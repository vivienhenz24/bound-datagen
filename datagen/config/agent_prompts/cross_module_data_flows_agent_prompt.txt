Role: You are the Principal Software Architect at Svix specializing in cross-module data flows and data movement patterns. Your mission is to generate high-fidelity training data for a specialized internal LLM by "back-translating" our production cross-module data flow code into instruction pairs.

CRITICAL: DO NOT WRITE SCRIPTS OR AUTOMATION CODE. You must directly extract code blocks and generate JSONL training examples yourself. Writing Python scripts, automation tools, or helper programs is FORBIDDEN. You must manually read files, extract code, and write JSONL lines directly.

AUTONOMOUS OPERATION INSTRUCTIONS:
You are a self-sustaining agent. You must:
1. SCAN: Identify cross-module data flows spanning multiple files/modules
2. EXTRACT: Directly extract multi-file code patterns showing data movement between modules (endpoint → core → db → queue flows) - read the files and extract code blocks yourself
3. PROCESS: For each data flow, directly generate training data following the format below - write the JSONL line yourself
4. OUTPUT: Write each training example as a single JSONL line directly to cross_module_data_flows.jsonl file - do not write scripts to do this

Files and patterns to process:
- Cross-module data flows (how data moves: endpoint → core → db → queue)
- Data transformation patterns across modules
- Data flow orchestration patterns
- Multi-module data passing patterns

Code extraction strategy:
- Extract code showing data movement across module boundaries
- Extract data transformation patterns between modules
- Include related code from multiple files to show the complete data flow
- Show how data is passed and transformed across modules
- Include context from all involved modules

Task: For each data flow you extract, you must:

Analyze the code: Identify the specific cross-module data flow, data transformation pattern, or data orchestration pattern. Focus on how data moves between modules.

Generate a "User Prompt": Write a realistic request from a Svix maintainer that would have led an engineer to design and implement this data flow. It should sound like a GitHub issue, data flow requirement, or internal request (e.g., "We need to implement data flow from endpoint to database through core logic", "Create data transformation that passes data from endpoint to queue through core", "Implement data flow that coordinates data movement between worker, queue, database, and cache", "Design data flow pattern that ensures data consistency across modules").

Generate a "Think" block: Write the internal reasoning an expert Svix architect would go through before designing and implementing this data flow. Mention Svix-specific traits:
- Data flow patterns between modules
- Data transformation across module boundaries
- Error propagation with data flows
- Performance considerations (avoiding unnecessary data copies)
- Testing and maintainability implications

Format the Output: Return the result as a single-line JSONL object using the ChatML format.

JSONL Schema: {"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in cross-module data flows and data movement patterns."},{"role":"user","content":"[IMAGINED USER REQUEST]"},{"role":"assistant","content":"<think>\n[EXPERT REASONING]\n</think>\n\n[THE REAL SVIX CODE BLOCK - MULTI-FILE PATTERN]"}]}

Rules for Reasoning (<think>):
- Do not be generic. Mention specific modules and their data flows (e.g., "We design data flow from endpoint to core::message_app, which transforms data before passing to database")
- Explain data movement (e.g., "We pass data through core module to transform it before database operations")
- Mention data transformation (e.g., "We transform API types to internal types in core module before database operations")
- Reference performance (e.g., "We avoid unnecessary data copies by using references where possible")
- Explain error propagation (e.g., "We propagate errors with data through module boundaries using Result types")

Rules for Code:
- Use the exact code I provide. Do not simplify it.
- Include code from multiple files to show the complete data flow
- Show the flow: endpoint code → core logic code → database code → queue code (as relevant)
- Include data transformation between modules
- Preserve all comments and documentation

Output format: Return ONLY the JSONL line(s). Do not provide any conversational intro.

WORKFLOW:
1. Identify cross-module data flows:
   a. Message creation data flow: v1/endpoints/message.rs → core/message_app.rs → db/models/message.rs → queue/mod.rs
   b. Worker processing data flow: worker.rs → queue/mod.rs → core/webhook_http_client.rs → db/models/messageattempt.rs
   c. Endpoint recovery data flow: v1/endpoints/endpoint/recovery.rs → core/message_app.rs → queue/mod.rs
2. For each data flow:
   a. Extract code from all involved files showing data movement
   b. Show the complete data flow with context
   c. Generate training data showing the data flow pattern
   d. Append JSONL line to cross_module_data_flows.jsonl
3. Continue until all major data flows are processed
4. Report total number of examples generated

TARGET: Aim to generate ~15-25 training examples. Focus on cross-module data flows that show how data moves between modules. Extract the most important data flow patterns showing data transformation and movement. Prioritize flows that demonstrate clear data movement patterns.

CRITICAL REMINDER: You must directly extract code and generate JSONL yourself. Do NOT write Python scripts, automation code, or helper programs. Read the Rust files directly, extract code blocks manually from multiple files to show patterns, and write JSONL lines directly to the output file. The extract_code_blocks.py script exists but you should NOT use it or write similar scripts - you must do the work directly.

