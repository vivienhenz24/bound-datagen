{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"We need the exact import list worker.rs relies on for the tokio worker loop so we can review queue dependencies."},{"role":"assistant","content":"<think>\nTokio workers poll omniqueue via TaskQueueConsumer::receive_all, so enumerating every import keeps the worker loop's dependency surface explicit during training.\n</think>\n\n```rust\nuse std::{\n    collections::HashMap,\n    sync::{\n        atomic::{AtomicU64, AtomicUsize, Ordering},\n        Arc, LazyLock,\n    },\n    time::Duration,\n};\n\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chrono::Utc;\nuse futures::future;\nuse http::{HeaderValue, StatusCode, Version};\nuse http_body_util::BodyExt as _;\nuse itertools::Itertools;\nuse rand::Rng;\nuse sea_orm::{\n    prelude::DateTimeUtc, ActiveModelTrait, ColumnTrait, DatabaseConnection, EntityTrait,\n    QueryFilter, QueryOrder, Set,\n};\nuse serde::{Deserialize, Serialize};\nuse tokio::time::sleep;\nuse tracing::Instrument;\n\nuse crate::{\n    cfg::Configuration,\n    core::{\n        cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue},\n        cryptography::Encryption,\n        message_app::{CreateMessageApp, CreateMessageEndpoint},\n        operational_webhooks::{\n            EndpointDisabledEventData, MessageAttemptEvent, OperationalWebhook,\n            OperationalWebhookSender,\n        },\n        types::{\n            ApplicationId, ApplicationUid, BaseId, EndpointHeaders, EndpointId,\n            EndpointSecretInternal, EndpointSecretType, MessageAttemptId,\n            MessageAttemptTriggerType, MessageId, MessageStatus, MessageUid, OrganizationId,\n        },\n        webhook_http_client::{Error as WebhookClientError, RequestBuilder, WebhookClient},\n    },\n    db::models::{endpoint, message, messageattempt, messagecontent, messagedestination},\n    error::{Error, ErrorType, HttpError, Result},\n    queue::{MessageTask, QueueTask, TaskQueueConsumer, TaskQueueProducer},\n    v1::utils::get_unix_timestamp,\n};\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Document the worker constants and header map type so we can reason about tokio loop jitter and omniqueue response sizing."},{"role":"assistant","content":"<think>\nOur tokio workers throttle omniqueue polling based on jitter and overload penalties, so keeping these constants visible ensures the batch loop behaves predictably under load.\n</think>\n\n```rust\npub type CaseSensitiveHeaderMap = HashMap<String, HeaderValue>;\n\n// The maximum variation from the retry schedule when applying jitter to a resent webhook event in\n// percent deviation\nconst JITTER_DELTA: f32 = 0.2;\nconst OVERLOAD_PENALTY_SECS: u64 = 60;\n\nconst USER_AGENT: &str = concat!("Svix-Webhooks/", env!("CARGO_PKG_VERSION"));\n\n/// Send the MessageAttemptFailingEvent after exceeding this number of failed attempts\nconst OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER: usize = 4;\n\nconst RESPONSE_MAX_SIZE: usize = 20000;\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Show the structures worker.rs uses to remember first-failure timestamps so the tokio worker loop knows when to stop polling omniqueue destinations."},{"role":"assistant","content":"<think>\nThe tokio queue loop leans on omniqueue metadata but still needs per-endpoint failure windows, so capturing the cache structs keeps disable decisions deterministic across worker restarts.\n</think>\n\n```rust\n/// A simple struct noting the context of the wrapped [`DateTimeUtc`]. This struct is returned when\n/// you are to disable disable an endpoint. This is optionally returned by [`process_failure_cache`]\n/// which is to be called after all retry events are exhausted.\n#[repr(transparent)]\nstruct EndpointDisableInfo {\n    first_failure_at: DateTimeUtc,\n}\n\n/// The first_failure_at time is only stored in Postgres after the endpoint has been disabled.\n/// Otherwise, it is stored in the cache with an expiration.\n#[derive(Deserialize, Serialize)]\npub struct FailureCacheValue {\n    pub first_failure_at: DateTimeUtc,\n}\n\nkv_def!(FailureCacheKey, FailureCacheValue);\n\nimpl FailureCacheKey {\n    pub fn new(\n        org_id: &OrganizationId,\n        app_id: &ApplicationId,\n        endp_id: &EndpointId,\n    ) -> FailureCacheKey {\n        FailureCacheKey(format!("SVIX_FAILURE_CACHE_{org_id}_{app_id}_{endp_id}"))\n    }\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Document how the worker loop clears the failure cache after a successful delivery so omniqueue polling doesn't disable healthy destinations."},{"role":"assistant","content":"<think>\nWhen a tokio-spawned worker finishes a delivery we immediately delete the cached failure marker so the omniqueue-driven loop won't misclassify the endpoint on future receive_all batches.\n</think>\n\n```rust\n/// Called upon the successful dispatch of an endpoint. Simply clears the cache of a\n/// [`FailureCacheKey`]/[`FailureCacheValue`] pair associated with a given endpoint. This is such\n/// that an endpoint that was previously not responding is not disabled after responding again.\n///\n/// If the key value pair does not already exist in the cache, indicating that the endpoint never\n/// stopped responding, no operation is performed.\n#[tracing::instrument(skip_all)]\nasync fn process_endpoint_success(\n    cache: &Cache,\n    app_id: &ApplicationId,\n    org_id: &OrganizationId,\n    endp: &CreateMessageEndpoint,\n) -> Result<()> {\n    let key = FailureCacheKey::new(org_id, app_id, &endp.id);\n\n    cache.delete(&key).await.map_err(Error::cache)\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Explain how the worker loop records endpoint failures so tokio receive_all batches know when to disable an omniqueue target."},{"role":"assistant","content":"<think>\nOur tokio loop needs to coordinate with omniqueue so that repeated failures in receive_all batches eventually flip an endpoint off, and this helper encodes the grace-period bookkeeping.\n</think>\n\n```rust\n/// Called upon endpoint failure. Returns whether to disable the endpoint based on the time of first\n/// failure stored in the cache.\n///\n/// If no failure has previously been reported, then now is cached as the time of first failure and\n/// the endpoint is not disabled.\n///\n/// If there has been a  previous failure, then it is compared to the configured grace period, where\n/// if there have been only failures within the grace period, then the endpoint is disabled.\n///\n/// All cache values are set with an expiration time greater that the grace period, so occasional\n/// failures will not cause an endpoint to be disabled.\n#[tracing::instrument(skip_all)]\nasync fn process_endpoint_failure(\n    cache: &Cache,\n    app_id: &ApplicationId,\n    org_id: &OrganizationId,\n    endp: &CreateMessageEndpoint,\n    disable_in: Duration,\n) -> Result<Option<EndpointDisableInfo>> {\n    let key = FailureCacheKey::new(org_id, app_id, &endp.id);\n    let now = Utc::now();\n\n    // If it already exists in the cache, see if the grace period has already elapsed\n    if let Some(FailureCacheValue { first_failure_at }) = cache\n        .get::<FailureCacheValue>(&key)\n        .await\n        .map_err(Error::generic)?\n    {\n        if now - first_failure_at\n            > chrono::Duration::from_std(disable_in).expect("Given `disable_in` is too large")\n        {\n            Ok(Some(EndpointDisableInfo { first_failure_at }))\n        } else {\n            Ok(None)\n        }\n    }\n    // If it does not yet exist in the cache, set the first_failure_at value to now\n    else {\n        cache\n            .set(\n                &key,\n                &FailureCacheValue {\n                    first_failure_at: now,\n                },\n                // Failures are forgiven after double the `disable_in` `Duration` with the expiry of\n                // the Redis key\n                disable_in * 2,\n            )\n            .await\n            .map_err(Error::generic)?;\n\n        Ok(None)\n    }\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Lay out the worker context structs and dispatch enums so we understand how tokio worker loops shuttle omniqueue data between stages."},{"role":"assistant","content":"<think>\nEach tokio task spawned from omniqueue batches threads a WorkerContext plus Pending/Completed dispatch state, so documenting these structs keeps the loop's data flow explicit.\n</think>\n\n```rust\n#[derive(Clone)]\nstruct WorkerContext<'a> {\n    cfg: &'a Configuration,\n    cache: &'a Cache,\n    db: &'a DatabaseConnection,\n    queue_tx: &'a TaskQueueProducer,\n    op_webhook_sender: &'a OperationalWebhookSender,\n    webhook_client: &'a WebhookClient,\n}\n\nstruct FailedDispatch(messageattempt::ActiveModel, Error);\nstruct SuccessfulDispatch(messageattempt::ActiveModel);\n\n#[allow(clippy::large_enum_variant)]\nenum IncompleteDispatch {\n    Pending(PendingDispatch),\n    #[allow(dead_code)]\n    Failed(FailedDispatch),\n}\n\nstruct PendingDispatch {\n    method: http::Method,\n    url: String,\n    headers: CaseSensitiveHeaderMap,\n    payload: String,\n    request_timeout: u64,\n    created_at: DateTimeUtc,\n}\n\n// Clippy fails to compute the first variant's size, stating it as\n// "at least 0 bytes". They're actually very similar in size.\n#[allow(clippy::large_enum_variant)]\nenum CompletedDispatch {\n    Failed(FailedDispatch),\n    Successful(SuccessfulDispatch),\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Show how the worker loop prepares a dispatch before tokio launches HTTP work so omniqueue batches reuse consistent metadata."},{"role":"assistant","content":"<think>\nBefore a tokio worker spawned from an omniqueue receive_all batch fires the HTTP call, it builds a PendingDispatch with signatures, headers, and timeout so retries stay deterministic.\n</think>\n\n```rust\n#[tracing::instrument(skip_all)]\nasync fn prepare_dispatch(\n    WorkerContext { cfg, .. }: &WorkerContext<'_>,\n    DispatchContext {\n        msg_task,\n        payload,\n        endp,\n        ..\n    }: DispatchContext<'_>,\n) -> Result<IncompleteDispatch> {\n    let attempt_created_at = Utc::now();\n\n    let headers = {\n        let keys = endp.valid_signing_keys();\n\n        let signatures = sign_msg(\n            &cfg.encryption,\n            attempt_created_at.timestamp(),\n            payload,\n            &msg_task.msg_id,\n            &keys,\n        );\n\n        generate_msg_headers(\n            attempt_created_at.timestamp(),\n            &msg_task.msg_id,\n            signatures,\n            cfg.whitelabel_headers,\n            endp.headers.as_ref(),\n            &endp.url,\n        )?\n    };\n\n    Ok(IncompleteDispatch::Pending(PendingDispatch {\n        method: http::Method::POST,\n        url: endp.url.clone(),\n        headers,\n        payload: payload.to_owned(),\n        request_timeout: cfg.worker_request_timeout as _,\n        created_at: attempt_created_at,\n    }))\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Capture the HTTP dispatch helper so tokio worker loops know exactly how omniqueue deliveries translate into recorded attempts."},{"role":"assistant","content":"<think>\nEvery tokio task pulled from omniqueue must build a WebhookClient request, store the attempt, and handle oversized bodies, so this function is key to the worker loop's correctness.\n</think>\n\n```rust\n#[tracing::instrument(skip_all)]\nasync fn make_http_call(\n    DispatchContext {\n        msg_task,\n        endp,\n        db_attempt_number,\n        ..\n    }: DispatchContext<'_>,\n    PendingDispatch {\n        method,\n        url,\n        headers,\n        payload,\n        request_timeout,\n        created_at,\n    }: PendingDispatch,\n    client: &WebhookClient,\n) -> Result<CompletedDispatch> {\n    let req = RequestBuilder::new()\n        .method(method)\n        .uri_str(&url)\n        .map_err(|e| Error::validation(format_args!("URL is invalid: {e:?}")))?\n        .headers(headers)\n        .body(payload.into(), HeaderValue::from_static("application/json"))\n        .version(Version::HTTP_11)\n        .timeout(Duration::from_secs(request_timeout))\n        .build()\n        .map_err(Error::generic)?;\n\n    let attempt = messageattempt::ActiveModel {\n        // Set both ID and created_at to the same timestamp\n        id: Set(MessageAttemptId::new(created_at.into(), None)),\n        created_at: Set(created_at.into()),\n        msg_id: Set(msg_task.msg_id.clone()),\n        endp_id: Set(endp.id.clone()),\n        msg_dest_id: Set(None),\n        url: Set(endp.url.clone()),\n        ended_at: Set(Some(Utc::now().into())),\n        trigger_type: Set(msg_task.trigger_type),\n        response_duration_ms: Set(0), // Default to 0, will be updated after the request\n        attempt_number: Set(db_attempt_number),\n        ..Default::default()\n    };\n\n    match client.execute(req).await {\n        Ok(res) => {\n            // Calculate the duration in milliseconds\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            let status_code = res.status().as_u16() as i16;\n            let status = if res.status().is_success() {\n                MessageStatus::Success\n            } else {\n                MessageStatus::Fail\n            };\n\n            let http_error = if !res.status().is_success() {\n                Some(WebhookClientError::FailureStatus(res.status()))\n            } else {\n                None\n            };\n\n            let body = match res.into_body().collect().await {\n                Ok(collected) => {\n                    let bytes = collected.to_bytes();\n                    if bytes.len() > RESPONSE_MAX_SIZE {\n                        bytes_to_string(bytes.slice(..RESPONSE_MAX_SIZE))\n                    } else {\n                        bytes_to_string(bytes)\n                    }\n                }\n                Err(err) => format!("Error reading response body: {err}"),\n            };\n\n            let attempt = messageattempt::ActiveModel {\n                response_status_code: Set(status_code),\n                response: Set(body),\n                status: Set(status),\n                response_duration_ms: Set(duration_ms),\n                ..attempt\n            };\n\n            match http_error {\n                Some(err) => Ok(CompletedDispatch::Failed(FailedDispatch(\n                    attempt,\n                    Error::generic(err),\n                ))),\n                None => Ok(CompletedDispatch::Successful(SuccessfulDispatch(attempt))),\n            }\n        }\n        Err(err) => {\n            // For errors, we still calculate the duration\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            Ok(CompletedDispatch::Failed(FailedDispatch(\n                messageattempt::ActiveModel {\n                    response_status_code: Set(0),\n                    response: Set(err.to_string()),\n                    status: Set(MessageStatus::Fail),\n                    response_duration_ms: Set(duration_ms),\n                    ..attempt\n                },\n                err.into(),\n            )))\n        }\n    }\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Show how the worker loop records a successful dispatch so tokio workers drain omniqueue batches while emitting recovery webhooks."},{"role":"assistant","content":"<think>\nAfter a tokio worker completes a delivery from omniqueue, we persist the attempt, clear the failure cache, and optionally emit recovery op webhooks, so this path explains the loop's happy case.\n</think>\n\n```rust\n#[tracing::instrument(skip_all, fields(response_code))]\nasync fn handle_successful_dispatch(\n    WorkerContext {\n        cache,\n        db,\n        op_webhook_sender,\n        ..\n    }: &WorkerContext<'_>,\n    DispatchContext {\n        org_id,\n        endp,\n        app_id,\n        app_uid,\n        msg_task,\n        msg_uid,\n        ..\n    }: DispatchContext<'_>,\n    SuccessfulDispatch(mut attempt): SuccessfulDispatch,\n) -> Result<()> {\n    attempt.ended_at = Set(Some(Utc::now().into()));\n    attempt.next_attempt = Set(None);\n    let attempt = attempt.insert(*db).await?;\n\n    process_endpoint_success(cache, app_id, org_id, endp).await?;\n\n    tracing::Span::current().record("response_code", attempt.response_status_code);\n    tracing::info!("Webhook success.");\n\n    if msg_task.attempt_count as usize >= OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER {\n        if let Err(e) = op_webhook_sender\n            .send_operational_webhook(\n                org_id,\n                OperationalWebhook::MessageAttemptRecovered(MessageAttemptEvent {\n                    app_id: app_id.clone(),\n                    app_uid: app_uid.cloned(),\n                    endpoint_id: msg_task.endpoint_id.clone(),\n                    msg_id: msg_task.msg_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await\n        {\n            tracing::error!(\n                "Failed sending MessageAttemptRecovered Operational Webhook: {}",\n                e\n            );\n        }\n    }\n\n    Ok(())\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Show the retry backoff helper so tokio worker loops apply the same jitter before requeueing work on omniqueue."},{"role":"assistant","content":"<think>\nThis function wraps the retry schedule with jitter so tokio workers requeue omniqueue deliveries with consistent penalties when 429s or timeouts occur.\n</think>\n\n```rust\nfn calculate_retry_delay(duration: Duration, err: &Error) -> Duration {\n    let duration = if matches!(err.typ, ErrorType::Timeout(_))\n        || matches!(err.typ, ErrorType::Http(HttpError { status, .. }) if status == StatusCode::TOO_MANY_REQUESTS)\n    {\n        std::cmp::max(duration, Duration::from_secs(OVERLOAD_PENALTY_SECS))\n    } else {\n        duration\n    };\n    // Apply jitter with a maximum variation of JITTER_DELTA\n    rand::thread_rng()\n        .gen_range(duration.mul_f32(1.0 - JITTER_DELTA)..=duration.mul_f32(1.0 + JITTER_DELTA))\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Detail the failure handling path so tokio worker loops know how to requeue omniqueue deliveries, send op webhooks, and disable endpoints."},{"role":"assistant","content":"<think>\nWhen a tokio worker popped from omniqueue fails a delivery, we jitter the retry delay, enqueue the next attempt, and potentially disable the endpoint, so this function captures the loop's resilience logic.\n</think>\n\n```rust\n#[tracing::instrument(skip_all, fields(response_code))]\nasync fn handle_failed_dispatch(\n    WorkerContext {\n        db,\n        cache,\n        op_webhook_sender,\n        cfg,\n        queue_tx,\n        ..\n    }: &WorkerContext<'_>,\n    DispatchContext {\n        org_id,\n        app_id,\n        app_uid,\n        msg_uid,\n        endp,\n        msg_task,\n        ..\n    }: DispatchContext<'_>,\n    FailedDispatch(mut attempt, err): FailedDispatch,\n) -> Result<()> {\n    attempt.ended_at = Set(Some(Utc::now().into()));\n\n    tracing::Span::current().record("response_code", attempt.response_status_code.try_as_ref());\n    tracing::info!("Webhook failure.");\n\n    let retry_schedule = &cfg.retry_schedule;\n\n    let attempt_count = msg_task.attempt_count as usize;\n    if msg_task.trigger_type == MessageAttemptTriggerType::Manual {\n        tracing::debug!("Manual retry failed");\n        attempt.next_attempt = Set(None);\n        attempt.insert(*db).await?;\n        Ok(())\n    } else if attempt_count < retry_schedule.len() {\n        let retry_delay = calculate_retry_delay(retry_schedule[attempt_count], &err);\n        let next_attempt_time =\n            Utc::now() + chrono::Duration::from_std(retry_delay).expect("Error parsing duration");\n\n        attempt.next_attempt = Set(Some(next_attempt_time.into()));\n        let attempt = attempt.insert(*db).await?;\n\n        tracing::debug!(\n            retry_delay = retry_delay.as_secs(),\n            "Worker failure retrying for attempt {}: {} {} {}",\n            attempt_count,\n            err,\n            &attempt.id,\n            &endp.id\n        );\n\n        if attempt_count == (OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER - 1) {\n            if let Err(e) = op_webhook_sender\n                .send_operational_webhook(\n                    org_id,\n                    OperationalWebhook::MessageAttemptFailing(MessageAttemptEvent {\n                        app_id: app_id.clone(),\n                        app_uid: app_uid.cloned(),\n                        endpoint_id: msg_task.endpoint_id.clone(),\n                        msg_id: msg_task.msg_id.clone(),\n                        msg_event_id: msg_uid.cloned(),\n                        last_attempt: attempt.into(),\n                    }),\n                )\n                .await\n            {\n                tracing::error!(\n                    "Failed sending MessageAttemptFailing Operational Webhook: {}",\n                    e\n                );\n            }\n        }\n        queue_tx\n            .send(\n                &QueueTask::MessageV1(MessageTask {\n                    attempt_count: msg_task.attempt_count + 1,\n                    ..msg_task.clone()\n                }),\n                Some(retry_delay),\n            )\n            .await?;\n\n        Ok(())\n    } else {\n        attempt.next_attempt = Set(None);\n        let attempt = attempt.insert(*db).await?;\n\n        tracing::debug!(\n            "Worker failure attempts exhausted: {} {} {}",\n            err,\n            &attempt.id,\n            &endp.id\n        );\n\n        // Send common operational webhook\n        op_webhook_sender\n            .send_operational_webhook(\n                org_id,\n                OperationalWebhook::MessageAttemptExhausted(MessageAttemptEvent {\n                    app_id: app_id.clone(),\n                    app_uid: app_uid.cloned(),\n                    endpoint_id: msg_task.endpoint_id.clone(),\n                    msg_id: msg_task.msg_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await?;\n\n        match process_endpoint_failure(\n            cache,\n            app_id,\n            org_id,\n            endp,\n            cfg.endpoint_failure_disable_after,\n        )\n        .await?\n        {\n            None => Ok(()),\n\n            Some(EndpointDisableInfo { first_failure_at }) => {\n                let endp = endpoint::Entity::secure_find_by_id(\n                    msg_task.app_id.clone(),\n                    msg_task.endpoint_id.clone(),\n                )\n                .one(*db)\n                .await?\n                .ok_or_else(|| {\n                    Error::generic(format_args!(\n                        "Endpoint not found {app_id} {}",\n                        &msg_task.endpoint_id\n                    ))\n                })?;\n\n                let endp = endpoint::ActiveModel {\n                    disabled: Set(true),\n                    first_failure_at: Set(Some(first_failure_at.into())),\n                    ..endp.into()\n                };\n                let _endp = endp.update(*db).await?;\n\n                // Send operational webhooks\n                op_webhook_sender\n                    .send_operational_webhook(\n                        org_id,\n                        OperationalWebhook::EndpointDisabled(EndpointDisabledEventData {\n                            app_id: app_id.clone(),\n                            app_uid: app_uid.cloned(),\n                            endpoint_id: msg_task.endpoint_id.clone(),\n                            // TODO:\n                            endpoint_uid: None,\n                            fail_since: first_failure_at,\n                        }),\n                    )\n                    .await\n            }\n        }\n    }\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Document the DispatchContext struct so tokio worker loops know which omniqueue identifiers ride along with each task."},{"role":"assistant","content":"<think>\nWe pass this context through tokio futures spawned from omniqueue batches so every attempt logs consistent org, app, and UID data.\n</think>\n\n```rust\n#[derive(Clone)]\nstruct DispatchContext<'a> {\n    msg_task: &'a MessageTask,\n    payload: &'a str,\n    endp: &'a CreateMessageEndpoint,\n    org_id: &'a OrganizationId,\n    app_id: &'a ApplicationId,\n    app_uid: Option<&'a ApplicationUid>,\n    msg_uid: Option<&'a MessageUid>,\n    // Like 'attempt_number' from the task, but adjusted to -1 for `MessageAttemptTriggerType::Manual`.\n    db_attempt_number: i16,\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Record the helper that normalizes response bodies so tokio worker loops can log omniqueue attempts even when payloads aren't UTF-8."},{"role":"assistant","content":"<think>\nTokio workers persist every omniqueue attempt, so we need this converter to base64 encode binary HTTP bodies before storing them.\n</think>\n\n```rust\nfn bytes_to_string(bytes: bytes::Bytes) -> String {\n    match std::str::from_utf8(&bytes) {\n        Ok(v) => v.to_owned(),\n        Err(_) => STANDARD.encode(&bytes),\n    }\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Capture the last-poll tracker so tokio worker supervisors know when the omniqueue loop last received a batch."},{"role":"assistant","content":"<think>\nWe expose LAST_QUEUE_POLL so ops can tell whether the tokio receive_all loop is still hitting omniqueue, and the helper simply updates the atomic timestamp each cycle.\n</think>\n\n```rust\npub static LAST_QUEUE_POLL: LazyLock<AtomicU64> = LazyLock::new(|| get_unix_timestamp().into());\n\nasync fn update_last_poll_time() {\n    LAST_QUEUE_POLL.swap(get_unix_timestamp(), Ordering::Relaxed);\n}\n```\n"}]}
{"type":"chatml","messages":[{"role":"system","content":"You are the Svix Maintainer AI, an expert in the internal svix-server Rust architecture, specializing in worker loop logic and batch processing."},{"role":"user","content":"Capture the full queue_handler so we can show how the tokio worker loop polls omniqueue via receive_all, manages concurrency, and acks deliveries."},{"role":"assistant","content":"<think>\nThis is the heart of our tokio loop: it enforces the worker cap, watches shutdown, calls receive_all on the omniqueue consumer, and spawns per-delivery tasks with explicit ack/nack handling.\n</think>\n\n```rust\n/// Listens on the message queue for new tasks\n#[allow(clippy::too_many_arguments)]\npub async fn queue_handler(\n    cfg: &Configuration,\n    cache: Cache,\n    db: DatabaseConnection,\n    queue_tx: TaskQueueProducer,\n    mut queue_rx: TaskQueueConsumer,\n    op_webhook_sender: OperationalWebhookSender,\n) -> Result<()> {\n    let recv_deadline = Duration::from_secs(cfg.queue_max_poll_secs.into());\n\n    static NUM_WORKERS: AtomicUsize = AtomicUsize::new(0);\n\n    let task_limit = cfg.worker_max_tasks;\n    if task_limit == 0 {\n        tracing::info!("Worker concurrent task limit: unlimited");\n    } else {\n        tracing::info!("Worker concurrent task limit: {}", task_limit);\n    }\n\n    let webhook_client = WebhookClient::new(\n        cfg.whitelist_subnets.clone(),\n        Some(Arc::new(vec!["backend".to_owned()])),\n        cfg.dangerous_disable_tls_verification,\n        cfg.proxy_config.as_ref(),\n    );\n\n    tokio::spawn(\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_millis(500));\n            loop {\n                interval.tick().await;\n                let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                if num_workers > 0 {\n                    tracing::info!("{} active workers", num_workers);\n                }\n            }\n        }\n        .instrument(tracing::error_span!(\n            "worker_monitor",\n            instance_id = tracing::field::Empty\n        )),\n    );\n\n    loop {\n        if task_limit > 0 {\n            let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n            if num_workers > task_limit.into() {\n                tokio::time::sleep(Duration::from_millis(100)).await;\n                continue;\n            }\n        }\n\n        if crate::is_shutting_down() {\n            tokio::join!(async move {\n                let mut interval = tokio::time::interval(Duration::from_millis(500));\n                loop {\n                    interval.tick().await;\n                    let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                    if num_workers > 0 {\n                        tracing::info!(\n                            "{} active workers, waiting to shut down worker.",\n                            num_workers\n                        );\n                    } else {\n                        tracing::info!("No active workers, shutting down worker.");\n                        break;\n                    }\n                }\n            });\n            break;\n        }\n\n        match queue_rx.receive_all(recv_deadline).await {\n            Ok(batch) => {\n                for delivery in batch {\n                    let cfg = cfg.clone();\n                    let cache = cache.clone();\n                    let db = db.clone();\n                    let queue_tx = queue_tx.clone();\n                    let queue_task = delivery.task.clone();\n                    let op_webhook_sender = op_webhook_sender.clone();\n                    let webhook_client = webhook_client.clone();\n\n                    tokio::spawn(async move {\n                        NUM_WORKERS.fetch_add(1, Ordering::Relaxed);\n                        let worker_context = WorkerContext {\n                            cfg: &cfg,\n                            db: &db,\n                            cache: &cache,\n                            op_webhook_sender: &op_webhook_sender,\n                            queue_tx: &queue_tx,\n                            webhook_client: &webhook_client,\n                        };\n\n                        let queue_task =\n                            Arc::try_unwrap(queue_task).unwrap_or_else(|arc| (*arc).clone());\n                        if process_queue_task(worker_context, queue_task)\n                            .await\n                            .is_err()\n                        {\n                            if let Err(err) = delivery.nack().await {\n                                tracing::error!(\n                                    "Error sending 'nack' to Redis after task execution error: {}",\n                                    err\n                                );\n                            }\n                        } else if let Err(err) = delivery.ack().await {\n                            tracing::error!(\n                                "Error sending 'ack' to Redis after successful task execution: {}",\n                                err\n                            );\n                        }\n\n                        NUM_WORKERS.fetch_sub(1, Ordering::Relaxed);\n                    });\n                }\n            }\n            Err(err) => {\n                tracing::error!("Error receiving task: {:?}", err);\n                sleep(tokio::time::Duration::from_millis(10)).await;\n            }\n        }\n\n        update_last_poll_time().await;\n    }\n\n    Ok(())\n}\n```\n"}]}
