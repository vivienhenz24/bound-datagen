{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct HttpPatchConfig {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub url: Option<String>,\n}\n\nimpl HttpPatchConfig {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new() -> Self {\n        Self { url: None }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/http_patch_config.rs", "node_type": "function_item", "line_range": [11, 13]}
{"prompt": "<|fim_prefix|>use std::{\n    str,\n    time::{Duration, Instant},\n};\n\nuse rdkafka::{\n    consumer::{CommitMode, Consumer as _},\n    error::KafkaError,\n    Message as _,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::api::{MessageCreateOptions, Svix},\n    CreateMessageRequest, JsObject, SenderInput, SenderOutputOpts, TransformationConfig,\n    TransformerInput, TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\nuse tokio::task::spawn_blocking;\n\nuse crate::{config::KafkaInputOpts, Error, Result};\n\npub struct KafkaConsumer {\n    name: String,\n    opts: KafkaInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n}\n\nimpl KafkaConsumer {\n    pub fn new(\n        name: String,\n        opts: KafkaInputOpts,\n        transformation: Option<TransformationConfig>,\n        output: SenderOutputOpts,\n    ) -> Result<Self> {\n        Ok(Self {\n            name,\n            transformation,\n            transformer_tx: None,\n            opts,\n            svix_client: match output {\n                SenderOutputOpts::Svix(output) => {\n                    Svix::new(output.token, output.options.map(Into::into))\n                }\n            },\n        })\n    }\n\n    #[tracing::instrument(skip_all)]\n    async fn process(&self, msg: &rdkafka::message::BorrowedMessage<'_>) -> Result<()> {\n        <|fim_suffix|>\n        let payload = if let Some(transformation) = &self.transformation {\n            let input = match transformation.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload =\n                        serde_json::from_slice(payload).map_err(Error::Deserialization)?;\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    let raw_payload = str::from_utf8(payload).map_err(Error::NonUtf8Payload)?;\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n\n            let script = transformation.source().clone();\n            let object = self.transform(script, input).await?;\n            serde_json::from_value(serde_json::Value::Object(object))\n                .map_err(Error::Deserialization)?\n        } else {\n            serde_json::from_slice(payload).map_err(Error::Deserialization)?\n        };\n\n        let CreateMessageRequest { app_id, message } = payload;\n\n        let KafkaInputOpts::Inner {\n            group_id, topic, ..\n        } = &self.opts;\n\n        let options = MessageCreateOptions {\n            with_content: None,\n            // If committing the message fails or the process crashes after posting the webhook but\n            // before committing, this makes sure that the next run of this fn with the same kafka\n            // message doesn't end up creating a duplicate webhook in svix.\n            idempotency_key: Some(format!(\n                \"svix_bridge_kafka_{group_id}_{topic}_{}\",\n                msg.offset()\n            )),\n        };\n\n        self.svix_client\n            .message()\n            .create(app_id, message, Some(options))\n            .await?;\n\n        Ok(())\n    }\n\n    async fn transform(&self, script: String, input: TransformerInput) -> Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx\n            .as_ref()\n            .ok_or_else(|| Error::transformation(\"transformations not configured\"))?\n            .send(job)\n            .map_err(|e| Error::transformation(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::transformation(\"transformation rx failed\"))\n            .and_then(|x| {\n                x.map_err(|_e| Error::transformation(\"transformation execution failed\"))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => Err(Error::transformation(\n                \"transformation produced unexpected value\",\n            )),\n        }\n    }\n\n    async fn run_inner(&self) -> Result<()> {\n        let opts = self.opts.clone();\n        // `ClientConfig::create` does blocking I/O.\n        // Same for subscribe, most likely.\n        let consumer = spawn_blocking(move || {\n            let KafkaInputOpts::Inner { topic, .. } = &opts;\n            let topic = topic.clone();\n\n            let consumer = opts.create_consumer()?;\n            tracing::debug!(\"Created StreamConsumer\");\n\n            consumer.subscribe(&[&topic])?;\n            tracing::debug!(topic, \"Subscribed\");\n\n            Ok::<_, KafkaError>(consumer)\n        })\n        .await\n        .expect(\"create_consumer task panicked\")?;\n\n        loop {\n            // It's fine to pull messages one-by-one without any buffering in our own code because\n            // rdkafka buffers messages internally through a background task / thread.\n            let msg = consumer.recv().await?;\n            tracing::debug!(\"Received a message\");\n\n            let mut process_error_count = 0;\n            while let Err(e) = self.process(&msg).await {\n                match e {\n                    // If the payload is invalid, log an error and continue.\n                    // It would fail the same way if retried.\n                    Error::MissingPayload\n                    | Error::Deserialization(_)\n                    | Error::NonUtf8Payload(_) => {\n                        tracing::error!(error = &e as &dyn std::error::Error, \"invalid payload\");\n                        break;\n                    }\n\n                    // If the error is (possibly) transient, retry a few times.\n                    // After that, bubble up the error so it's logged at error level.\n                    Error::Kafka(_) | Error::SvixClient(_) | Error::Transformation { .. } => {\n                        process_error_count += 1;\n                        if process_error_count >= 3 {\n                            return Err(e);\n                        }\n\n                        tracing::warn!(\n                            error = &e as &dyn std::error::Error,\n                            \"failed to process payload from kafka\"\n                        );\n\n                        // retry\n                    }\n                }\n            }\n\n            // FIXME(jplatte): Should we introduce logic to only commit every N messages to reduce\n            // back and forth on the Kafka connection / disk writes inside Kafka?\n            //\n            // Background: messages in Kafka are not committed individually, rather what this call\n            // does is update the stored stream position for the consumer group.\n            consumer.commit_message(&msg, CommitMode::Async)?;\n        }\n    }\n}\n\n#[async_trait]\nimpl SenderInput for KafkaConsumer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        let mut fails: u64 = 0;\n        let mut last_fail = Instant::now();\n\n        let KafkaInputOpts::Inner { topic, .. } = &self.opts;\n        tracing::info!(topic, \"Starting to listen for messages\");\n\n        loop {\n            if let Err(e) = self.run_inner().await {\n                tracing::error!(\"{e}\");\n            }\n\n            if last_fail.elapsed() > Duration::from_secs(10) {\n                // reset the fail count if we didn't have a hiccup in the past short while.\n                tracing::trace!(\"been a while since last fail, resetting count\");\n                fails = 0;\n            } else {\n                fails += 1;\n            }\n\n            last_fail = Instant::now();\n            tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n        }\n    }\n}\n<|fim_middle|>", "completion": "let payload = msg.payload().ok_or_else(|| Error::MissingPayload)?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/input.rs", "node_type": "let_declaration", "line_range": [51, 51]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse sea_orm::{\n    ColumnTrait, DatabaseConnection, DbBackend, DeleteResult, EntityTrait, QueryFilter,\n    SqlxPostgresConnector,\n};\nuse sqlx::postgres::PgPoolOptions;\n\nuse crate::{cfg::Configuration, core::types::OrganizationId};\n\npub mod models;\nuse models::{application, endpoint, eventtype, message, messageattempt};\n\nstatic MIGRATIONS: sqlx::migrate::Migrator = sqlx::migrate!();\n\nasync fn connect(dsn: &str, max_pool_size: u16) -> sqlx::Pool<sqlx::Postgres> {\n    if DbBackend::Postgres.is_prefix_of(dsn) {\n        PgPoolOptions::new()\n            .max_connections(max_pool_size.into())\n            .connect(dsn)\n            .await\n            .expect(\"Error connecting to Postgres\")\n    } else {\n        panic!(\"db_dsn format not recognized. {dsn}\")\n    }\n}\n\npub async fn init_db(cfg: &Configuration) -> DatabaseConnection {\n    SqlxPostgresConnector::from_sqlx_postgres_pool(connect(&cfg.db_dsn, cfg.db_pool_max_size).await)\n}\n\np<|fim_suffix|>\n/// Wipe an organization from existence in a way that ensures the operation can be tried again on\n/// failure.\npub async fn wipe_org(cfg: &Configuration, org_id: OrganizationId) {\n    let db = init_db(cfg).await;\n\n    let applications: Vec<application::Model> = application::Entity::secure_find(org_id.clone())\n        .all(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error fetching applications associated with org ID {org_id}\"));\n\n    for application in applications {\n        let endpoints: Vec<endpoint::Model> = endpoint::Entity::secure_find(application.id.clone())\n            .all(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error fetching endpoints associated with application ID {}\",\n                    application.id\n                )\n            });\n\n        for endpoint in endpoints {\n            // First [`messageattempt`]s, then [`messagedestination`]s\n            let _: DeleteResult = messageattempt::Entity::delete_many()\n                .filter(messageattempt::Column::EndpId.eq(endpoint.id.clone()))\n                .exec(&db)\n                .await\n                .unwrap_or_else(|_| {\n                    panic!(\n                        \"Error deleting messageattempts associated with endpoint ID {}\",\n                        endpoint.id\n                    )\n                });\n        }\n\n        // Then [`message`]s, then [`endpoint`]s\n        let _: DeleteResult = message::Entity::delete_many()\n            .filter(message::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting messages associated with application ID {}\",\n                    application.id\n                )\n            });\n\n        let _: DeleteResult = endpoint::Entity::delete_many()\n            .filter(endpoint::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting endpoints associated with application ID {}\",\n                    application.id\n                )\n            });\n    }\n\n    // Then [`application`]s, then [`eventtype`]s\n    let _: DeleteResult = application::Entity::delete_many()\n        .filter(application::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting applications associated with org ID {org_id}\"));\n\n    let _: DeleteResult = eventtype::Entity::delete_many()\n        .filter(eventtype::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting event types associated with org ID {org_id}\"));\n}\n<|fim_middle|>", "completion": "pub async fn run_migrations(cfg: &Configuration) {\n    let db = connect(&cfg.db_dsn, cfg.db_pool_max_size).await;\n    MIGRATIONS.run(&db).await.unwrap();\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/mod.rs", "node_type": "function_item", "line_range": [33, 36]}
{"prompt": "<|fim_prefix|>wrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Long key\n    let secret_2 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n    // Asymmetric key\n    let secret_3 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    assert_eq!(\n        secret_3.serialize_public_key(),\n        \"whpk_1SiA4o9hyqTCpIqC5V9HUakiiaeACeqfZTInDBbOir4=\"\n    );\n\n    let mut ep_in = EndpointIn {\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    ep_in.key = Some(secret_2.clone());\n    let endp_2 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // We rotate the key after because it's easier than setting json! for everything\n    ep_in.key = Some(secret_2.clone());\n    let endp_3 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp_3.id),\n            json!({\n                \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\",\n            }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    for (secret, ep) in [(secret_1, endp_1), (secret_2, endp_2), (secret_3, endp_3)] {\n        assert_eq!(\n            secret.serialize_public_key(),\n            client\n                .get::<EndpointSecretOutTest>(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                    StatusCode::OK\n                )\n                .await\n                .unwrap()\n                .key\n        );\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_encryption() {\n    let org_id = OrganizationId::new(None, None);\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let cfg = get_default_test_config();\n    let (client, jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n    jh.abort();\n\n    // Now add encryption and check the secret is still fine\n    let mut cfg = get_default_test_config();\n    cfg.encryption = Encryption::new([1; 32]);\n    let (client, jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading the existing secret works\n    assert_eq!(secret, secret2);\n\n    // Generate a new encrypted secret\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": secret }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading and saving works for encrypted\n    assert_eq!(secret, secret2);\n    jh.abort();\n\n    // Make sure we can't read it with the secret unset\n    let cfg = get_default_test_config();\n    l<|fim_suffix|>    client\n        .get::<IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::INTERNAL_SERVER_ERROR,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_invalid_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_too_short = \"whsec_C2FVsBQIhrscChlQIM+b5sSYspob\".to_owned();\n    let secret_too_long =\n        \"whsec_V09IYXZUaFJoSnFobnpJQkpPMXdpdGFNWnJsRzAxdXZCeTVndVpwRmxSSXFsc0oyYzBTRWRUekJhYnlaZ0JSRGNPQ3BGZG1xYjFVVmRGQ3UK\"\n            .to_owned();\n    let invalid_prefix = \"hwsec_C2FVsBQIhrscChlQIM+b5sSYspob7oDazfgh\".to_owned();\n\n    for sec in [secret_too_short, secret_too_long, invalid_prefix] {\n        let _: IgnoredAny = client\n            .post(\n                &format!(\"api/v1/app/{app_id}/endpoint/\"),\n                json!({\n                    \"url\": \"http://www.example.com\",\n                    \"version\": 1,\n                    \"secret\": sec,\n                }),\n                StatusCode::UNPROCESSABLE_ENTITY,\n            )\n            .await\n            .unwrap();\n    }\n}\n\nfn new_message_attempt_at_time(\n    timestamp: DateTime<Utc>,\n    status: MessageStatus,\n    endp_id: &EndpointId,\n    msg_id: &MessageId,\n) -> messageattempt::ActiveModel {\n    messageattempt::ActiveModel {\n        endp_id: Set(endp_id.clone()),\n        msg_id: Set(msg_id.clone()),\n        id: Set(MessageAttemptId::new(timestamp.into(), None)),\n        status: Set(status),\n        created_at: Set(timestamp.into()),\n        url: Set(\"http://www.example.com\".into()),\n        response_status_code: Set(200),\n        response_duration_ms: Set(1000),\n        response: Set(\"{}\".into()),\n        trigger_type: Set(MessageAttemptTriggerType::Scheduled),\n\n        ..ActiveModelTrait::default()\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_stats() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp_id = create_test_endpoint(&client, &app_id, \"https://gabagool.deli\")\n        .await\n        .unwrap()\n        .id;\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 0);\n    assert_eq!(stats.sending, 0);\n\n    let last_msg_time = {\n        // Create the relevant Stats records manually, otherwise\n        // it's difficult to test exact state of messagedestinations.\n\n        let cfg = get_default_test_config();\n        let db = Arc::new(cfg);\n        let db = svix_server::db::init_db(&db).await;\n\n        let now = Utc::now();\n\n        let msg = message::ActiveModel {\n            app_id: Set(app_id.clone()),\n            org_id: Set(OrganizationId::new(None, None)),\n            expiration: Set(Utc::now().into()),\n            event_type: Set(EventTypeName(\"test.ing\".into())),\n            created_at: Set((now - chrono::Duration::minutes(65)).into()),\n            id: Set(MessageId::new(\n                (now - chrono::Duration::minutes(65)).into(),\n                None,\n            )),\n            ..message::ActiveModel::new()\n        }\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(60),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(45),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(30),\n            MessageStatus::Sending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwr<|fim_middle|>", "completion": "let (client, _jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1395, 1395]}
{"prompt": "<|fim_prefix|>\"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"2\"] {\n            let list_failed: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_failed.data.len() == 2);\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_3));\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_4));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // No messages should still be listed as `sending`\n    let l: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status=3\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(l.data.len(), 0);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_sending() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"3\"] {\n            let list_sending: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_sending.data.len() == 2);\n            anyhow::ensure!(list_sending.data.iter().any(|x| x.msg == msg_3));\n            anyhow::ensure!(list_sending.data.iter().any(|x| x.msg == msg_4));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\nstruct FailFirstSucceedSecond {\n    first_done: Arc<Mutex<bool>>,\n}\n\nimpl FailFirstSucceedSecond {\n    f<|fim_suffix|>}\n\nimpl Respond for FailFirstSucceedSecond {\n    fn respond(&self, _request: &wiremock::Request) -> ResponseTemplate {\n        if *self.first_done.lock().unwrap() {\n            return ResponseTemplate::new(200);\n        }\n        let mut first_done = self.first_done.lock().unwrap();\n        *first_done = true;\n        ResponseTemplate::new(500)\n    }\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_success_are_not_sending() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mock_server = MockServer::start().await;\n    let responder = FailFirstSucceedSecond::new();\n    Mock::given(matchers::method(\"POST\"))\n        .respond_with(responder)\n        .mount(&mock_server)\n        .await;\n\n    let endp_id = create_test_endpoint(&client, &app_id, &mock_server.uri())\n        .await\n        .unwrap()\n        .id;\n\n    let _msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        let l: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status=0\"),\n                StatusCode::OK,\n            )\n            .await?;\n\n        anyhow::ensure!(l.data.len() == 1);\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // Message should not still be listed as `sending`\n    let l: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status=3\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(l.data.len(), 0);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_destinations() {\n    use svix_server::v1::endpoints::attempt::MessageEndpointOut;\n\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testListAttemptedDestinations\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver_1 =\n        TestReceiver::start_with_body(StatusCode::OK, axum::Json(json!({ \"ok\": true })));\n    let endp_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap();\n\n    let mut receiver_2 =\n        TestReceiver::start_with_body(StatusCode::OK, axum::Json(json!({ \"ok\": true })));\n    let endp_2 = create_test_endpoint(&client, &app_id, &receiver_2.endpoint)\n        .await\n        .unwrap();\n\n    let mut receiver_3 =\n        TestReceiver::start_with_body(StatusCode::OK, axum::Json(json!({ \"ok\": true })));\n    let endp_3 = create_test_endpoint(&client, &app_id, &receiver_3.endpoint)\n        .await\n        .unwrap();\n\n    let msg_id = create_test_message(&client, &app_id, json!({ \"test\": \"data\" }))\n        .await\n        .unwrap()\n        .id;\n\n    receiver_1.recv_body_value().await;\n    receiver_2.recv_body_value().await;\n    receiver_3.recv_body_value().await;\n\n    let destinations = run_with_retries(async || {\n        let list: ListResponse<MessageEndpointOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/\"),\n                StatusCode::OK,\n            )\n            .await?;\n        anyhow::ensure!(list.data.len() == 3);\n\n        Ok(list)\n    })\n    .await\n    .unwrap();\n\n    let endpoint_ids: Vec<_> = destinations.data.iter().map(|d| d.id.clone()).collect();\n    assert_eq!(\n        endpoint_ids,\n        vec![endp_1.id.clone(), endp_2.id.clone(), endp_3.id.clone()]\n    );\n\n    // Delete one endpoint\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", endp_2.id),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Should still see all 3 endpoints (including deleted)\n    let destinations_after_delete: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap(<|fim_middle|>", "completion": "fn new() -> Self {\n        Self {\n            first_done: Arc::new(Mutex::new(false)),\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "function_item", "line_range": [374, 378]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct SinkTransformIn {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub code: Option<String>,\n}\n\nimpl SinkTransformIn {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new() -> Self {\n        Self { code: None }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/sink_transform_in.rs", "node_type": "function_item", "line_range": [11, 13]}
{"prompt": "<|fim_prefix|>//! Extensions of the auto-generated \"models\" (schema structs).\n\nuse std::str::FromStr;\n\nuse serde_json::json;\n\nuse crate::{\n    api::{ConnectorProduct, MessageStatus, Ordering, StatusCodeClass},\n    models::MessageIn,\n};\n\nimpl MessageIn {\n    /// Create a new message with a raw string payload.\n    ///\n    /// The payload is not normalized on the server. Normally, payloads are\n    /// required to be JSON, and Svix will minify the payload before sending\n    /// the webhook (for example, by removing extraneous whitespace or\n    /// unnecessarily escaped characters in strings). With this constructor,\n    /// the payload will be sent \"as is\", without any minification or other\n    /// processing.\n    ///\n    /// The default `content-type` of `application/json` will still be used for\n    /// the webhook sent by Svix, unless overwritten with\n    /// [`with_content_type`][Self::with_content_type].\n    pub fn new_raw_payload(event_type: String, payload: String) -> Self {\n        Self {\n            transformations_params: Some(json!({ \"rawPayload\": payload })),\n            ..Self::new(event_type, json!({}))\n        }\n    }\n\n    /// Set the `content-type` header to use for the webhook sent by Svix.\n    pub fn with_content_type(mut self, content_type: String) -> Self {\n        let transformations_params = self.transformations_params.get_or_insert_with(|| json!({}));\n\n        // This will panic if transformations_params, its headers field, or the\n        // headers' content-type field already exists as an array, bool, number\n        // or string.\n        // That would make the whole parameter struct invalid anyways though,\n        // and can hardly happen accidentally.\n        transformations_params[\"headers\"][\"content-type\"] = content_type.into();\n\n        self\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\n#[error(\"invalid value for productType\")]\npub struct ConnectorProductFromStrError;\n\nimpl FromStr for ConnectorProduct {\n    type Err = ConnectorProductFromStrError;\n\n    fn from_str(s: &str) -> Result<Self, Self::Err> {\n        match s {\n            \"Dispatch\" => Ok(Self::Dispatch),\n            \"Stream\" => Ok(Self::Stream),\n            _ => Err(ConnectorProductFromStrError),\n        }\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\n#[error(\"invalid value for ordering\")]\npub struct OrderingFromStrError;\n\nimpl FromStr for Ordering {\n    type Err = OrderingFromStrError;\n\n    fn from_str(s: &str) -> Result<Self, Self::Err> {\n        match s {\n            \"ascending\" => Ok(Self::Ascending),\n            \"descending\" => Ok(Self::Descending),\n            _ => Err(OrderingFromStrError),\n        }\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\n#[error(\"invalid value for messageStatus\")]\npub struct MessageStatusFromStrError;\n\nimpl FromStr for MessageStatus {\n    type Err = MessageStatusFromStrError;\n\n    fn from_str(s: &str) -> Result<Self, Self::Err> {\n        match s {\n            \"0\" | \"success\" => Ok(Self::Success),\n            \"1\" | \"pending\" => Ok(Self::Pending),\n            \"2\" | \"fail\" => Ok(Self::Fail),\n            \"3\" | \"sending\" => Ok(Self::Sending),\n            _ => Err(MessageStatusFromStrError),\n        }\n    }\n}\n\n#[derive(Debug, thiserror::Error)]\n#[error(\"invalid value for statusCodeClass\")]\npub struct StatusCodeClassFromStrError;\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl FromStr for StatusCodeClass {\n    type Err = StatusCodeClassFromStrError;\n\n    fn from_str(s: &str) -> Result<Self, Self::Err> {\n        match s {\n            \"0\" => Ok(Self::CodeNone),\n            \"100\" => Ok(Self::Code1xx),\n            \"200\" => Ok(Self::Code2xx),\n            \"300\" => Ok(Self::Code3xx),\n            \"400\" => Ok(Self::Code4xx),\n            \"500\" => Ok(Self::Code5xx),\n            _ => Err(StatusCodeClassFromStrError),\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/model_ext.rs", "node_type": "impl_item", "line_range": [101, 115]}
{"prompt": "<|fim_prefix|>est to each.\n#[tokio::test]\nasync fn test_transformation_json() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let mut input = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            input.insert(\"__TRANSFORMED__\".into(), json!(true));\n            let out = json!({ \"payload\": input });\n\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"transformed\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: Some(\n                    \"handler = (x) => ({ payload: {__TRANSFORMED__: true, ...x }})\".into(),\n                ),\n            },\n        ),\n        (\n            \"as-is\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The `__TRANSFORMED__` key should have been added\n    assert_eq!(\n        json!(forwarded),\n        json!({\"a\": true, \"__TRANSFORMED__\": true})\n    );\n\n    let request = Request::builder()\n        .uri(\"/webhook/as-is\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    // The same payload should come through, without any transformation.\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_transformation_string() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let out = match x.input {\n                TransformerInput::String(input) => json!({\"payload\": { \"got\": input }})\n                    .as_object()\n                    .cloned()\n                    .unwrap(),\n                _ => unreachable!(),\n            };\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"transformed\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: Some(TransformationConfig::Explicit {\n                format: TransformerInputFormat::String,\n                src: String::from(\"handler = (x) => ({ payload: { got: x }})\"),\n            }),\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"text/plain\")\n        .body(axum::body::Body::from(\"plain text\"))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The plain text message should have been added in the key \"got\"\n    assert_eq!(json!(forwarded), json!({\"got\": \"plain text\"}));\n\n    assert!(a_rx.try_recv().is_err());\n}\n\n// Two different bodies - one used during signing, then the other is what we send in the request.\n// This should result in a bad response status.\n#[tokio::test]\nasync fn test_forwarding_svix_verification_mismatch() {\n    let signed_payload_bytes = serde_json::to_vec(&json!({\"a\": true})).unwrap();\n    let sent_payload_bytes = serde_json::to_vec(&json!({\"a\": false})).unwrap();\n\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n\n    let webhook = Arc::new(Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap());\n\n    let timestamp = chrono::Utc::now().timestamp();\n    let signature = webhook\n        .sign(\"msg_valid\", timestamp, &signed_payload_bytes)\n        .unwrap();\n\n    let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: SvixVerifier::new(webhook).into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .header(\"svix-id\", \"msg_valid\")\n                .header(\"svix-signature\", signature.clone())\n                .header(\"svix-timestamp\", &format!(\"{timestamp}\"))\n                .body(axum::body::Body::from(sent_payload_bytes))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n\n    // Expect a rejection due to signature verification failure.\n    assert_eq!(response.status(), StatusCode::BAD_REQUEST);\n    // There should be noting in the channel since the request should _not have been forwarded_.\n    assert!(a_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_forwarding_svix_verification_match() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n\n    let webhook = Arc::new(Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap());\n\n    let payload = json!({\"a\": true});\n    let payload_bytes = serde_json::to_vec(&payload).unwrap();\n    let timestamp = chrono::Utc::now().timestamp();\n    <|fim_suffix|>\n\n    let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: SvixVerifier::new(webhook).into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .header(\"svix-id\", \"msg_valid\")\n                .header(\"svix-signature\", signature.clone())\n                .header(\"svix-timestamp\", timestamp.to_string())\n                .body(axum::body::Body::from(payload_bytes))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({ \"a\": true }));\n}\n<|fim_middle|>", "completion": "let signature = webhook\n        .sign(\"msg_valid\", timestamp, &payload_bytes)\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/tests.rs", "node_type": "let_declaration", "line_range": [362, 364]}
{"prompt": "<|fim_prefix|>de(default)]\n    pub include_archived: bool,\n    /// When `true` the full item (including the schema) is included in the response\n    #[serde(default)]\n    pub with_content: bool,\n}\n\n/// Return the list of event types.\n#[aide_annotate(op_id = \"v1.event-type.list\")]\nasync fn list_event_types(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<Pagination<ReversibleIterator<EventTypeName>>>,\n    fetch_options: ValidatedQuery<ListFetchQueryParams>,\n    permissions::ReadAll {\n        org_id,\n        feature_flags,\n        ..\n    }: permissions::ReadAll,\n) -> Result<Json<ListResponse<EventTypeOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    let iterator = pagination.iterator;\n    let iter_direction = iterator\n        .as_ref()\n        .map_or(IteratorDirection::Normal, |iter| iter.direction());\n\n    let mut query = eventtype::Entity::secure_find(org_id);\n\n    if !fetch_options.include_archived {\n        query = query.filter(eventtype::Column::Deleted.eq(false));\n    }\n\n    if let permissions::AllowedFeatureFlags::Some(flags) = feature_flags {\n        query = eventtype::Entity::filter_feature_flags(query, flags);\n    }\n\n    let query = apply_pagination(\n        query,\n        eventtype::Column::Name,\n        limit,\n        iterator,\n        Ordering::Ascending,\n    );\n\n    Ok(Json(EventTypeOut::list_response(\n        query\n            .all(db)\n            .await?\n            .into_iter()\n            .map(|x| {\n                if !fetch_options.with_content {\n                    EventTypeOut::without_payload(x)\n                } else {\n                    x.into()\n                }\n            })\n            .collect(),\n        limit as usize,\n        iter_direction,\n    )))\n}\n\n/// Create new or unarchive existing event type.\n///\n/// Unarchiving an event type will allow endpoints to filter on it and messages to be sent with it.\n/// Endpoints filtering on the event type before archival will continue to filter on it.\n/// This operation does not preserve the description and schemas.\n#[aide_annotate(op_id = \"v1.event-type.create\")]\nasync fn create_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<EventTypeIn>,\n) -> Result<JsonStatus<201, EventTypeOut>> {\n    let evtype = eventtype::Entity::secure_find_by_name(org_id.clone(), data.name.to_owned())\n        .one(db)\n        .await?;\n    let ret = match evtype {\n        Some(evtype) => {\n            if evtype.deleted {\n                let mut evtype: eventtype::ActiveModel = evtype.into();\n                evtype.deleted = Set(false);\n                data.update_model(&mut evtype);\n                evtype.update(db).await?\n            } else {\n                return Err(HttpError::conflict(\n                    Some(\"event_type_exists\".to_owned()),\n                    Some(\"An event_type with this name already exists\".to_owned()),\n                )\n                .into());\n            }\n        }\n        None => {\n            let evtype = eventtype::ActiveModel {\n                org_id: Set(org_id),\n                ..data.into()\n            };\n            evtype.insert(db).await.map_err(http_error_on_conflict)?\n        }\n    };\n    Ok(JsonStatus(ret.into()))\n}\n\n/// Get an event type.\n#[aide_annotate(op_id = \"v1.event-type.get\")]\nasync fn get_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(EventTypeNamePath { event_type_name }): Path<EventTypeNamePath>,\n    permissions::ReadAll {\n        org_id,\n        feature_flags,\n        ..\n    }: permissions::ReadAll,\n) -> Result<Json<EventTypeOut>> {\n    let mut query = eventtype::Entity::secure_find_by_name(org_id, event_type_name);\n    if let permissions::AllowedFeatureFlags::Some(flags) = feature_flags {\n        query = eventtype::Entity::filter_feature_flags(query, flags);\n    }\n    let evtype = query\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    Ok(Json(evtype.into()))\n}\n\n/// Update an event type.\n#[aide_annotate(op_id = \"v1.event-type.update\")]\nasync fn update_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(EventTypeNamePath { event_type_name }): Path<EventTypeNamePath>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<EventTypeUpdate>,\n) -> Result<JsonStatusUpsert<EventTypeOut>> {\n    let evtype = eventtype::Entity::secure_find_by_name(org_id.clone(), event_type_name.clone())\n        .one(db)\n        .await?;\n\n    match evtype {\n        Some(evtype) => {\n            let mut evtype: eventtype::ActiveModel = evtype.into();\n            data.update_model(&mut evtype);\n            let ret = evtype.update(db).await.map_err(http_error_on_conflict)?;\n\n            Ok(JsonStatusUpsert::Updated(ret.into()))\n        }\n        None => {\n            l<|fim_suffix|>\n            Ok(JsonStatusUpsert::Created(ret.into()))\n        }\n    }\n}\n\n/// Partially update an event type.\n#[aide_annotate]\nasync fn patch_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(EventTypeNamePath { event_type_name }): Path<EventTypeNamePath>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<EventTypePatch>,\n) -> Result<Json<EventTypeOut>> {\n    let evtype = eventtype::Entity::secure_find_by_name(org_id, event_type_name)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut evtype: eventtype::ActiveModel = evtype.into();\n    data.update_model(&mut evtype);\n\n    let ret = evtype.update(db).await.map_err(http_error_on_conflict)?;\n    Ok(Json(ret.into()))\n}\n\n#[derive(Debug, Deserialize, Serialize, Validate, JsonSchema)]\nstruct DeleteEventTypeQueryParams {\n    /// By default event types are archived when \"deleted\". Passing this to `true` deletes them\n    /// entirely.\n    #[serde(default)]\n    expunge: bool,\n}\n\n/// Archive an event type.\n///\n/// Endpoints already configured to filter on an event type will continue to do so after archival.\n/// However, new messages can not be sent with it and endpoints can not filter on it.\n/// An event type can be unarchived with the\n/// [create operation](#operation/create_event_type_api_v1_event_type__post).\n#[aide_annotate(op_id = \"v1.event-type.delete\")]\nasync fn delete_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(EventTypeNamePath { event_type_name }): Path<EventTypeNamePath>,\n    ValidatedQuery(DeleteEventTypeQueryParams { expunge }): ValidatedQuery<\n        DeleteEventTypeQueryParams,\n    >,\n    permissions::Organization { org_id }: permissions::Organization,\n) -> Result<NoContent> {\n    let evtype = eventtype::Entity::secure_find_by_name(org_id, event_type_name)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut evtype: eventtype::ActiveModel = evtype.into();\n\n    if expunge {\n        evtype.delete(db).await?;\n    } else {\n        evtype.deleted = Set(true);\n        evtype.update(db).await?;\n    }\n    Ok(NoContent)\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Event Type\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/event-type\",\n            post_with(create_event_type, create_event_type_operation)\n                .get_with(list_event_types, list_event_types_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/event-type/:event_type_name\",\n            get_with(get_event_type, get_event_type_operation)\n                .put_with(update_event_type, update_event_type_operation)\n                .patch_with(patch_event_type, patch_event_type_operation)\n                .delete_with(delete_event_type, delete_event_type_operation),\n            &tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n\n    use super::ListFetchQueryParams;\n\n    #[test]\n    fn test_list_fetch_options_default() {\n        let l: ListFetchQueryParams = serde_json::from_value(json!({})).unwrap();\n        assert!(!l.include_archived);\n        assert!(!l.with_content);\n    }\n}\n<|fim_middle|>", "completion": "let ret = eventtype::ActiveModel {\n                org_id: Set(org_id),\n                name: Set(event_type_name),\n                ..data.into()\n            }\n            .insert(db)\n            .await\n            .map_err(http_error_on_conflict)?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/event_type.rs", "node_type": "let_declaration", "line_range": [371, 378]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct StreamEventTypeOut {\n    pub archived: bool,\n\n    #[serde(rename = \"createdAt\")]\n    pub created_at: String,\n\n    pub deprecated: bool,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub description: Option<String>,\n\n    #[serde(rename = \"featureFlags\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub feature_flags: Option<Vec<String>>,\n\n    /// The event type's name\n    pub name: String,\n\n    #[serde(rename = \"updatedAt\")]\n    pub updated_at: String,\n}\n\nimpl StreamEventTypeOut {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(\n        archived: bool,\n        created_at: String,\n        deprecated: bool,\n        name: String,\n        updated_at: String,\n    ) -> Self {\n        Self {\n            archived,\n            created_at,\n            deprecated,\n            description: None,\n            feature_flags: None,\n            name,\n            updated_at,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/stream_event_type_out.rs", "node_type": "function_item", "line_range": [28, 44]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse chrono::Utc;\nuse sea_orm::{entity::prelude::*, sea_query::OnConflict, ActiveValue::Set, TryIntoModel};\n\nu<|fim_suffix|>\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"applicationmetadata\")]\npub struct Model {\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: ApplicationId,\n    pub created_at: DateTimeWithTimeZone,\n    pub updated_at: DateTimeWithTimeZone,\n    pub data: Metadata,\n}\n\nimpl Model {\n    pub fn metadata(self) -> Metadata {\n        self.data\n    }\n\n    pub fn new(app_id: ApplicationId) -> Self {\n        ActiveModel::new(app_id, None)\n            .try_into_model()\n            .expect(\"ActiveModel::create(...) should have set all fields\")\n    }\n}\n\n#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]\npub enum Relation {\n    #[sea_orm(\n        belongs_to = \"super::application::Entity\",\n        from = \"Column::Id\",\n        to = \"super::application::Column::Id\",\n        on_update = \"NoAction\",\n        on_delete = \"Restrict\"\n    )]\n    Application,\n}\n\nimpl Related<super::application::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Application.def()\n    }\n}\n\n#[axum::async_trait]\nimpl ActiveModelBehavior for ActiveModel {\n    async fn before_save<C>(mut self, _db: &C, _insert: bool) -> Result<Self, DbErr>\n    where\n        C: ConnectionTrait,\n    {\n        self.updated_at = Set(Utc::now().into());\n        Ok(self)\n    }\n}\n\nimpl ActiveModel {\n    pub fn new(app_id: ApplicationId, metadata: impl Into<Option<Metadata>>) -> Self {\n        let id = Set(app_id);\n        let data = Set(metadata.into().unwrap_or_default());\n        let timestamp = Utc::now();\n        Self {\n            id,\n            data,\n            created_at: Set(timestamp.into()),\n            updated_at: Set(timestamp.into()),\n        }\n    }\n\n    /// Upserts the record if it's new or updated, AND data is nonempty. Otherwise the record is\n    /// ignored or destroyed as appropriate.\n    pub async fn upsert_or_delete(self, db: &impl ConnectionTrait) -> error::Result<Model> {\n        let data = self.data.clone().take().unwrap_or_default();\n\n        if data.is_empty() {\n            let model = self.clone().try_into_model()?;\n            self.delete(db).await?;\n            return Ok(model);\n        }\n\n        Ok(Entity::upsert(self).exec_with_returning(db).await?)\n    }\n}\n\nimpl Entity {\n    pub fn secure_find(app_id: ApplicationId) -> sea_orm::Select<Entity> {\n        Self::find().filter(Column::Id.eq(app_id))\n    }\n\n    pub fn upsert(am: ActiveModel) -> sea_orm::Insert<ActiveModel> {\n        Self::insert(am).on_conflict(\n            OnConflict::column(Column::Id)\n                .update_columns([Column::Data, Column::UpdatedAt])\n                .to_owned(),\n        )\n    }\n}\n<|fim_middle|>", "completion": "use crate::{\n    core::types::{metadata::Metadata, ApplicationId},\n    error,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/applicationmetadata.rs", "node_type": "use_declaration", "line_range": [7, 10]}
{"prompt": "<|fim_prefix|>Set(None);\n        let attempt = attempt.insert(*db).await?;\n\n        tracing::debug!(\n            \"Worker failure attempts exhausted: {} {} {}\",\n            err,\n            &attempt.id,\n            &endp.id\n        );\n\n        // Send common operational webhook\n        op_webhook_sender\n            .send_operational_webhook(\n                org_id,\n                OperationalWebhook::MessageAttemptExhausted(MessageAttemptEvent {\n                    app_id: app_id.clone(),\n                    app_uid: app_uid.cloned(),\n                    endpoint_id: msg_task.endpoint_id.clone(),\n                    msg_id: msg_task.msg_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await?;\n\n        match process_endpoint_failure(\n            cache,\n            app_id,\n            org_id,\n            endp,\n            cfg.endpoint_failure_disable_after,\n        )\n        .await?\n        {\n            None => Ok(()),\n\n            Some(EndpointDisableInfo { first_failure_at }) => {\n                let endp = endpoint::Entity::secure_find_by_id(\n                    msg_task.app_id.clone(),\n                    msg_task.endpoint_id.clone(),\n                )\n                .one(*db)\n                .await?\n                .ok_or_else(|| {\n                    Error::generic(format_args!(\n                        \"Endpoint not found {app_id} {}\",\n                        &msg_task.endpoint_id\n                    ))\n                })?;\n\n                let endp = endpoint::ActiveModel {\n                    disabled: Set(true),\n                    first_failure_at: Set(Some(first_failure_at.into())),\n                    ..endp.into()\n                };\n                let _endp = endp.update(*db).await?;\n\n                // Send operational webhooks\n                op_webhook_sender\n                    .send_operational_webhook(\n                        org_id,\n                        OperationalWebhook::EndpointDisabled(EndpointDisabledEventData {\n                            app_id: app_id.clone(),\n                            app_uid: app_uid.cloned(),\n                            endpoint_id: msg_task.endpoint_id.clone(),\n                            // TODO:\n                            endpoint_uid: None,\n                            fail_since: first_failure_at,\n                        }),\n                    )\n                    .await\n            }\n        }\n    }\n}\n\n#[derive(Clone)]\nstruct DispatchContext<'a> {\n    msg_task: &'a MessageTask,\n    payload: &'a str,\n    endp: &'a CreateMessageEndpoint,\n    org_id: &'a OrganizationId,\n    app_id: &'a ApplicationId,\n    app_uid: Option<&'a ApplicationUid>,\n    msg_uid: Option<&'a MessageUid>,\n    // Like 'attempt_number' from the task, but adjusted to -1 for `MessageAttemptTriggerType::Manual`.\n    db_attempt_number: i16,\n}\n\n/// Dispatches one webhook\n#[tracing::instrument(\n    skip_all,\n    level = \"error\",\n    fields(\n        endp_id = msg_task.endpoint_id.0.as_str(),\n    )\n)]\nasync fn dispatch_message_task(\n    worker_context: &WorkerContext<'_>,\n    msg: &message::Model,\n    app: &CreateMessageApp,\n    msg_task: MessageTask,\n    payload: &str,\n    endp: CreateMessageEndpoint,\n    status: MessageStatus,\n) -> Result<()> {\n    let WorkerContext { webhook_client, .. } = worker_context;\n\n    tracing::trace!(\"Dispatch start\");\n\n    if (status != MessageStatus::Pending && status != MessageStatus::Sending)\n        && (msg_task.trigger_type != MessageAttemptTriggerType::Manual)\n    {\n        // TODO: it happens when this message destination is \"resent\". This leads to 2 queue tasks with the same message destination\n        tracing::debug!(\n            id = msg_task.msg_id.0,\n            \"MessageDestination is not pending (it's {:?}).\",\n            status,\n        );\n        return Ok(());\n    }\n\n    let db_attempt_number = match msg_task.trigger_type {\n        MessageAttemptTriggerType::Manual => -1,\n        MessageAttemptTriggerType::Scheduled => msg_task.attempt_count as _,\n    };\n\n    l<|fim_suffix|>\n    let dispatch = prepare_dispatch(worker_context, dispatch_context.clone()).await?;\n    let completed = match dispatch {\n        IncompleteDispatch::Pending(pending) => {\n            make_http_call(dispatch_context.clone(), pending, webhook_client).await?\n        }\n        IncompleteDispatch::Failed(failed) => CompletedDispatch::Failed(failed),\n    };\n\n    match completed {\n        CompletedDispatch::Successful(success) => {\n            handle_successful_dispatch(worker_context, dispatch_context, success).await\n        }\n        CompletedDispatch::Failed(failed) => {\n            handle_failed_dispatch(worker_context, dispatch_context, failed).await\n        }\n    }\n}\n\nfn bytes_to_string(bytes: bytes::Bytes) -> String {\n    match std::str::from_utf8(&bytes) {\n        Ok(v) => v.to_owned(),\n        Err(_) => STANDARD.encode(&bytes),\n    }\n}\n\n/// Manages preparation and execution of a QueueTask type\n#[tracing::instrument(\n    skip_all,\n    level = \"error\",\n    fields(msg_id, app_id, org_id, instance_id, task_type = queue_task.task_type())\n)]\nasync fn process_queue_task(\n    worker_context: WorkerContext<'_>,\n    queue_task: QueueTask,\n) -> Result<()> {\n    process_queue_task_inner(worker_context, queue_task)\n        .await\n        .map_err(|e| {\n            tracing::error!(\"{e}\");\n            e\n        })\n}\n\n/// Manages preparation and execution of a QueueTask type\nasync fn process_queue_task_inner(\n    worker_context: WorkerContext<'_>,\n    queue_task: QueueTask,\n) -> Result<()> {\n    let WorkerContext { db, cache, .. }: WorkerContext<'_> = worker_context;\n    let span = tracing::Span::current();\n\n    let (mut msg, msg_content, force_endpoint, trigger_type, attempt_count, status) =\n        match queue_task {\n            QueueTask::HealthCheck => return Ok(()),\n            QueueTask::MessageV1(task) => {\n                let (msg, msg_content) = message::Entity::find_by_id(task.msg_id.clone())\n                    .find_also_related(messagecontent::Entity)\n                    .one(db)\n                    .await?\n                    .ok_or_else(|| {\n                        Error::generic(format_args!(\n                            \"Unexpected: message doesn't exist {}\",\n                            task.msg_id\n                        ))\n                    })?;\n\n                let latest_attempt = messageattempt::Entity::secure_find_by_msg(msg.id.clone())\n                    .filter(messageattempt::Column::EndpId.eq(task.endpoint_id.clone()))\n                    .order_by_desc(messageattempt::Column::Id)\n                    .one(db)\n                    .await?;\n\n                let status = match latest_attempt {\n                    Some(attempt) => {\n                        match attempt.next_attempt {\n                            Some(_) => MessageStatus::Sending,\n                            // If here then we're processing old tasks from before\n                            // messagedest was removed, so we'll need to query messagedestination\n                            None if attempt.msg_dest_id.is_some() => {\n                                messagedestination::Entity::secure_find_by_msg(task.msg_id.clone())\n                                    .filter(\n                                        messagedestination::Column::EndpId\n                                            .eq(task.endpoint_id.clone()),\n                                    )\n                                    .one(db)\n                                    .await?\n                                    .ok_or_else(|| {\n                                        Error::generic(format!(\n                                            \"MessageDestination not found for message {}\",\n                                            &task.msg_id\n                                        ))\n                                    })?\n                                    .status\n                            }\n                            None => MessageStatus::Fail,\n                        }\n                    }\n                    None => MessageStatus::Sending,\n                };\n\n             <|fim_middle|>", "completion": "let dispatch_context = DispatchContext {\n        msg_task: &msg_task,\n        payload,\n        endp: &endp,\n        org_id: &app.org_id,\n        app_id: &app.id,\n        app_uid: app.uid.as_ref(),\n        msg_uid: msg.uid.as_ref(),\n        db_attempt_number,\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "let_declaration", "line_range": [708, 717]}
{"prompt": "<|fim_prefix|><|fim_suffix|>\n\nuse clap::Parser;\nuse itertools::{Either, Itertools};\nuse once_cell::sync::Lazy;\nuse opentelemetry::trace::TracerProvider as _;\nuse opentelemetry_otlp::WithExportConfig;\nuse opentelemetry_sdk::{\n    metrics::{data::Temporality, reader::TemporalitySelector, InstrumentKind, SdkMeterProvider},\n    runtime::Tokio,\n};\nuse svix_bridge_types::{PollerInput, SenderInput, TransformerJob};\nuse svix_ksuid::{KsuidLike as _, KsuidMs};\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\nuse tikv_jemallocator::Jemalloc;\nuse tracing_subscriber::{layer::SubscriberExt as _, util::SubscriberInitExt as _};\n\nuse self::config::Config;\n\nmod allocator;\nmod config;\nmod http_output;\nmod metrics;\nmod runtime;\nmod webhook_receiver;\n\nuse crate::{\n    allocator::{get_allocator_stat_mibs, get_allocator_stats},\n    config::{EitherReceiver, PollerReceiverConfig, WebhookReceiverConfig},\n    metrics::CommonMetrics,\n};\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\n#[global_allocator]\nstatic GLOBAL: Jemalloc = Jemalloc;\n\n#[cfg(all(target_env = \"msvc\", feature = \"jemalloc\"))]\ncompile_error!(\"jemalloc cannot be enabled on msvc\");\n\n// Seems like it would be useful to be able to configure this.\n// In some docker setups, hostname is sometimes the container id, and advertising this can be\n// helpful.\nstatic INSTANCE_ID: Lazy<String> = Lazy::new(|| KsuidMs::new(None, None).to_string());\n\nfn get_svc_identifiers(cfg: &Config) -> opentelemetry_sdk::Resource {\n    opentelemetry_sdk::Resource::new(vec![\n        opentelemetry::KeyValue::new(\n            \"service.name\",\n            cfg.opentelemetry\n                .as_ref()\n                .and_then(|x| x.service_name.as_deref())\n                .unwrap_or(\"svix-bridge\")\n                .to_owned(),\n        ),\n        opentelemetry::KeyValue::new(\"instance_id\", INSTANCE_ID.to_owned()),\n    ])\n}\n\nfn setup_tracing(cfg: &Config) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        const CRATE_NAME: &str = env!(\"CARGO_CRATE_NAME\");\n        let level = cfg.log_level.to_string();\n        let var = [\n            format!(\"{CRATE_NAME}={level}\"),\n            // XXX: Assuming this applies to the Producer side (aka `og-ingester`) when we fold it back in.\n            format!(\"tower_http={level}\"),\n        ];\n        var.join(\",\")\n    });\n\n    let otel_layer = cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        // Configure the OpenTelemetry tracing layer\n        opentelemetry::global::set_text_map_propagator(\n            opentelemetry_sdk::propagation::TraceContextPropagator::new(),\n        );\n\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        let tracer = opentelemetry_otlp::new_pipeline()\n            .tracing()\n            .with_exporter(exporter)\n            .with_trace_config(\n                opentelemetry_sdk::trace::Config::default()\n                    .with_sampler(\n                        otel_cfg\n                            .sample_ratio\n                            .map(opentelemetry_sdk::trace::Sampler::TraceIdRatioBased)\n                            .unwrap_or(opentelemetry_sdk::trace::Sampler::AlwaysOn),\n                    )\n                    .with_resource(get_svc_identifiers(cfg)),\n            )\n            .install_batch(Tokio)\n            .unwrap()\n            .tracer(\"svix_bridge\");\n\n        tracing_opentelemetry::layer().with_tracer(tracer)\n    });\n\n    // Then create a subscriber with an additional layer printing to stdout.\n    // This additional layer is either formatted normally or in JSON format.\n    match cfg.log_format {\n        config::LogFormat::Default => {\n            let stdout_layer = tracing_subscriber::fmt::layer();\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n        config::LogFormat::Json => {\n            let fmt = tracing_subscriber::fmt::format().json().flatten_event(true);\n            let json_fields = tracing_subscriber::fmt::format::JsonFields::new();\n\n            let stdout_layer = tracing_subscriber::fmt::layer()\n                .event_format(fmt)\n                .fmt_fields(json_fields);\n\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n    };\n}\n\n/// Delta temporality selector as recommended by upstream:\n/// https://github.com/open-telemetry/opentelemetry-rust/discussions/1511#discussioncomment-8386721\nstruct DeltaTemporalitySelector;\n\nimpl TemporalitySelector for DeltaTemporalitySelector {\n    fn temporality(&self, kind: InstrumentKind) -> Temporality {\n        match kind {\n            InstrumentKind::UpDownCounter => Temporality::Cumulative,\n            InstrumentKind::ObservableUpDownCounter => Temporality::Cumulative,\n            _ => Temporality::Delta,\n        }\n    }\n}\n\npub fn setup_metrics(cfg: &Config) -> Option<SdkMeterProvider> {\n    cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        opentelemetry_otlp::new_pipeline()\n            .metrics(Tokio)\n            .with_temporality_selector(DeltaTemporalitySelector)\n            .with_exporter(exporter)\n            .with_resource(get_svc_identifiers(cfg))\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn supervise_senders(inputs: Vec<Box<dyn SenderInput>>) -> Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"sender input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnec<|fim_middle|>", "completion": "use std::{\n    io::{Error, Result},\n    path::PathBuf,\n    time::Duration,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "use_declaration", "line_range": [1, 5]}
{"prompt": "<|fim_prefix|>use std::{\n    io::{Error, Result},\n    path::PathBuf,\n    time::Duration,\n};\n\nuse clap::Parser;\nuse itertools::{Either, Itertools};\n<|fim_suffix|>\nuse opentelemetry::trace::TracerProvider as _;\nuse opentelemetry_otlp::WithExportConfig;\nuse opentelemetry_sdk::{\n    metrics::{data::Temporality, reader::TemporalitySelector, InstrumentKind, SdkMeterProvider},\n    runtime::Tokio,\n};\nuse svix_bridge_types::{PollerInput, SenderInput, TransformerJob};\nuse svix_ksuid::{KsuidLike as _, KsuidMs};\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\nuse tikv_jemallocator::Jemalloc;\nuse tracing_subscriber::{layer::SubscriberExt as _, util::SubscriberInitExt as _};\n\nuse self::config::Config;\n\nmod allocator;\nmod config;\nmod http_output;\nmod metrics;\nmod runtime;\nmod webhook_receiver;\n\nuse crate::{\n    allocator::{get_allocator_stat_mibs, get_allocator_stats},\n    config::{EitherReceiver, PollerReceiverConfig, WebhookReceiverConfig},\n    metrics::CommonMetrics,\n};\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\n#[global_allocator]\nstatic GLOBAL: Jemalloc = Jemalloc;\n\n#[cfg(all(target_env = \"msvc\", feature = \"jemalloc\"))]\ncompile_error!(\"jemalloc cannot be enabled on msvc\");\n\n// Seems like it would be useful to be able to configure this.\n// In some docker setups, hostname is sometimes the container id, and advertising this can be\n// helpful.\nstatic INSTANCE_ID: Lazy<String> = Lazy::new(|| KsuidMs::new(None, None).to_string());\n\nfn get_svc_identifiers(cfg: &Config) -> opentelemetry_sdk::Resource {\n    opentelemetry_sdk::Resource::new(vec![\n        opentelemetry::KeyValue::new(\n            \"service.name\",\n            cfg.opentelemetry\n                .as_ref()\n                .and_then(|x| x.service_name.as_deref())\n                .unwrap_or(\"svix-bridge\")\n                .to_owned(),\n        ),\n        opentelemetry::KeyValue::new(\"instance_id\", INSTANCE_ID.to_owned()),\n    ])\n}\n\nfn setup_tracing(cfg: &Config) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        const CRATE_NAME: &str = env!(\"CARGO_CRATE_NAME\");\n        let level = cfg.log_level.to_string();\n        let var = [\n            format!(\"{CRATE_NAME}={level}\"),\n            // XXX: Assuming this applies to the Producer side (aka `og-ingester`) when we fold it back in.\n            format!(\"tower_http={level}\"),\n        ];\n        var.join(\",\")\n    });\n\n    let otel_layer = cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        // Configure the OpenTelemetry tracing layer\n        opentelemetry::global::set_text_map_propagator(\n            opentelemetry_sdk::propagation::TraceContextPropagator::new(),\n        );\n\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        let tracer = opentelemetry_otlp::new_pipeline()\n            .tracing()\n            .with_exporter(exporter)\n            .with_trace_config(\n                opentelemetry_sdk::trace::Config::default()\n                    .with_sampler(\n                        otel_cfg\n                            .sample_ratio\n                            .map(opentelemetry_sdk::trace::Sampler::TraceIdRatioBased)\n                            .unwrap_or(opentelemetry_sdk::trace::Sampler::AlwaysOn),\n                    )\n                    .with_resource(get_svc_identifiers(cfg)),\n            )\n            .install_batch(Tokio)\n            .unwrap()\n            .tracer(\"svix_bridge\");\n\n        tracing_opentelemetry::layer().with_tracer(tracer)\n    });\n\n    // Then create a subscriber with an additional layer printing to stdout.\n    // This additional layer is either formatted normally or in JSON format.\n    match cfg.log_format {\n        config::LogFormat::Default => {\n            let stdout_layer = tracing_subscriber::fmt::layer();\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n        config::LogFormat::Json => {\n            let fmt = tracing_subscriber::fmt::format().json().flatten_event(true);\n            let json_fields = tracing_subscriber::fmt::format::JsonFields::new();\n\n            let stdout_layer = tracing_subscriber::fmt::layer()\n                .event_format(fmt)\n                .fmt_fields(json_fields);\n\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n    };\n}\n\n/// Delta temporality selector as recommended by upstream:\n/// https://github.com/open-telemetry/opentelemetry-rust/discussions/1511#discussioncomment-8386721\nstruct DeltaTemporalitySelector;\n\nimpl TemporalitySelector for DeltaTemporalitySelector {\n    fn temporality(&self, kind: InstrumentKind) -> Temporality {\n        match kind {\n            InstrumentKind::UpDownCounter => Temporality::Cumulative,\n            InstrumentKind::ObservableUpDownCounter => Temporality::Cumulative,\n            _ => Temporality::Delta,\n        }\n    }\n}\n\npub fn setup_metrics(cfg: &Config) -> Option<SdkMeterProvider> {\n    cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        opentelemetry_otlp::new_pipeline()\n            .metrics(Tokio)\n            .with_temporality_selector(DeltaTemporalitySelector)\n            .with_exporter(exporter)\n            .with_resource(get_svc_identifiers(cfg))\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn supervise_senders(inputs: Vec<Box<dyn SenderInput>>) -> Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"sender input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consum<|fim_middle|>", "completion": "use once_cell::sync::Lazy;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "use_declaration", "line_range": [9, 9]}
{"prompt": "<|fim_prefix|>   }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageOut {\n    /// Optional unique identifier for the message\n    #[serde(rename = \"eventId\")]\n    pub uid: Option<MessageUid>,\n    pub event_type: EventTypeName,\n    #[schemars(example = \"example_payload\")]\n    pub payload: RawPayload,\n    /// List of free-form identifiers that endpoints can filter by\n    #[schemars(length(min = 1, max = 5), example = \"example_channel_set\")]\n    pub channels: Option<EventChannelSet>,\n    pub id: MessageId,\n    #[serde(rename = \"timestamp\")]\n    pub created_at: DateTime<Utc>,\n}\n\nimpl MessageOut {\n    pub fn from_msg_and_payload(\n        model: message::Model,\n        content: Option<Vec<u8>>,\n        with_content: bool,\n    ) -> Self {\n        let payload = if with_content {\n            let payload = content\n                .and_then(|p| match serde_json::from_slice(&p) {\n                    Ok(v) => Some(v),\n                    Err(e) => {\n                        tracing::error!(\"Failed to parse content: {e}\");\n                        None\n                    }\n                })\n                .or(model.legacy_payload);\n            RawPayload::from_string(match payload {\n                Some(payload) => serde_json::to_string(&payload).expect(\"Can never fail\"),\n                None => r#\"{\"expired\":true}\"#.to_string(),\n            })\n            .expect(\"Can never fail\")\n        } else {\n            RawPayload::from_string(\"{}\".to_string()).expect(\"Can never fail\")\n        };\n\n        Self {\n            uid: model.uid,\n            event_type: model.event_type,\n            payload,\n            channels: model.channels,\n            id: model.id,\n            created_at: model.created_at.into(),\n        }\n    }\n}\n\nfn default_true() -> bool {\n    true\n}\n\nfn default_90() -> i64 {\n    90\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct ListMessagesQueryParams {\n    /// Filter response based on the channel\n    #[validate]\n    channel: Option<EventChannel>,\n    /// Only include items created before a certain date\n    before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    after: Option<DateTime<Utc>>,\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// List all of the application's messages.\n///\n/// The `before` parameter lets you filter all items created before a certain date and is ignored if an iterator is passed.\n/// The `after` parameter lets you filter all items created after a certain date and is ignored if an iterator is passed.\n/// `before` and `after` cannot be used simultaneously.\n#[aide_annotate(op_id = \"v1.message.list\")]\nasync fn list_messages(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<PaginationDescending<ReversibleIterator<MessageId>>>,\n    ValidatedQuery(ListMessagesQueryParams {\n        channel,\n        with_content,\n        before,\n        after,\n    }): ValidatedQuery<ListMessagesQueryParams>,\n    EventTypesQueryParams(event_types): EventTypesQueryParams,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<ListResponse<MessageOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n\n    let mut query = message::Entity::secure_find(app.id);\n\n    if let Some(EventTypeNameSet(event_types)) = event_types {\n        query = query.filter(message::Column::EventType.is_in(event_types));\n    }\n\n    if let Some(channel) = channel {\n        query = query.filter(Expr::cust_with_values(\"channels @> $1\", [channel.jsonb()]));\n    }\n\n    let (query, iter_direction) = filter_and_paginate_time_limited(\n        query,\n        message::Column::Id,\n        limit,\n        pagination.iterator,\n        before,\n        after,\n    );\n    let query = query.find_also_related(messagecontent::Entity);\n\n    let msgs_and_content: Vec<(message::Model, Option<messagecontent::Model>)> =\n        query.all(db).await?.into_iter().collect();\n    l<|fim_suffix|>\n    Ok(Json(MessageOut::list_response(\n        msgs_and_content.into_iter().map(into).collect(),\n        limit as usize,\n        iter_direction,\n    )))\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct CreateMessageQueryParams {\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// Creates a new message and dispatches it to all of the application's endpoints.\n///\n/// The `eventId` is an optional custom unique ID. It's verified to be unique only up to a day, after that no verification will be made.\n/// If a message with the same `eventId` already exists for any application in your environment, a 409 conflict error will be returned.\n///\n/// The `eventType` indicates the type and schema of the event. All messages of a certain `eventType` are expected to have the same schema. Endpoints can choose to only listen to specific event types.\n/// Messages can also have `channels`, which similar to event types let endpoints filter by them. Unlike event types, messages can have multiple channels, and channels don't imply a specific message content or schema.\n///\n/// The `payload` property is the webhook's body (the actual webhook message). Svix supports payload sizes of up to ~350kb, though it's generally a good idea to keep webhook payloads small, probably no larger than 40kb.\n#[aide_annotate(op_id = \"v1.message.create\")]\nasync fn create_message(\n    State(AppState {\n        ref db,\n        queue_tx,\n        cache,\n        ..\n    }): State<AppState>,\n    ValidatedQuery(CreateMessageQueryParams { with_content }): ValidatedQuery<\n        CreateMessageQueryParams,\n    >,\n    Path(ApplicationPath { app_id }): Path<ApplicationPath>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<MessageIn>,\n) -> Result<JsonStatus<202, MessageOut>> {\n    Ok(JsonStatus(\n        create_message_inner(\n            db,\n            queue_tx,\n            cache,\n            with_content,\n            None,\n            data,\n            org_id,\n            app_id,\n        )\n        .await?,\n    ))\n}\n\n#[allow(clippy::too_many_arguments)]\npub(crate) async fn create_message_inner(\n    db: &DatabaseConnection,\n    queue_tx: TaskQueueProducer,\n    cache: Cache,\n    with_content: bool,\n    force_endpoint: Option<EndpointId>,\n    data: MessageIn,\n    org_id: OrganizationId,\n    app_id: ApplicationIdOrUid,\n) -> Result<MessageOut> {\n    app_id.validate().map_err(|e| {\n        HttpError::unprocessable_entity(validation_errors(\n            vec![\"path\".to_owned(), \"app_id_or_uid\".to_owned()],\n            e,\n        ))\n    })?;\n\n    let app_from_path_app_id =\n        application::Entity::secure_find_by_id_or_uid(org_id.clone(), app_id.to_owned())\n            .one(db)\n            .await?;\n\n    let app = match (&data.application, app_from_path_app_id) {\n        (None, None) => {\n            return Err(\n                HttpError::not_found(None, Some(\"Application not found\".to_string())).into(),\n            );\n        }\n\n        (_, Some(app_from_path_param)) => app_from_path_param,\n        (Some(cmg_app), None) => {\n            validate_create_app_uid(&app_id, cmg_app)?;\n            let (app, _metadata) = create_app_from_app_in(db, cmg_app.to_owned(), org_id).await?;\n\n            app\n        }\n    };\n\n    let create_message_app = CreateMessageApp::layered_fetch(\n        &cache,\n        db,\n        Some(app.clone()),\n        app.org_id.clone(),\n        app.id.clone(),\n        std::time::Duration::from_secs(30),\n    )\n    .await?\n    // Should never happen since you're giving it an existing Application, but just in case\n    .ok_or_else(|| Error::generic(format_args!(\"Application doesn't exist: {}\", app.id)))?;\n\n    let payload = data.payload();\n    let msg = message::ActiveModel {\n        app_id: Set(app.id.clone()),\n        org_id: Set(app.org_id),\n        ..data.into()\n    };\n\n    let (msg, msg_content) = db\n        .transaction(|txn| {\n            async move {\n                let msg = msg.insert(txn).await.map_err(http_<|fim_middle|>", "completion": "let into = |(msg, content): (message::Model, Option<messagecontent::Model>)| {\n        MessageOut::from_msg_and_payload(msg, content.map(|c| c.payload), with_content)\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/message.rs", "node_type": "let_declaration", "line_range": [312, 314]}
{"prompt": "<|fim_prefix|>n) -> Self {\n        self.version = Some(version);\n        self\n    }\n\n    pub fn timeout(mut self, timeout: Duration) -> Self {\n        self.timeout = Some(timeout);\n        self\n    }\n\n    pub fn user_agent(mut self, user_agent: HeaderValue) -> Self {\n        self.user_agent = Some(user_agent);\n        self\n    }\n}\n\nimpl Default for RequestBuilder {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl RequestBuilder {\n    fn validate(&self) -> Result<(), RequestBuildError> {\n        let mut errs: Vec<BuildError> = Vec::new();\n        if self.uri.is_none() {\n            errs.push(BuildError::UriMissing);\n        }\n        if self.version.is_none() {\n            errs.push(BuildError::VersionMissing);\n        }\n\n        if !errs.is_empty() {\n            Err(RequestBuildError(errs))\n        } else {\n            Ok(())\n        }\n    }\n\n    pub fn build(self) -> Result<Request, RequestBuildError> {\n        self.validate()?;\n\n        let custom_headers = self.headers.unwrap_or_default();\n\n        let uri = self.uri.unwrap();\n        let authority = uri.authority().expect(\"Missing authority\");\n        let host = match authority.port() {\n            Some(port) => HeaderValue::from_str(&format!(\"{}:{port}\", authority.host())),\n            None => HeaderValue::from_str(authority.host()),\n        }\n        .unwrap();\n\n        let mut headers = HeaderMap::with_capacity(3 + custom_headers.len());\n\n        // Ensure that host header is first -- even though this is technically\n        // not required by HTTP spec, some clients fail if it's not first:\n        headers.insert(http::header::HOST, host);\n        headers.insert(\n            http::header::ACCEPT,\n            self.accept.unwrap_or(HeaderValue::from_static(\"*/*\")),\n        );\n        headers.insert(\n            http::header::CONTENT_TYPE,\n            self.content_type\n                .unwrap_or(HeaderValue::from_static(\"application/json\")),\n        );\n\n        headers.extend(custom_headers);\n\n        if let Some(user_agent) = self.user_agent {\n            headers.insert(http::header::USER_AGENT, user_agent);\n        }\n\n        if let Some(auth_header) = self.basic_auth {\n            if !headers.contains_key(http::header::AUTHORIZATION) {\n                headers.insert(\n                    http::header::AUTHORIZATION,\n                    HeaderValue::from_bytes(&auth_header).unwrap(),\n                );\n            }\n        }\n\n        Ok(Request {\n            method: self.method.unwrap_or(Method::POST),\n            uri,\n            headers,\n            header_names: self.header_names,\n            body: self.body,\n            timeout: self.timeout,\n            version: self.version.unwrap(),\n        })\n    }\n}\n\n/// HTTP connector that blocks outgoing requests to private IPs with support\n/// for HTTPS and optionally proxying via SOCKS5 or HTTP(S).\n#[derive(Clone)]\nenum SvixHttpsConnector {\n    Regular(HttpsConnector<NonLocalHttpConnector>),\n    Socks5Proxy {\n        proxy: HttpsConnector<SocksV5<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n    HttpProxy {\n        proxy: HttpsConnector<Tunnel<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n}\n\nimpl SvixHttpsConnector {\n    fn new(\n        inner: NonLocalHttpConnector,\n        proxy_cfg: Option<&ProxyConfig>,\n        disable_tls_verification: bool,\n    ) -> Result<Self, Box<dyn std::error::Error>> {\n        let https =\n            HttpsConnector::with_connector(inner.clone(), ssl_builder(disable_tls_verification))?;\n\n        let matcher = |proxy_url: String, noproxy: Option<ProxyBypassCfg>| -> Arc<Matcher> {\n            let mut matcher = Matcher::builder().all(proxy_url);\n            if let Some(noproxy) = noproxy {\n                matcher = matcher.no(noproxy.0);\n            }\n            Arc::new(matcher.build())\n        };\n\n        match proxy_cfg {\n            Some(proxy_cfg) => match proxy_cfg.addr.clone() {\n                ProxyAddr::Socks5(proxy_addr) => {\n                    <|fim_suffix|>\n                    let socks = SocksV5::new(proxy_addr, inner).local_dns(true);\n                    let socks_https = HttpsConnector::with_connector(\n                        socks,\n                        ssl_builder(disable_tls_verification),\n                    )?;\n                    Ok(Self::Socks5Proxy {\n                        proxy: socks_https,\n                        bypass: https,\n                        matcher,\n                    })\n                }\n                ProxyAddr::Http(proxy_addr) => {\n                    let matcher = matcher(proxy_addr.to_string(), proxy_cfg.noproxy.clone());\n                    let tunnel = Tunnel::new(proxy_addr, inner);\n                    let tunnel_https = HttpsConnector::with_connector(\n                        tunnel,\n                        ssl_builder(disable_tls_verification),\n                    )?;\n                    Ok(Self::HttpProxy {\n                        proxy: tunnel_https,\n                        bypass: https,\n                        matcher,\n                    })\n                }\n            },\n            None => Ok(Self::Regular(https)),\n        }\n    }\n}\n\nimpl Service<Uri> for SvixHttpsConnector {\n    type Response = MaybeHttpsStream<TokioIo<TcpStream>>;\n    type Error = Box<dyn std::error::Error + Send + Sync>;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(\n        &mut self,\n        cx: &mut std::task::Context<'_>,\n    ) -> std::task::Poll<Result<(), Self::Error>> {\n        match self {\n            Self::Regular(inner) => inner.poll_ready(cx),\n            Self::Socks5Proxy { proxy, bypass, .. } => {\n                ready!(proxy.poll_ready(cx)?);\n                ready!(bypass.poll_ready(cx)?);\n                Poll::Ready(Ok(()))\n            }\n            Self::HttpProxy { proxy, bypass, .. } => {\n                ready!(proxy.poll_ready(cx)?);\n                ready!(bypass.poll_ready(cx)?);\n                Poll::Ready(Ok(()))\n            }\n        }\n    }\n\n    fn call(&mut self, req: Uri) -> Self::Future {\n        match self {\n            Self::Regular(inner) => Box::pin(inner.call(req)),\n            Self::Socks5Proxy {\n                proxy,\n                bypass,\n                matcher,\n                ..\n            } => match matcher.intercept(&req) {\n                Some(_) => Box::pin(proxy.call(req)),\n                None => Box::pin(bypass.call(req)),\n            },\n            Self::HttpProxy {\n                proxy,\n                bypass,\n                matcher,\n                ..\n            } => match matcher.intercept(&req) {\n                Some(_) => Box::pin(proxy.call(req)),\n                None => Box::pin(bypass.call(req)),\n            },\n        }\n    }\n}\n\n/// A plain-HTTP connector that blocks outgoing requests to private IPs.\ntype NonLocalHttpConnector = HttpConnector<NonLocalDnsResolver>;\n\n/// A DNS resolver that produces an error for names that resolve to private IPs.\n///\n/// Specific private subnets or domain names may be whitelisted.\n#[derive(Clone, Debug)]\nstruct NonLocalDnsResolver {\n    state: Arc<Mutex<DnsState>>,\n    whitelist_nets: Arc<Vec<IpNet>>,\n    whitelist_names: Arc<Vec<String>>,\n}\n\n#[derive(Clone, Debug)]\nenum DnsState {\n    Init,\n    Ready(Arc<TokioResolver>),\n}\n\nimpl NonLocalDnsResolver {\n    pub fn new(whitelist_nets: Arc<Vec<IpNet>>, whitelist_names: Arc<Vec<String>>) -> Self {\n        NonLocalDnsResolver {\n            state: Arc::new(Mutex::new(DnsState::Init)),\n            whitelist_nets,\n            whitelist_names,\n        }\n    }\n}\n\nimpl Service<Name> for NonLocalDnsResolver {\n    type Response = SocketAddrs;\n    type Error = Error;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(&mut self, _cx: &mut std::task::Context<'_>) -> Poll<Result<(), Self::Error>> {\n        Poll::Ready(Ok(()))\n    }\n\n    fn call(&mut self, name: Name) -> Self::Future {\n        let resolver = self.clone();\n        let whitelist_nets = self.whitelist_nets.clone();\n        let whitelist_names = <|fim_middle|>", "completion": "let matcher = matcher(proxy_addr.to_string(), proxy_cfg.noproxy.clone());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [489, 489]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse chrono::Utc;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set, Condition};\n\nuse crate::core::types::{\n    ApplicationId, BaseId, EventChannelSet, EventTypeName, MessageId, MessageIdOrUid, MessageUid,\n    OrganizationId,\n};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"message\")]\npub struct Model {\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: MessageId,\n    pub created_at: DateTimeWithTimeZone,\n    pub org_id: OrganizationId,\n    pub app_id: ApplicationId,\n    pub event_type: EventTypeName,\n    pub uid: Option<MessageUid>,\n    #[sea_orm(column_type = \"JsonBinary\", column_name = \"payload\", nullable)]\n    pub legacy_payload: Option<Json>,\n    pub channels: Option<EventChannelSet>,\n    pub expiration: DateTimeWithTimeZone,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]\npub enum Relation {\n    #[sea_orm(\n        belongs_to = \"super::application::Entity\",\n        from = \"Column::AppId\",\n        to = \"super::application::Column::Id\",\n        on_update = \"NoAction\",\n        on_delete = \"Cascade\"\n    )]\n    Application,\n    #[sea_orm(has_one = \"super::messagecontent::Entity\")]\n    Messagecontent,\n    #[sea_orm(has_many = \"super::messageattempt::Entity\")]\n    Messageattempt,\n}\n\nimpl Related<super::application::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Application.def()\n    }\n}\n\nimpl Related<super::messagecontent::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Messagecontent.def()\n    }\n}\n\ni<|fim_suffix|>\nimpl ActiveModelBehavior for ActiveModel {\n    fn new() -> Self {\n        let timestamp = Utc::now();\n        Self {\n            id: Set(MessageId::new(timestamp.into(), None)),\n            created_at: Set(timestamp.into()),\n            ..ActiveModelTrait::default()\n        }\n    }\n}\n\nimpl Entity {\n    pub fn secure_find(app_id: ApplicationId) -> Select<Entity> {\n        Self::find().filter(Column::AppId.eq(app_id))\n    }\n\n    pub fn secure_find_by_id(app_id: ApplicationId, id: MessageId) -> Select<Entity> {\n        Self::secure_find(app_id).filter(Column::Id.eq(id))\n    }\n\n    pub fn secure_find_by_id_or_uid(\n        app_id: ApplicationId,\n        id_or_uid: MessageIdOrUid,\n    ) -> Select<Entity> {\n        Self::secure_find(app_id).filter(\n            Condition::any()\n                .add(Column::Id.eq(id_or_uid.to_owned()))\n                .add(Column::Uid.eq(id_or_uid)),\n        )\n    }\n}\n<|fim_middle|>", "completion": "impl Related<super::messageattempt::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Messageattempt.def()\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/message.rs", "node_type": "impl_item", "line_range": [56, 60]}
{"prompt": "<|fim_prefix|>data));\n    data.update_model(&mut endp);\n\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.insert(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointCreated(EndpointEvent::new(app.uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Create a new endpoint for the application.\n///\n/// When `secret` is `null` the secret is automatically generated (recommended)\n#[aide_annotate(op_id = \"v1.endpoint.create\")]\npub(super) async fn create_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        op_webhooks,\n        ..\n    }): State<AppState>,\n    _: Path<ApplicationPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointIn>,\n) -> Result<JsonStatus<201, EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let (endp, metadata) = create_endp_from_data(db, cfg, &op_webhooks, app, data)\n        .await\n        .trace()?;\n\n    Ok(JsonStatus((endp, metadata.data).into()))\n}\n\n/// Get an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.get\")]\npub(super) async fn get_endpoint(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointOut>> {\n    let (endp, metadata) = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .find_also_related(endpointmetadata::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let metadata = metadata.map(|m| m.data).unwrap_or_default();\n\n    Ok(Json((endp, metadata).into()))\n}\n\nasync fn update_endp_from_data(\n    db: &DatabaseConnection,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    endp: endpoint::ActiveModel,\n    metadata: endpointmetadata::ActiveModel,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.update(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    let app_uid = app.uid;\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointUpdated(EndpointEvent::new(app_uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Update an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.update\")]\npub(super) async fn update_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(mut data): ValidatedJson<EndpointUpdate>,\n) -> Result<JsonStatusUpsert<EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let models = endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n        .await\n        .trace()?;\n\n    if let Some((mut endp, mut metadata)) = models {\n        metadata.data = Set(mem::take(&mut data.metadata));\n        data.update_model(&mut endp);\n        let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Updated((endp, metadata.data).into()))\n    } else {\n        <|fim_suffix|>\n        let (endp, metadata) = create_endp_from_data(db, cfg, op_webhooks, app, data)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Created((endp, metadata.data).into()))\n    }\n}\n\n/// Partially update an endpoint.\n#[aide_annotate]\npub(super) async fn patch_endpoint(\n    State(AppState {\n        ref db,\n        cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointPatch>,\n) -> Result<Json<EndpointOut>> {\n    if let UnrequiredNullableField::Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    if let UnrequiredField::Some(url) = &data.url {\n        validate_endpoint_url(url, cfg.endpoint_https_only)?;\n    }\n\n    let (mut endp, mut metadata) =\n        endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut patch_data = data; // need to alias so we can use data for `patch_field_non_nullable!`\n\n    let data = mem::take(&mut patch_data.metadata);\n    patch_field_non_nullable!(metadata, data);\n    patch_data.update_model(&mut endp);\n    let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n        .await\n        .trace()?;\n\n    Ok(Json((endp, metadata.data).into()))\n}\n\n/// Delete an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.delete\")]\npub(super) async fn delete_endpoint(\n    State(AppState {\n        ref db,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<NoContent> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    // Cloning the ID/UID out of endp before it's consumed below\n    let endpoint_id = endp.id.clone();\n    let endpoint_uid = endp.uid.clone();\n\n    let mut endp: endpoint::ActiveModel = endp.into();\n    endp.deleted = Set(true);\n    endp.uid = Set(None); // We don't want deleted UIDs to clash\n    endp.update(db).await?;\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointDeleted(EndpointEvent {\n                app_id: app.id,\n                app_uid: app.uid,\n                endpoint_id,\n                endpoint_uid,\n            }),\n        )\n        .await?;\n\n    Ok(NoContent)\n}\n\n/// This module is here so that our Result override doesn't conflict\nmod hack {\n    use sea_orm::FromQueryResult;\n\n    use crate::core::types::EventTypeName;\n\n    #[derive(Debug, FromQueryResult)]\n    pub struct EventTypeNameResult {\n        pub name: EventTypeName,\n    }\n}\n\nasync fn validate_event_types(\n    db: &DatabaseConnection,\n    event_types_ids: &EventTypeNameSet,\n    org_id: &OrganizationId,\n) -> Result<()> {\n    let event_types: Vec<EventTypeNameResult> = eventtype::Entity::secure_find(org_id.clone())\n        .filter(eventtype::Column::Deleted.eq(false))\n        .select_only()\n        .column(eventtype::Column::Name)\n        .into_model::<EventTypeNameResult>()\n        .all(db)\n        .await?;\n    let event_types: HashSet<EventTypeName> =\n        HashSet::from_iter(event_types.into_iter().map(|x| x.name));\n    let missing: Vec<&EventTypeName> = event_types_ids\n        .0\n        .iter()\n        .filter(|x| !event_types.contains(x))\n        .collect();\n\n    if missing.is_empty() {\n        Ok(())\n    } else {\n        let missing = missing\n            .into_iter()\n            .map(|x| &(x.0[..]))\n            .collect::<Vec<&str>>()\n            .join(\", \");\n        Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n            loc: vec![\"body\".to_owned(), \"filterTypes\".to_owned()],\n            msg: for<|fim_middle|>", "completion": "let data = data.into_in_with_default_key();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/crud.rs", "node_type": "let_declaration", "line_range": [205, 205]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for PATCH requests focused mostly around non-required field types.\n\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse validator::Validate;\n\n/// This is an enum that will wrap every nullable field for a PATCH request.\n///\n/// Nonnullable fields can be represented via an [`UnrequiredField`]. This\n/// differs from an [`Option`] in that it distinguishes null values and absent\n/// values such that an optional value in a model may be made `None` via\n/// PATCHing while allowing omitted fields to be skipped when updating.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredNullableField<T> {\n    #[default]\n    Absent,\n    None,\n    Some(T),\n}\n\n/// This enum is a non-nullable equivalent to [`UnrequiredNullableField`].\n///\n/// This is effectively an [`Option`] with the additional context that any field\n/// which uses this type is a member of a PATCH request model and that the field\n/// may be absent, meaning it is not to be updated. In comparison, [`Option`]s\n/// are used in other [`ModelIn`]s to define a field, that when absent, is\n/// `null`.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredField<T> {\n    #[default]\n    Absent,\n    Some(T),\n}\n\nimpl<T> UnrequiredNullableField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredNullableField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredNullableField<U> {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> UnrequiredField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredField<U> {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> From<Option<T>> for UnrequiredNullableField<T> {\n    fn from(opt: Option<T>) -> Self {\n        match opt {\n            Some(v) => UnrequiredNullableField::Some(v),\n            None => UnrequiredNullableField::None,\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredNullableField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n            UnrequiredNullableField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredField::Absent => Ok(()),\n            UnrequiredField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredNullableField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone + Copy> Copy for UnrequiredNullableField<T> {}\nimpl<T: Clone + Copy> Copy for UnrequiredField<T> {}\n\ni<|fim_suffix|>\nimpl<'de, T> Deserialize<'de> for UnrequiredField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        T::deserialize(deserializer).map(UnrequiredField::Some)\n    }\n}\n\nimpl<T> Serialize for UnrequiredNullableField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredNullableField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredNullableField must skip serializing if field is absent\",\n            )),\n            UnrequiredNullableField::None => serializer.serialize_none(),\n            UnrequiredNullableField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\nimpl<T> Serialize for UnrequiredField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredField must skip serializing if field is absent\",\n            )),\n            UnrequiredField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"Unrequired_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<T>()\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredNullableField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"UnrequiredNullable_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<Option<T>>()\n    }\n}\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`Option`] is `Some`, but\n/// performs no operation in the case it is `None`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The nullable equivalent which is used for [`UnrequiredNullableField`] is [`patch_field_nullable`].\nmacro_rules! patch_field_non_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n}\npub(crate) use patch_field_non_nullable;\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`UnrequiredNullableField`] is\n/// `Some` and unsetting the field when it is `None`, but performs no operation in the case it is\n///  `Absent`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The non-nullable equivalent which is used for [`Option`] is [`patch_field_non_nullable`].\nmacro_ru<|fim_middle|>", "completion": "impl<'de, T> Deserialize<'de> for UnrequiredNullableField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        Option::deserialize(deserializer).map(Into::into)\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/patch.rs", "node_type": "impl_item", "line_range": [120, 130]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct IngestEndpointListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n}\n\nimpl From<IngestEndpointListOptions> for svix::api::IngestEndpointListOptions {\n    fn from(value: IngestEndpointListOptions) -> Self {\n        let IngestEndpointListOptions {\n            limit,\n            iterator,\n            order,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            order,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct IngestEndpointCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<IngestEndpointCreateOptions> for svix::api::IngestEndpointCreateOptions {\n    fn from(value: IngestEndpointCreateOptions) -> Self {\n        let IngestEndpointCreateOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct IngestEndpointRotateSecretOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<IngestEndpointRotateSecretOptions> for svix::api::IngestEndpointRotateSecretOptions {\n    <|fim_suffix|>\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct IngestEndpointArgs {\n    #[command(subcommand)]\n    pub command: IngestEndpointCommands,\n}\n\n#[derive(Subcommand)]\npub enum IngestEndpointCommands {\n    /// List ingest endpoints.\n    List {\n        source_id: String,\n        #[clap(flatten)]\n        options: IngestEndpointListOptions,\n    },\n    /// Create an ingest endpoint.\n    Create {\n        source_id: String,\n        ingest_endpoint_in: crate::json::JsonOf<IngestEndpointIn>,\n        #[clap(flatten)]\n        options: IngestEndpointCreateOptions,\n    },\n    /// Get an ingest endpoint.\n    Get {\n        source_id: String,\n        endpoint_id: String,\n    },\n    /// Update an ingest endpoint.\n    Update {\n        source_id: String,\n        endpoint_id: String,\n        ingest_endpoint_update: crate::json::JsonOf<IngestEndpointUpdate>,\n    },\n    /// Delete an ingest endpoint.\n    Delete {\n        source_id: String,\n        endpoint_id: String,\n    },\n    /// Get the additional headers to be sent with the ingest.\n    GetHeaders {\n        source_id: String,\n        endpoint_id: String,\n    },\n    /// Set the additional headers to be sent to the endpoint.\n    UpdateHeaders {\n        source_id: String,\n        endpoint_id: String,\n        ingest_endpoint_headers_in: crate::json::JsonOf<IngestEndpointHeadersIn>,\n    },\n    /// Get an ingest endpoint's signing secret.\n    ///\n    /// This is used to verify the authenticity of the webhook.\n    /// For more information please refer to [the consuming webhooks docs](https://docs.svix.com/consuming-webhooks/).\n    GetSecret {\n        source_id: String,\n        endpoint_id: String,\n    },\n    /// Rotates an ingest endpoint's signing secret.\n    ///\n    /// The previous secret will remain valid for the next 24 hours.\n    RotateSecret {\n        source_id: String,\n        endpoint_id: String,\n        ingest_endpoint_secret_in: Option<crate::json::JsonOf<IngestEndpointSecretIn>>,\n        #[clap(flatten)]\n        options: IngestEndpointRotateSecretOptions,\n    },\n    /// Get the transformation code associated with this ingest endpoint.\n    GetTransformation {\n        source_id: String,\n        endpoint_id: String,\n    },\n    /// Set or unset the transformation code associated with this ingest endpoint.\n    SetTransformation {\n        source_id: String,\n        endpoint_id: String,\n        ingest_endpoint_transformation_patch:\n            Option<crate::json::JsonOf<IngestEndpointTransformationPatch>>,\n    },\n}\n\nimpl IngestEndpointCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { source_id, options } => {\n                let resp = client\n                    .ingest()\n                    .endpoint()\n                    .list(source_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                source_id,\n                ingest_endpoint_in,\n                options,\n            } => {\n                let resp = client\n                    .ingest()\n                    .endpoint()\n                    .create(\n                        source_id,\n                        ingest_endpoint_in.into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get {\n                source_id,\n                endpoint_id,\n            } => {\n                let resp = client\n                    .ingest()\n                    .endpoint()\n                    .get(source_id, endpoint_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                source_id,\n                endpoint_id,\n                ingest_endpoint_update,\n            } => {\n                let resp = client\n                    .ingest()\n                    .endpoint()\n                    .update(source_id, endpoint_id, ingest_endpoint_update.into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete {\n                source_id,\n                endpoint_id,\n            } => {\n                client\n                    .ingest()\n                    .endpoint()\n                    .delete(source_id, endpoint_id)\n                    .await?;\n            }\n            Self::GetHeaders {\n                source_id,\n                endpoint_id,\n            } => {\n                let resp = client\n                    .ingest()\n                    .endpoint()\n                    .get_headers(source_id, endpoint_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::UpdateHeaders {\n                source_id,\n                endpoint_id,\n                ingest_endpoint_headers_in,\n            } => {\n                client\n                    .ingest()\n                    .endpoint()\n                    .update_headers(\n                        source_id,\n                        endpoint_id,\n                        ingest_endpoint_headers_in.into_inner(),\n                    )\n                    .await?;\n            }\n            Self::GetSecret {\n                source_id,\n                endpoint_id,\n            } => {\n                let resp = client\n                    .ingest()\n                    .endpoint()\n                    .get_secret(source_id, endpoint_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::RotateSecret {\n                source_id,\n                endpoint_id,\n                ingest_endpoint_secret_in,\n                options,\n            } => {\n                client\n                    .ingest()\n                    .endpoint()\n                    .rotate_secret(\n                        source_id,\n                        endpoint_id,\n                        ingest_endpoint_secret_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n            Self::GetTransformation {\n                source_id,\n                endpoint_id,\n            } => {\n                let resp = client\n                    .ingest()\n                    .endpoint()\n                    .get_transformation(source_id, endpoint_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n    <|fim_middle|>", "completion": "fn from(value: IngestEndpointRotateSecretOptions) -> Self {\n        let IngestEndpointRotateSecretOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/ingest_endpoint.rs", "node_type": "function_item", "line_range": [53, 56]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse chrono::Utc;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set, Condition};\n\nuse crate::core::types::{\n    ApplicationId, BaseId, EventChannelSet, EventTypeName, MessageId, MessageIdOrUid, MessageUid,\n    OrganizationId,\n};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"message\")]\npub struct Model {\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: MessageId,\n    pub created_at: DateTimeWithTimeZone,\n    pub org_id: OrganizationId,\n    pub app_id: ApplicationId,\n    pub event_type: EventTypeName,\n    pub uid: Option<MessageUid>,\n    #[sea_orm(column_type = \"JsonBinary\", column_name = \"payload\", nullable)]\n    pub legacy_payload: Option<Json>,\n    pub channels: Option<EventChannelSet>,\n    pub expiration: DateTimeWithTimeZone,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]\npub enum Relation {\n    #[sea_orm(\n        belongs_to = \"super::application::Entity\",\n        from = \"Column::AppId\",\n        to = \"super::application::Column::Id\",\n        on_update = \"NoAction\",\n        on_delete = \"Cascade\"\n    )]\n    Application,\n    #[sea_orm(has_one = \"super::messagecontent::Entity\")]\n    Messagecontent,\n    #[sea_orm(has_many = \"super::messageattempt::Entity\")]\n    Messageattempt,\n}\n\nimpl Related<super::application::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Application.def()\n    }\n}\n\ni<|fim_suffix|>\nimpl Related<super::messageattempt::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Messageattempt.def()\n    }\n}\n\nimpl ActiveModelBehavior for ActiveModel {\n    fn new() -> Self {\n        let timestamp = Utc::now();\n        Self {\n            id: Set(MessageId::new(timestamp.into(), None)),\n            created_at: Set(timestamp.into()),\n            ..ActiveModelTrait::default()\n        }\n    }\n}\n\nimpl Entity {\n    pub fn secure_find(app_id: ApplicationId) -> Select<Entity> {\n        Self::find().filter(Column::AppId.eq(app_id))\n    }\n\n    pub fn secure_find_by_id(app_id: ApplicationId, id: MessageId) -> Select<Entity> {\n        Self::secure_find(app_id).filter(Column::Id.eq(id))\n    }\n\n    pub fn secure_find_by_id_or_uid(\n        app_id: ApplicationId,\n        id_or_uid: MessageIdOrUid,\n    ) -> Select<Entity> {\n        Self::secure_find(app_id).filter(\n            Condition::any()\n                .add(Column::Id.eq(id_or_uid.to_owned()))\n                .add(Column::Uid.eq(id_or_uid)),\n        )\n    }\n}\n<|fim_middle|>", "completion": "impl Related<super::messagecontent::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Messagecontent.def()\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/message.rs", "node_type": "impl_item", "line_range": [50, 54]}
{"prompt": "<|fim_prefix|>.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn delete(&self, endpoint: &str, expected_code: StatusCode) -> Result<()> {\n        let mut req = self.client.delete(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        if expected_code == StatusCode::NO_CONTENT {\n            let res_body = resp.text().await.context(\"error receiving response\")?;\n            anyhow::ensure!(res_body.is_empty());\n        }\n\n        Ok(())\n    }\n\n    pub async fn patch<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn patch_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n}\n\npub fn get_default_test_config() -> ConfigurationInner {\n    let _ = dotenvy::dotenv();\n    let cfg = svix_server::cfg::load().unwrap();\n\n    cfg.as_ref().clone()\n}\n\npub async fn start_svix_server() -> (TestClient, tokio::task::JoinHandle<()>) {\n    start_svix_server_with_cfg(&get_default_test_config()).await\n}\n\npub async fn start_svix_server_with_cfg(\n    cfg: &ConfigurationInner,\n) -> (TestClient, tokio::task::JoinHandle<()>) {\n    start_svix_server_with_cfg_and_org_id(cfg, OrganizationId::new(None, None)).await\n}\n\npub async fn start_svix_server_with_cfg_and_org_id(\n    cfg: &ConfigurationInner,\n    org_id: OrganizationId,\n) -> (TestClient, tokio::task::JoinHandle<()>) {\n    let prefix = svix_ksuid::Ksuid::new(None, None).to_string();\n    start_svix_server_with_cfg_and_org_id_and_prefix(cfg, org_id, prefix).await\n}\n\npub async fn start_svix_server_with_cfg_and_org_id_and_prefix(\n    cfg: &ConfigurationInner,\n    org_id: OrganizationId,\n    prefix: String,\n) -> (TestClient, tokio::task::JoinHandle<()>) {\n    let (tracing_subscriber, _guard) = setup_tracing(cfg, /* for_test = */ true);\n\n    let cfg = Arc::new(cfg.clone());\n\n    let token = generate_org_token(&cfg.jwt_signing_config, org_id).unwrap();\n    let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n    let base_uri = format!(\"http://{}\", listener.local_addr().unwrap());\n\n    // Could update this fn to take a tokio TcpListener instead, but that's a pretty large diff\n    // for very little benefit (since this is just test code anyways).\n    listener.set_nonblocking(true).unwrap();\n    let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n    let jh = tokio::spawn(\n        svix_server::run_with_prefix(Some(prefix), cfg, Some(listener))\n            .with_subscriber(tracing_subscriber),\n    );\n\n    (TestClient::new(base_uri, &token), jh)\n}\n\n#[derive(Debug)]\npub struct TestReceiver {\n    pub endpoint: String,\n    pub jh: tokio::task::JoinHandle<()>,\n    pub data_recv: mpsc::Receiver<serde_json::Value>,\n    pub header_recv: mpsc::Receiver<HeaderMap>,\n    pub response_status_code: Arc<Mutex<ResponseStatusCode>>,\n}\n\n#[derive(Clone)]\npub struct TestAppState<T: IntoResponse + Clone> {\n    tx: mpsc::Sender<serde_json::Value>,\n    header_tx: mpsc::Sender<HeaderMap>,\n    response_status_code: Arc<Mutex<ResponseStatusCode>>,\n    response_body: T,\n}\n\n#[derive(Debug, Clone)]\npub struct ResponseStatusCode {\n    pub status_code: axum::http::StatusCode,\n}\n\nimpl TestReceiver {\n    <|fim_suffix|>\n\n    pub fn start_with_body<T>(resp_with: axum::http::StatusCode, body: T) -> Self\n    where\n        T: IntoResponse + Clone + Send + Sync + 'static,\n    {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n        let endpoint = format!(\"http://{}/\", listener.local_addr().unwrap());\n\n        let (tx, data_recv) = mpsc::channel(32);\n        let (header_tx, header_recv) = mpsc::channel(32);\n\n        let response_status_code = Arc::new(Mutex::new(ResponseStatusCode {\n            status_code: resp_with,\n        }));\n\n        let routes = axum::Router::new()\n            .route(\n                \"/\",\n                axum::routing::post(test_receiver_route).get(test_receiver_route),\n            )\n            .with_state(TestAppState {\n                tx,\n                header_tx,\n                response_status_code: response_status_code.clone(),\n                response_body: body,\n            })\n            .into_make_service();\n\n        let jh = tokio::spawn(async move {\n            axum::serve(listener, routes).await.unwrap();\n        });\n\n        TestReceiver {\n            endpoint,\n            jh,\n            data_recv,\n            header_recv,\n            response_status_code,\n        }\n    }\n\n    pub(crate) fn try_recv_body_value(\n        &mut self,\n    ) -> Result<serde_json::Value, mpsc::error::TryRecvError> {\n        let payload = self.data_recv.try_recv()?;\n        Ok(serde_json::from_value(payload).unwrap())\n    }\n\n    pub(crate) async fn recv_body(&mut self) -> Option<serde_json::Value> {\n        self.data_recv.recv().await\n    }\n\n    pub(crate) async fn recv_body_value(&mut self) -> Option<serde_json::Value> {\n        let payload = tokio::time::timeout(Duration::from_secs(30), self.data_recv.recv())\n            .await\n            .expect(\"timed out\")?;\n        Some(serde_json::from_value(payload).unwrap())\n    }\n\n    pub fn set_response_status_code(&self, resp_with: axum::http::StatusCode) {\n        self.response_status_code.lock().unwrap().status_code = resp_with;\n    }\n}\n\nasync fn test_receiver_route<T: IntoResponse + Clone>(\n    axum::extract::State(TestAppState {\n        tx,\n        header_tx,\n        response_status_code,\n        response_body,\n    }): axum::extract::State<TestAppState<T>>,\n    headers: HeaderMap,\n    axum::Json(json): axum::Json<serde_json::Value>,\n) -> (axum::http::StatusCode, impl IntoResponse) {\n    tx.send(json).await.unwrap();\n    header_tx.send(headers).await.unwrap();\n    (\n        response_status_code.lock().unwrap().status_code,\n        response_body,\n    )\n}\n\npub async fn run_with_retries<O, F, C>(f: C) -> Result<O>\nwhere\n    F: Future<Output = Result<O>>,\n    C: Fn() -> F,\n{\n    for attempt in 0..50 {\n        let out = f().await;\n        if out.is_ok() {\n            return out;\n        } else if let Err(err) = out {\n            println!(\"Attempt {attempt}: {err}\");\n        }\n\n        tokio::time::sleep(std::time::Duration::from_millis(50)).await;\n    }\n\n    anyhow::bail!(\"All attempts failed\");\n}\n<|fim_middle|>", "completion": "pub fn start(resp_with: axum::http::StatusCode) -> Self {\n        Self::start_with_body(resp_with, ())\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/mod.rs", "node_type": "function_item", "line_range": [388, 390]}
{"prompt": "<|fim_prefix|>lass {\n    CodeNone = 0,\n    Code1xx = 100,\n    Code2xx = 200,\n    Code3xx = 300,\n    Code4xx = 400,\n    Code5xx = 500,\n}\n\njsonschema_for_repr_enum! {\n    StatusCodeClass,\n    i16,\n    \"The different classes of HTTP status codes:\\n- CodeNone = 0\\n- Code1xx = 100\\n- Code2xx = 200\\n- Code3xx = 300\\n- Code4xx = 400\\n- Code5xx = 500\",\n    CodeNone, Code1xx, Code2xx, Code3xx, Code4xx, Code5xx\n}\n\nenum_wrapper!(MessageAttemptTriggerType);\nenum_wrapper!(MessageStatus);\nenum_wrapper!(StatusCodeClass);\n\n#[derive(Clone, Debug, Hash, Eq, PartialEq, Serialize)]\npub struct FeatureFlag(pub String);\n\ncommon_jsonschema_impl!(\n    FeatureFlag,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            min_length: None,\n            max_length: Some(256),\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"cool-new-feature\".to_string()),\n    }\n);\n\nstring_wrapper_impl!(FeatureFlag);\n\nimpl<'de> Deserialize<'de> for FeatureFlag {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).and_then(|s| {\n            validate_limited_str(&s).map_err(serde::de::Error::custom)?;\n            Ok(FeatureFlag(s))\n        })\n    }\n}\n\npub type FeatureFlagSet = HashSet<FeatureFlag>;\n\n#[cfg(test)]\nmod tests {\n    use std::collections::HashMap;\n\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use validator::Validate;\n\n    use super::{\n        validate_header_map, ApplicationId, ApplicationUid, EndpointHeaders, EndpointHeadersPatch,\n        EndpointSecret, EventChannel, EventTypeName,\n    };\n    use crate::core::cryptography::AsymmetricKey;\n\n    #[test]\n    fn test_id_validation() {\n        let app_id = ApplicationId(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_id.validate().unwrap();\n\n        let app_id = ApplicationId(\"badprefix_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_id.validate().is_err());\n\n        let app_uid = ApplicationUid(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_uid.validate().unwrap();\n\n        // With a space\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3 \".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        // Check all allowed\n        let app_uid = ApplicationUid(\"azAZ09-_.\".to_owned());\n        app_uid.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let app_id = ApplicationId(long_str.clone());\n        assert!(app_id.validate().is_err());\n        let app_uid = ApplicationUid(long_str);\n        assert!(app_uid.validate().is_err());\n\n        let empty_str: String = \"\".to_owned();\n        let app_id = ApplicationId(empty_str.clone());\n        assert!(app_id.validate().is_err());\n        let app_uid = ApplicationUid(empty_str);\n        assert!(app_uid.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_names_validation() {\n        // With a space\n        let evt_name = EventTypeName(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventTypeName(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let evt_name = EventTypeName(long_str);\n        assert!(evt_name.validate().is_err());\n\n        let empty_str = \"\".to_owned();\n        let evt_name = EventTypeName(empty_str);\n        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_channel_validation() {\n        // With a space\n        let evt_name = EventChannel(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventChannel(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        l<|fim_suffix|>        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), \"true\".to_owned()),\n            (\"also-valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        validate_header_map(&endpoint_headers.0).unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n            (\"also-valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        endpoint_headers.validate().unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n    }\n\n    #[test]\n    fn test_endpoint_secret_validation() {\n        let secret = EndpointSecret::Symmetric(STANDARD.decode(\"bm90LXZhbGlkCg==\").unwrap());\n        assert!(secret.validate().is_err());\n\n        let secret =\n            EndpointSecret::Symmetric(STANDARD.decode(\"C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap());\n        secret.validate().unwrap();\n\n        let secret = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n        secret.validate().unwrap();\n\n        let secret = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlaaaaaaaaaaAJ6p9lMicMFs6Kvg==\").unwrap());\n        assert!(secret.validate().is_err());\n    }\n\n    #[derive(serde::Deserialize)]\n    struct EndpointSecretTestStruct {\n        key: EndpointSecret,\n    }\n\n    #[test]\n    fn test_endpoint_secret_deserialization() {\n        for key in [\n            \"w\",\n            \"whsec_%\",\n            \"whsec_wronglength\",\n            \"whpk_1SiA4o9hyqTCpIqC5V9HUakiiaeACeqfZTInDBbOir4=\", // Public key\n            \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kv\", // Bad SK\n            \"hwsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\",\n        ] {\n            let js = serde_json::json!({ \"ke<|fim_middle|>", "completion": "let evt_name = EventChannel(long_str);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "let_declaration", "line_range": [1499, 1499]}
{"prompt": "<|fim_prefix|>mmetricKey;\n\n    #[test]\n    fn test_id_validation() {\n        let app_id = ApplicationId(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_id.validate().unwrap();\n\n        let app_id = ApplicationId(\"badprefix_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_id.validate().is_err());\n\n        let app_uid = ApplicationUid(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_uid.validate().unwrap();\n\n        // With a space\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3 \".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        // Check all allowed\n        let app_uid = ApplicationUid(\"azAZ09-_.\".to_owned());\n        app_uid.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let app_id = ApplicationId(long_str.clone());\n        assert!(app_id.validate().is_err());\n        let app_uid = ApplicationUid(long_str);\n        assert!(app_uid.validate().is_err());\n\n        let empty_str: String = \"\".to_owned();\n        let app_id = ApplicationId(empty_str.clone());\n        assert!(app_id.validate().is_err());\n        let app_uid = ApplicationUid(empty_str);\n        assert!(app_uid.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_names_validation() {\n        // With a space\n        let evt_name = EventTypeName(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventTypeName(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let evt_name = EventTypeName(long_str);\n        assert!(evt_name.validate().is_err());\n\n        let empty_str = \"\".to_owned();\n        let evt_name = EventTypeName(empty_str);\n        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_channel_validation() {\n        // With a space\n        let evt_name = EventChannel(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventChannel(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let evt_name = EventChannel(long_str);\n        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), \"true\".to_owned()),\n            (\"also-valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        validate_header_map(&endpoint_headers.0).unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n            (\"also-valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        endpoint_headers.validate().unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n    }\n\n    #[test]\n    fn test_endpoint_secret_validation() {\n        let secret = EndpointSecret::Symmetric(STANDARD.decode(\"bm90LXZhbGlkCg==\").unwrap());\n        assert!(secret.validate().is_err());\n\n        let secret =\n            EndpointSecret::Symmetric(STANDARD.decode(\"C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap());\n        secret.validate().unwrap();\n\n        let secret = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n        secret.validate().unwrap();\n\n        l<|fim_suffix|>        assert!(secret.validate().is_err());\n    }\n\n    #[derive(serde::Deserialize)]\n    struct EndpointSecretTestStruct {\n        key: EndpointSecret,\n    }\n\n    #[test]\n    fn test_endpoint_secret_deserialization() {\n        for key in [\n            \"w\",\n            \"whsec_%\",\n            \"whsec_wronglength\",\n            \"whpk_1SiA4o9hyqTCpIqC5V9HUakiiaeACeqfZTInDBbOir4=\", // Public key\n            \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kv\", // Bad SK\n            \"hwsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\",\n        ] {\n            let js = serde_json::json!({ \"key\": key });\n            assert!(serde_json::from_value::<EndpointSecretTestStruct>(js).is_err());\n        }\n\n        let js = serde_json::json!({ \"key\": \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\" });\n        let ep = serde_json::from_value::<EndpointSecretTestStruct>(js).unwrap();\n        if let EndpointSecret::Symmetric(key) = ep.key {\n            assert_eq!(\n                STANDARD.decode(\"C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap(),\n                key\n            );\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n\n        // Too long secret\n        let js = serde_json::json!({ \"key\": \"whsec_V09IYXZUaFJoSnFobnpJQkpPMXdpdGFNWnJsRzAxdXZCeTVndVpwRmxSSXFsc0oyYzBTRWRUekJhYnlaZ0JSRGNPQ3BGZG1xYjFVVmRGQ3UK\" });\n        let ep = serde_json::from_value::<EndpointSecretTestStruct>(js).unwrap();\n        assert!(ep.key.validate().is_err());\n\n        // Valid long secret\n        let long_sec = \"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\";\n        let js = serde_json::json!({ \"key\": format!(\"whsec_{long_sec}\") });\n        let ep = serde_json::from_value::<EndpointSecretTestStruct>(js).unwrap();\n        if let EndpointSecret::Symmetric(key) = ep.key {\n            assert_eq!(STANDARD.decode(long_sec).unwrap(), key);\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n\n        // Asymmetric key\n        let asym_sec = \"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\";\n        let js = serde_json::json!({ \"key\": format!(\"whsk_{asym_sec}\") });\n        let ep = serde_json::from_value::<EndpointSecretTestStruct>(js).unwrap();\n        if let EndpointSecret::Asymmetric(key) = ep.key {\n            assert_eq!(STANDARD.decode(asym_sec).unwrap(), key.0.sk.as_slice());\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n    }\n}\n<|fim_middle|>", "completion": "let secret = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlaaaaaaaaaaAJ6p9lMicMFs6Kvg==\").unwrap());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "let_declaration", "line_range": [1579, 1579]}
{"prompt": "<|fim_prefix|>$string_schema);\n\n        impl BaseId for $name_id {\n            const PREFIX: &'static str = $key_prefix;\n            type Output = Self;\n\n            fn new(dt: Option<DateTime<Utc>>, payload: Option<&[u8]>) -> Self::Output {\n                Self(Self::generate_(dt, payload))\n            }\n        }\n\n        impl Validate for $name_id {\n            fn validate(&self) -> Result<(), validator::ValidationErrors> {\n                self.validate_()\n            }\n        }\n\n        impl sea_orm::TryFromU64 for $name_id {\n            fn try_from_u64(_: u64) -> Result<Self, sea_orm::DbErr> {\n                Err(sea_orm::DbErr::Exec(sea_orm::error::RuntimeErr::Internal(\n                    format!(\"{} cannot be converted from u64\", stringify!($type)),\n                )))\n            }\n        }\n    };\n}\n\nmacro_rules! create_all_id_types {\n    ($name_id:ident, $name_uid:ident, $name_id_or_uid:ident, $key_prefix:literal) => {\n        // Id\n        create_id_type!(\n            $name_id,\n            $key_prefix,\n            $crate::core::types::StringSchema::schema_for_ids($key_prefix)\n        );\n\n        // Uid\n        string_wrapper!(\n            $name_uid,\n            $crate::core::types::StringSchema::schema_for_uids($key_prefix)\n        );\n\n        impl BaseUid for $name_uid {\n            const ID_PREFIX: &'static str = $key_prefix;\n        }\n\n        impl Validate for $name_uid {\n            fn validate(&self) -> Result<(), validator::ValidationErrors> {\n                self.validate_()\n            }\n        }\n\n        impl From<$name_uid> for $name_id_or_uid {\n            fn from(v: $name_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        // Id or uid\n        #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\n        pub struct $name_id_or_uid(pub String);\n\n        common_jsonschema_impl!(\n            $name_id_or_uid,\n            $crate::core::types::StringSchema::schema_for_uids($key_prefix)\n        );\n\n        impl From<$name_id_or_uid> for $name_uid {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        impl From<$name_id_or_uid> for $name_id {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        impl From<$name_id_or_uid> for sea_orm::Value {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self::String(Some(Box::new(v.0)))\n            }\n        }\n\n        impl Validate for $name_id_or_uid {\n            fn validate(&self) -> Result<(), validator::ValidationErrors> {\n                validate_limited_str(&self.0)\n            }\n        }\n    };\n}\n\ncreate_id_type!(OrganizationId, \"org_\");\ncreate_id_type!(\n    MessageAttemptId,\n    \"atmpt_\",\n    crate::core::types::StringSchema {\n        string_validation: None,\n        example: Some(\"atmpt_1srOrx2ZWZBpBUvZwXKQmoEYga2\".to_string()),\n    }\n);\ncreate_id_type!(MessageEndpointId, \"msgep_\");\ncreate_id_type!(EventTypeId, \"evtype_\");\ncreate_id_type!(QueueBackgroundTaskId, \"qtask_\");\n\ncreate_all_id_types!(ApplicationId, ApplicationUid, ApplicationIdOrUid, \"app_\");\ncreate_all_id_types!(EndpointId, EndpointUid, EndpointIdOrUid, \"ep_\");\ncreate_all_id_types!(MessageId, MessageUid, MessageIdOrUid, \"msg_\");\n\nstring_wrapper!(\n    EventTypeName,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            max_length: Some(256),\n            min_length: None,\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"user.signup\".to_string()),\n    }\n);\n\nimpl Validate for EventTypeName {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        validate_limited_str(&self.0)\n    }\n}\n\nstring_wrapper!(\n    EventChannel,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            max_length: Some(128),\n            min_length: None,\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"project_1337\".to_string()),\n    }\n);\n\ni<|fim_suffix|>\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventChannelSet(pub HashSet<EventChannel>);\njson_wrapper!(EventChannelSet);\n\nimpl Validate for EventChannelSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventTypeNameSet(pub HashSet<EventTypeName>);\njson_wrapper!(EventTypeNameSet);\n\nimpl Validate for EventTypeNameSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\npub struct ExpiringSigningKeys(pub Vec<ExpiringSigningKey>);\njson_wrapper!(ExpiringSigningKeys);\n\nimpl ExpiringSigningKeys {\n    pub const MAX_OLD_KEYS: usize = 10;\n    pub const OLD_KEY_EXPIRY_HOURS: i64 = 24;\n}\n\n/// The type of encryption key\n#[repr(u8)]\n#[derive(Clone, Debug, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum EndpointSecretType {\n    Hmac256 = 1,\n    Ed25519 = 2,\n    // Reserved = 3,\n}\n\nimpl EndpointSecretType {\n    pub const fn secret_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whsk_\",\n        }\n    }\n\n    pub const fn public_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whpk_\",\n        }\n    }\n}\n\n/// Properties of the encryption key\n#[derive(Clone, Debug, PartialEq, Eq)]\nstruct EndpointSecretMarker {\n    type_: EndpointSecretType,\n    encrypted: bool,\n}\n\nimpl EndpointSecretMarker {\n    const ENCRYPTED_FLAG: u8 = 0b1000_0000;\n\n    fn from_u8(v: u8) -> crate::error::Result<Self> {\n        let encrypted = (v & Self::ENCRYPTED_FLAG) != 0;\n        let v = v & !Self::ENCRYPTED_FLAG;\n        let type_ = EndpointSecretType::try_from(v)\n            .map_err(|_| crate::error::Error::generic(\"Invalid marker value\"))?;\n\n        Ok(Self { type_, encrypted })\n    }\n\n    fn to_u8(&self) -> u8 {\n        let mut ret = self.type_.clone().into();\n        if self.encrypted {\n            ret |= Self::ENCRYPTED_FLAG;\n        }\n        ret\n    }\n\n    fn type_(&self) -> &EndpointSecretType {\n        &self.type_\n    }\n}\n\n/// The internal representation of the endpoint secret.\n/// This is used to store it securely in the database and cache, and to ensure it doesn't get\n/// sent externally.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub struct EndpointSecretInternal {\n    marker: EndpointSecretMarker,\n\n    key: Vec<u8>,\n}\n\nimpl EndpointSecretInternal {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MINUS_ONE: usize = Self::KEY_SIZE - 1;\n\n    fn new(\n        encryption: &Encryption,\n        type_: EndpointSecretType,\n        key: &[u8],\n    ) -> crate::error::Result<Self> {\n        Ok(Self {\n            marker: EndpointSecretMarker {\n                type_,\n                encrypted: encryption.enabled(),\n            },\n            key: encryption.encrypt(key)?,\n        })\n    }\n\n    pub fn generate_symmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let buf: [u8; Self::KEY_SIZE] = rand::thread_rng().gen();\n        Self::new(encryption, EndpointSecretType::Hmac256, &buf)\n    }\n\n    pub fn generate_asymmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let key = AsymmetricKey::generate();\n        Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())\n    }\n\n    fn into_vec(mut self) -> Vec<u8> {\n        let marker: u8 = self.marker.to_u8();\n\n        let mut vec = vec![marker];\n        vec.append(<|fim_middle|>", "completion": "impl Validate for EventChannel {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        validate_limited_str(&self.0)\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "impl_item", "line_range": [602, 606]}
{"prompt": "<|fim_prefix|>tput, into_sender_input};\nuse self::error::Error;\n\n/// Newtype for [`omniqueue::queue::Delivery`].\n///\n/// Mostly vestigial at this point, though it doesn't hurt to have something to act as a facade to\n/// group helper functions for handling payload details.\npub struct DeliveryWrapper(Delivery);\n\nimpl From<Delivery> for DeliveryWrapper {\n    fn from(value: Delivery) -> Self {\n        Self(value)\n    }\n}\n\nimpl DeliveryWrapper {\n    /// Delegates to the inner delivery types ack method.\n    async fn ack(self) -> Result<(), QueueError> {\n        self.0.ack().await.map_err(|(e, _)| e)\n    }\n    /// Delegates to the inner delivery types nack method.\n    async fn nack(self) -> Result<(), QueueError> {\n        self.0.nack().await.map_err(|(e, _)| e)\n    }\n\n    /// Decodes the inner delivery as String.\n    fn raw_payload(&self) -> Result<&str, QueueError> {\n        // TODO: used to be unsupported for redis. Is it now? Check for skipped tests to prove it.\n        let bytes = self.0.borrow_payload().ok_or(QueueError::NoData)?;\n        std::str::from_utf8(bytes).map_err(QueueError::generic)\n    }\n\n    /// Decodes the inner delivery as `serde_json::Value`.\n    fn payload(&self) -> Result<serde_json::Value, QueueError> {\n        self.0.payload_serde_json()?.ok_or(QueueError::NoData)\n    }\n}\n\n#[async_trait]\ntrait Consumer {\n    /// The source of the stream of messages, e.g. the name or id for the queue, subscription, etc.\n    fn source(&self) -> &str;\n    /// The name of the messaging system, e.g. rabbitmq, sqs, etc.\n    fn system(&self) -> &str;\n    /// Gets the channel sender for running transformations.\n    fn transformer_tx(&self) -> Option<&TransformerTx>;\n    /// The js source for the transformation to run on each payload.\n    fn transformation(&self) -> Option<&TransformationConfig>;\n    /// The client to use when creating messages in svix.\n    fn svix_client(&self) -> &Svix;\n\n    async fn transform(\n        &self,\n        script: String,\n        input: TransformerInput,\n    ) -> std::io::Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx()\n            .as_ref()\n            .expect(\"transformations not configured\")\n            .send(job)\n            .map_err(|e| Error::Generic(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::Generic(\"transformation rx failed\".to_string()))\n            .and_then(|x| {\n                x.map_err(|_e| Error::Generic(\"transformation execution failed\".to_string()))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => {\n                Err(Error::Generic(\"transformation produced unexpected value\".to_string()).into())\n            }\n        }\n    }\n\n    /// Gets consumer (likely based on a config value), called by [`consume`].\n    async fn consumer(&self) -> std::io::Result<DynConsumer>;\n\n    /// Main consumer loop\n    async fn consume(&self) -> std::io::Result<()> {\n        let mut consumer = self.consumer().await?;\n        tracing::debug!(\"{} consuming: {}\", self.system(), self.source(),);\n        loop {\n            self.receive(&mut consumer).await?;\n        }\n    }\n\n    /// Pulls N messages off the queue and feeds them to [`Self::process`].\n    #[tracing::instrument(skip_all,\n    fields(\n        otel.kind = \"CONSUMER\",\n        messaging.system = self.system(),\n        messaging.operation = \"receive\",\n        messaging.source = self.source(),\n        svix_bridge_plugin.name = crate::PLUGIN_NAME,\n        svix_bridge_plugin.vers = crate::PLUGIN_VERS,\n    )\n    )]\n    async fn receive(&self, consumer: &mut DynConsumer) -> std::io::Result<()> {\n        // FIXME: omniqueue has a fixed batch size of 1 afaict. Would be nicer to pull N at a time.\n        let delivery = consumer.receive().await.map_err(Error::from)?;\n        self.process(delivery.into()).await?;\n        Ok(())\n    }\n\n    /// Parses the delivery as JSON and feeds it into [`create_svix_message`].\n    /// Will nack the delivery if either the JSON parse, transformation, or the request to svix fails.\n    #[tracing::instrument(skip_all, fields(messaging.operation = \"process\"))]\n    async fn process(&self, delivery: DeliveryWrapper) -> std::io::Result<()> {\n        let payload = if let Some(xform_cfg) = self.transformation() {\n            let input = match xform_cfg.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload = match delivery.payload() {\n                        Ok(p) => p,\n                        Err(e) => {\n                            tracing::warn!(\"{e}\");\n                            delivery.nack().await.map_err(Error::from)?;\n                            return Ok(());\n                        }\n                    };\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    // N.b. our redis backend doesn't support string payloads, but higher up in the\n                    // call stack, during the plugin construction, we should be catching this and\n                    // giving an error about bad config.\n                    // If we get here somehow with a redis delivery, this call will panic.\n                    <|fim_suffix|>\n                    // FIXME: if we add a lifetime to `TransformerInput` we might avoid this allocation.\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n            let script = xform_cfg.source().clone();\n            match self.transform(script, input).await {\n                Err(e) => {\n                    tracing::error!(\"nack: {e}\");\n                    delivery.nack().await.map_err(Error::from)?;\n                    return Ok(());\n                }\n                Ok(x) => serde_json::from_value(serde_json::Value::Object(x))?,\n            }\n        } else {\n            // Parse as JSON when not using a transformation because Create Message requires JSON.\n            // If this fails, the config needs to change.\n            let json_payload = match delivery.payload() {\n                Ok(p) => p,\n                Err(e) => {\n                    tracing::warn!(\"{e}\");\n                    delivery.nack().await.map_err(Error::from)?;\n                    return Ok(());\n                }\n            };\n            serde_json::from_value(json_payload)?\n        };\n\n        match create_svix_message(self.svix_client(), payload).await {\n            Ok(_) => {\n                tracing::trace!(\"ack\");\n                delivery.ack().await.map_err(Error::from)?\n            }\n            Err(e) => {\n                tracing::error!(\"nack: {e}\");\n                delivery.nack().await.map_err(Error::from)?\n            }\n        }\n        Ok(())\n    }\n}\n\nasync fn run_inner(consumer: &(impl Consumer + Send + Sync)) -> ! {\n    let mut fails: u64 = 0;\n    let mut last_fail = Instant::now();\n    let system_name = consumer.system();\n    let source = consumer.source();\n\n    tracing::info!(\"{system_name} starting: {source}\");\n\n    loop {\n        if let Err(e) = consumer.consume().await {\n            tracing::error!(\"{e}\");\n        }\n\n        tracing::error!(\"{system_name} disconnected: {source}\");\n\n        if last_fail.elapsed() > Duration::from_secs(10) {\n            // reset the fail count if we didn't have a hiccup in the past short while.\n            tracing::trace!(\"been a while since last fail, resetting count\");\n            fails = 0;\n        } else {\n            fails += 1;\n        }\n\n        last_fail = Instant::now();\n        tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n    }\n}\n\n#[tracing::instrument(skip_all, level = \"error\", fields(\n    app_id,\n    event_type = message.event_type\n))]\nasync fn create_svix_message(\n    svix: &Svix,\n    CreateMessageRequest { app_id, message }: CreateMessageRequest,\n) -> std::io::Result<()> {\n    svix.message()\n        .create(\n            app_id, message,\n            // FIXME: add a way for the caller to give an idempotency key like we have in kafka\n            None,\n        )\n        .await\n        .map_err(Error::from)?;\n    Ok(())\n}\n<|fim_middle|>", "completion": "let raw_payload = match delivery.raw_payload() {\n                        Ok(p) => p,\n                        Err(e) => {\n                            tracing::warn!(\"{e}\");\n                            delivery.nack().await.map_err(Error::from)?;\n                            return Ok(());\n                        }\n                    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/src/lib.rs", "node_type": "let_declaration", "line_range": [151, 158]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse aide::axum::{\n    routing::{get_with, post_with},\n    ApiRouter,\n};\nuse axum::{\n    extract::{Path, State},\n    Json,\n};\nuse chrono::{DateTime, Utc};\nuse futures::FutureExt;\nuse schemars::JsonSchema;\nuse sea_orm::{ActiveModelTrait, ActiveValue::Set, DatabaseConnection, TransactionTrait};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::{aide_annotate, ModelOut};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::{\n        permissions,\n        types::{metadata::Metadata, ApplicationId, ApplicationUid, OrganizationId},\n    },\n    db::models::{application, applicationmetadata},\n    error::{http_error_on_conflict, HttpError, Result, Traceable},\n    v1::utils::{\n        apply_pagination, openapi_tag,\n        patch::{\n            patch_field_non_nullable, patch_field_nullable, UnrequiredField,\n            UnrequiredNullableField,\n        },\n        validate_no_control_characters, validate_no_control_characters_unrequired,\n        validation_error, ApplicationPath, IteratorDirection, JsonStatusUpsert, ListResponse,\n        ModelIn, ModelOut, NoContent, Ordering, Pagination, PaginationLimit, ReversibleIterator,\n        ValidatedJson, ValidatedQuery,\n    },\n    AppState,\n};\n\nfn application_name_example() -> &'static str {\n    \"My first application\"\n}\n\n#[derive(Clone, Debug, Default, PartialEq, Eq, Serialize, Deserialize, Validate, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct ApplicationIn {\n    #[validate(\n        length(min = 1, message = \"Application names must be at least one character\"),\n        custom = \"validate_no_control_characters\"\n    )]\n    #[schemars(example = \"application_name_example\")]\n    pub name: String,\n\n    #[validate(range(min = 1, message = \"Application rate limits must be at least 1 if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n    /// Optional unique identifier for the application\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<ApplicationUid>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for ApplicationIn {\n    type ActiveModel = (application::ActiveModel, applicationmetadata::ActiveModel);\n\n    fn update_model(self, (app, app_metadata): &mut Self::ActiveModel) {\n        let ApplicationIn {\n            name,\n            rate_limit,\n            uid,\n            metadata,\n        } = self;\n\n        app.name = Set(name);\n        app.rate_limit = Set(rate_limit.map(|x| x.into()));\n        app.uid = Set(uid);\n        app_metadata.data = Set(metadata);\n    }\n}\n\n#[derive(Deserialize, Serialize, Validate, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct ApplicationPatch {\n    #[serde(default, skip_serializing_if = \"UnrequiredField::is_absent\")]\n    #[validate(\n        custom = \"validate_name_length_patch\",\n        custom = \"validate_no_control_characters_unrequired\"\n    )]\n    pub name: UnrequiredField<String>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredNullableField::is_absent\")]\n    #[validate(custom = \"validate_rate_limit_patch\")]\n    pub rate_limit: UnrequiredNullableField<u16>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredNullableField::is_absent\")]\n    #[validate]\n    pub uid: UnrequiredNullableField<ApplicationUid>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredField::is_absent\")]\n    pub metadata: UnrequiredField<Metadata>,\n}\n\nimpl ModelIn for ApplicationPatch {\n    type ActiveModel = (application::ActiveModel, applicationmetadata::ActiveModel);\n\n    fn update_model(self, (app, app_metadata): &mut Self::ActiveModel) {\n        let ApplicationPatch {\n            name,\n            rate_limit,\n            uid,\n            metadata,\n        } = self;\n\n        // `model`'s version of `rate_limit` is an i32, while `self`'s is a u16.\n        let rate_limit_map = |x: u16| -> i32 { x.into() };\n        l<|fim_suffix|>\n        patch_field_non_nullable!(app, name);\n        patch_field_nullable!(app, rate_limit, rate_limit_map);\n        patch_field_nullable!(app, uid);\n        patch_field_non_nullable!(app_metadata, data);\n    }\n}\n\nfn validate_name_length_patch(name: &UnrequiredField<String>) -> Result<(), ValidationError> {\n    match name {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(s) => {\n            if s.is_empty() {\n                Err(validation_error(\n                    Some(\"length\"),\n                    Some(\"Application names must be at least one character\"),\n                ))\n            } else {\n                Ok(())\n            }\n        }\n    }\n}\n\nfn validate_rate_limit_patch(\n    rate_limit: &UnrequiredNullableField<u16>,\n) -> Result<(), ValidationError> {\n    match rate_limit {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(rate_limit) => {\n            if *rate_limit > 0 {\n                Ok(())\n            } else {\n                Err(validation_error(\n                    Some(\"range\"),\n                    Some(\"Application rate limits must be at least 1 if set\"),\n                ))\n            }\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct ApplicationOut {\n    // FIXME: Do we want to use serde(flatten) or just duplicate the keys?\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<ApplicationUid>,\n    #[schemars(example = \"application_name_example\")]\n    pub name: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    pub id: ApplicationId,\n    pub created_at: DateTime<Utc>,\n    pub updated_at: DateTime<Utc>,\n    pub metadata: Metadata,\n}\n\nimpl From<(application::Model, applicationmetadata::Model)> for ApplicationOut {\n    fn from((app, metadata): (application::Model, applicationmetadata::Model)) -> Self {\n        Self {\n            uid: app.uid,\n            name: app.name,\n            rate_limit: app.rate_limit.map(|x| x as u16),\n            id: app.id,\n            created_at: app.created_at.into(),\n            updated_at: app.updated_at.into(),\n            metadata: metadata.metadata(),\n        }\n    }\n}\n\n/// List of all the organization's applications.\n#[aide_annotate(op_id = \"v1.application.list\")]\nasync fn list_applications(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<Pagination<ReversibleIterator<ApplicationId>>>,\n    permissions::Organization { org_id }: permissions::Organization,\n) -> Result<Json<ListResponse<ApplicationOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    let iterator = pagination.iterator;\n    let iter_direction = iterator\n        .as_ref()\n        .map_or(IteratorDirection::Normal, |iter| iter.direction());\n\n    let query = apply_pagination(\n        application::Entity::secure_find(org_id),\n        application::Column::Id,\n        limit,\n        iterator,\n        pagination.order.unwrap_or(Ordering::Ascending),\n    );\n\n    let results: Vec<ApplicationOut> = query\n        .find_also_related(applicationmetadata::Entity)\n        .all(db)\n        .await?\n        .into_iter()\n        .map(|(app, metadata)| {\n            let metadata =\n                metadata.unwrap_or_else(|| applicationmetadata::Model::new(app.id.clone()));\n            (app, metadata)\n        })\n        .map(ApplicationOut::from)\n        .collect();\n\n    Ok(Json(ApplicationOut::list_response(\n        results,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\nfn default_as_false() -> bool {\n    false\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct CreateApplicationQueryParams {\n    /// Get an existing application, or create a new one if doesn't exist. It's two separate functions in the libs.\n    #[serde(default = \"default_as_false\")]\n    get_if_exists: bool,\n}\n\n/// Create a new application.\n#[aide_annotate(op_id = \"v1.application.create\")]\nasync fn create_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    query: ValidatedQuery<Crea<|fim_middle|>", "completion": "let data = metadata;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/application.rs", "node_type": "let_declaration", "line_range": [121, 121]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::fmt::Debug;\n\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chacha20poly1305::{\n    aead::{Aead, KeyInit},\n    Key, XChaCha20Poly1305, XNonce,\n};\nuse ed25519_compact::*;\nuse rand::Rng;\n\nuse crate::error::Result;\n\n// Asymmetric Signature keys\n#[derive(Clone, Eq)]\npub struct AsymmetricKey(pub KeyPair);\n\nimpl AsymmetricKey {\n    pub fn generate() -> AsymmetricKey {\n        AsymmetricKey(KeyPair::from_seed(Seed::generate()))\n    }\n\n    pub fn from_slice(bytes: &[u8]) -> Result<Self> {\n        Ok(AsymmetricKey(KeyPair::from_slice(bytes).map_err(|_| {\n            crate::error::Error::generic(\"Failed parsing key.\")\n        })?))\n    }\n\n    pub fn from_base64(b64: &str) -> Result<Self> {\n        let bytes = STANDARD\n            .decode(b64)\n            .map_err(|_| crate::error::Error::generic(\"Failed parsing base64\"))?;\n\n        Self::from_slice(bytes.as_slice())\n    }\n\n    pub fn pubkey(&self) -> &[u8] {\n        &self.0.pk[..]\n    }\n}\n\nimpl Debug for AsymmetricKey {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"<AsymmetricKey sk=*** pk={}>\",\n            STANDARD.encode(self.0.pk.as_slice())\n        )\n    }\n}\n\nimpl PartialEq for AsymmetricKey {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.as_slice() == other.0.as_slice()\n    }\n}\n\n#[derive(Clone, Debug)]\npub struct Encryption(Option<Key>);\n\nimpl Encryption {\n    const NONCE_SIZE: usize = 24;\n\n    pub fn new_noop() -> Self {\n        Self(None)\n    }\n\n    pub fn new(key: [u8; 32]) -> Self {\n        Self(Some(Key::from_slice(&key).to_owned()))\n    }\n\n    pub fn encrypt(&self, data: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce: [u8; Self::NONCE_SIZE] = rand::thread_rng().gen();\n            let nonce = XNonce::from_slice(&nonce);\n            let mut ciphertext = cipher\n                .encrypt(nonce, data)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))?;\n            let mut ret = nonce.to_vec();\n            ret.append(&mut ciphertext);\n            Ok(ret)\n        } else {\n            Ok(data.to_vec())\n        }\n    }\n\n    pub fn decrypt(&self, ciphertext: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce = &ciphertext[..Self::NONCE_SIZE];\n            let ciphertext = &ciphertext[Self::NONCE_SIZE..];\n            cipher\n                .decrypt(XNonce::from_slice(nonce), ciphertext)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))\n        } else {\n            Ok(ciphertext.to_vec())\n        }\n    }\n\n    pub fn enabled(&self) -> bool {\n        self.0.is_some()\n    }\n}\n\nimpl Default for Encryption {\n    fn default() -> Self {\n        Self::new_noop()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::Encryption;\n\n    #[test]\n    fn test_encryption() {\n        let clear = b\"hello world\";\n        let encryption = Encryption::new([1; 32]);\n        l<|fim_suffix|>        let clear2 = encryption.decrypt(&ciphertext).unwrap();\n        assert_eq!(&clear[..], &clear2[..]);\n    }\n}\n<|fim_middle|>", "completion": "let ciphertext = encryption.encrypt(clear).unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cryptography.rs", "node_type": "let_declaration", "line_range": [122, 122]}
{"prompt": "<|fim_prefix|>// Dead code is allowed because not everything is used in all of the tests\n#![allow(dead_code)]\n\nuse std::{\n    future::Future,\n    net::TcpListener,\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse anyhow::{Context, Result};\nuse axum::response::IntoResponse;\nuse http::HeaderMap;\nuse reqwest::{Client, RequestBuilder, StatusCode};\nuse serde::{de::DeserializeOwned, Serialize};\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::ConfigurationInner,\n    core::{\n        security::generate_org_token,\n        types::{BaseId, OrganizationId},\n    },\n    setup_tracing,\n};\nuse tokio::sync::mpsc;\nuse tracing::instrument::WithSubscriber;\n\npub mod common_calls;\n\n#[derive(Clone)]\npub struct TestClient {\n    base_uri: String,\n    auth_header: String,\n    client: Client,\n}\n\nimpl TestClient {\n    pub fn set_auth_header(&mut self, auth_header: String) {\n        self.auth_header = format!(\"Bearer {auth_header}\");\n    }\n}\n\nimpl TestClient {\n    pub fn new(base_uri: String, auth_token: &str) -> TestClient {\n        TestClient {\n            base_uri,\n            auth_header: format!(\"Bearer {auth_token}\"),\n            client: Client::new(),\n        }\n    }\n\n    <|fim_suffix|>\n\n    fn add_headers(&self, request: RequestBuilder) -> RequestBuilder {\n        request.header(\"Authorization\", &self.auth_header)\n    }\n\n    pub async fn get<O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.get(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn get_without_response(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.get(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn post<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                resp.json()\n                    .await\n                    .context(\"error receiving/parsing response\")\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                Ok(())\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_with_idempotency<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        idempotency_key: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self\n            .add_headers(req)\n            .header(\"idempotency-key\", idempotency_key)\n            .json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json().await.context(\"error receiving/paring response\")\n    }\n\n    pub async fn put<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn put_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn delete(&self, endpoint: &str, expected_code: StatusCode) -> Result<()> {\n        let mut req = self.client.delete(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        if expected_code == StatusCode::NO_CONTENT {\n            let res_body = resp.text().await.context(\"error receiving response\")?;\n            anyhow::ensure!(res_body.is_empty());\n        }\n\n        Ok(())\n    }\n\n    pub async fn patch<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn patch_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_<|fim_middle|>", "completion": "fn build_uri(&self, endpoint: &str) -> String {\n        format!(\"{}/{endpoint}\", self.base_uri)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/mod.rs", "node_type": "function_item", "line_range": [52, 54]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\nuse reqwest::StatusCode;\nuse serde::de::IgnoredAny;\nuse svix_server::{\n    cfg::CacheType,\n    core::{\n        security::generate_org_token,\n        types::{ApplicationUid, BaseId, OrganizationId},\n    },\n    v1::endpoints::application::{ApplicationIn, ApplicationOut},\n};\n\nuse crate::utils::{\n    common_calls::{application_in, common_test_list, metadata},\n    get_default_test_config, start_svix_server,\n};\n\n// NOTE: PATCHing must be tested exhaustively as if any of the boilerplate is missed then the\n// operation could fail. This should probably be made into a macro if at all possible.\n#[tokio::test]\nasync fn test_patch() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(\"first_name\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that PUT with an invalid ID creates an application\n    let _: ApplicationOut = client\n        .put(\n            \"api/v1/app/fake-id/\",\n            application_in(\"first_name\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that name may be set while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json! ({\n                \"name\": \"second_name\"\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert change was made when later fetched\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.name, \"second_name\".to_owned());\n    // Assert that no other field was changed\n    assert_eq!(out.rate_limit, None);\n    assert_eq!(out.uid, None);\n    assert_eq!(out.metadata, metadata(\"{}\"));\n\n    // Test that rate_limit may be set while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json! ({\n                \"rateLimit\": 1,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.rate_limit, Some(1));\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.uid, None);\n\n    // Test that rate_limit may be unset while the rest are omitted\n    l<|fim_suffix|>\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.rate_limit, None);\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.uid, None);\n\n    // Test that uid may be set while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json! ({\n                \"uid\": \"test_uid\"\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.uid, Some(ApplicationUid(\"test_uid\".to_owned())));\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.rate_limit, None);\n\n    // Test that uid may be unset while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({ \"uid\": null }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.uid, None);\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.rate_limit, None);\n\n    // Test that metadata may be changed while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({\n                \"metadata\": {\n                    \"foo\": \"bar\",\n                    \"bizz\": \"baz\",\n                },\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#), out.metadata);\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.rate_limit, None);\n}\n\n#[tokio::test]\nasync fn test_crud() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1_1: &str = \"v1ApplicationCrudTest11\";\n    const APP_NAME_1_2: &str = \"v1ApplicationCrudTest12\";\n    const APP_NAME_2_1: &str = \"v1ApplicationCrudTest21\";\n    const APP_NAME_2_2: &str = \"v1ApplicationCrudTest22\";\n\n    // CREATE\n    let app_1: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(APP_NAME_1_1),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_1.name, APP_NAME_1_1);\n\n    let app_2: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(APP_NAME_2_1),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_2.name, APP_NAME_2_1);\n\n    // READ\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::OK)\n            .await\n            .unwrap(),\n        app_1\n    );\n\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_2\n    );\n\n    // UPDATE\n    let app_1_id = app_1.id;\n    let app_1: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{app_1_id}/\"),\n            application_in(APP_NAME_1_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    let app_2_id = app_2.id;\n    let app_2: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{app_2_id}/\"),\n            application_in(APP_NAME_2_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // CONFIRM UPDATE\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{app_1_id}/\"), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_1\n    );\n\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{app_2_id}/\"), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_2\n    );\n\n    // DELETE\n    client\n        .delete(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n    client\n        .delete(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n\n    // CONFIRM DELETION\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n\n    let app: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            serde_json::json!({\n                \"name\": \"Apps all around\",\n            }),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(app.metadata, metadata(r#\"{}\"#));\n\n    let updated: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),<|fim_middle|>", "completion": "let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({ \"rateLimit\": null }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_application.rs", "node_type": "let_declaration", "line_range": [90, 97]}
{"prompt": "<|fim_prefix|>//! Use the `testing-docker-compose.yml` in the repo root to run the dependencies for testing,\n//! including the gcloud pubsub emulator.\n//!\n//! Use `run-tests.sh` to use the requisite environment for testing.\n\nuse std::time::Duration;\n\nuse gcloud_googleapis::pubsub::v1::{DeadLetterPolicy, PubsubMessage};\nuse gcloud_pubsub::{\n    client::{Client, ClientConfig},\n    subscription::{Subscription, SubscriptionConfig},\n    topic::Topic,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_queue::{\n    config::{GcpPubSubInputOpts, QueueInputOpts},\n    sender_input::QueueSender,\n};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nconst DEFAULT_PUBSUB_EMULATOR_HOST: &str = \"localhost:8085\";\n\nfn get_test_plugin(\n    svix_url: String,\n    subscription_id: String,\n    use_transformation: Option<TransformerInputFormat>,\n) -> QueueSender {\n    QueueSender::new(\n        \"test\".into(),\n        QueueInputOpts::GcpPubSub(GcpPubSubInputOpts {\n            subscription_id,\n            credentials_file: None,\n        }),\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n}\n\nasync fn mq_connection() -> Client {\n    // The `Default` impl for `ClientConfig` looks for this env var. When set it branches for\n    // local-mode use using the addr in the env var and a hardcoded project id of `local-project`.\n    if std::env::var(\"PUBSUB_EMULATOR_HOST\").is_err() {\n        std::env::set_var(\"PUBSUB_EMULATOR_HOST\", DEFAULT_PUBSUB_EMULATOR_HOST);\n    }\n    Client::new(ClientConfig::default()).await.unwrap()\n}\n\nfn random_chars() -> impl Iterator<Item = char> {\n    std::iter::repeat_with(fastrand::alphanumeric)\n}\n\nasync fn create_test_queue(client: &Client) -> (Topic, Subscription) {\n    let topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();\n    // Need to define a dead letter topic to avoid the \"bad\" test cases from pulling the nacked\n    // messages again and again.\n    <|fim_suffix|>\n    let subscription_name: String = \"subscription-\"\n        .chars()\n        .chain(random_chars().take(8))\n        .collect();\n\n    let topic = client.create_topic(&topic_name, None, None).await.unwrap();\n    let dead_letter_topic = client\n        .create_topic(&dead_letter_topic_name, None, None)\n        .await\n        .unwrap();\n    let subscription = client\n        .create_subscription(\n            &subscription_name,\n            &topic_name,\n            SubscriptionConfig {\n                // Messages published to the topic need to supply a unique ID to make use of this\n                enable_exactly_once_delivery: true,\n                dead_letter_policy: Some(DeadLetterPolicy {\n                    dead_letter_topic: dead_letter_topic.fully_qualified_name().into(),\n                    max_delivery_attempts: MAX_DELIVERY_ATTEMPTS,\n                }),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();\n\n    (topic, subscription)\n}\n\nasync fn publish(topic: &Topic, payload: &str) {\n    let publisher = topic.new_publisher(None);\n    let awaiter = publisher\n        .publish(PubsubMessage {\n            data: payload.to_owned().into_bytes(),\n            message_id: random_chars().take(6).collect(),\n            ..Default::default()\n        })\n        .await;\n    awaiter.get().await.unwrap();\n}\n\n/// General \"pause while we wait for messages to travel\" beat. If you're seeing flakes, bump this up.\nconst WAIT_MS: u64 = 100;\n/// Controls how many times a message can be nack'd before it lands on the dead letter topic.\nconst MAX_DELIVERY_ATTEMPTS: i32 = 5;\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), subscription.id(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&topic, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        // The transformed bit of the payload\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        subscription.id(),\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duratio<|fim_middle|>", "completion": "let dead_letter_topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/gcp_pubsub_consumer.rs", "node_type": "let_declaration", "line_range": [73, 73]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{string::FromUtf8Error, time::Duration};\n\nuse ::redis::RedisError;\nuse axum::async_trait;\nuse enum_dispatch::enum_dispatch;\nuse serde::{de::DeserializeOwned, Serialize};\n\nuse crate::core::retry::run_with_retries;\n\npub mod memory;\npub mod none;\npub mod redis;\n\n/// Errors internal to the cache\n#[derive(thiserror::Error, Debug)]\npub enum Error {\n    #[error(\"error deserializing Redis value\")]\n    Deserialization(#[from] serde_json::error::Error),\n\n    #[error(\"error deserializing Redis value\")]\n    DeserializationOther,\n\n    #[error(\"error deserializing byte array\")]\n    DeserializationBytes(#[from] FromUtf8Error),\n\n    #[error(\"Redis pool error: {0}\")]\n    Pool(#[from] bb8::RunError<RedisError>),\n\n    #[error(\"Redis database error: {0}\")]\n    Database(#[from] RedisError),\n\n    #[error(\"input error: {0}\")]\n    Input(String),\n}\ntype Result<T> = std::result::Result<T, Error>;\n\n/// A valid key value for the cache -- usually just a wrapper around a [`String`]\npub trait CacheKey: AsRef<str> + Send + Sync {}\n\n/// A cache key for setting/getting raw [`String`]s -- this is just a marker\n/// trait added in the `string_kv_def macro`\npub trait StringCacheKey: AsRef<str> + Send + Sync {}\n\n/// Any (de)serializable structure usable as a value in the cache -- it is associated with a\n/// given key type to ensure type checking on creation or reading of values from the cache\npub trait CacheValue: DeserializeOwned + Serialize + Send + Sync {\n    type Key: CacheKey;\n}\n\npub trait StringCacheValue: ToString + TryFrom<String> + Send + Sync {\n    type Key: CacheKey;\n}\n\n/// An inner macro which defines everything common to the below macro. Not really meant to be used,\n/// but it can't be made private or else it couldn't be used in the outer macro.\nmacro_rules! kv_def_inner {\n    ($key_id:ident, $val_struct:ident) => {\n        #[derive(Clone, Debug)]\n        pub struct $key_id(String);\n\n        impl AsRef<str> for $key_id {\n            fn as_ref(&self) -> &str {\n                &self.0\n            }\n        }\n\n        impl CacheValue for $val_struct {\n            type Key = $key_id;\n        }\n    };\n}\npub(crate) use kv_def_inner;\n\n/// A macro that creates a [`CacheKey`] and ties it to any value that implements\n/// [`DeserializeOwned`] and [`Serialize`]\nmacro_rules! kv_def {\n    ($key_id:ident, $val_struct:ident) => {\n        crate::core::cache::kv_def_inner!($key_id, $val_struct);\n\n        impl CacheKey for $key_id {}\n    };\n}\npub(crate) use kv_def;\n\n/// An inner macro which defines everything common to the below macro. Not really meant to be used,\n/// but it can't be made private or else it couldn't be used in the outer macro.\n#[allow(unused_macros)]\nmacro_rules! string_kv_def_inner {\n    ($key_id:ident) => {\n        #[derive(Clone, Debug)]\n        pub struct $key_id(String);\n\n        impl AsRef<str> for $key_id {\n            fn as_ref(&self) -> &str {\n                &self.0\n            }\n        }\n    };\n}\n#[allow(unused_imports)]\np<|fim_suffix|>\n#[cfg(test)]\nmacro_rules! string_kv_def {\n    ($key_id:ident) => {\n        crate::core::cache::string_kv_def_inner!($key_id);\n\n        impl crate::core::cache::StringCacheKey for $key_id {}\n        // so key can work w/ other methods, like delete:\n        impl crate::core::cache::CacheKey for $key_id {}\n    };\n}\n#[cfg(test)]\npub(crate) use string_kv_def;\n\n#[derive(Clone)]\n#[enum_dispatch]\npub enum Cache {\n    Memory(memory::MemoryCache),\n    Redis(redis::RedisCache),\n    None(none::NoCache),\n}\n\nimpl Cache {\n    pub fn is_none(&self) -> bool {\n        matches!(*self, Cache::None(none::NoCache))\n    }\n}\n\nconst RETRY_SCHEDULE: &[Duration] = &[\n    Duration::from_millis(10),\n    Duration::from_millis(20),\n    Duration::from_millis(40),\n];\n\n#[async_trait]\n#[enum_dispatch(Cache)]\npub trait CacheBehavior: Sync + Send {\n    fn should_retry(&self, e: &Error) -> bool;\n    async fn get<T: CacheValue>(&self, key: &T::Key) -> Result<Option<T>> {\n        run_with_retries(\n            || async move {\n                self.get_raw(key.as_ref().as_bytes())\n                    .await?\n                    .map(|x| {\n                        String::from_utf8(x)\n                            .map_err(|e| e.into())\n                            .and_then(|json| serde_json::from_str(&json).map_err(|e| e.into()))\n                    })\n                    .transpose()\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>>;\n\n    async fn get_string<T: StringCacheKey>(&self, key: &T) -> Result<Option<String>> {\n        run_with_retries(\n            || async move {\n                self.get_raw(key.as_ref().as_bytes())\n                    .await?\n                    .map(|x| String::from_utf8(x).map_err(|e| e.into()))\n                    .transpose()\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set<T: CacheValue>(&self, key: &T::Key, value: &T, ttl: Duration) -> Result<()> {\n        run_with_retries(\n            || async move {\n                self.set_raw(\n                    key.as_ref().as_bytes(),\n                    serde_json::to_string(value)?.as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()>;\n\n    async fn set_string<T: StringCacheKey>(\n        &self,\n        key: &T,\n        value: &str,\n        ttl: Duration,\n    ) -> Result<()> {\n        run_with_retries(\n            || async move {\n                self.set_raw(key.as_ref().as_bytes(), value.to_string().as_bytes(), ttl)\n                    .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()>;\n\n    async fn set_if_not_exists<T: CacheValue>(\n        &self,\n        key: &T::Key,\n        value: &T,\n        ttl: Duration,\n    ) -> Result<bool> {\n        run_with_retries(\n            || async move {\n                self.set_raw_if_not_exists(\n                    key.as_ref().as_bytes(),\n                    serde_json::to_string(value)?.as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool>;\n\n    async fn set_string_if_not_exists<T: StringCacheKey>(\n        &self,\n        key: &T,\n        value: &str,\n        ttl: Duration,\n    ) -> Result<bool> {\n        run_with_retries(\n            || async move {\n                self.set_raw_if_not_exists(\n                    key.as_ref().as_bytes(),\n                    value.to_string().as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub(crate) use string_kv_def_inner;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/mod.rs", "node_type": "use_declaration", "line_range": [104, 104]}
{"prompt": "<|fim_prefix|>use deno_core::JsRuntime;\nuse serde_json::json;\nuse svix_bridge_types::{TransformerInput, TransformerOutput};\n\nuse super::{run_script_inner, validate_script};\n\nfn get_test_rt() -> JsRuntime {\n    JsRuntime::new(Default::default())\n}\n\n// Really just trying to figure out if the deno runtime is working the way I hope.\n#[test]\nfn test_happy_fn() {\n    let src = r#\"\n    function handler(input) {\n        return { \"x\": 123, ...input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({ \"y\": 456 }).into(), src).unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n            assert_eq!(v[\"y\"].as_i64(), Some(456));\n        }\n        TransformerOutput::Invalid => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\nfn test_invalid_output_bool() {\n    let src = r#\"\n    function handler(input) {\n        return false;\n    }\n    \"#\n    .to_string();\n\n    let mut rt = get_test_rt();\n    <|fim_suffix|>\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\n// FIXME: serde decodes arrays with keys like \"0\", \"1\"... in this situation, failing the test.\n#[ignore]\nfn test_invalid_output_array() {\n    let src = r#\"\n    function handler(input) {\n        return [1, 2];\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => {\n            panic!(\"got unexpected return value\");\n        }\n    }\n}\n\n/// Receives a string input, parses as JSON in js, then returns the result back to rust.\n#[test]\nfn test_string_input() {\n    let src = r#\"\n    function handler(input) {\n        return JSON.parse(input);\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(r#\"{\"x\": 123}\"#)),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n/// Take the string input and just add it to a field in the returned object.\n/// The string should make it through, back to rust, as-is.\n#[test]\nfn test_string_input2() {\n    let src = r#\"\n    function handler(input) {\n        return { \"payload\": input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(\"Hello World\")),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"payload\"].as_str(), Some(\"Hello World\"));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n#[test]\nfn test_validate_script_bad_syntax_is_err() {\n    assert!(validate_script(\"let 123 = ';\").is_err());\n}\n\n#[test]\nfn test_validate_script_empty_handler_is_ok() {\n    assert!(validate_script(\"function handler() { }\").is_ok());\n}\n\n#[test]\nfn test_validate_script_arrow_fn_is_ok() {\n    assert!(validate_script(\"const handler = () => ({ a: 123 })\").is_ok());\n}\n\n/// Technically, this should be legal though the utility is questionable.\n#[test]\nfn test_validate_script_empty_is_ok() {\n    assert!(validate_script(\"\").is_ok());\n    assert!(validate_script(\"    \").is_ok());\n}\n<|fim_middle|>", "completion": "let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/tests.rs", "node_type": "let_declaration", "line_range": [41, 41]}
{"prompt": "<|fim_prefix|>g_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await?;\n\n        match process_endpoint_failure(\n            cache,\n            app_id,\n            org_id,\n            endp,\n            cfg.endpoint_failure_disable_after,\n        )\n        .await?\n        {\n            None => Ok(()),\n\n            Some(EndpointDisableInfo { first_failure_at }) => {\n                let endp = endpoint::Entity::secure_find_by_id(\n                    msg_task.app_id.clone(),\n                    msg_task.endpoint_id.clone(),\n                )\n                .one(*db)\n                .await?\n                .ok_or_else(|| {\n                    Error::generic(format_args!(\n                        \"Endpoint not found {app_id} {}\",\n                        &msg_task.endpoint_id\n                    ))\n                })?;\n\n                let endp = endpoint::ActiveModel {\n                    disabled: Set(true),\n                    first_failure_at: Set(Some(first_failure_at.into())),\n                    ..endp.into()\n                };\n                let _endp = endp.update(*db).await?;\n\n                // Send operational webhooks\n                op_webhook_sender\n                    .send_operational_webhook(\n                        org_id,\n                        OperationalWebhook::EndpointDisabled(EndpointDisabledEventData {\n                            app_id: app_id.clone(),\n                            app_uid: app_uid.cloned(),\n                            endpoint_id: msg_task.endpoint_id.clone(),\n                            // TODO:\n                            endpoint_uid: None,\n                            fail_since: first_failure_at,\n                        }),\n                    )\n                    .await\n            }\n        }\n    }\n}\n\n#[derive(Clone)]\nstruct DispatchContext<'a> {\n    msg_task: &'a MessageTask,\n    payload: &'a str,\n    endp: &'a CreateMessageEndpoint,\n    org_id: &'a OrganizationId,\n    app_id: &'a ApplicationId,\n    app_uid: Option<&'a ApplicationUid>,\n    msg_uid: Option<&'a MessageUid>,\n    // Like 'attempt_number' from the task, but adjusted to -1 for `MessageAttemptTriggerType::Manual`.\n    db_attempt_number: i16,\n}\n\n/// Dispatches one webhook\n#[tracing::instrument(\n    skip_all,\n    level = \"error\",\n    fields(\n        endp_id = msg_task.endpoint_id.0.as_str(),\n    )\n)]\nasync fn dispatch_message_task(\n    worker_context: &WorkerContext<'_>,\n    msg: &message::Model,\n    app: &CreateMessageApp,\n    msg_task: MessageTask,\n    payload: &str,\n    endp: CreateMessageEndpoint,\n    status: MessageStatus,\n) -> Result<()> {\n    let WorkerContext { webhook_client, .. } = worker_context;\n\n    tracing::trace!(\"Dispatch start\");\n\n    if (status != MessageStatus::Pending && status != MessageStatus::Sending)\n        && (msg_task.trigger_type != MessageAttemptTriggerType::Manual)\n    {\n        // TODO: it happens when this message destination is \"resent\". This leads to 2 queue tasks with the same message destination\n        tracing::debug!(\n            id = msg_task.msg_id.0,\n            \"MessageDestination is not pending (it's {:?}).\",\n            status,\n        );\n        return Ok(());\n    }\n\n    let db_attempt_number = match msg_task.trigger_type {\n        MessageAttemptTriggerType::Manual => -1,\n        MessageAttemptTriggerType::Scheduled => msg_task.attempt_count as _,\n    };\n\n    let dispatch_context = DispatchContext {\n        msg_task: &msg_task,\n        payload,\n        endp: &endp,\n        org_id: &app.org_id,\n        app_id: &app.id,\n        app_uid: app.uid.as_ref(),\n        msg_uid: msg.uid.as_ref(),\n        db_attempt_number,\n    };\n\n    let dispatch = prepare_dispatch(worker_context, dispatch_context.clone()).await?;\n    let completed = match dispatch {\n        IncompleteDispatch::Pending(pending) => {\n            make_http_call(dispatch_context.clone(), pending, webhook_client).await?\n        }\n        IncompleteDispatch::Failed(failed) => CompletedDispatch::Failed(failed),\n    };\n\n    m<|fim_suffix|>}\n\nfn bytes_to_string(bytes: bytes::Bytes) -> String {\n    match std::str::from_utf8(&bytes) {\n        Ok(v) => v.to_owned(),\n        Err(_) => STANDARD.encode(&bytes),\n    }\n}\n\n/// Manages preparation and execution of a QueueTask type\n#[tracing::instrument(\n    skip_all,\n    level = \"error\",\n    fields(msg_id, app_id, org_id, instance_id, task_type = queue_task.task_type())\n)]\nasync fn process_queue_task(\n    worker_context: WorkerContext<'_>,\n    queue_task: QueueTask,\n) -> Result<()> {\n    process_queue_task_inner(worker_context, queue_task)\n        .await\n        .map_err(|e| {\n            tracing::error!(\"{e}\");\n            e\n        })\n}\n\n/// Manages preparation and execution of a QueueTask type\nasync fn process_queue_task_inner(\n    worker_context: WorkerContext<'_>,\n    queue_task: QueueTask,\n) -> Result<()> {\n    let WorkerContext { db, cache, .. }: WorkerContext<'_> = worker_context;\n    let span = tracing::Span::current();\n\n    let (mut msg, msg_content, force_endpoint, trigger_type, attempt_count, status) =\n        match queue_task {\n            QueueTask::HealthCheck => return Ok(()),\n            QueueTask::MessageV1(task) => {\n                let (msg, msg_content) = message::Entity::find_by_id(task.msg_id.clone())\n                    .find_also_related(messagecontent::Entity)\n                    .one(db)\n                    .await?\n                    .ok_or_else(|| {\n                        Error::generic(format_args!(\n                            \"Unexpected: message doesn't exist {}\",\n                            task.msg_id\n                        ))\n                    })?;\n\n                let latest_attempt = messageattempt::Entity::secure_find_by_msg(msg.id.clone())\n                    .filter(messageattempt::Column::EndpId.eq(task.endpoint_id.clone()))\n                    .order_by_desc(messageattempt::Column::Id)\n                    .one(db)\n                    .await?;\n\n                let status = match latest_attempt {\n                    Some(attempt) => {\n                        match attempt.next_attempt {\n                            Some(_) => MessageStatus::Sending,\n                            // If here then we're processing old tasks from before\n                            // messagedest was removed, so we'll need to query messagedestination\n                            None if attempt.msg_dest_id.is_some() => {\n                                messagedestination::Entity::secure_find_by_msg(task.msg_id.clone())\n                                    .filter(\n                                        messagedestination::Column::EndpId\n                                            .eq(task.endpoint_id.clone()),\n                                    )\n                                    .one(db)\n                                    .await?\n                                    .ok_or_else(|| {\n                                        Error::generic(format!(\n                                            \"MessageDestination not found for message {}\",\n                                            &task.msg_id\n                                        ))\n                                    })?\n                                    .status\n                            }\n                            None => MessageStatus::Fail,\n                        }\n                    }\n                    None => MessageStatus::Sending,\n                };\n\n                (\n                    msg,\n                    msg_content,\n                    Some(task.endpoint_id),\n                    task.trigger_type,\n                    task.attempt_count,\n                    status,\n                )\n            }\n            QueueTask::MessageBatch(task) => {\n                let (msg, msg_content) = message::Entity::find_by_id(task.msg_id.clone())\n                    .find_also_related(messagecontent::Entity)\n                    .one(db)\n                    .await?\n                    .ok_or_else(|| {\n                        Error::generic(format_args!(\n                            \"Unexpected: message doesn't exist {}\",\n               <|fim_middle|>", "completion": "match completed {\n        CompletedDispatch::Successful(success) => {\n            handle_successful_dispatch(worker_context, dispatch_context, success).await\n        }\n        CompletedDispatch::Failed(failed) => {\n            handle_failed_dispatch(worker_context, dispatch_context, failed).await\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "match_expression", "line_range": [727, 734]}
{"prompt": "<|fim_prefix|> serde::Serializer,\n    {\n        serializer.serialize_str(&STANDARD.encode(self.clone().into_vec()))\n    }\n}\n\nimpl<'de> Deserialize<'de> for EndpointSecretInternal {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n\n        String::deserialize(deserializer).and_then(|string| {\n            // For backwards compat when loading from ExpiringSigningKeys. Going forward we just b64 it\n            if string.starts_with(EndpointSecretType::Hmac256.secret_prefix()) {\n                Ok(Self {\n                    marker: EndpointSecretMarker {\n                        type_: EndpointSecretType::Hmac256,\n                        encrypted: false,\n                    },\n                    key: string\n                        .get(EndpointSecretType::Hmac256.secret_prefix().len()..)\n                        .ok_or_else(|| Error::custom(\"invalid prefix\".to_string()))\n                        .and_then(|string| {\n                            STANDARD\n                                .decode(string)\n                                .map_err(|err| Error::custom(err.to_string()))\n                        })?,\n                })\n            } else {\n                let buf = STANDARD\n                    .decode(string)\n                    .map_err(|err| Error::custom(err.to_string()))?;\n                Self::from_vec(buf).map_err(|err| Error::custom(err.to_string()))\n            }\n        })\n    }\n}\n\nimpl From<EndpointSecretInternal> for sea_orm::Value {\n    fn from(v: EndpointSecretInternal) -> Self {\n        Self::Bytes(Some(Box::new(v.into_vec())))\n    }\n}\n\nimpl sea_orm::TryGetable for EndpointSecretInternal {\n    fn try_get_by<I: sea_orm::ColIdx>(\n        res: &sea_orm::QueryResult,\n        index: I,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get_by(res, index) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }\n\n    fn try_get(\n        res: &sea_orm::QueryResult,\n        pre: &str,\n        col: &str,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get(res, pre, col) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }\n}\n\nimpl sea_orm::sea_query::Nullable for EndpointSecretInternal {\n    fn null() -> sea_orm::Value {\n        sea_orm::Value::Bytes(None)\n    }\n}\n\nimpl sea_orm::sea_query::ValueType for EndpointSecretInternal {\n    fn try_from(v: sea_orm::Value) -> Result<Self, sea_orm::sea_query::ValueTypeErr> {\n        match v {\n            sea_orm::Value::Bytes(Some(x)) => {\n                EndpointSecretInternal::from_vec(*x).map_err(|_| sea_orm::sea_query::ValueTypeErr)\n            }\n            _ => Err(sea_orm::sea_query::ValueTypeErr),\n        }\n    }\n\n    fn type_name() -> String {\n        stringify!(EndpointSecretInternal).to_owned()\n    }\n\n    fn column_type() -> sea_orm::sea_query::ColumnType {\n        sea_orm::sea_query::ColumnType::Binary(\n            Self::KEY_SIZE\n                .try_into()\n                .expect(\"Key size is not more than u32::MAX\"),\n        )\n    }\n\n    fn array_type() -> sea_orm::sea_query::ArrayType {\n        sea_orm::sea_query::ArrayType::Bytes\n    }\n}\n\n/// The external representation of the endpoint secret.\n/// This one is used for serializing to and from customers.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum EndpointSecret {\n    Symmetric(Vec<u8>),\n    Asymmetric(AsymmetricKey),\n}\n\nimpl EndpointSecret {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MAX: usize = 75;\n\n    pub fn serialize_secret_key(&self) -> String {\n        m<|fim_suffix|>    }\n\n    pub fn serialize_public_key(&self) -> String {\n        match self {\n            Self::Symmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Hmac256.public_prefix(),\n                    STANDARD.encode(key)\n                )\n            }\n            Self::Asymmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Ed25519.public_prefix(),\n                    &STANDARD.encode(key.pubkey())\n                )\n            }\n        }\n    }\n}\n\nimpl Serialize for EndpointSecret {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        serializer.serialize_str(&self.serialize_public_key())\n    }\n}\n\nimpl<'de> Deserialize<'de> for EndpointSecret {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n        let invalid_prefix = Error::custom(\"invalid prefix\".to_string());\n        String::deserialize(deserializer).and_then(|string| {\n            if string.starts_with(EndpointSecretType::Ed25519.secret_prefix()) {\n                Ok(Self::Asymmetric(\n                    AsymmetricKey::from_base64(\n                        string\n                            .get(EndpointSecretType::Ed25519.secret_prefix().len()..)\n                            .ok_or(invalid_prefix)?,\n                    )\n                    .map_err(|e| Error::custom(e.to_string()))?,\n                ))\n            } else if string.starts_with(EndpointSecretType::Hmac256.secret_prefix()) {\n                Ok(Self::Symmetric(\n                    string\n                        .get(EndpointSecretType::Hmac256.secret_prefix().len()..)\n                        .ok_or(invalid_prefix)\n                        .and_then(|string| {\n                            STANDARD\n                                .decode(string)\n                                .map_err(|err| Error::custom(err.to_string()))\n                        })?,\n                ))\n            } else {\n                Err(invalid_prefix)\n            }\n        })\n    }\n}\n\nimpl Validate for EndpointSecret {\n    fn validate(&self) -> Result<(), ValidationErrors> {\n        let mut errors = ValidationErrors::new();\n\n        match self {\n            Self::Symmetric(bytes) => {\n                if bytes.len() < Self::KEY_SIZE || bytes.len() > Self::KEY_SIZE_MAX {\n                    errors.add(\n                        ALL_ERROR,\n                        validation_error(Some(\"length\"), Some(\"secret length invalid\")),\n                    );\n                }\n            }\n            Self::Asymmetric(key) => {\n                let test_msg = b\"123\";\n                let signature = key.0.sk.sign(test_msg, None);\n                if key.0.pk.verify(test_msg, &signature).is_err() {\n                    errors.add(\n                        ALL_ERROR,\n                        validation_error(\n                            Some(\"invalid_key\"),\n                            Some(\"Invalid key, failed signing test msg\"),\n                        ),\n                    );\n                }\n            }\n        }\n\n        if errors.is_empty() {\n            Ok(())\n        } else {\n            Err(errors)\n        }\n    }\n}\n\nimpl JsonSchema for EndpointSecret {\n    fn schema_name() -> String {\n        \"EndpointSecret\".to_string()\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        const KEY_PATTERN: &str = \"^(whsec_)?[a-zA-Z0-9+/=]{32,100}$\";\n        let mut schema = String::json_schema(gen);\n        if let schemars::schema::Schema::Object(ref mut obj) = schema {\n            obj.string = Some(Box::new(schemars::schema::StringValidation {\n                pattern: Some(KEY_PATTERN.to_string()),\n                ..Default::default()\n            }));\n            obj.metadata = Some(Box::new(schemars::schema::Metadata{\n                description: Some(\"The endpoint's verification secret. If `null` is passed, a secret is a<|fim_middle|>", "completion": "match self {\n            Self::Symmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Hmac256.secret_prefix(),\n                    STANDARD.encode(key)\n                )\n            }\n            Self::Asymmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Ed25519.secret_prefix(),\n                    &STANDARD.encode(key.0.sk.as_slice())\n                )\n            }\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "match_expression", "line_range": [957, 972]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n<|fim_suffix|>\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct StreamSinkOut {\n    #[serde(rename = \"batchSize\")]\n    pub batch_size: i32,\n\n    #[serde(rename = \"createdAt\")]\n    pub created_at: String,\n\n    #[serde(rename = \"currentIterator\")]\n    pub current_iterator: String,\n\n    #[serde(rename = \"eventTypes\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub event_types: Option<Vec<String>>,\n\n    #[serde(rename = \"failureReason\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub failure_reason: Option<String>,\n\n    /// The sink's ID.\n    pub id: String,\n\n    #[serde(rename = \"maxWaitSecs\")]\n    pub max_wait_secs: i32,\n\n    pub metadata: std::collections::HashMap<String, String>,\n\n    #[serde(rename = \"nextRetryAt\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub next_retry_at: Option<String>,\n\n    pub status: SinkStatus,\n\n    /// The sink's UID.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<String>,\n\n    #[serde(rename = \"updatedAt\")]\n    pub updated_at: String,\n\n    #[serde(flatten)]\n    pub config: StreamSinkOutConfig,\n}\n\n#[derive(Clone, Debug, PartialEq, Deserialize, Serialize)]\n#[serde(tag = \"type\", content = \"config\")]\npub enum StreamSinkOutConfig {\n    #[serde(rename = \"poller\")]\n    Poller,\n    #[serde(rename = \"azureBlobStorage\")]\n    AzureBlobStorage(AzureBlobStorageConfig),\n    #[serde(rename = \"otelTracing\")]\n    OtelTracing(SinkOtelV1Config),\n    #[serde(rename = \"http\")]\n    Http(SinkHttpConfig),\n    #[serde(rename = \"amazonS3\")]\n    AmazonS3(S3Config),\n    #[serde(rename = \"googleCloudStorage\")]\n    GoogleCloudStorage(GoogleCloudStorageConfig),\n}\n\n#[allow(clippy::derivable_impls)]\nimpl Default for StreamSinkOutConfig {\n    fn default() -> Self {\n        Self::Poller\n    }\n}\n<|fim_middle|>", "completion": "use super::{\n    azure_blob_storage_config::AzureBlobStorageConfig,\n    google_cloud_storage_config::GoogleCloudStorageConfig, s3_config::S3Config,\n    sink_http_config::SinkHttpConfig, sink_otel_v1_config::SinkOtelV1Config,\n    sink_status::SinkStatus,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/stream_sink_out.rs", "node_type": "use_declaration", "line_range": [4, 9]}
{"prompt": "<|fim_prefix|>use js_option::JsOption;\nuse std::{collections::HashSet, time::Duration};\nuse svix::{\n    api::{ApplicationIn, EndpointIn, EndpointPatch, EventTypeIn},\n    error::Error,\n};\nuse wiremock::{Mock, MockServer, ResponseTemplate};\n\nuse crate::utils::test_client::TestClientBuilder;\n\nfn check_for_conflict(e: Error) {\n    match e {\n        Error::Http(e) => {\n            assert_eq!(\n                e.status,\n                http02::StatusCode::CONFLICT,\n                \"conflicts are expected but other statuses are not\"\n            );\n        }\n        _ => panic!(\"unexpected error: {e}\"),\n    }\n}\n\n// Opt-in with `cargo test --ignored`\n#[ignore]\n#[tokio::test]\nasync fn test_endpoint_crud() {\n    let client = TestClientBuilder::new().build();\n    let client = client.client;\n\n    <|fim_suffix|>\n\n    if let Err(e) = client\n        .event_type()\n        .create(\n            EventTypeIn {\n                name: String::from(\"event.started\"),\n                description: String::from(\"Something started\"),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n    {\n        check_for_conflict(e);\n    }\n\n    if let Err(e) = client\n        .event_type()\n        .create(\n            EventTypeIn {\n                name: String::from(\"event.ended\"),\n                description: String::from(\"Something ended\"),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n    {\n        check_for_conflict(e);\n    }\n\n    let ep = client\n        .endpoint()\n        .create(\n            app.id.clone(),\n            EndpointIn {\n                channels: Some(vec![String::from(\"ch0\"), String::from(\"ch1\")]),\n                url: String::from(\"https://example.svix.com/\"),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();\n\n    let want_channels: HashSet<_> = [String::from(\"ch0\"), String::from(\"ch1\")]\n        .into_iter()\n        .collect();\n    let got_channels = ep.channels.clone().unwrap().into_iter().collect();\n    assert_eq!(want_channels, got_channels);\n    assert_eq!(0, ep.filter_types.unwrap_or_default().len());\n\n    let ep_patched = client\n        .endpoint()\n        .patch(\n            app.id.clone(),\n            ep.id.clone(),\n            EndpointPatch {\n                filter_types: JsOption::Some(vec![\n                    String::from(\"event.started\"),\n                    String::from(\"event.ended\"),\n                ]),\n                ..Default::default()\n            },\n        )\n        .await\n        .unwrap();\n\n    let want_filter_types: HashSet<_> =\n        [String::from(\"event.started\"), String::from(\"event.ended\")]\n            .into_iter()\n            .collect();\n    let got_channels = ep_patched.channels.clone().unwrap().into_iter().collect();\n    let got_filter_types = ep_patched\n        .filter_types\n        .clone()\n        .unwrap()\n        .into_iter()\n        .collect();\n    assert_eq!(want_channels, got_channels);\n    assert_eq!(want_filter_types, got_filter_types);\n\n    // Should complete without error if the deserialization handles empty bodies\n    // correctly.\n    client\n        .endpoint()\n        .delete(app.id.clone(), ep.id)\n        .await\n        .unwrap();\n\n    client.application().delete(app.id).await.unwrap()\n}\n\n#[tokio::test]\nasync fn test_default_retries() {\n    let mock_server: MockServer = MockServer::start().await;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .and(wiremock::matchers::header(\"svix-retry-count\", 1))\n        .and(wiremock::matchers::header_exists(\"svix-req-id\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .and(wiremock::matchers::header(\"svix-retry-count\", 2))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    let t0 = std::time::Instant::now();\n    let client = TestClientBuilder::new()\n        .url(mock_server.uri())\n        .token(\"test\".to_string())\n        .build()\n        .client;\n\n    let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await;\n    assert!(app.is_err());\n\n    let diff = std::time::Instant::now() - t0;\n    assert!(diff.as_millis() >= 60);\n\n    mock_server.verify().await;\n}\n\n#[tokio::test]\nasync fn test_custom_retries() {\n    let mock_server: MockServer = MockServer::start().await;\n    let num_retries = 6;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    for i in 1..=num_retries {\n        Mock::given(wiremock::matchers::method(\"POST\"))\n            .and(wiremock::matchers::path(\"/api/v1/app\"))\n            .and(wiremock::matchers::header(\"svix-retry-count\", i))\n            .respond_with(ResponseTemplate::new(500))\n            .up_to_n_times(1)\n            .expect(1)\n            .mount(&mock_server)\n            .await;\n    }\n\n    let t0 = std::time::Instant::now();\n    let client = TestClientBuilder::new()\n        .url(mock_server.uri())\n        .token(\"test\".to_string())\n        .retries(num_retries)\n        .build()\n        .client;\n\n    let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await;\n    assert!(app.is_err());\n\n    let diff = std::time::Instant::now() - t0;\n    let expected: u32 = (1..=num_retries).map(|x| 20 * x).sum();\n    assert!(diff.as_millis() >= u128::from(expected));\n\n    mock_server.verify().await;\n}\n\n#[tokio::test]\nasync fn test_custom_retry_schedule() {\n    let mock_server: MockServer = MockServer::start().await;\n    let retry_schedule_in_ms = [50, 100, 200, 400];\n    let retry_schedule = retry_schedule_in_ms.map(Duration::from_millis).into();\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    for i in 1..=retry_schedule_in_ms.len() {\n        Mock::given(wiremock::matchers::method(\"POST\"))\n            .and(wiremock::matchers::path(\"/api/v1/app\"))\n            .and(wiremock::matchers::header(\"svix-retry-count\", i))\n            .respond_with(ResponseTemplate::new(500))\n            .up_to_n_times(1)\n            .expect(1)\n            .mount(&mock_server)\n            .await;\n    }\n\n    let t0 = std::time::Instant::now();\n    let client = TestClientBuilder::new()\n        .url(mock_server.uri())\n        .token(\"test\".to_string())\n        .retry_schedule(retry_schedule)\n        .build()\n        .client;\n\n    let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await;\n    assert!(app.is_err());\n\n    let diff = std::time::Instant::now() - t0;\n    let expected: u64 = retry_schedule_in_ms.iter().sum();\n    assert!(diff.as_millis() >= u128::from(expected));\n\n    mock_server.verify().await;\n}\n<|fim_middle|>", "completion": "let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/tests/it/kitchen_sink.rs", "node_type": "let_declaration", "line_range": [31, 41]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\nuse super::message_poller::MessagePollerArgs;\n\n#[derive(Args, Clone)]\npub struct MessageListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filter response based on the channel.\n    #[arg(long)]\n    pub channel: Option<String>,\n    /// Only include items created before a certain date.\n    #[arg(long)]\n    pub before: Option<chrono::DateTime<chrono::Utc>>,\n    /// Only include items created after a certain date.\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n    /// When `true` message payloads are included in the response.\n    #[arg(long)]\n    pub with_content: Option<bool>,\n    /// Filter messages matching the provided tag.\n    #[arg(long)]\n    pub tag: Option<String>,\n    /// Filter response based on the event type\n    #[arg(long)]\n    pub event_types: Option<Vec<String>>,\n}\n\nimpl From<MessageListOptions> for svix::api::MessageListOptions {\n    fn from(value: MessageListOptions) -> Self {\n        let MessageListOptions {\n            limit,\n            iterator,\n            channel,\n            before,\n            after,\n            with_content,\n            tag,\n            event_types,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            channel,\n            before: before.map(|dt| dt.to_rfc3339()),\n            after: after.map(|dt| dt.to_rfc3339()),\n            with_content,\n            tag,\n            event_types,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageCreateOptions {\n    /// When `true`, message payloads are included in the response.\n    #[arg(long)]\n    pub with_content: Option<bool>,\n\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<MessageCreateOptions> for svix::api::MessageCreateOptions {\n    fn from(value: MessageCreateOptions) -> Self {\n        let MessageCreateOptions {\n            with_content,\n            idempotency_key,\n        } = value;\n        Self {\n            with_content,\n            idempotency_key,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageExpungeAllContentsOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<MessageExpungeAllContentsOptions> for svix::api::MessageExpungeAllContentsOptions {\n    <|fim_suffix|>\n}\n\n#[derive(Args, Clone)]\npub struct MessageGetOptions {\n    /// When `true` message payloads are included in the response.\n    #[arg(long)]\n    pub with_content: Option<bool>,\n}\n\nimpl From<MessageGetOptions> for svix::api::MessageGetOptions {\n    fn from(value: MessageGetOptions) -> Self {\n        let MessageGetOptions { with_content } = value;\n        Self { with_content }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct MessageArgs {\n    #[command(subcommand)]\n    pub command: MessageCommands,\n}\n\n#[derive(Subcommand)]\npub enum MessageCommands {\n    Poller(MessagePollerArgs),\n    /// List all of the application's messages.\n    ///\n    /// The `before` and `after` parameters let you filter all items created before or after a certain date. These can be\n    /// used alongside an iterator to paginate over results within a certain window.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    List {\n        app_id: String,\n        #[clap(flatten)]\n        options: MessageListOptions,\n    },\n    /// Creates a new message and dispatches it to all of the application's endpoints.\n    ///\n    /// The `eventId` is an optional custom unique ID. It's verified to be unique only up to a day, after that no verification will be made.\n    /// If a message with the same `eventId` already exists for the application, a 409 conflict error will be returned.\n    ///\n    /// The `eventType` indicates the type and schema of the event. All messages of a certain `eventType` are expected to have the same schema. Endpoints can choose to only listen to specific event types.\n    /// Messages can also have `channels`, which similar to event types let endpoints filter by them. Unlike event types, messages can have multiple channels, and channels don't imply a specific message content or schema.\n    ///\n    /// The `payload` property is the webhook's body (the actual webhook message). Svix supports payload sizes of up to 1MiB, though it's generally a good idea to keep webhook payloads small, probably no larger than 40kb.\n    Create {\n        app_id: String,\n        message_in: crate::json::JsonOf<MessageIn>,\n        #[clap(flatten)]\n        options: MessageCreateOptions,\n    },\n    /// Delete all message payloads for the application.\n    ///\n    /// This operation is only available in the <a href=\"https://svix.com/pricing\" target=\"_blank\">Enterprise</a> plan.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"application.purge_content\",\n    ///   \"data\": {\n    ///     \"messagesPurged\": 150\n    ///   }\n    /// }\n    /// ```\n    ExpungeAllContents {\n        app_id: String,\n        #[clap(flatten)]\n        options: MessageExpungeAllContentsOptions,\n    },\n    /// Get a message by its ID or eventID.\n    Get {\n        app_id: String,\n        id: String,\n        #[clap(flatten)]\n        options: MessageGetOptions,\n    },\n    /// Delete the given message's payload.\n    ///\n    /// Useful in cases when a message was accidentally sent with sensitive content.\n    /// The message can't be replayed or resent once its payload has been deleted or expired.\n    ExpungeContent {\n        app_id: String,\n        id: String,\n    },\n}\n\nimpl MessageCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Poller(args) => {\n                args.command.exec(client, color_mode).await?;\n            }\n            Self::List { app_id, options } => {\n                let resp = client.message().list(app_id, Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                app_id,\n                message_in,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .create(app_id, message_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ExpungeAllContents { app_id, options } => {\n                let resp = client\n                    .message()\n                    .expunge_all_contents(app_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get {\n                app_id,\n                id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .get(app_id, id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ExpungeContent { app_id, id } => {\n                client.message().expunge_content(app_id, id).await?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "fn from(value: MessageExpungeAllContentsOptions) -> Self {\n        let MessageExpungeAllContentsOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/message.rs", "node_type": "function_item", "line_range": [90, 93]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct EndpointListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct EndpointCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EndpointRecoverOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EndpointReplayMissingOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EndpointRotateSecretOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EndpointSendExampleOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EndpointGetStatsOptions {\n    /// Filter the range to data starting from this date.\n    ///\n    /// RFC3339 date string.\n    pub since: Option<String>,\n\n    /// Filter the range to data ending by this date.\n    ///\n    /// RFC3339 date string.\n    pub until: Option<String>,\n}\n\npub struct Endpoint<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> Endpoint<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List the application's endpoints.\n    pub async fn list(\n        &self,\n        app_id: String,\n        options: Option<EndpointListOptions>,\n    ) -> Result<ListResponseEndpointOut> {\n        let EndpointListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/endpoint\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create a new endpoint for the application.\n    ///\n    /// When `secret` is `null` the secret is automatically generated\n    /// (recommended).\n    pub async fn create(\n        &self,\n        app_id: String,\n        endpoint_in: EndpointIn,\n        options: Option<EndpointCreateOptions>,\n    ) -> Result<EndpointOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/app/{app_id}/endpoint\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(endpoint_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an endpoint.\n    pub async fn get(&self, app_id: String, endpoint_id: String) -> Result<EndpointOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Update an endpoint.\n    pub async fn update(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_update: EndpointUpdate,\n    ) -> Result<EndpointOut> {\n        crate::request::Request::new(\n            http1::Method::PUT,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_update)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Delete an endpoint.\n    pub async fn delete(&self, app_id: String, endpoint_id: String) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Partially update an endpoint.\n    pub async fn patch(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_patch: EndpointPatch,\n    ) -> Result<EndpointOut> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_patch)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get the additional headers to be sent with the webhook.\n    pub async fn get_headers(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n    ) -> Result<EndpointHeadersOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/headers\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Set the additional headers to be sent with the webhook.\n    pub async fn update_headers(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_headers_in: EndpointHeadersIn,\n    ) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::PUT,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/headers\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_headers_in)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Partially set the additional headers to be sent with the webhook.\n    pub async fn patch_headers(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_headers_patch_in: EndpointHeadersPatchIn,\n    ) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/headers\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_headers_patch_in)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Resend all failed messages since a given time.\n    ///\n    /// Messages that were sent successfully, even if failed initially, are not\n    /// resent.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"endpoint.recover\",\n    ///   \"data\": {\n    ///     \"messagesSent\": 2\n    ///   }\n    /// }\n    /// ```\n    pub async fn recover(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        recover_in: RecoverIn,\n        options: Option<EndpointRecoverOptions>,\n    ) -> Result<RecoverOut> {\n        let EndpointRecoverOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/recover\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .with_body_param(recover_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Replays messages to the endpoint.\n    ///\n    /// Only messages that were created after `since` will be sent.\n    /// Messages that were previously sent to the endpoint are not resent.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"endpoint.replay\",\n    ///   \"data\": {\n    ///     \"messagesSent\": 2\n    ///   }\n    /// }\n    /// ```\n    pub async fn replay_missing(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        replay_in: ReplayIn,\n        options: Option<EndpointReplayMissingOptions>,\n    ) -> Result<ReplayOut> {\n        let EndpointR<|fim_middle|>", "completion": "let EndpointCreateOptions { idempotency_key } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/endpoint.rs", "node_type": "let_declaration", "line_range": [94, 94]}
{"prompt": "<|fim_prefix|>use aide::OperationInput;\nuse axum::{\n    async_trait,\n    extract::{FromRequestParts, Path},\n    http::request::Parts,\n};\n\nuse super::{\n    security::{permissions_from_bearer, AccessLevel, Permissions},\n    types::{ApplicationId, ApplicationIdOrUid, FeatureFlagSet, OrganizationId},\n};\nuse crate::{\n    db::models::{application, applicationmetadata},\n    error::{Error, HttpError, Result, Traceable},\n    AppState,\n};\n\npub struct ReadAll {\n    pub org_id: OrganizationId,\n    pub feature_flags: AllowedFeatureFlags,\n}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for ReadAll {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n        let org_id = permissions.org_id();\n        let feature_flags = match permissions.access_level {\n            AccessLevel::Organization(_) => AllowedFeatureFlags::All,\n            AccessLevel::Application(_, _) => AllowedFeatureFlags::Some(permissions.feature_flags),\n        };\n        Ok(Self {\n            org_id,\n            feature_flags,\n        })\n    }\n}\n\nimpl OperationInput for ReadAll {}\n\npub struct Organization {\n    pub org_id: OrganizationId,\n}\n\nimpl OperationInput for Organization {}\n\nimpl Permissions {\n    fn check_app_is_permitted(&self, app_id: &ApplicationId) -> Result<()> {\n        if let Some(ref permitted_app_id) = self.app_id() {\n            <|fim_suffix|>\n        }\n        Ok(())\n    }\n}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for Organization {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n\n        let org_id = match permissions.access_level {\n            AccessLevel::Organization(org_id) => org_id,\n            _ => return Err(HttpError::permission_denied(None, None).into()),\n        };\n\n        Ok(Self { org_id })\n    }\n}\n\npub struct Application {\n    pub app: application::Model,\n}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for Application {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n\n        let Path(ApplicationPathParams { app_id }) =\n            Path::<ApplicationPathParams>::from_request_parts(parts, state).await?;\n        let app =\n            application::Entity::secure_find_by_id_or_uid(permissions.org_id(), app_id.to_owned())\n                .one(&state.db)\n                .await?\n                .ok_or_else(|| HttpError::not_found(None, None))?;\n\n        permissions.check_app_is_permitted(&app.id)?;\n\n        Ok(Self { app })\n    }\n}\n\nimpl OperationInput for Application {}\n\n// Organization level privileges, with the requested application\npub struct OrganizationWithApplication {\n    pub app: application::Model,\n}\n\nimpl OperationInput for OrganizationWithApplication {}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for OrganizationWithApplication {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let Organization { org_id } = Organization::from_request_parts(parts, state)\n            .await\n            .trace()?;\n\n        let Path(ApplicationPathParams { app_id }) =\n            Path::<ApplicationPathParams>::from_request_parts(parts, state).await?;\n        let app = application::Entity::secure_find_by_id_or_uid(org_id, app_id.to_owned())\n            .one(&state.db)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n        Ok(OrganizationWithApplication { app })\n    }\n}\n\npub struct ApplicationWithMetadata {\n    pub app: application::Model,\n    pub metadata: applicationmetadata::Model,\n}\n\nimpl OperationInput for ApplicationWithMetadata {}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for ApplicationWithMetadata {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n\n        let Path(ApplicationPathParams { app_id }) =\n            Path::<ApplicationPathParams>::from_request_parts(parts, state).await?;\n        let (app, metadata) = application::Model::fetch_with_metadata(\n            &state.db,\n            permissions.org_id(),\n            app_id.to_owned(),\n        )\n        .await\n        .trace()?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n        permissions.check_app_is_permitted(&app.id)?;\n\n        Ok(Self { app, metadata })\n    }\n}\n\n#[derive(serde::Deserialize)]\nstruct ApplicationPathParams {\n    app_id: ApplicationIdOrUid,\n}\n\n/// Denotes what features gated by feature flags the requester has access to.\npub enum AllowedFeatureFlags {\n    /// Requester has access to all features regardless of flags. No checking\n    /// of feature flags should be done for this request.\n    All,\n    /// Requester has access to a limited set of features. The set may be empty.\n    Some(FeatureFlagSet),\n}\n<|fim_middle|>", "completion": "if permitted_app_id != app_id {\n                return Err(HttpError::not_found(None, None).into());\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/permissions.rs", "node_type": "if_expression", "line_range": [52, 54]}
{"prompt": "<|fim_prefix|>use anyhow::Context;\nuse clap::Args;\nuse rand::{rngs::StdRng, seq::SliceRandom, SeedableRng};\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse svix::api::*;\n\n#[derive(Args)]\nstruct SeedOptions {\n    /// Will clear out all the applications and event types\n    #[arg(long, default_value = \"false\")]\n    pub reset: bool,\n\n    /// The number of endpoints to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=10) , default_value = \"2\")]\n    pub endpoint_count: u8,\n\n    /// The number of messages to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=100) , default_value = \"10\")]\n    pub message_count: u8,\n}\n\n#[derive(Args)]\npub struct SeedArgs {\n    #[clap(flatten)]\n    options: SeedOptions,\n}\n\n#[derive(Debug, Serialize, Default)]\n#[serde(rename_all = \"camelCase\")]\nstruct SeedOut {\n    application: ApplicationOut,\n    endpoints: Vec<String>,\n    event_types: Vec<String>,\n    messages: Vec<String>,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct PlayTokenOut {\n    token: String,\n}\n\nconst PLAY_TOKEN_GENERATE_URL: &str = \"https://api.play.svix.com/api/v1/token/generate/\";\nconst USER_EVENT_TYPES: [&str; 4] = [\"signup\", \"signin\", \"signout\", \"deleted\"];\n\npub async fn exec(\n    client: &Svix,\n    args: SeedArgs,\n    color_mode: colored_json::ColorMode,\n) -> anyhow::Result<()> {\n    let mut seed_out = SeedOut {\n        ..Default::default()\n    };\n\n    if args.options.reset {\n        let confirmation = dialoguer::Confirm::new()\n         .with_prompt(\"This will clear out all the applications and event types! Do you want to continue? \")\n         .interact()\n         .unwrap_or(false);\n\n        if confirmation {\n            reset_application(client).await?;\n            reset_event_type(client).await?;\n        } else {\n            return Ok(());\n        }\n    }\n\n    let application_in = ApplicationIn {\n        name: \"Test application\".to_string(),\n        ..Default::default()\n    };\n    let application_out = client.application().create(application_in, None).await?;\n\n    seed_out.application = application_out.clone();\n\n    let app_id = application_out.id;\n\n    let mut handles = Vec::new();\n\n    for _ in 0..args.options.endpoint_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(async move {\n            create_endpoint(client, app_id).await\n        }))\n    }\n\n    for h in handles {\n        let eo = h.await??;\n        seed_out.endpoints.push(eo.url);\n    }\n\n    for typ in USER_EVENT_TYPES {\n        let event_type_in = EventTypeIn {\n            name: format!(\"user.{typ}\"),\n            description: \"\".to_string(),\n            schemas: Some(json!(schema_example())),\n            ..Default::default()\n        };\n        let res = client.event_type().create(event_type_in, None).await;\n\n        match res {\n            Ok(event_type_out) => {\n                seed_out.event_types.push(event_type_out.name);\n            }\n            Err(err) => {\n                eprintln!(\"Failed to create event type: {err}\");\n                continue;\n            }\n        }\n    }\n    let mut handles = Vec::new();\n\n    for _ in 0..args.options.message_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(\n            async move { create_message(client, app_id).await },\n        ))\n    }\n\n    for h in handles {\n        let message_out = h.await??;\n        seed_out.messages.push(message_out.id);\n    }\n\n    let summary = format!(\n        \"Seeded {} endpoints, {} event types, {} messages to application \\\"{}\\\"\",\n        seed_out.endpoints.len(),\n        seed_out.event_types.len(),\n        seed_out.messages.len(),\n        seed_out.application.name\n    );\n\n    crate::json::print_json_output(&seed_out, color_mode)?;\n    println!(\"{summary}\");\n\n    Ok(())\n}\n\nasync fn create_endpoint(client: Svix, app_id: String) -> anyhow::Result<EndpointOut> {\n    let req_client = reqwest::Client::new();\n\n    let resp = req_client\n        .post(PLAY_TOKEN_GENERATE_URL)\n        .send()\n        .await?\n        .json::<PlayTokenOut>()\n        .await\n        .context(\"Failed to get token from public api\")?;\n\n    let endpoint_in = EndpointIn {\n        url: format!(\"https://play.svix.com/in/{}/\", resp.token),\n        ..Default::default()\n    };\n    let endpoint_out = client.endpoint().create(app_id, endpoint_in, None).await?;\n    Ok(endpoint_out)\n}\n\nasync fn create_message(client: Svix, app_id: String) -> anyhow::Result<MessageOut> {\n    let mut rng = StdRng::from_entropy();\n\n    let event_type = USER_EVENT_TYPES\n        .choose(&mut rng)\n        .context(\"Couldn't pick a random event type while creating a message\")?;\n\n    let message_in = MessageIn {\n        event_type: event_type.to_string(),\n        payload: json!({\n            \"userId\": \"41376126-35bf-4eda-81ef-83d741b0e026\",\n            \"firstName\": \"John\",\n            \"lastName\": \"Doe\",\n        }),\n        ..Default::default()\n    };\n\n    let message_out = client.message().create(app_id, message_in, None).await?;\n    Ok(message_out)\n}\n\nasync fn reset_application(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.application().list(None).await?;\n    let mut handles = Vec::new();\n\n    for app_out in resp.data {\n        let client = client.clone();\n\n        handles.push(tokio::spawn(async move {\n            if let Err(err) = client.application().delete(app_out.id.clone()).await {\n                eprintln!(\"Failed to delete application {}: {}\", app_out.id, err);\n            }\n        }));\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\nasync fn reset_event_type(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.event_type().list(None).await?;\n    let mut handles = Vec::new();\n\n    for event_type_out in resp.data {\n        let client = client.clone();\n\n        <|fim_suffix|>\n        handles.push(handle);\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\npub fn schema_example() -> serde_json::Value {\n    serde_json::json!({\n        \"1\": {\n            \"description\": \"A user signed up\",\n            \"properties\": {\n                \"userId\": {\n                    \"description\": \"The user id\",\n                    \"type\": \"string\"\n                },\n                \"firstName\": {\n                    \"description\": \"User first name\",\n                    \"type\": \"string\"\n                },\n                \"lastName\": {\n                    \"description\": \"User last name\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"userId\",\n                \"firstName\",\n                \"lastName\",\n            ],\n            \"title\": \"User signed up Event\",\n            \"type\": \"object\",\n        }\n    })\n}\n<|fim_middle|>", "completion": "let handle = tokio::spawn(async move {\n            let _ = client\n                .event_type()\n                .delete(\n                    event_type_out.name,\n                    Some(EventTypeDeleteOptions {\n                        expunge: Some(true),\n                    }),\n                )\n                .await;\n        });", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/seed.rs", "node_type": "let_declaration", "line_range": [213, 223]}
{"prompt": "<|fim_prefix|>\n}\n\npub async fn post_endpoint(\n    client: &TestClient,\n    app_id: &str,\n    ep: EndpointIn,\n) -> Result<EndpointOut> {\n    client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep,\n            StatusCode::CREATED,\n        )\n        .await\n}\n\npub async fn put_endpoint(\n    client: &TestClient,\n    app_id: &str,\n    ep_id: &str,\n    ep: EndpointIn,\n) -> Result<EndpointOut> {\n    client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n            ep,\n            StatusCode::OK,\n        )\n        .await\n}\n\n// Message\n\npub fn message_in<T: Serialize>(event_type: &str, payload: T) -> Result<MessageIn> {\n    Ok(MessageIn {\n        event_type: EventTypeName(event_type.to_owned()),\n        payload: RawPayload::from_string(serde_json::to_string(&payload)?)?,\n        payload_retention_period: 5,\n        channels: None,\n        uid: None,\n        extra_params: None,\n        application: None,\n    })\n}\n\npub async fn create_test_message(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n) -> Result<MessageOut> {\n    client\n        .post(\n            &format!(\"api/v1/app/{}/msg/\", &app_id),\n            message_in(\"event.type\", payload)?,\n            StatusCode::ACCEPTED,\n        )\n        .await\n}\n\npub async fn create_test_msg_with(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n    event_type: &str,\n    channel: impl IntoIterator<Item = &str>,\n) -> MessageOut {\n    let channels: HashSet<EventChannel> = channel\n        .into_iter()\n        .map(|x| EventChannel(x.to_string()))\n        .collect();\n\n    let mut message_in = json!({\n        \"eventType\": event_type,\n        \"payload\": payload,\n        \"payloadRetentionPeriod\": 5,\n    });\n    if !channels.is_empty() {\n        message_in[\"channels\"] = json!(channels);\n    }\n\n    client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in,\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap()\n}\n\npub fn event_type_in(\n    name: &str,\n    schema: impl Into<Option<serde_json::Value>>,\n) -> Result<EventTypeIn> {\n    Ok(EventTypeIn {\n        name: EventTypeName(name.to_owned()),\n        description: \"test-event-description\".to_owned(),\n        deleted: false,\n        schemas: schema.into().map(|s| serde_json::from_value(s).unwrap()),\n        feature_flag: None,\n        deprecated: false,\n    })\n}\n\n// Common tests\npub async fn common_test_list<\n    ModelOut: DeserializeOwned + PartialEq + std::fmt::Debug,\n    ModelIn: Serialize,\n>(\n    client: &TestClient,\n    path: &str,\n    create_model: fn(usize) -> ModelIn,\n    sort_asc: bool,\n    supports_reverse: bool,\n) -> Result<()> {\n    let mut items = Vec::new();\n    for i in 0..10 {\n        let item: ModelOut = client\n            .post(path, create_model(i), StatusCode::CREATED)\n            .await\n            .unwrap();\n        // Sleep for 5ms because KsuidMs has 4ms accuracy so things got out of order\n        tokio::time::sleep(Duration::from_millis(5)).await;\n        items.push(item);\n    }\n\n    let original_list = run_with_retries(|| async {\n        let list = client\n            .get::<ListResponse<ModelOut>>(&format!(\"{path}?with_content=true\"), StatusCode::OK)\n            .await\n            .unwrap();\n\n        assert_eq!(list.data.len(), 10);\n\n        Ok(list)\n    })\n    .await\n    .unwrap();\n\n    if sort_asc {\n        for i in 0..10 {\n            assert_eq!(items.get(i), original_list.data.get(i));\n        }\n    } else {\n        for i in 0..10 {\n            assert_eq!(items.get(9 - i), original_list.data.get(i));\n        }\n    }\n\n    // Limit results\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=1\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 1);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=50\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    l<|fim_suffix|>\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=6\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 6);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=6&iterator={}\", list.iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 4);\n    assert!(list.done);\n\n    let prev = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=3&iterator={}\", list.prev_iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(prev.data.len(), 3);\n    assert_eq!(\n        prev.data.first().unwrap(),\n        original_list.data.get(3).unwrap()\n    );\n\n    let _list = client\n        .get::<IgnoredAny>(\n            &format!(\"{path}?limit=6&iterator=BAD-$$$ITERATOR\"),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    if supports_reverse {\n        let opposite_order = if sort_asc { \"descending\" } else { \"ascending\" };\n\n        let opposite_1 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\"{path}?limit=3&order={opposite_order}\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let opposite_2 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\n                    \"{path}?limit=3&order={opposite_order}&iterator={}\",\n                    opposite_1.iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let opposite_prev = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\n                    \"{path}?limit=3&order={opposite_order}&iterator={}\",\n                    opposite_2.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for i in 0..3 {\n            assert_eq!(original_list.data.get(9 - i), opposite_1.data.get(i));\n            assert_eq!(original_list.data.get(6 - i), opposite_2.data.get(i));\n            assert_eq!(original_list.data.get(9 - i), opposite_prev.data.get(i));\n        }\n    }\n\n    // Test limits -- ten models were created previously and the default hard/soft cap is 250 so\n    // 10..251 is the sane range here.\n    for i in 10..251 {\n        let _: ModelOut = client\n            .post(path, create_model(i), StatusCode::CREATED)\n            .await\n            .unwrap();\n    }\n\n    // If limits are hard, it will be a 422 UNPROCESSABLE_ENTITY response, otherwise it'll be capped\n    // to 250\n    if client\n        .get::<IgnoredAny>(\n            &format!(\"{path}?limit=300\"),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .is_ok()\n        || client\n            .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=300\"), StatusCode::OK)\n            .await\n            .unwrap()\n            .data\n            .len()\n            == 250\n    {\n        Ok(())\n    } else {\n        panic!(\"Error with soft/hard caps to pagination limits\")\n    }\n}\n\npub async fn recover_webhooks(client: &TestClient, since: DateTime<Utc>, url: &str) {\n    client\n        .post_without_response(url, RecoverIn { since, until: None }, StatusCode::ACCEPTED)\n        .await\n        .unwrap();\n}\n\npub async fn get_msg_attempt_list_and_assert_count(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    msg_id: &MessageId,\n    expected_count: usize,\n) -> Result<ListResponse<MessageAttemptOut>> {\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{msg_id}/\"),\n                StatusCode::OK,\n            )\n            .await?;\n\n        if list.data.len() != expected_count {\n            anyhow::bail!(\n                \"Attempt count {} does not match expe<|fim_middle|>", "completion": "let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=10\"), StatusCode::OK)\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/common_calls.rs", "node_type": "let_declaration", "line_range": [252, 255]}
{"prompt": "<|fim_prefix|>0\\n- Manual = 1\",\n    Scheduled, Manual\n}\n\n#[repr(i16)]\n#[derive(Clone, Debug, Copy, PartialEq, IntoPrimitive, TryFromPrimitive, Hash, Eq)]\npub enum MessageStatus {\n    Success = 0,\n    Pending = 1,\n    Fail = 2,\n    Sending = 3,\n}\n\njsonschema_for_repr_enum! {\n    MessageStatus,\n    i16,\n    \"The sending status of the message:\\n- Success = 0\\n- Pending = 1\\n- Fail = 2\\n- Sending = 3\",\n    Success, Pending, Fail, Sending\n}\n\n#[derive(Clone, Debug, Copy, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub enum MessageStatusText {\n    Success,\n    Pending,\n    #[serde(alias = \"failed\")]\n    Fail,\n    Sending,\n}\n\nimpl From<MessageStatus> for MessageStatusText {\n    fn from(status: MessageStatus) -> Self {\n        match status {\n            MessageStatus::Success => Self::Success,\n            MessageStatus::Pending => Self::Pending,\n            MessageStatus::Fail => Self::Fail,\n            MessageStatus::Sending => Self::Sending,\n        }\n    }\n}\n\n#[repr(i16)]\n#[derive(Clone, Debug, Copy, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum StatusCodeClass {\n    CodeNone = 0,\n    Code1xx = 100,\n    Code2xx = 200,\n    Code3xx = 300,\n    Code4xx = 400,\n    Code5xx = 500,\n}\n\njsonschema_for_repr_enum! {\n    StatusCodeClass,\n    i16,\n    \"The different classes of HTTP status codes:\\n- CodeNone = 0\\n- Code1xx = 100\\n- Code2xx = 200\\n- Code3xx = 300\\n- Code4xx = 400\\n- Code5xx = 500\",\n    CodeNone, Code1xx, Code2xx, Code3xx, Code4xx, Code5xx\n}\n\nenum_wrapper!(MessageAttemptTriggerType);\nenum_wrapper!(MessageStatus);\nenum_wrapper!(StatusCodeClass);\n\n#[derive(Clone, Debug, Hash, Eq, PartialEq, Serialize)]\npub struct FeatureFlag(pub String);\n\ncommon_jsonschema_impl!(\n    FeatureFlag,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            min_length: None,\n            max_length: Some(256),\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"cool-new-feature\".to_string()),\n    }\n);\n\nstring_wrapper_impl!(FeatureFlag);\n\nimpl<'de> Deserialize<'de> for FeatureFlag {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).and_then(|s| {\n            validate_limited_str(&s).map_err(serde::de::Error::custom)?;\n            Ok(FeatureFlag(s))\n        })\n    }\n}\n\npub type FeatureFlagSet = HashSet<FeatureFlag>;\n\n#[cfg(test)]\nmod tests {\n    use std::collections::HashMap;\n\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use validator::Validate;\n\n    use super::{\n        validate_header_map, ApplicationId, ApplicationUid, EndpointHeaders, EndpointHeadersPatch,\n        EndpointSecret, EventChannel, EventTypeName,\n    };\n    use crate::core::cryptography::AsymmetricKey;\n\n    #[test]\n    fn test_id_validation() {\n        let app_id = ApplicationId(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_id.validate().unwrap();\n\n        let app_id = ApplicationId(\"badprefix_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_id.validate().is_err());\n\n        let app_uid = ApplicationUid(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_uid.validate().unwrap();\n\n        // With a space\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3 \".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        // Check all allowed\n        let app_uid = ApplicationUid(\"azAZ09-_.\".to_owned());\n        app_uid.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let app_id = ApplicationId(long_str.clone());\n        assert!(app_id.validate().is_err());\n        let app_uid = ApplicationUid(long_str);\n        assert!(app_uid.validate().is_err());\n\n        let empty_str: String = \"\".to_owned();\n        let app_id = ApplicationId(empty_str.clone());\n        assert!(app_id.validate().is_err());\n        l<|fim_suffix|>        assert!(app_uid.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_names_validation() {\n        // With a space\n        let evt_name = EventTypeName(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventTypeName(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let evt_name = EventTypeName(long_str);\n        assert!(evt_name.validate().is_err());\n\n        let empty_str = \"\".to_owned();\n        let evt_name = EventTypeName(empty_str);\n        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_channel_validation() {\n        // With a space\n        let evt_name = EventChannel(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventChannel(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let evt_name = EventChannel(long_str);\n        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), \"true\".to_owned()),\n            (\"also-valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        validate_header_map(&endpoint_headers.0).unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n            (\"also-valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        endpoint_headers.validate().unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n    }\n\n    #[test]\n    fn test_endpoint_secret_validation() {\n        let secret = EndpointSecret::Symmetric(STANDARD.decode(\"bm90LXZhbGlkCg==\").unwrap());\n        assert!(secret.validate().is_err());\n\n        let secret =\n            EndpointSecret::Symmetric(STANDA<|fim_middle|>", "completion": "let app_uid = ApplicationUid(empty_str);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "let_declaration", "line_range": [1463, 1463]}
{"prompt": "<|fim_prefix|>x<usize>>`]\n    /// that points to the count of the server such that its internal state may be monitored.\n    async fn start_service(\n        wait: Option<std::time::Duration>,\n    ) -> (JoinHandle<()>, String, Arc<Mutex<u16>>) {\n        dotenvy::dotenv().ok();\n\n        let cache = cache::memory::new();\n\n        let count = Arc::new(Mutex::new(0));\n\n        let listener = TcpListener::bind(\"127.0.0.1:0\").await.unwrap();\n        let endpoint = format!(\"http://{}/\", listener.local_addr().unwrap());\n\n        let jh = tokio::spawn({\n            let count = count.clone();\n            async move {\n                let svc = Router::new()\n                    .route(\"/\", post(service_endpoint).get(service_endpoint))\n                    .layer(\n                        ServiceBuilder::new().layer_fn(move |service| IdempotencyService {\n                            cache: cache.clone(),\n                            service,\n                        }),\n                    )\n                    .with_state(TestAppState { count, wait })\n                    .into_make_service();\n                serve(listener, svc).await.unwrap();\n            }\n        });\n\n        (jh, endpoint, count)\n    }\n\n    /// Only to be used via [`start_service`] -- this is the actual endpoint implementation\n    async fn service_endpoint(State(TestAppState { wait, count }): State<TestAppState>) -> String {\n        let mut count = count.lock().await;\n        *count += 1;\n\n        if let Some(wait) = wait {\n            tokio::time::sleep(wait).await;\n        }\n\n        format!(\"{count}\")\n    }\n\n    #[tokio::test]\n    async fn test_basic_idempotency() {\n        let (_jh, endpoint, count) = start_service(None).await;\n        let client = Client::new();\n\n        // Generate a new token so that keys are unique\n        dotenvy::dotenv().ok();\n        let cfg = crate::cfg::load().unwrap();\n        let token = generate_org_token(&cfg.jwt_signing_config, OrganizationId::new(None, None))\n            .unwrap()\n            .to_string();\n\n        // Sanity check on test service\n        assert_eq!(*count.lock().await, 0);\n        let _ = client.post(&endpoint).send().await;\n        assert_eq!(*count.lock().await, 1);\n\n        // Idempotency key not yet used -- should increment\n        let resp_1 = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"1\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await\n            .unwrap();\n        assert_eq!(*count.lock().await, 2);\n\n        // Now used the count should not increment\n        let resp_2 = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"1\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await\n            .unwrap();\n        assert_eq!(*count.lock().await, 2);\n\n        // And the responses should be equivalent\n        assert_eq!(resp_1.status(), resp_2.status());\n        //assert_eq!(resp_1.headers(), resp_2.headers());\n        assert_eq!(resp_1.text().await.unwrap(), resp_2.text().await.unwrap());\n\n        // No key -- should increment\n        let _ = client.post(&endpoint).send().await;\n        assert_eq!(*count.lock().await, 3);\n\n        // Same key -- should not increment\n        let _ = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"1\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await;\n        assert_eq!(*count.lock().await, 3);\n\n        // New key -- should increment\n        let resp_1 = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"2\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await\n            .unwrap();\n        assert_eq!(*count.lock().await, 4);\n\n        // Old key -- shouldn't increment\n        let _ = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"1\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await;\n        assert_eq!(*count.lock().await, 4);\n\n        // Key 2, shouldn't increment and should equal resp_1\n        l<|fim_suffix|>        assert_eq!(*count.lock().await, 4);\n\n        assert_eq!(resp_1.status(), resp_2.status());\n        assert_eq!(resp_1.headers(), resp_2.headers());\n        assert_eq!(resp_1.text().await.unwrap(), resp_2.text().await.unwrap());\n\n        // Key 2, but with GET should increment as it is not a POST request\n        let _ = client\n            .get(&endpoint)\n            .header(\"idempotency-key\", \"2\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await\n            .unwrap();\n        assert_eq!(*count.lock().await, 5);\n    }\n\n    #[tokio::test]\n    async fn test_lock() {\n        let sleep_duration = std::time::Duration::from_millis(300);\n\n        let (_jh, endpoint, _count) = start_service(Some(sleep_duration)).await;\n        let client = Client::new();\n\n        // Generate a new token so that keys are unique\n        dotenvy::dotenv().ok();\n        let cfg = crate::cfg::load().unwrap();\n\n        let token = generate_org_token(&cfg.jwt_signing_config, OrganizationId::new(None, None))\n            .unwrap()\n            .to_string();\n\n        let start = std::time::Instant::now();\n\n        let resp_1_jh = tokio::spawn(\n            client\n                .post(&endpoint)\n                .header(\"idempotency-key\", \"1\")\n                .header(\"Authorization\", &token)\n                .send(),\n        );\n\n        let resp_2_jh = tokio::spawn(\n            client\n                .post(&endpoint)\n                .header(\"idempotency-key\", \"1\")\n                .header(\"Authorization\", &token)\n                .send(),\n        );\n\n        let resp_1 = resp_1_jh.await.unwrap().unwrap();\n        let resp_1_instant = std::time::Instant::now();\n\n        let resp_2 = resp_2_jh.await.unwrap().unwrap();\n        let resp_2_instant = std::time::Instant::now();\n\n        // resp_1 should take some variable amount of time thanks to the sleep.\n        assert!(resp_1_instant - start >= sleep_duration);\n        // resp_2 should take less than the sleep (300) since it's just waiting for the same\n        // response given to the first request.\n        assert!(resp_2_instant - resp_1_instant < sleep_duration);\n\n        // And the responses should be equivalent\n        assert_eq!(resp_1.status(), resp_2.status());\n        //assert_eq!(resp_1.headers(), resp_2.headers());\n        assert_eq!(resp_1.text().await.unwrap(), resp_2.text().await.unwrap());\n    }\n\n    /// Starts a server just like [`start_service`] but it returns an empty body. The count is\n    /// recorded in the HTTP status code.\n    async fn start_empty_service() -> (JoinHandle<()>, String, Arc<Mutex<u16>>) {\n        dotenvy::dotenv().ok();\n\n        let cache = cache::memory::new();\n\n        let count = Arc::new(Mutex::new(199));\n\n        let listener = TcpListener::bind(\"127.0.0.1:0\").await.unwrap();\n        let endpoint = format!(\"http://{}/\", listener.local_addr().unwrap());\n\n        let jh = tokio::spawn({\n            let count = count.clone();\n            async move {\n                let svc = Router::new()\n                    .route(\"/\", post(empty_service_endpoint))\n                    .layer(\n                        ServiceBuilder::new().layer_fn(move |service| IdempotencyService {\n                            cache: cache.clone(),\n                            service,\n                        }),\n                    )\n                    .with_state(TestAppState { count, wait: None })\n                    .into_make_service();\n                serve(listener, svc).await.unwrap();\n            }\n        });\n\n        (jh, endpoint, count)\n    }\n\n    /// Only to be used via [`start_empty_service`] -- this is the actual endpoint implementation\n    async fn empty_service_endpoint(\n        State(TestAppState { count, .. }): State<TestAppState>,\n    ) -> StatusCode {\n        let mut count = count.lock().await;\n        *count += 1;\n\n        StatusCode::from_u16(*count).unwrap()\n    }\n\n    #[tokio::test]\n    async fn test_empty_body() {\n        let (_jh, endpoint, count) = start_empty_service().await;\n        let client = Client::new();\n\n        // Generate a new t<|fim_middle|>", "completion": "let resp_2 = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"2\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/idempotency.rs", "node_type": "let_declaration", "line_range": [504, 510]}
{"prompt": "<|fim_prefix|>r) -> bool {\n    (addr.segments()[0] == 0x2001) && (addr.segments()[1] == 0xdb8)\n}\n\n#[cfg(test)]\nmod tests {\n    use std::{\n        net::{IpAddr, TcpListener},\n        path::PathBuf,\n        str::FromStr,\n        sync::Arc,\n    };\n\n    use axum::{routing, Router};\n    use axum_server::tls_openssl::{OpenSSLAcceptor, OpenSSLConfig};\n    use http::{HeaderValue, Method, Version};\n    use ipnet::IpNet;\n\n    use super::{is_allowed, CaseSensitiveHeaderMap, RequestBuilder, WebhookClient};\n\n    #[test]\n    fn is_allowed_test() {\n        // Copied shamelessly from the standard library `is_global` docs\n        assert!(!is_allowed(IpAddr::from([10, 254, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([192, 168, 10, 65])));\n        assert!(!is_allowed(IpAddr::from([172, 16, 10, 65])));\n        assert!(!is_allowed(IpAddr::from([0, 1, 2, 3])));\n        assert!(!is_allowed(IpAddr::from([0, 0, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([127, 0, 0, 1])));\n        assert!(!is_allowed(IpAddr::from([169, 254, 45, 1])));\n        assert!(!is_allowed(IpAddr::from([255, 255, 255, 255])));\n        assert!(!is_allowed(IpAddr::from([192, 0, 2, 255])));\n        assert!(!is_allowed(IpAddr::from([198, 51, 100, 65])));\n        assert!(!is_allowed(IpAddr::from([203, 0, 113, 6])));\n        assert!(!is_allowed(IpAddr::from([100, 100, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([192, 0, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([192, 0, 0, 255])));\n        assert!(!is_allowed(IpAddr::from([250, 10, 20, 30])));\n        assert!(!is_allowed(IpAddr::from([198, 18, 0, 0])));\n\n        assert!(is_allowed(IpAddr::from([1, 1, 1, 1])));\n\n        assert!(!is_allowed(IpAddr::from([0, 0, 0, 0, 0, 0, 0, 0x1])));\n\n        assert!(is_allowed(IpAddr::from([0, 0, 0, 0xffff, 0, 0, 0, 0x1])));\n        assert!(is_allowed(\n            \"2001:4860:4860::8888\".parse::<IpAddr>().unwrap()\n        ));\n        assert!(is_allowed(\"::ffff:8.8.8.8\".parse::<IpAddr>().unwrap()));\n        assert!(!is_allowed(\"::ffff:127.0.0.1\".parse::<IpAddr>().unwrap()));\n        assert!(!is_allowed(\"::ffff:0.0.0.1\".parse::<IpAddr>().unwrap()));\n    }\n\n    #[test]\n    fn test_builder() {\n        match RequestBuilder::new().build() {\n            Err(e) => assert_eq!(\"Build failed: uri missing; version missing\", e.to_string()),\n            Ok(_) => panic!(),\n        }\n\n        assert!(RequestBuilder::new()\n            .version(Version::HTTP_11)\n            .build()\n            .is_err());\n\n        assert!(RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .is_ok());\n    }\n\n    #[test]\n    fn test_header_casings() {\n        let hdrs = CaseSensitiveHeaderMap::from([(\n            \"tEsT-header-1\".to_owned(),\n            HeaderValue::from_static(\"value\"),\n        )]);\n\n        let req = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .headers(hdrs)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req.header_names\n                .unwrap()\n                .get(\"test-header-1\".parse().unwrap())\n                .unwrap(),\n            \"tEsT-header-1\".as_bytes()\n        );\n        assert_eq!(\n            req.headers.get(\"test-header-1\").unwrap(),\n            HeaderValue::from_static(\"value\")\n        );\n    }\n\n    #[test]\n    fn test_url_basic_auth() {\n        let req = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://test:123@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req.headers.get(\"authorization\").unwrap(),\n            \"Basic dGVzdDoxMjM=\".as_bytes()\n        );\n\n        let req_user_only = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://test:@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_user_only.headers.get(\"authorization\").unwrap(),\n            \"Basic dGVzdDo=\".as_bytes()\n        );\n\n        let req_pass_only = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://:123@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_pass_only.headers.get(\"authorization\").unwrap(),\n            \"Basic OjEyMw==\".as_bytes()\n        );\n\n        let req_no_basic_auth = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert!(req_no_basic_auth.headers.get(\"authorization\").is_none());\n\n        let req_special_chars = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://test==:123==@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_special_chars.headers.get(\"authorization\").unwrap(),\n            \"Basic dGVzdD09OjEyMz09\".as_bytes()\n        );\n    }\n\n    #[test]\n    fn test_host_header() {\n        let req = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(req.headers.get(\"host\").unwrap(), \"127.0.0.1\".as_bytes());\n\n        let req_with_port = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1:8000/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_with_port.headers.get(\"host\").unwrap(),\n            \"127.0.0.1:8000\".as_bytes()\n        );\n    }\n\n    #[tokio::test]\n    async fn test_tls_verification_disable() {\n        // Self-signed certificates are expected to be found in `server/svix-server/tests/static` from\n        // the repository root.\n        //\n        // Some have been pre-generated into that directory via the following command:\n        //\n        // ```\n        // openssl req -x509 -newkey rsa:4096 -keyout ex_key.pem -out ex_cert.pem -sha256 \\\n        // -days 36500 -nodes\n        // ```\n        //\n        // Then, via the interactive prompt, a `.` was entered for all fields but the common name,\n        // which was set to `localhost`.\n        //\n        // NOTE: It doesn't really matter the contents of these files as long as they are a valid key\n        // and certificate that is self-signed, expired, or otherwise unable to pass verification.\n        let dir: PathBuf = [env!(\"CARGO_MANIFEST_DIR\"), \"tests\", \"static\"]\n            .iter()\n            .collect();\n        let config =\n            OpenSSLConfig::from_pem_file(dir.join(\"ex_cert.pem\"), dir.join(\"ex_key.pem\")).unwrap();\n        <|fim_suffix|>\n\n        let tcp = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        let url = format!(\"https://{}/\", tcp.local_addr().unwrap());\n\n        let app = Router::new().route(\"/\", routing::any(|| async { \"Hello\" }));\n\n        let _jh = tokio::spawn(async {\n            axum_server::from_tcp(tcp)\n                .acceptor(acceptor)\n                .serve(app.into_make_service())\n                .await\n                .unwrap();\n        });\n\n        let request = RequestBuilder::new()\n            .method(Method::GET)\n            .uri_str(&url)\n            .unwrap()\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        let whitelist = Arc::new(vec![IpNet::new(\"127.0.0.1\".parse().unwrap(), 0).unwrap()]);\n\n        // Assert that a [`WebhookClient`] without the disabled flag will err on making to a request\n        // to this server with the self-signed certificate\n        let whc_with_validation = WebhookClient::new(Some(whitelist.clone()), None, false, None);\n        assert!(whc_with_validation.execute(request.clone()).await.is_err());\n\n        // And assert that when the flag is enabled, that it will succeed\n        let whc_without_validation = WebhookClient::new(Some(whitelist), None, true, None);\n        assert!(whc_without_validation.execute(request).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "let acceptor = OpenSSLAcceptor::new(config);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [932, 932]}
{"prompt": "<|fim_prefix|>usCode::BAD_REQUEST\n                    })?)\n                }\n                TransformerInputFormat::Json => {\n                    TransformerInput::Json(payload.as_json().map_err(|_| {\n                        tracing::error!(\"Unable to parse request body as json\");\n                        http::StatusCode::BAD_REQUEST\n                    })?)\n                }\n            };\n            transform(input, xform.source().clone(), transformer_tx).await\n        }\n        // Keep the original payload as-is if there's no transformation specified, but stuff the\n        // whole thing into the payload field.\n        // The as_json() only gets us to `Value`, so we also need a `from_value` call to marshal\n        // into a [`ForwardRequest`] type.\n        None => Ok(ForwardRequest {\n            payload: payload.as_json().map_err(|_| {\n                tracing::error!(\"Unable to parse request body as json\");\n                http::StatusCode::BAD_REQUEST\n            })?,\n        }),\n    }\n}\n\n/// Attempts to run the payload through a js transformation.\nasync fn transform(\n    input: TransformerInput,\n    script: String,\n    tx: TransformerTx,\n) -> Result<ForwardRequest, http::StatusCode> {\n    let (job, callback) = TransformerJob::new(script, input);\n    if let Err(e) = tx.send(job) {\n        tracing::error!(\"transformations are not available: {}\", e);\n        return Err(http::StatusCode::INTERNAL_SERVER_ERROR);\n    }\n\n    match callback.await {\n        // This is the only \"good\" outcome giving a RHS value for the assignment.\n        // All other match arms should bail with a non-2xx status.\n        Ok(Ok(TransformerOutput::Object(obj))) => Ok(serde_json::from_value(\n            serde_json::Value::Object(obj),\n        )\n        .map_err(|e| {\n            tracing::error!(\"transformation produced invalid payload: {}\", e);\n            http::StatusCode::INTERNAL_SERVER_ERROR\n        })?),\n        Ok(Ok(TransformerOutput::Invalid)) => {\n            tracing::error!(\"transformation produced invalid payload\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n        _ => {\n            tracing::error!(\"transformation failed\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n    }\n}\n\nstruct SvixEventsPoller {\n    name: String,\n    input_opts: PollerInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n    output: Arc<Box<dyn ReceiverOutput>>,\n}\n\n#[async_trait]\nimpl PollerInput for SvixEventsPoller {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        run_inner(self).await\n    }\n}\n\nimpl PollerReceiverConfig {\n    pub async fn into_poller_input(\n        self,\n        transformer_tx: TransformerTx,\n    ) -> std::io::Result<Box<dyn PollerInput>> {\n        let svix_client = self\n            .input\n            .svix_client()\n            .expect(\"only one poller type; svix client required\");\n        let name = self.name.clone();\n        let input_opts = self.input.clone();\n        let transformation = self.transformation.clone();\n        let output = Arc::new(\n            self.into_receiver_output()\n                .await\n                .map_err(std::io::Error::other)?,\n        );\n        Ok(Box::new(SvixEventsPoller {\n            name,\n            input_opts,\n            transformation,\n            transformer_tx: Some(transformer_tx.clone()),\n            svix_client,\n            output,\n        }))\n    }\n}\n\nasync fn run_inner(poller: &SvixEventsPoller) -> ! {\n    const MIN_SLEEP: Duration = Duration::from_millis(10);\n    const MAX_SLEEP: Duration = Duration::from_secs(300);\n    const NO_SLEEP: Duration = Duration::ZERO;\n    let mut sleep_time = NO_SLEEP;\n\n    let PollerInputOpts::SvixMessagePoller {\n        consumer_id,\n        token: _,\n        app_id,\n        sink_id,\n        svix_options: _,\n    } = &poller.input_opts;\n\n    let mut iterator = None;\n\n    'outer: loop {\n        tracing::trace!(app_id, sink_id, \"polling poller\");\n        match poller\n            .svix_client\n            .message()\n            .poller()\n            .consumer_poll(\n                app_id.clone(),\n                sink_id.clone(),\n                consumer_id.clone(),\n                Some(MessagePollerConsumerPollOptions {\n                    limit: None,\n                    iterator: iterator.clone(),\n                }),\n            )\n            .await\n        {\n            Ok(resp) => {\n                let mut has_failure = false;\n                tracing::trace!(count = resp.data.len(), \"got messages\");\n                for msg in resp.data {\n                    let msg_id = msg.id.clone();\n                    <|fim_suffix|>\n                }\n\n                // Retry the current iterator if we see failures while handling any of the messages\n                // in the batch.\n                if has_failure {\n                    // BACKOFF\n                    sleep_time = (sleep_time * 2).clamp(MIN_SLEEP, MAX_SLEEP);\n                } else {\n                    tracing::trace!(\n                        ?iterator,\n                        next_iterator = ?resp.iterator,\n                        \"batch handled, updating local iterator\"\n                    );\n                    // Update the iterator _after we've handled all the messages in the batch_.\n                    iterator = Some(resp.iterator.clone());\n                    // If the iterator is \"done\" we can backoff to wait for new messages to arrive.\n                    sleep_time = if resp.done {\n                        // BACKOFF\n                        (sleep_time * 2).clamp(MIN_SLEEP, MAX_SLEEP)\n                    } else {\n                        NO_SLEEP\n                    };\n                }\n            }\n\n            Err(err) => {\n                match &err {\n                    Error::Http(x)\n                        if x.status.as_u16() == http::StatusCode::BAD_REQUEST.as_u16() =>\n                    {\n                        if let Some(\"invalid_iterator\") =\n                            x.payload.as_ref().map(|p| p.code.as_str())\n                        {\n                            tracing::error!(\n                                error = ?err,\n                                ?iterator,\n                                \"request failed, iterator is invalid syncing with server...\"\n                            );\n                            iterator = None;\n                            continue 'outer;\n                        }\n                    }\n                    _ => {}\n                }\n\n                tracing::error!(\n                    error = ?err,\n                    ?iterator,\n                    \"request failed, retrying current iterator\"\n                );\n                // BACKOFF\n                sleep_time = (sleep_time * 2).clamp(MIN_SLEEP, MAX_SLEEP);\n            }\n        }\n\n        if !sleep_time.is_zero() {\n            tracing::trace!(?sleep_time, \"sleeping\");\n            tokio::time::sleep(sleep_time).await;\n        }\n    }\n}\n\n#[tracing::instrument(skip_all, fields(msg_id = msg.id))]\nasync fn handle_poller_msg(\n    msg: PollingEndpointMessageOut,\n    poller: &SvixEventsPoller,\n) -> Result<(), (u16, &'static str)> {\n    let payload = parse_payload(\n        &SerializablePayload::Standard(\n            // FIXME: for svix-event pollers we already know the payload is json so\n            //   there's some wasted ser/deser/ser cycles.\n            serde_json::to_vec(&msg).expect(\"just fetched as json, must be serializable\"),\n        ),\n        poller.transformation.as_ref(),\n        poller\n            .transformer_tx\n            .clone()\n            .expect(\"transformer tx is required\"),\n    )\n    .await\n    .map_err(|status| (status.as_u16(), \"error while parsing polled message\"))?;\n\n    handle(payload, Arc::clone(&poller.output))\n        .await\n        // FIXME: need to refactor handle to not give http status codes so we can report what happened here.\n        .map_err(|status| (status.as_u16(), \"error while handling polled message\"))?;\n\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests;\n<|fim_middle|>", "completion": "if let Err((status, message)) = handle_poller_msg(msg, poller).await {\n                        tracing::error!(msg_id, status, message);\n                        has_failure = true;\n                    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/mod.rs", "node_type": "if_expression", "line_range": [324, 327]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for crating `tracing` spans compatible with OpenTelemetry's\n//! conventions.\nu<|fim_suffix|>\nuse axum::extract::{ConnectInfo, MatchedPath};\nuse http::header;\nuse opentelemetry::trace::TraceContextExt;\nuse svix_ksuid::{KsuidLike, KsuidMs};\nuse tower_http::{\n    classify::ServerErrorsFailureClass,\n    trace::{MakeSpan, OnFailure, OnResponse},\n};\nuse tracing::field::{debug, Empty};\nuse tracing_opentelemetry::OpenTelemetrySpanExt;\n\n/// An implementor of [`MakeSpan`] which creates `tracing` spans populated with information about\n/// the request received by an `axum` web server.\n#[derive(Clone, Copy)]\npub struct AxumOtelSpanCreator;\n\nimpl<B> MakeSpan<B> for AxumOtelSpanCreator {\n    fn make_span(&mut self, request: &http::Request<B>) -> tracing::Span {\n        let user_agent = request\n            .headers()\n            .get(header::USER_AGENT)\n            .and_then(|header| header.to_str().ok());\n\n        let host = request\n            .headers()\n            .get(header::HOST)\n            .and_then(|header| header.to_str().ok());\n\n        let http_route = request\n            .extensions()\n            .get::<MatchedPath>()\n            .map(|p| p.as_str());\n\n        let client_ip = request\n            .extensions()\n            .get::<ConnectInfo<SocketAddr>>()\n            .map(|ConnectInfo(ip)| debug(ip));\n\n        let request_id = request\n            .headers()\n            .get(\"x-request-id\")\n            .and_then(|id| id.to_str().map(ToOwned::to_owned).ok())\n            // If `x-request-id` isn't set, check `svix-req-id`. If the `svix-req-id` isn't a\n            // valid `str`, or it isn't set, then fallback to a random [`KsuidMs`]\n            .or_else(|| {\n                request\n                    .headers()\n                    .get(\"svix-req-id\")\n                    .and_then(|v| v.to_str().map(ToOwned::to_owned).ok())\n            })\n            .unwrap_or_else(|| KsuidMs::new(None, None).to_string());\n\n        let remote_context = opentelemetry::global::get_text_map_propagator(|p| {\n            p.extract(&opentelemetry_http::HeaderExtractor(request.headers()))\n        });\n        let remote_span = remote_context.span();\n        let span_context = remote_span.span_context();\n        let trace_id = span_context\n            .is_valid()\n            .then(|| span_context.trace_id().to_string());\n\n        let idempotency_key = request\n            .headers()\n            .get(\"idempotency-key\")\n            .and_then(|v| v.to_str().ok());\n\n        let span = tracing::error_span!(\n            \"HTTP request\",\n            grpc.code = Empty,\n            http.client_ip = client_ip,\n            http.versions = ?request.version(),\n            http.host = host,\n            http.method = ?request.method(),\n            http.route = http_route,\n            http.scheme = request.uri().scheme().map(debug),\n            http.status_code = Empty,\n            http.target = request.uri().path_and_query().map(|p| p.as_str()),\n            http.user_agent = user_agent,\n            otel.kind = \"server\",\n            otel.status_code = Empty,\n            request_id,\n            trace_id,\n            idempotency_key,\n            org_id = tracing::field::Empty,\n            app_id = tracing::field::Empty,\n        );\n\n        span.set_parent(remote_context);\n\n        span\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnResponse;\n\nimpl<B> OnResponse<B> for AxumOtelOnResponse {\n    fn on_response(\n        self,\n        response: &http::Response<B>,\n        latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        let status = response.status().as_u16().to_string();\n        span.record(\"http.status_code\", tracing::field::display(status));\n        span.record(\"otel.status_code\", \"OK\");\n\n        tracing::debug!(\n            \"finished processing request latency={} ms status={}\",\n            latency.as_millis(),\n            response.status().as_u16(),\n        );\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnFailure;\n\nimpl OnFailure<ServerErrorsFailureClass> for AxumOtelOnFailure {\n    fn on_failure(\n        &mut self,\n        failure_classification: ServerErrorsFailureClass,\n        _latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        match failure_classification {\n            ServerErrorsFailureClass::StatusCode(status) if status.is_server_error() => {\n                span.record(\"otel.status_code\", \"ERROR\");\n            }\n            _ => {}\n        }\n    }\n}\n<|fim_middle|>", "completion": "use std::net::SocketAddr;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/otel_spans.rs", "node_type": "use_declaration", "line_range": [6, 6]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct StreamingStreamListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct StreamingStreamCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct StreamingStream<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> StreamingStream<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List of all the organization's streams.\n    pub async fn list(\n        &self,\n        options: Option<StreamingStreamListOptions>,\n    ) -> Result<ListResponseStreamOut> {\n        let StreamingStreamListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/stream\")\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Creates a new stream.\n    pub async fn create(\n        &self,\n        stream_in: StreamIn,\n        options: Option<StreamingStreamCreateOptions>,\n    ) -> Result<StreamOut> {\n        let StreamingStreamCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/stream\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(stream_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get a stream by id or uid.\n    pub async fn get(&self, stream_id: String) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Update a stream.\n    pub async fn update(&self, stream_id: String, stream_in: StreamIn) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_body_param(stream_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete a stream.\n    <|fim_suffix|>\n\n    /// Partially update a stream.\n    pub async fn patch(&self, stream_id: String, stream_patch: StreamPatch) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::PATCH, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_body_param(stream_patch)\n            .execute(self.cfg)\n            .await\n    }\n}\n<|fim_middle|>", "completion": "pub async fn delete(&self, stream_id: String) -> Result<()> {\n        crate::request::Request::new(http1::Method::DELETE, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .returns_nothing()\n            .execute(self.cfg)\n            .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/streaming_stream.rs", "node_type": "function_item", "line_range": [82, 88]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct StreamingStreamListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct StreamingStreamCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct StreamingStream<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> StreamingStream<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List of all the organization's streams.\n    pub async fn list(\n        &self,\n        options: Option<StreamingStreamListOptions>,\n    ) -> Result<ListResponseStreamOut> {\n        let StreamingStreamListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/stream\")\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Creates a new stream.\n    pub async fn create(\n        &self,\n        stream_in: StreamIn,\n        options: Option<StreamingStreamCreateOptions>,\n    ) -> Result<StreamOut> {\n        let StreamingStreamCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/stream\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(stream_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get a stream by id or uid.\n    <|fim_suffix|>\n\n    /// Update a stream.\n    pub async fn update(&self, stream_id: String, stream_in: StreamIn) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_body_param(stream_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete a stream.\n    pub async fn delete(&self, stream_id: String) -> Result<()> {\n        crate::request::Request::new(http1::Method::DELETE, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .returns_nothing()\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Partially update a stream.\n    pub async fn patch(&self, stream_id: String, stream_patch: StreamPatch) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::PATCH, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_body_param(stream_patch)\n            .execute(self.cfg)\n            .await\n    }\n}\n<|fim_middle|>", "completion": "pub async fn get(&self, stream_id: String) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .execute(self.cfg)\n            .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/streaming_stream.rs", "node_type": "function_item", "line_range": [65, 70]}
{"prompt": "<|fim_prefix|>e.\n    Normal(T),\n    /// Reversed iteration - forwards in time.\n    Prev(T),\n}\n\nimpl<T: Validate> ReversibleIterator<T> {\n    pub(crate) fn direction(&self) -> IteratorDirection {\n        match self {\n            Self::Normal(_) => IteratorDirection::Normal,\n            Self::Prev(_) => IteratorDirection::Prev,\n        }\n    }\n}\n\nimpl<'de, T: 'static + Deserialize<'de> + Validate + From<String>> Deserialize<'de>\n    for ReversibleIterator<T>\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).map(|s| {\n            if let Some(s) = s.strip_prefix('-') {\n                ReversibleIterator::Prev(T::from(s.to_owned()))\n            } else {\n                ReversibleIterator::Normal(T::from(s))\n            }\n        })\n    }\n}\n\nimpl<T: Validate> Validate for ReversibleIterator<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            ReversibleIterator::Normal(val) => val.validate(),\n            ReversibleIterator::Prev(val) => val.validate(),\n        }\n    }\n}\n\nimpl<T: Validate + JsonSchema> JsonSchema for ReversibleIterator<T> {\n    fn schema_name() -> String {\n        format!(\"ReversibleIterator_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        T::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\n/// Applies sorting and filtration to a query from its iterator, sort column, and limit\n/// queries based on time\n/// Our rules for limiting queries are as follows\n///\n/// If `before` is passed:\n/// * lower limit on query is `before - LIMITED_QUERY_DURATION`\n/// * upper limit is `before`\n///\n/// If `after` is passed:\n/// * lower limit is `after`\n/// * upper limit is `now + FUTURE_QUERY_LIMIT`\n///\n/// If prev-iterator is passed:\n/// * lower limit is `prev-iterator`\n/// * upper limit is `prev-iterator + LIMITED_QUERY_DURATION`\n///\n/// If (normal) iterator is passed:\n/// * lower limit is `iterator - LIMITED_QUERY_DURATION`\n/// * upper limit is `iterator`\n///\n/// If no iterator is passed:\n/// * lower limit is `now() - LIMITED_QUERY_DURATION` if\n///   neither `before` nor `after` were passed\npub(crate) fn filter_and_paginate_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }\n\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            i<|fim_suffix|>        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket limit on future\n        .filter(sort_column.lt(I::start_id(future_limit)));\n\n    (query, iter_direction)\n}\n\n/// Marker trait for any type that is used for iterating through results\n/// in the public API.\npub trait IdIterator: Validate + Into<sea_orm::Value> {}\n\nimpl<T: BaseId + Validate + Into<sea_orm::Value>> IdIterator for T {}\nimpl IdIterator for EventTypeName {}\n\npub fn apply_pagination<\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    C: ColumnTrait,\n    I: IdIterator,\n>(\n    query: Q,\n    sort_column: C,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    ordering: Ordering,\n) -> Q {\n    use Ordering::*;\n    use ReversibleIterator::*;\n\n    // Query for an extra element to be able to tell whether there's more\n    // data than the user requested.\n    let query = query.limit(limit + 1);\n\n    let iterator = if let Some(it) = iterator {\n        it\n    } else {\n        return match ordering {\n            Ascending => query.order_by_asc(sort_column),\n            Descending => query.order_by_desc(sort_column),\n        };\n    };\n\n    match (iterator, ordering) {\n        (Prev(id), Ascending) | (Normal(id), Descending) => {\n            query.order_by_desc(sort_column).filter(sort_column.lt(id))\n        }\n        (Prev(id), Descending) | (Normal(id), Ascending) => {\n            query.order_by_asc(sort_column).filter(sort_column.gt(id))\n        }\n    }\n}\n\n/// A response with no body content and a specific response code, specified by\n/// the generic parameter `N`.\npub struct NoContentWithCode<const N: u16>;\n\nimpl<const N: u16> IntoResponse for NoContentWithCode<N> {\n    fn into_response(self) -> axum::response::Response {\n        (StatusCode::from_u16(N).unwrap(), ()).into_response()\n    }\n}\n\nimpl<const N: u16> OperationOutput for NoContentWithCode<N> {\n    type Inner = Self;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        <() as OperationOutput>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(response) = Self::operation_response(ctx, operation) {\n            vec![(Some(N), response)]\n        } else {\n            vec![]\n        }\n    }\n}\n\n/// A response with no body content and HTTP status code 204, the standard code\n/// for such responses.\n#[derive(OperationIo)]\n#[aide(output_with = \"()\")]\npub struct NoContent;\n\nimpl IntoResponse for NoContent {\n    fn into_response(self) -> axum::response::Response {\n        NoContentWithCode::<204>::into_response(NoContentWithCode)\n    }\n}\n\n#[derive(Serialize, JsonSchema)]\npub struct EmptyResponse {}\n\n// If you change the internal representation of this then you must also update\n// it in the `JsonSchema` impl below to match.\n#[derive(Serialize, Deserialize, Clone)]\n#[serde(rename_all = \"camelCase\")]\npub struct ListResponse<T> {\n    pub data: Vec<T>,\n    pub iterator: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub prev_iterator: Option<String>,\n    pub done: bool,\n}\n\nimpl<T> ListResponse<T> {\n    pub fn empty() -> Self {\n        Self {\n            data: Vec::new(),\n            iterator: None,\n            prev_iterator: None,\n            done: true,\n        }\n    }\n}\n\n// This custom impl is needed because we want to customize the name of the\n// schema that goes into the spec, but that can only be done by having a custom\n// `JsonSchema` implementation.\n// Tracking issue: https://github.com/GREsau/schemars/issues/193\nimpl<T: JsonSchema> JsonSchema for ListResponse<T> {\n    fn schema_na<|fim_middle|>", "completion": "if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "if_expression", "line_range": [251, 253]}
{"prompt": "<|fim_prefix|>ashSet<EventTypeName>);\njson_wrapper!(EventTypeNameSet);\n\nimpl Validate for EventTypeNameSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\npub struct ExpiringSigningKeys(pub Vec<ExpiringSigningKey>);\njson_wrapper!(ExpiringSigningKeys);\n\nimpl ExpiringSigningKeys {\n    pub const MAX_OLD_KEYS: usize = 10;\n    pub const OLD_KEY_EXPIRY_HOURS: i64 = 24;\n}\n\n/// The type of encryption key\n#[repr(u8)]\n#[derive(Clone, Debug, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum EndpointSecretType {\n    Hmac256 = 1,\n    Ed25519 = 2,\n    // Reserved = 3,\n}\n\nimpl EndpointSecretType {\n    pub const fn secret_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whsk_\",\n        }\n    }\n\n    pub const fn public_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whpk_\",\n        }\n    }\n}\n\n/// Properties of the encryption key\n#[derive(Clone, Debug, PartialEq, Eq)]\nstruct EndpointSecretMarker {\n    type_: EndpointSecretType,\n    encrypted: bool,\n}\n\nimpl EndpointSecretMarker {\n    const ENCRYPTED_FLAG: u8 = 0b1000_0000;\n\n    fn from_u8(v: u8) -> crate::error::Result<Self> {\n        let encrypted = (v & Self::ENCRYPTED_FLAG) != 0;\n        let v = v & !Self::ENCRYPTED_FLAG;\n        let type_ = EndpointSecretType::try_from(v)\n            .map_err(|_| crate::error::Error::generic(\"Invalid marker value\"))?;\n\n        Ok(Self { type_, encrypted })\n    }\n\n    fn to_u8(&self) -> u8 {\n        let mut ret = self.type_.clone().into();\n        if self.encrypted {\n            ret |= Self::ENCRYPTED_FLAG;\n        }\n        ret\n    }\n\n    fn type_(&self) -> &EndpointSecretType {\n        &self.type_\n    }\n}\n\n/// The internal representation of the endpoint secret.\n/// This is used to store it securely in the database and cache, and to ensure it doesn't get\n/// sent externally.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub struct EndpointSecretInternal {\n    marker: EndpointSecretMarker,\n\n    key: Vec<u8>,\n}\n\nimpl EndpointSecretInternal {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MINUS_ONE: usize = Self::KEY_SIZE - 1;\n\n    fn new(\n        encryption: &Encryption,\n        type_: EndpointSecretType,\n        key: &[u8],\n    ) -> crate::error::Result<Self> {\n        Ok(Self {\n            marker: EndpointSecretMarker {\n                type_,\n                encrypted: encryption.enabled(),\n            },\n            key: encryption.encrypt(key)?,\n        })\n    }\n\n    pub fn generate_symmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let buf: [u8; Self::KEY_SIZE] = rand::thread_rng().gen();\n        Self::new(encryption, EndpointSecretType::Hmac256, &buf)\n    }\n\n    pub fn generate_asymmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let key = AsymmetricKey::generate();\n        Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())\n    }\n\n    fn into_vec(mut self) -> Vec<u8> {\n        let marker: u8 = self.marker.to_u8();\n\n        let mut vec = vec![marker];\n        vec.append(&mut self.key);\n        vec\n    }\n\n    fn from_vec(v: Vec<u8>) -> crate::error::Result<Self> {\n        // Legacy had exact size\n        match v.len() {\n            0..=Self::KEY_SIZE_MINUS_ONE => Err(crate::error::Error::generic(\"Value too small\")),\n            Self::KEY_SIZE => Ok(Self {\n                marker: EndpointSecretMarker {\n                    type_: EndpointSecretType::Hmac256,\n                    encrypted: false,\n                },\n                key: v,\n            }),\n            _ => {\n                l<|fim_suffix|>                Ok(Self {\n                    marker,\n                    key: v[1..].to_vec(),\n                })\n            }\n        }\n    }\n\n    pub fn into_endpoint_secret(\n        self,\n        encryption: &Encryption,\n    ) -> crate::error::Result<EndpointSecret> {\n        let key = self.key(encryption)?;\n        Ok(match self.type_() {\n            EndpointSecretType::Hmac256 => EndpointSecret::Symmetric(key),\n            EndpointSecretType::Ed25519 => {\n                EndpointSecret::Asymmetric(AsymmetricKey::from_slice(&key[..])?)\n            }\n        })\n    }\n\n    pub fn from_endpoint_secret(\n        endpoint_secret: EndpointSecret,\n        encryption: &Encryption,\n    ) -> crate::error::Result<Self> {\n        Ok(match endpoint_secret {\n            EndpointSecret::Symmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Hmac256, &key)?\n            }\n            EndpointSecret::Asymmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())?\n            }\n        })\n    }\n\n    pub fn sign(&self, encryption: &Encryption, bytes: &[u8]) -> Vec<u8> {\n        let key = self.key(encryption).unwrap();\n        // FIXME: remove unwrap\n        match self.marker.type_() {\n            EndpointSecretType::Hmac256 => hmac_sha256::HMAC::mac(bytes, key).to_vec(),\n            EndpointSecretType::Ed25519 => AsymmetricKey::from_slice(&key[..])\n                .unwrap()\n                .0\n                .sk\n                .sign(bytes, None)\n                .to_vec(),\n        }\n    }\n\n    fn key(&self, encryption: &Encryption) -> crate::error::Result<Vec<u8>> {\n        Ok(if self.marker.encrypted {\n            if encryption.enabled() {\n                encryption.decrypt(&self.key)?\n            } else {\n                return Err(crate::error::Error::generic(\n                    \"main_secret unset, can't decrypt key\",\n                ));\n            }\n        } else {\n            self.key.to_vec()\n        })\n    }\n\n    pub fn type_(&self) -> &EndpointSecretType {\n        self.marker.type_()\n    }\n}\n\nimpl Serialize for EndpointSecretInternal {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        serializer.serialize_str(&STANDARD.encode(self.clone().into_vec()))\n    }\n}\n\nimpl<'de> Deserialize<'de> for EndpointSecretInternal {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n\n        String::deserialize(deserializer).and_then(|string| {\n            // For backwards compat when loading from ExpiringSigningKeys. Going forward we just b64 it\n            if string.starts_with(EndpointSecretType::Hmac256.secret_prefix()) {\n                Ok(Self {\n                    marker: EndpointSecretMarker {\n                        type_: EndpointSecretType::Hmac256,\n                        encrypted: false,\n                    },\n                    key: string\n                        .get(EndpointSecretType::Hmac256.secret_prefix().len()..)\n                        .ok_or_else(|| Error::custom(\"invalid prefix\".to_string()))\n                        .and_then(|string| {\n                            STANDARD\n                                .decode(string)\n                                .map_err(|err| Error::custom(err.to_string()))\n                        })?,\n                })\n            } else {\n                let buf = STANDARD\n                    .decode(string)\n                    .map_err(|err| Error::custom(err.to_string()))?;\n                Self::from_vec(buf).map_err(|err| Error::custom(err.to_string()))\n            }\n        })\n    }\n}\n\nimpl From<EndpointSecretInternal> for sea_orm::Value {\n    fn from(v: EndpointSecretInternal) -> Self {\n        Self::Bytes(Some(Box::new(v.into_vec())))\n    }\n}\n\nimpl sea_orm::TryGetable for EndpointSecretInternal {\n    fn try_get_by<I: sea_orm::ColIdx>(\n        res: &sea_orm::QueryResult,\n        index: I,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec<|fim_middle|>", "completion": "let marker = EndpointSecretMarker::from_u8(v[0])?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "let_declaration", "line_range": [764, 764]}
{"prompt": "<|fim_prefix|>e tried again.\n//!\n//! This implementation uses the following data structures:\n//! - A \"tasks to be processed\" stream - which is what the consumer listens to for tasks.\n//!   AKA: Main\n//! - A ZSET for delayed tasks with the sort order being the time-to-be-delivered\n//!   AKA: Delayed\n//!\n//! - Tasks in the delayed queue are prefixed with a ksuid so that we can know the timestamp of when\n//!   they should be executed.\n//!\n//! The implementation spawns an additional worker that monitors both the zset delayed tasks and\n//! the tasks currently processing. It monitors the zset task set for tasks that should be\n//! processed now, and the currently processing queue for tasks that have timed out and should be\n//! put back on the main queue.\n\n// This lint warns on `let _: () = ...` which is used throughout this file for Redis commands which\n// have generic return types. This is cleaner than the turbofish operator in my opinion.\n#![allow(clippy::let_unit_value)]\n\nuse std::{num::NonZeroUsize, sync::Arc, time::Duration};\n\nuse omniqueue::backends::{redis::DeadLetterQueueConfig, RedisBackend, RedisConfig};\nuse redis::{AsyncCommands as _, RedisResult};\n\nuse super::{QueueTask, TaskQueueConsumer, TaskQueueProducer};\nuse crate::{\n    cfg::{Configuration, QueueType},\n    error::Result,\n    metrics::RedisQueueType,\n    redis::{RedisConnection, RedisManager},\n};\n\n/// This is the key of the main queue. As a KV store, redis places the entire stream under this key.\n/// Confusingly, each message in the queue may have any number of KV pairs.\nconst MAIN: &str = \"{queue}_svix_v3_main\";\n\n/// The key for the DELAYED queue in which scheduled messages are placed. This is the same DELAYED\n/// queue as v2 of the queue implementation.\nconst DELAYED: &str = \"{queue}_svix_delayed\";\n\n/// The key for the lock guarding the delayed queue background task.\nconst DELAYED_LOCK: &str = \"{queue}_svix_delayed_lock\";\n\n/// The key for the DLQ\nconst DLQ: &str = \"{queue}_svix_dlq\";\n\n// v2 KEY CONSTANTS\nconst LEGACY_V2_MAIN: &str = \"{queue}_svix_main\";\nconst LEGACY_V2_PROCESSING: &str = \"{queue}_svix_processing\";\n\n// v1 KEY CONSTANTS\nconst LEGACY_V1_MAIN: &str = \"svix_queue_main\";\nconst LEGACY_V1_PROCESSING: &str = \"svix_queue_processing\";\nconst LEGACY_V1_DELAYED: &str = \"svix_queue_delayed\";\n\n/// Consumer group name constant -- each consumer group is able to read and acknowledge messages\n/// from the queue, and messages are read by all consumer groups.\nconst WORKERS_GROUP: &str = \"svix_workers_group\";\n/// Consumer group consumer name constant -- consumer groups contain consumers which receive\n/// messages in a round-robin manner. Every worker uses the same consumer name such that they race\n/// for messages instead of having them evenly distributed.\nconst WORKER_CONSUMER: &str = \"svix_workers_consumer\";\n\n/// Special ID for XADD command's which generates a stream ID automatically\nconst GENERATE_STREAM_ID: &str = \"*\";\n\n/// Each queue item has a set of KV pairs associated with it, for simplicity a sing key, \"data\" is\n/// used with the entire [`QueueTask`] as the value in serialized JSON\nconst QUEUE_KV_KEY: &str = \"data\";\n\n/// Generates a [`TaskQueueProducer`] and a [`TaskQueueConsumer`] backed by Redis.\npub async fn new_pair(\n    cfg: &Configuration,\n    prefix: Option<&str>,\n) -> (TaskQueueProducer, TaskQueueConsumer) {\n    new_pair_inner(\n        cfg,\n        Duration::from_secs(cfg.redis_pending_duration_secs),\n        prefix.unwrap_or_default(),\n        MAIN,\n        DELAYED,\n        DELAYED_LOCK,\n        DLQ,\n    )\n    .await\n}\n\n/// Runs Redis queue migrations with the given delay schedule. Migrations are run on this schedule\n/// such that if an old instance of the server is online after the migrations are made, that no data\n/// will be lost assuming the old server is taken offline before the last scheduled delay.\nasync fn run_migration_schedule(delays: &[Duration], pool: RedisManager) {\n    let mut conn = pool\n        .get()\n        .await\n        .expect(\"Error retrieving connection from Redis pool\");\n\n    for delay in delays {\n        // drain legacy queues:\n        <|fim_suffix|>\n        if let Err(e) = migrate_v2_to_v3_queues(&mut conn).await {\n            tracing::error!(\"Error migrating queue: {}\", e);\n            tokio::time::sleep(*delay).await;\n            continue;\n        }\n\n        tokio::time::sleep(*delay).await;\n    }\n}\n\n/// An inner function allowing key constants to be variable for testing purposes\nasync fn new_pair_inner(\n    cfg: &Configuration,\n    pending_duration: Duration,\n    queue_prefix: &str,\n    main_queue_name: &'static str,\n    delayed_queue_name: &'static str,\n    delayed_lock_name: &'static str,\n    dlq_name: &'static str,\n) -> (TaskQueueProducer, TaskQueueConsumer) {\n    let main_queue_name = format!(\"{queue_prefix}{main_queue_name}\");\n    let delayed_queue_name = format!(\"{queue_prefix}{delayed_queue_name}\");\n    let delayed_lock_name = format!(\"{queue_prefix}{delayed_lock_name}\");\n    let dlq_name = format!(\"{queue_prefix}{dlq_name}\");\n\n    // This fn is only called from\n    // - `queue::new_pair` if the queue type is redis and a DSN is set\n    // - redis tests that only makes sense to run with the DSN set\n    let dsn = cfg.redis_dsn.as_deref().unwrap();\n    let pool =\n        RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await;\n\n    // Create the stream and consumer group for the MAIN queue should it not already exist. The\n    // consumer is created automatically upon use so it does not have to be created here.\n    {\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n\n        let consumer_group_resp: RedisResult<()> = conn\n            .xgroup_create_mkstream(&main_queue_name, WORKERS_GROUP, 0i8)\n            .await;\n\n        // If the error is a BUSYGROUP error, then the stream or consumer group already exists. This does\n        // not impact functionality, so continue as usual.\n        if let Err(e) = consumer_group_resp {\n            if !e.to_string().contains(\"BUSYGROUP\") {\n                panic!(\n                    \"error creating consumer group or stream: {:?}, {:?}, {:?}, {:?}, {e:?}\",\n                    e.kind(),\n                    e.detail(),\n                    e.code(),\n                    e.category()\n                )\n            };\n        }\n    }\n\n    // Redis durations are given in integer numbers of milliseconds, so the pending_duration (the\n    // time in which a task is allowed to be processing before being restarted) must be converted to\n    // one.\n    let pending_duration: i64 = pending_duration\n        .as_millis()\n        .try_into()\n        .expect(\"Pending duration out of bounds\");\n\n    // Migrate v1 queues to v2 and v2 queues to v3 on a loop with exponential backoff.\n    tokio::spawn({\n        let pool = pool.clone();\n\n        async move {\n            let delays = [\n                // 11.25 min\n                Duration::from_secs(60 * 11 + 15),\n                // 22.5 min\n                Duration::from_secs(60 * 22 + 30),\n                // 45 min\n                Duration::from_secs(60 * 45),\n                // 1.5 hours\n                Duration::from_secs(60 * 30 * 3),\n                // 3 hours\n                Duration::from_secs(60 * 60 * 3),\n                // 6 hours\n                Duration::from_secs(60 * 60 * 6),\n                // 12 hours\n                Duration::from_secs(60 * 60 * 12),\n                // 24 hours\n                Duration::from_secs(60 * 60 * 24),\n            ];\n\n            run_migration_schedule(&delays, pool).await;\n        }\n    });\n\n    // Metrics task\n    tokio::spawn({\n        let pool = pool.clone();\n        let main_queue_name = main_queue_name.clone();\n        let delayed_queue_name = delayed_queue_name.clone();\n        let deadletter_queue_name = dlq_name.clone();\n\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_secs(1));\n            let main_queue = RedisQueueType::Stream(&main_queue_name);\n            let pending = RedisQueueType::StreamPending {\n                stream: &main_queue_name,\n                group: WORKERS_GROUP,\n            };<|fim_middle|>", "completion": "if let Err(e) = migrate_v1_to_v2_queues(&mut conn).await {\n            tracing::error!(\"Error migrating queue: {}\", e);\n            tokio::time::sleep(*delay).await;\n            continue;\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "if_expression", "line_range": [109, 113]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ApiTokenOut {\n    #[serde(rename = \"createdAt\")]\n    pub created_at: String,\n\n    #[serde(rename = \"expiresAt\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub expires_at: Option<String>,\n\n    pub id: String,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub name: Option<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub scopes: Option<Vec<String>>,\n\n    pub token: String,\n}\n\nimpl ApiTokenOut {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(created_at: String, id: String, token: String) -> Self {\n        Self {\n            created_at,\n            expires_at: None,\n            id,\n            name: None,\n            scopes: None,\n            token,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/api_token_out.rs", "node_type": "function_item", "line_range": [25, 34]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct OperationalWebhookEndpointListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n}\n\nimpl From<OperationalWebhookEndpointListOptions>\n    for svix::api::OperationalWebhookEndpointListOptions\n{\n    fn from(value: OperationalWebhookEndpointListOptions) -> Self {\n        let OperationalWebhookEndpointListOptions {\n            limit,\n            iterator,\n            order,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            order,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct OperationalWebhookEndpointCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<OperationalWebhookEndpointCreateOptions>\n    for svix::api::OperationalWebhookEndpointCreateOptions\n{\n    fn from(value: OperationalWebhookEndpointCreateOptions) -> Self {\n        let OperationalWebhookEndpointCreateOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct OperationalWebhookEndpointRotateSecretOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<OperationalWebhookEndpointRotateSecretOptions>\n    for svix::api::OperationalWebhookEndpointRotateSecretOptions\n{\n    fn from(value: OperationalWebhookEndpointRotateSecretOptions) -> Self {\n        let OperationalWebhookEndpointRotateSecretOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct OperationalWebhookEndpointArgs {\n    #[command(subcommand)]\n    pub command: OperationalWebhookEndpointCommands,\n}\n\n#[derive(Subcommand)]\npub enum OperationalWebhookEndpointCommands {\n    /// List operational webhook endpoints.\n    List {\n        #[clap(flatten)]\n        options: OperationalWebhookEndpointListOptions,\n    },\n    /// Create an operational webhook endpoint.\n    Create {\n        operational_webhook_endpoint_in: crate::json::JsonOf<OperationalWebhookEndpointIn>,\n        #[clap(flatten)]\n        options: OperationalWebhookEndpointCreateOptions,\n    },\n    /// Get an operational webhook endpoint.\n    Get { endpoint_id: String },\n    /// Update an operational webhook endpoint.\n    Update {\n        endpoint_id: String,\n        operational_webhook_endpoint_update: crate::json::JsonOf<OperationalWebhookEndpointUpdate>,\n    },\n    /// Delete an operational webhook endpoint.\n    Delete { endpoint_id: String },\n    /// Get the additional headers to be sent with the operational webhook.\n    GetHeaders { endpoint_id: String },\n    /// Set the additional headers to be sent with the operational webhook.\n    UpdateHeaders {\n        endpoint_id: String,\n        operational_webhook_endpoint_headers_in:\n            crate::json::JsonOf<OperationalWebhookEndpointHeadersIn>,\n    },\n    /// Get an operational webhook endpoint's signing secret.\n    ///\n    /// This is used to verify the authenticity of the webhook.\n    /// For more information please refer to [the consuming webhooks docs](https://docs.svix.com/consuming-webhooks/).\n    GetSecret { endpoint_id: String },\n    /// Rotates an operational webhook endpoint's signing secret.\n    ///\n    /// The previous secret will remain valid for the next 24 hours.\n    RotateSecret {\n        endpoint_id: String,\n        operational_webhook_endpoint_secret_in:\n            Option<crate::json::JsonOf<OperationalWebhookEndpointSecretIn>>,\n        #[clap(flatten)]\n        options: OperationalWebhookEndpointRotateSecretOptions,\n    },\n}\n\nimpl OperationalWebhookEndpointCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { options } => {\n                let resp = client\n                    .operational_webhook()\n                    .endpoint()\n                    .list(Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                operational_webhook_endpoint_in,\n                options,\n            } => {\n                let resp = client\n                    .operational_webhook()\n                    .endpoint()\n                    .create(\n                        operational_webhook_endpoint_in.into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { endpoint_id } => {\n                <|fim_suffix|>\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                endpoint_id,\n                operational_webhook_endpoint_update,\n            } => {\n                let resp = client\n                    .operational_webhook()\n                    .endpoint()\n                    .update(\n                        endpoint_id,\n                        operational_webhook_endpoint_update.into_inner(),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete { endpoint_id } => {\n                client\n                    .operational_webhook()\n                    .endpoint()\n                    .delete(endpoint_id)\n                    .await?;\n            }\n            Self::GetHeaders { endpoint_id } => {\n                let resp = client\n                    .operational_webhook()\n                    .endpoint()\n                    .get_headers(endpoint_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::UpdateHeaders {\n                endpoint_id,\n                operational_webhook_endpoint_headers_in,\n            } => {\n                client\n                    .operational_webhook()\n                    .endpoint()\n                    .update_headers(\n                        endpoint_id,\n                        operational_webhook_endpoint_headers_in.into_inner(),\n                    )\n                    .await?;\n            }\n            Self::GetSecret { endpoint_id } => {\n                let resp = client\n                    .operational_webhook()\n                    .endpoint()\n                    .get_secret(endpoint_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::RotateSecret {\n                endpoint_id,\n                operational_webhook_endpoint_secret_in,\n                options,\n            } => {\n                client\n                    .operational_webhook()\n                    .endpoint()\n                    .rotate_secret(\n                        endpoint_id,\n                        operational_webhook_endpoint_secret_in\n                            .unwrap_or_default()\n                            .into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let resp = client\n                    .operational_webhook()\n                    .endpoint()\n                    .get(endpoint_id)\n                    .await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/operational_webhook_endpoint.rs", "node_type": "let_declaration", "line_range": [149, 153]}
{"prompt": "<|fim_prefix|>use rdkafka::{\n    error::KafkaError,\n    producer::{FutureProducer, FutureRecord},\n    util::Timeout,\n};\nuse svix_bridge_types::{async_trait, BoxError, ForwardRequest, ReceiverOutput};\n\nuse crate::config::KafkaOutputOpts;\n\n/// Forwards webhook payloads to kafka.\npub struct KafkaProducer {\n    name: String,\n    topic: String,\n    producer: FutureProducer,\n}\n\nimpl KafkaProducer {\n    pub fn new(name: String, opts: KafkaOutputOpts) -> Result<Self, KafkaError> {\n        let KafkaOutputOpts::Inner { topic, .. } = &opts;\n        let topic = topic.clone();\n        <|fim_suffix|>\n\n        Ok(Self {\n            name,\n            topic,\n            producer,\n        })\n    }\n}\n\n#[async_trait]\nimpl ReceiverOutput for KafkaProducer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.producer\n            .send(\n                FutureRecord::<(), _>::to(&self.topic)\n                    .payload(&serde_json::to_vec(&request.payload)?),\n                Timeout::Never,\n            )\n            .await\n            .map_err(|(e, _msg)| e)?;\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let producer = opts.create_producer()?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/output.rs", "node_type": "let_declaration", "line_range": [21, 21]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nu<|fim_suffix|>\nuse aide::{\n    transform::{TransformOperation, TransformPathItem},\n    OperationInput, OperationIo, OperationOutput,\n};\nuse axum::{\n    async_trait,\n    extract::{\n        rejection::{BytesRejection, FailedToBufferBody},\n        FromRequest, FromRequestParts, Query, Request,\n    },\n    response::IntoResponse,\n};\nuse chrono::{DateTime, Utc};\nuse http::{request::Parts, StatusCode};\nuse regex::Regex;\nuse schemars::JsonSchema;\nuse sea_orm::{ColumnTrait, QueryFilter, QueryOrder, QuerySelect};\nuse serde::{de::DeserializeOwned, Deserialize, Serialize};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::types::{\n        ApplicationIdOrUid, BaseId, EndpointIdOrUid, EventTypeName, EventTypeNameSet,\n        MessageAttemptId, MessageIdOrUid,\n    },\n    error::{Error, HttpError, Result, ValidationErrorItem},\n};\n\npub mod patch;\nuse patch::UnrequiredField;\n\nconst fn default_limit() -> PaginationLimit {\n    PaginationLimit(50)\n}\n\nconst PAGINATION_LIMIT_CAP_HARD: bool = true;\nconst PAGINATION_LIMIT_CAP_LIMIT: u64 = 250;\nstatic PAGINATION_LIMIT_ERROR: LazyLock<String> =\n    LazyLock::new(|| format!(\"Given limit must not exceed {PAGINATION_LIMIT_CAP_LIMIT}\"));\n\nstatic FUTURE_QUERY_LIMIT: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::hours(1));\nstatic LIMITED_QUERY_DURATION: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::days(90));\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct PaginationDescending<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct Pagination<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Clone, Debug, JsonSchema)]\n#[schemars(transparent)]\npub struct PaginationLimit(pub u64);\n\nimpl<'de> Deserialize<'de> for PaginationLimit {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let limit = u64::deserialize(deserializer)?;\n\n        // Want hard limits to stay the same so they can be validated\n        if !PAGINATION_LIMIT_CAP_HARD && limit > PAGINATION_LIMIT_CAP_LIMIT {\n            Ok(PaginationLimit(PAGINATION_LIMIT_CAP_LIMIT))\n        } else {\n            Ok(PaginationLimit(limit))\n        }\n    }\n}\n\nimpl Validate for PaginationLimit {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        let mut errs = validator::ValidationErrors::new();\n\n        if self.0 > PAGINATION_LIMIT_CAP_LIMIT {\n            errs.add(\n                \"limit\",\n                validation_error(Some(\"pagination\"), Some(&PAGINATION_LIMIT_ERROR)),\n            );\n        }\n\n        if errs.is_empty() {\n            Ok(())\n        } else {\n            Err(errs)\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum ReversibleIterator<T: Validate> {\n    /// Regular iteration - backwards in time.\n    Normal(T),\n    /// Reversed iteration - forwards in time.\n    Prev(T),\n}\n\nimpl<T: Validate> ReversibleIterator<T> {\n    pub(crate) fn direction(&self) -> IteratorDirection {\n        match self {\n            Self::Normal(_) => IteratorDirection::Normal,\n            Self::Prev(_) => IteratorDirection::Prev,\n        }\n    }\n}\n\nimpl<'de, T: 'static + Deserialize<'de> + Validate + From<String>> Deserialize<'de>\n    for ReversibleIterator<T>\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).map(|s| {\n            if let Some(s) = s.strip_prefix('-') {\n                ReversibleIterator::Prev(T::from(s.to_owned()))\n            } else {\n                ReversibleIterator::Normal(T::from(s))\n            }\n        })\n    }\n}\n\nimpl<T: Validate> Validate for ReversibleIterator<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            ReversibleIterator::Normal(val) => val.validate(),\n            ReversibleIterator::Prev(val) => val.validate(),\n        }\n    }\n}\n\nimpl<T: Validate + JsonSchema> JsonSchema for ReversibleIterator<T> {\n    fn schema_name() -> String {\n        format!(\"ReversibleIterator_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        T::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\n/// Applies sorting and filtration to a query from its iterator, sort column, and limit\n/// queries based on time\n/// Our rules for limiting queries are as follows\n///\n/// If `before` is passed:\n/// * lower limit on query is `before - LIMITED_QUERY_DURATION`\n/// * upper limit is `before`\n///\n/// If `after` is passed:\n/// * lower limit is `after`\n/// * upper limit is `now + FUTURE_QUERY_LIMIT`\n///\n/// If prev-iterator is passed:\n/// * lower limit is `prev-iterator`\n/// * upper limit is `prev-iterator + LIMITED_QUERY_DURATION`\n///\n/// If (normal) iterator is passed:\n/// * lower limit is `iterator - LIMITED_QUERY_DURATION`\n/// * upper limit is `iterator`\n///\n/// If no iterator is passed:\n/// * lower limit is `now() - LIMITED_QUERY_DURATION` if\n///   neither `before` nor `after` were passed\npub(crate) fn filter_and_paginate_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }\n\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket limit on future\n        .filter(sort_column.lt(I::start_id(future_limit)));\n\n    (query, iter_direction)\n}\n\n/// Marker trait for any type that <|fim_middle|>", "completion": "use std::{\n    borrow::Cow,\n    collections::HashSet,\n    error::Error as StdError,\n    ops::Deref,\n    sync::LazyLock,\n    time::{SystemTime, UNIX_EPOCH},\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "use_declaration", "line_range": [4, 11]}
{"prompt": "<|fim_prefix|>           json!({\n                    \"eventType\": event_name,\n                    \"payload\": {},\n                    \"payloadRetentionPeriod\": 5,\n                }),\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        tokio::time::sleep(Duration::from_millis(10)).await;\n\n        let _list =\n            get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, expected_count)\n                .await\n                .unwrap();\n    }\n}\n\n#[tokio::test]\nasync fn test_msg_channels_filter() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ec = EventChannelSet(HashSet::from([EventChannel(\"tag1\".to_owned())]));\n\n    for channels in [Some(ec.clone()), None] {\n        let _endp = post_endpoint(\n            &client,\n            &app_id,\n            EndpointIn {\n                channels,\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n        )\n        .await\n        .unwrap();\n    }\n\n    for (channels, expected_count) in [(Some(&ec), 2), (None, 1)] {\n        let mut message_in = json!({\n            \"eventType\": \"et1\",\n            \"payload\": {},\n            \"payloadRetentionPeriod\": 5,\n        });\n        if let Some(channels) = channels {\n            message_in[\"channels\"] = json!(channels);\n        }\n\n        let msg: MessageOut = client\n            .post(\n                &format!(\"api/v1/app/{}/msg/\", &app_id),\n                message_in,\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        let _list =\n            get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, expected_count)\n                .await\n                .unwrap();\n\n        let msg: MessageOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(msg.channels.as_ref(), channels);\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_headers_manipulation() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap();\n\n    let patched_headers_in = EndpointHeadersPatchIn {\n        headers: EndpointHeadersPatch(HashMap::from([\n            (\"x-test-3\".to_owned(), Some(\"4\".to_owned())),\n            (\"x-test-2\".to_owned(), None),\n        ])),\n    };\n\n    client\n        .patch_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            patched_headers_in,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let recvd_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([(\"x-test-3\".to_owned(), \"4\".to_owned()),]),\n        recvd_headers.headers\n    );\n\n    let endp = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap();\n\n    for bad_hdr in [\n        \"content-length\",\n        \"some:thing\",\n        \"some\\u{0000}thing\",\n        \"svix-foo\",\n        \"x-svix-foo\",\n        \"x-amzn-foo\",\n    ] {\n        let _: IgnoredAny = client\n            .put(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                json!({\n                    \"headers\": { bad_hdr: \"123\" },\n                }),\n                StatusCode::UNPROCESSABLE_ENTITY,\n            )\n            .await\n            .unwrap();\n    }\n\n    let org_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };\n\n    l<|fim_suffix|>\n    for hdrs in [&org_headers, &updated_headers] {\n        client\n            .put_without_response(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                hdrs,\n                StatusCode::NO_CONTENT,\n            )\n            .await\n            .unwrap();\n\n        let recvd_headers: EndpointHeadersOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(hdrs.headers.0, recvd_headers.headers);\n    }\n\n    let patched_headers_in = EndpointHeadersPatchIn {\n        headers: EndpointHeadersPatch(HashMap::from([\n            (\"x-test-3\".to_owned(), Some(\"4\".to_owned())),\n            (\"x-test-2\".to_owned(), None),\n        ])),\n    };\n\n    client\n        .patch_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            &patched_headers_in,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let recvd_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([\n            (\"x-test-1\".to_owned(), \"3\".to_owned()),\n            (\"x-test-3\".to_owned(), \"4\".to_owned()),\n        ]),\n        recvd_headers.headers\n    );\n\n    let redacted_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"authorization\".to_owned(), \"secret\".to_owned()),\n        ])),\n    };\n\n    client\n        .put_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            redacted_headers,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let recvd_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([(\"x-test-1\".to_owned(), \"1\".to_owned())]),\n        recvd_headers.headers\n    );\n\n    assert_eq!(\n        HashSet::from([\"authorization\".to_owned()]),\n        recvd_headers.sensitive\n    );\n}\n\n#[tokio::test]\nasync fn test_endpoint_headers_sending() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let endp = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };\n\n    client\n        .put_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            &headers,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n\n    for (k, v) in &headers.headers.0 {\n        assert_eq!(v, last_headers.get(k).unwrap().to_str().unwrap());\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_header_key_capitalization() {\n    let (client, _jk) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let endp = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([(\n            \"X-Api-Test\".to_owned(),\n            \"test-value\".to_owned(),\n        )])),\n    };\n\n    client\n        .put_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}<|fim_middle|>", "completion": "let updated_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"3\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [2108, 2113]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse axum::async_trait;\n\nuse super::{Cache, CacheBehavior, CacheKey, CacheValue, Result, StringCacheKey};\n\npub fn new() -> Cache {\n    tracing::warn!(\"Running with caching disabled will negatively affect performance. Idempotency is not supported without a cache.\");\n    NoCache {}.into()\n}\n\n#[derive(Clone)]\npub struct NoCache;\n\n#[async_trait]\nimpl CacheBehavior for NoCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get<T: CacheValue>(&self, _key: &T::Key) -> Result<Option<T>> {\n        Ok(None)\n    }\n\n    async fn get_raw(&self, _key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(None)\n    }\n\n    async fn get_string<T: StringCacheKey>(&self, _key: &T) -> Result<Option<String>> {\n        Ok(None)\n    }\n\n    async fn set<T: CacheValue>(&self, _key: &T::Key, _value: &T, _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_raw(&self, _key: &[u8], _value: &[u8], _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_string<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<()> {\n        Ok(())\n    }\n\n    async fn delete<T: CacheKey>(&self, _key: &T) -> Result<()> {\n        Ok(())\n    }\n\n    a<|fim_suffix|>\n    async fn set_raw_if_not_exists(\n        &self,\n        _key: &[u8],\n        _value: &[u8],\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n\n    async fn set_string_if_not_exists<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n}\n<|fim_middle|>", "completion": "async fn set_if_not_exists<T: CacheValue>(\n        &self,\n        _key: &T::Key,\n        _value: &T,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/none.rs", "node_type": "function_item", "line_range": [57, 64]}
{"prompt": "<|fim_prefix|>use std::{num::NonZeroUsize, str::FromStr};\n\nuse anyhow::Result;\n<|fim_suffix|>\nuse deno_ast::{MediaType, ParseParams};\nuse deno_core::{\n    serde_v8, url,\n    v8::{self},\n    JsRuntime,\n};\nuse svix_bridge_types::{JsObject, TransformerInput, TransformerOutput};\nuse tokio::sync::oneshot;\n\nstruct Executor {\n    tx: std::sync::mpsc::Sender<Job>,\n    _handle: std::thread::JoinHandle<()>,\n}\n\nimpl Default for Executor {\n    fn default() -> Self {\n        let (tx, rx) = std::sync::mpsc::channel::<Job>();\n        let _handle = std::thread::spawn(move || {\n            let mut runtime = JsRuntime::new(Default::default());\n            for Job { input, script, cb } in rx {\n                let ret = run_script_inner(&mut runtime, input, script);\n                if cb.send(ret).is_err() {\n                    tracing::error!(\"failed to send script output to caller\");\n                }\n            }\n        });\n        Self { tx, _handle }\n    }\n}\n\ntype Callback = oneshot::Sender<Result<TransformerOutput>>;\n\nstruct Job {\n    input: TransformerInput,\n    script: String,\n    cb: Callback,\n}\n\nimpl Executor {\n    async fn execute(\n        &mut self,\n        input: TransformerInput,\n        script: String,\n    ) -> Result<TransformerOutput> {\n        let (tx, rx) = oneshot::channel();\n        self.tx.send(Job {\n            input,\n            script,\n            cb: tx,\n        })?;\n        rx.await?\n    }\n}\n\n#[derive(Clone)]\npub struct JsPooler {\n    executors: Pool<Executor>,\n}\n\nimpl JsPooler {\n    pub fn new(pool_size: NonZeroUsize) -> Self {\n        let pool_size = pool_size.get();\n        let mut items = Vec::with_capacity(pool_size);\n        for _ in 0..pool_size {\n            items.push(Executor::default());\n        }\n        Self {\n            executors: Pool::from(items),\n        }\n    }\n\n    pub async fn run_script(\n        &self,\n        input: TransformerInput,\n        script: String,\n    ) -> Result<TransformerOutput> {\n        let pool = self.executors.clone();\n        let mut executor = pool.get().await;\n\n        executor\n            .as_mut()\n            .map_err(|e| anyhow::anyhow!(\"{e:?}\"))?\n            .execute(input, script)\n            .await\n    }\n}\n\n/// Checks that the input parses as valid JavaScript, giving the parser's error back on failure.\npub fn validate_script(src: &str) -> Result<()> {\n    Ok(deno_ast::parse_script(ParseParams {\n        specifier: url::Url::from_str(\"file:///x.js\").expect(\"static string\"),\n        text: src.into(),\n        media_type: MediaType::JavaScript,\n        capture_tokens: false,\n        scope_analysis: false,\n        maybe_syntax: None,\n    })\n    .map(|_| ())?)\n}\n\nfn run_script_inner(\n    runtime: &mut JsRuntime,\n    input: TransformerInput,\n    script: String,\n) -> Result<TransformerOutput> {\n    let input = serde_json::to_string(&input)?;\n    let res = runtime.execute_script(\n        \"<anon>\",\n        format!(\n            // Wrap the user script, and invocation of `handler`, in a self-calling closure.\n            // The hope is we'll prevent the globals space from being polluted call after call.\n            r#\"\n    (function () {{\n        {script}\n        return handler({input});\n    }})()\n    \"#,\n        ),\n    );\n    match res {\n        Ok(global) => {\n            let scope = &mut runtime.handle_scope();\n            let local = v8::Local::new(scope, global);\n            match serde_v8::from_v8::<JsObject>(scope, local) {\n                Ok(v) => Ok(TransformerOutput::Object(v)),\n                Err(e @ serde_v8::Error::ExpectedObject(_)) => {\n                    tracing::error!(\"{e}\");\n                    Ok(TransformerOutput::Invalid)\n                }\n                Err(e) => {\n                    tracing::error!(\"{e}\");\n                    Err(e)?\n                }\n            }\n        }\n        Err(err) => Err(anyhow::format_err!(\"Evaling error: {:?}\", err)),\n    }\n}\n\n#[cfg(test)]\nmod tests;\n<|fim_middle|>", "completion": "use deadpool::unmanaged::Pool;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/mod.rs", "node_type": "use_declaration", "line_range": [4, 4]}
{"prompt": "<|fim_prefix|> = \"jemalloc\"))]\n#[global_allocator]\nstatic GLOBAL: Jemalloc = Jemalloc;\n\n#[cfg(all(target_env = \"msvc\", feature = \"jemalloc\"))]\ncompile_error!(\"jemalloc cannot be enabled on msvc\");\n\n// Seems like it would be useful to be able to configure this.\n// In some docker setups, hostname is sometimes the container id, and advertising this can be\n// helpful.\nstatic INSTANCE_ID: Lazy<String> = Lazy::new(|| KsuidMs::new(None, None).to_string());\n\nfn get_svc_identifiers(cfg: &Config) -> opentelemetry_sdk::Resource {\n    opentelemetry_sdk::Resource::new(vec![\n        opentelemetry::KeyValue::new(\n            \"service.name\",\n            cfg.opentelemetry\n                .as_ref()\n                .and_then(|x| x.service_name.as_deref())\n                .unwrap_or(\"svix-bridge\")\n                .to_owned(),\n        ),\n        opentelemetry::KeyValue::new(\"instance_id\", INSTANCE_ID.to_owned()),\n    ])\n}\n\nfn setup_tracing(cfg: &Config) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        const CRATE_NAME: &str = env!(\"CARGO_CRATE_NAME\");\n        let level = cfg.log_level.to_string();\n        let var = [\n            format!(\"{CRATE_NAME}={level}\"),\n            // XXX: Assuming this applies to the Producer side (aka `og-ingester`) when we fold it back in.\n            format!(\"tower_http={level}\"),\n        ];\n        var.join(\",\")\n    });\n\n    let otel_layer = cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        // Configure the OpenTelemetry tracing layer\n        opentelemetry::global::set_text_map_propagator(\n            opentelemetry_sdk::propagation::TraceContextPropagator::new(),\n        );\n\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        let tracer = opentelemetry_otlp::new_pipeline()\n            .tracing()\n            .with_exporter(exporter)\n            .with_trace_config(\n                opentelemetry_sdk::trace::Config::default()\n                    .with_sampler(\n                        otel_cfg\n                            .sample_ratio\n                            .map(opentelemetry_sdk::trace::Sampler::TraceIdRatioBased)\n                            .unwrap_or(opentelemetry_sdk::trace::Sampler::AlwaysOn),\n                    )\n                    .with_resource(get_svc_identifiers(cfg)),\n            )\n            .install_batch(Tokio)\n            .unwrap()\n            .tracer(\"svix_bridge\");\n\n        tracing_opentelemetry::layer().with_tracer(tracer)\n    });\n\n    // Then create a subscriber with an additional layer printing to stdout.\n    // This additional layer is either formatted normally or in JSON format.\n    match cfg.log_format {\n        config::LogFormat::Default => {\n            let stdout_layer = tracing_subscriber::fmt::layer();\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n        config::LogFormat::Json => {\n            let fmt = tracing_subscriber::fmt::format().json().flatten_event(true);\n            let json_fields = tracing_subscriber::fmt::format::JsonFields::new();\n\n            let stdout_layer = tracing_subscriber::fmt::layer()\n                .event_format(fmt)\n                .fmt_fields(json_fields);\n\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n    };\n}\n\n/// Delta temporality selector as recommended by upstream:\n/// https://github.com/open-telemetry/opentelemetry-rust/discussions/1511#discussioncomment-8386721\nstruct DeltaTemporalitySelector;\n\nimpl TemporalitySelector for DeltaTemporalitySelector {\n    fn temporality(&self, kind: InstrumentKind) -> Temporality {\n        <|fim_suffix|>\n    }\n}\n\npub fn setup_metrics(cfg: &Config) -> Option<SdkMeterProvider> {\n    cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        opentelemetry_otlp::new_pipeline()\n            .metrics(Tokio)\n            .with_temporality_selector(DeltaTemporalitySelector)\n            .with_exporter(exporter)\n            .with_resource(get_svc_identifiers(cfg))\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn supervise_senders(inputs: Vec<Box<dyn SenderInput>>) -> Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"sender input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n#[derive(Parser)]\npub struct Args {\n    #[arg(long, env = \"SVIX_BRIDGE_CFG_FILE\", help = \"Path to the config file.\")]\n    cfg_file: Option<PathBuf>,\n    #[arg(\n        long,\n        env = \"SVIX_BRIDGE_CFG\",\n        help = \"Config data as a string (instead of a file on disk).\",\n        conflicts_with = \"cfg_file\"\n    )]\n    cfg: Option<String>,\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let args = Args::<|fim_middle|>", "completion": "match kind {\n            InstrumentKind::UpDownCounter => Temporality::Cumulative,\n            InstrumentKind::ObservableUpDownCounter => Temporality::Cumulative,\n            _ => Temporality::Delta,\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "match_expression", "line_range": [143, 147]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct OperationalWebhookEndpointHeadersIn {\n    pub headers: std::collections::HashMap<String, String>,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl OperationalWebhookEndpointHeadersIn {\n    pub fn new(headers: std::collections::HashMap<String, String>) -> Self {\n        Self { headers }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/operational_webhook_endpoint_headers_in.rs", "node_type": "impl_item", "line_range": [9, 13]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct EnvironmentExportOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EnvironmentExportOptions> for svix::api::EnvironmentExportOptions {\n    fn from(value: EnvironmentExportOptions) -> Self {\n        let EnvironmentExportOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct EnvironmentImportOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EnvironmentImportOptions> for svix::api::EnvironmentImportOptions {\n    fn from(value: EnvironmentImportOptions) -> Self {\n        let EnvironmentImportOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct EnvironmentArgs {\n    #[command(subcommand)]\n    pub command: EnvironmentCommands,\n}\n\n#[derive(Subcommand)]\npub enum EnvironmentCommands {\n    /// Download a JSON file containing all org-settings and event types.\n    ///\n    /// Note that the schema for [`EnvironmentOut`] is subject to change. The fields\n    /// herein are provided for convenience but should be treated as JSON blobs.\n    Export {\n        #[clap(flatten)]\n        options: EnvironmentExportOptions,\n    },\n    /// Import a configuration into the active organization.\n    ///\n    /// It doesn't delete anything, only adds / updates what was passed to it.\n    ///\n    /// Note that the schema for [`EnvironmentIn`] is subject to change. The fields\n    /// herein are provided for convenience but should be treated as JSON blobs.\n    Import {\n        environment_in: Option<crate::json::JsonOf<EnvironmentIn>>,\n        #[clap(flatten)]\n        options: EnvironmentImportOptions,\n    },\n}\n\nimpl EnvironmentCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Export { options } => {\n                <|fim_suffix|>\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Import {\n                environment_in,\n                options,\n            } => {\n                client\n                    .environment()\n                    .import(\n                        environment_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let resp = client.environment().export(Some(options.into())).await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/environment.rs", "node_type": "let_declaration", "line_range": [69, 69]}
{"prompt": "<|fim_prefix|>or.unwrap();\n    let prev_page: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/?limit=1&iterator={prev_iter}\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(prev_page.data.len(), 1);\n    let endpoint_ids: Vec<_> = prev_page.data.iter().map(|d| &d.id).collect();\n    assert_eq!(endpoint_ids, vec![&endp_1.id]);\n\n    receiver_1.jh.abort();\n    receiver_2.jh.abort();\n    receiver_3.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_list_attempts_by_endpoint() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1AttemptListAttemptsByEndpointTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n    let endp_id_2 = create_test_endpoint(&client, &app_id, &receiver_2.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"user.exploded\",\n        [\"obits\"],\n    )\n    .await;\n\n    // And wait at most one second for all attempts to be processed\n    run_with_retries(|| async {\n        for endp_id in [endp_id_1.clone(), endp_id_2.clone()] {\n            let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id}/\"),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_1: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_1}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    let list_2: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    for list in [list_1, list_2] {\n        let message_ids: Vec<_> = list.data.into_iter().map(|amo| amo.msg_id).collect();\n        assert!(message_ids.contains(&msg_1.id));\n        assert!(message_ids.contains(&msg_2.id));\n        assert!(message_ids.contains(&msg_3.id));\n    }\n\n    let foo_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?channel=foo\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert!(foo_attempts.data.is_empty());\n\n    let obits_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?channel=obits\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(obits_attempts.data.len(), 1);\n\n    let exploded_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types=user.exploded\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(exploded_attempts.data.len(), 1);\n\n    let regular_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types[]=event.type\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(regular_attempts.data.len(), 2);\n\n    l<|fim_suffix|>    assert_eq!(all_attempts_1.data.len(), 3);\n\n    let all_attempts_2: ListResponse<MessageAttemptOut> = client\n    .get(\n        &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types=event.type,user.exploded\"),\n        StatusCode::OK,\n    )\n    .await\n    .unwrap();\n    assert_eq!(all_attempts_2.data.len(), 3);\n\n    receiver_1.jh.abort();\n    receiver_2.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_message_attempts() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = (0..2).map(|_| Duration::from_millis(1)).collect();\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    for (status_code, msg_status, attempt_count) in [\n        // Success\n        (StatusCode::OK, MessageStatus::Success, Some(1)),\n        // HTTP 400\n        (StatusCode::FORBIDDEN, MessageStatus::Fail, None),\n        // HTTP 500\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None),\n    ] {\n        let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n        let receiver = TestReceiver::start(status_code);\n\n        let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n            .await\n            .unwrap()\n            .id;\n\n        let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n            .await\n            .unwrap();\n\n        let list = get_msg_attempt_list_and_assert_count(\n            &client,\n            &app_id,\n            &msg.id,\n            attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n        )\n        .await\n        .unwrap();\n\n        for i in list.data.iter() {\n            assert_eq!(i.status, msg_status);\n            println!(\"{} {status_code}\", i.response_status_code);\n            assert_eq!(\n                i.response_status_code,\n                TryInto::<i16>::try_into(status_code.as_u16()).unwrap()\n            );\n            assert_eq!(i.endpoint_id, endp_id);\n        }\n        receiver.jh.abort();\n    }\n\n    // non-HTTP-related failures:\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    // stop receiver before beginning tests:\n    receiver.jh.abort();\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        &cfg.retry_schedule.len() + 1,\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, MessageStatus::Fail);\n        assert_eq!(i.response_status_code, 0);\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n}\n\n#[tokio::test]\nasync fn test_message_attempts_empty_retry_schedule() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![];\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let (status_code, msg_status, attempt_count) =\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None);\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(status_code);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, msg_status);\n        println!(\"{} {status_code}\", i.response_status_code);\n        assert_eq!(\n            i.response_status_code,\n            TryInto::<i16>::try_into(status_code.as_u16()).unwrap()\n        );\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync f<|fim_middle|>", "completion": "let all_attempts_1: ListResponse<MessageAttemptOut> = client\n    .get(\n        &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types[0]=event.type&event_types[1]=user.exploded\"),\n        StatusCode::OK,\n    )\n    .await\n    .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [701, 707]}
{"prompt": "<|fim_prefix|>or;\n\n/// Newtype for [`omniqueue::queue::Delivery`].\n///\n/// Mostly vestigial at this point, though it doesn't hurt to have something to act as a facade to\n/// group helper functions for handling payload details.\npub struct DeliveryWrapper(Delivery);\n\nimpl From<Delivery> for DeliveryWrapper {\n    fn from(value: Delivery) -> Self {\n        Self(value)\n    }\n}\n\nimpl DeliveryWrapper {\n    /// Delegates to the inner delivery types ack method.\n    async fn ack(self) -> Result<(), QueueError> {\n        self.0.ack().await.map_err(|(e, _)| e)\n    }\n    /// Delegates to the inner delivery types nack method.\n    async fn nack(self) -> Result<(), QueueError> {\n        self.0.nack().await.map_err(|(e, _)| e)\n    }\n\n    /// Decodes the inner delivery as String.\n    fn raw_payload(&self) -> Result<&str, QueueError> {\n        // TODO: used to be unsupported for redis. Is it now? Check for skipped tests to prove it.\n        let bytes = self.0.borrow_payload().ok_or(QueueError::NoData)?;\n        std::str::from_utf8(bytes).map_err(QueueError::generic)\n    }\n\n    /// Decodes the inner delivery as `serde_json::Value`.\n    fn payload(&self) -> Result<serde_json::Value, QueueError> {\n        self.0.payload_serde_json()?.ok_or(QueueError::NoData)\n    }\n}\n\n#[async_trait]\ntrait Consumer {\n    /// The source of the stream of messages, e.g. the name or id for the queue, subscription, etc.\n    fn source(&self) -> &str;\n    /// The name of the messaging system, e.g. rabbitmq, sqs, etc.\n    fn system(&self) -> &str;\n    /// Gets the channel sender for running transformations.\n    fn transformer_tx(&self) -> Option<&TransformerTx>;\n    /// The js source for the transformation to run on each payload.\n    fn transformation(&self) -> Option<&TransformationConfig>;\n    /// The client to use when creating messages in svix.\n    fn svix_client(&self) -> &Svix;\n\n    async fn transform(\n        &self,\n        script: String,\n        input: TransformerInput,\n    ) -> std::io::Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx()\n            .as_ref()\n            .expect(\"transformations not configured\")\n            .send(job)\n            .map_err(|e| Error::Generic(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::Generic(\"transformation rx failed\".to_string()))\n            .and_then(|x| {\n                x.map_err(|_e| Error::Generic(\"transformation execution failed\".to_string()))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => {\n                Err(Error::Generic(\"transformation produced unexpected value\".to_string()).into())\n            }\n        }\n    }\n\n    /// Gets consumer (likely based on a config value), called by [`consume`].\n    async fn consumer(&self) -> std::io::Result<DynConsumer>;\n\n    /// Main consumer loop\n    async fn consume(&self) -> std::io::Result<()> {\n        let mut consumer = self.consumer().await?;\n        tracing::debug!(\"{} consuming: {}\", self.system(), self.source(),);\n        loop {\n            self.receive(&mut consumer).await?;\n        }\n    }\n\n    /// Pulls N messages off the queue and feeds them to [`Self::process`].\n    #[tracing::instrument(skip_all,\n    fields(\n        otel.kind = \"CONSUMER\",\n        messaging.system = self.system(),\n        messaging.operation = \"receive\",\n        messaging.source = self.source(),\n        svix_bridge_plugin.name = crate::PLUGIN_NAME,\n        svix_bridge_plugin.vers = crate::PLUGIN_VERS,\n    )\n    )]\n    async fn receive(&self, consumer: &mut DynConsumer) -> std::io::Result<()> {\n        // FIXME: omniqueue has a fixed batch size of 1 afaict. Would be nicer to pull N at a time.\n        let delivery = consumer.receive().await.map_err(Error::from)?;\n        self.process(delivery.into()).await?;\n        Ok(())\n    }\n\n    /// Parses the delivery as JSON and feeds it into [`create_svix_message`].\n    /// Will nack the delivery if either the JSON parse, transformation, or the request to svix fails.\n    #[tracing::instrument(skip_all, fields(messaging.operation = \"process\"))]\n    async fn process(&self, delivery: DeliveryWrapper) -> std::io::Result<()> {\n        let payload = if let Some(xform_cfg) = self.transformation() {\n            let input = match xform_cfg.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload = match delivery.payload() {\n                        Ok(p) => p,\n                        Err(e) => {\n                            tracing::warn!(\"{e}\");\n                            delivery.nack().await.map_err(Error::from)?;\n                            return Ok(());\n                        }\n                    };\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    // N.b. our redis backend doesn't support string payloads, but higher up in the\n                    // call stack, during the plugin construction, we should be catching this and\n                    // giving an error about bad config.\n                    // If we get here somehow with a redis delivery, this call will panic.\n                    let raw_payload = match delivery.raw_payload() {\n                        Ok(p) => p,\n                        Err(e) => {\n                            tracing::warn!(\"{e}\");\n                            delivery.nack().await.map_err(Error::from)?;\n                            return Ok(());\n                        }\n                    };\n                    // FIXME: if we add a lifetime to `TransformerInput` we might avoid this allocation.\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n            let script = xform_cfg.source().clone();\n            match self.transform(script, input).await {\n                Err(e) => {\n                    tracing::error!(\"nack: {e}\");\n                    delivery.nack().await.map_err(Error::from)?;\n                    return Ok(());\n                }\n                Ok(x) => serde_json::from_value(serde_json::Value::Object(x))?,\n            }\n        } else {\n            // Parse as JSON when not using a transformation because Create Message requires JSON.\n            // If this fails, the config needs to change.\n            let json_payload = match delivery.payload() {\n                Ok(p) => p,\n                Err(e) => {\n                    tracing::warn!(\"{e}\");\n                    delivery.nack().await.map_err(Error::from)?;\n                    return Ok(());\n                }\n            };\n            serde_json::from_value(json_payload)?\n        };\n\n        match create_svix_message(self.svix_client(), payload).await {\n            Ok(_) => {\n                tracing::trace!(\"ack\");\n                delivery.ack().await.map_err(Error::from)?\n            }\n            Err(e) => {\n                tracing::error!(\"nack: {e}\");\n                delivery.nack().await.map_err(Error::from)?\n            }\n        }\n        Ok(())\n    }\n}\n\nasync fn run_inner(consumer: &(impl Consumer + Send + Sync)) -> ! {\n    let mut fails: u64 = 0;\n    let mut last_fail = Instant::now();\n    let system_name = consumer.system();\n    let source = consumer.source();\n\n    tracing::info!(\"{system_name} starting: {source}\");\n\n    loop {\n        if let Err(e) = consumer.consume().await {\n            tracing::error!(\"{e}\");\n        }\n\n        tracing::error!(\"{system_name} disconnected: {source}\");\n\n        <|fim_suffix|>\n\n        last_fail = Instant::now();\n        tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n    }\n}\n\n#[tracing::instrument(skip_all, level = \"error\", fields(\n    app_id,\n    event_type = message.event_type\n))]\nasync fn create_svix_message(\n    svix: &Svix,\n    CreateMessageRequest { app_id, message }: CreateMessageRequest,\n) -> std::io::Result<()> {\n    svix.message()\n        .create(\n            app_id, message,\n            // FIXME: add a way for the caller to give an idempotency key like we have in kafka\n            None,\n        )\n        .await\n        .map_err(Error::from)?;\n    Ok(())\n}\n<|fim_middle|>", "completion": "if last_fail.elapsed() > Duration::from_secs(10) {\n            // reset the fail count if we didn't have a hiccup in the past short while.\n            tracing::trace!(\"been a while since last fail, resetting count\");\n            fails = 0;\n        } else {\n            fails += 1;\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/src/lib.rs", "node_type": "if_expression", "line_range": [215, 221]}
{"prompt": "<|fim_prefix|>use anyhow::{Context, Result};\nuse clap::Args;\n\nuse crate::config::{get_config_file_path, Config};\n\n#[derive(Args)]\npub struct ListenArgs {\n    /// The local URL to forward webhooks to\n    url: url::Url,\n    /// Disable TLS certificate verification when connecting to the local URL\n    #[arg(long)]\n    disable_tls_verification: bool,\n}\n\nimpl ListenArgs {\n    pub async fn exec(self, cfg: &Config) -> Result<()> {\n        let token = match cfg.relay_token.as_ref() {\n            None => {\n                let token = crate::relay::token::generate_token()?;\n                <|fim_suffix|>\n                updated_cfg.relay_token = Some(token.clone());\n\n                let cfg_path = get_config_file_path()?;\n                if let Err(e) = updated_cfg.save_to_disk(&cfg_path).context(format!(\n                    \"failed to save relay token to config file at `{}`\",\n                    cfg_path.display()\n                )) {\n                    eprintln!(\"{e:#}\");\n                }\n                token\n            }\n            Some(token) => token.clone(),\n        };\n        crate::relay::listen(\n            self.url,\n            token,\n            cfg.relay_debug_hostname.as_deref(),\n            cfg.relay_disable_security.unwrap_or_default(),\n            self.disable_tls_verification,\n        )\n        .await?;\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let mut updated_cfg = cfg.clone();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/listen.rs", "node_type": "let_declaration", "line_range": [20, 20]}
{"prompt": "<|fim_prefix|>xisting response before\n//! routing to the given endpoint's function, and caches any such results\n//! such that subsequent requests to that endpoint with the same key will return\n//! the same response.\n//!\n//! Responses are cached for twelve hours by default.\n\nuse std::{collections::HashMap, convert::Infallible, future::Future, pin::Pin, time::Duration};\n\nuse axum::{\n    body::Body,\n    extract::Request,\n    http::StatusCode,\n    response::{IntoResponse, Response},\n};\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse blake2::{Blake2b512, Digest};\nuse http::request::Parts;\nuse http_body_util::BodyExt as _;\nuse serde::{Deserialize, Serialize};\nuse tower::Service;\n\nuse super::cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue};\nuse crate::error::Error;\n\n/// Returns the default expiry period for cached responses\nconst fn expiry_default() -> Duration {\n    Duration::from_secs(60 * 60 * 12)\n}\n\n/// Returns the default expiry period for the starting lock\nconst fn expiry_starting() -> Duration {\n    Duration::from_secs(5)\n}\n\n/// Returns the duration to sleep before retrying to find a [`SerializedResponse::Finished`] in the\n/// cache\nconst fn wait_duration() -> Duration {\n    Duration::from_millis(200)\n}\n\n/// The data structure containing all necessary components of a response ready to be (de)serialized\n/// from/into the cache\n#[derive(Deserialize, Serialize)]\nenum SerializedResponse {\n    Start,\n    Finished {\n        code: u16,\n        headers: Option<HashMap<String, Vec<u8>>>,\n        body: Option<Vec<u8>>,\n    },\n}\n\nkv_def!(IdempotencyKey, SerializedResponse);\n\nimpl IdempotencyKey {\n    fn new(auth_token: &str, key: &str, url: &str) -> IdempotencyKey {\n        let mut hasher = Blake2b512::new();\n\n        hasher.update(auth_token);\n        hasher.update(\":\");\n        hasher.update(key);\n        hasher.update(\":\");\n        hasher.update(url);\n\n        let res = hasher.finalize();\n        // FIXME: add (previously omitted) prefix: `SVIX_IDEMPOTENCY_CACHE`\n        IdempotencyKey(STANDARD.encode(res))\n    }\n}\n\n#[derive(thiserror::Error, Debug)]\npub enum ConversionToResponseError {\n    #[error(\"the status code is out of bounds\")]\n    StatusError(#[from] http::status::InvalidStatusCode),\n\n    #[error(\"a header name is invalid\")]\n    FromStr(#[from] http::header::InvalidHeaderName),\n    #[error(\"a header value is invalid\")]\n    InvalidHeaderValue(#[from] http::header::InvalidHeaderValue),\n}\n\n/// Will never error as long as Redis doesn't corrupt -- never use this with anything but values\n/// from Redis which were put in via the idempotency service from known good requests.\nfn finished_serialized_response_to_response(\n    code: u16,\n    headers: Option<HashMap<String, Vec<u8>>>,\n    body: Option<Vec<u8>>,\n) -> Result<Response, ConversionToResponseError> {\n    let mut out = body.unwrap_or_default().into_response();\n\n    let status = out.status_mut();\n    *status = code.try_into()?;\n\n    if let Some(resp_headers) = headers {\n        let headers = out.headers_mut();\n        *headers = resp_headers\n            .iter()\n            .map(|(k, v)| Ok((k.parse()?, http::HeaderValue::from_bytes(v)?)))\n            .collect::<Result<_, ConversionToResponseError>>()?;\n    }\n\n    Ok(out)\n}\n\nasync fn resolve_service<S>(mut service: S, req: Request) -> Response\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    match service.call(req).await {\n        Ok(res) => res.into_response(),\n        Err(e) => match e {},\n    }\n}\n\n/// The idempotency middleware itself -- used via the [`Router::layer`] method\n#[derive(Clone)]\npub struct IdempotencyService<S: Clone> {\n    pub cache: Cache,\n    pub service: S,\n}\n\nimpl<S> Service<Request> for IdempotencyService<S>\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    type Response = Response;\n    type Error = Infallible;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    f<|fim_suffix|>\n    fn call(&mut self, req: Request) -> Self::Future {\n        let mut service = self.service.clone();\n        let cache = self.cache.clone();\n\n        if !cache.is_none() {\n            Box::pin(async move {\n                let (parts, body) = req.into_parts();\n\n                // If not a POST request, simply resolve the service as usual\n                if parts.method != http::Method::POST {\n                    return Ok(resolve_service(service, Request::from_parts(parts, body)).await);\n                }\n\n                // Retrieve `IdempotencyKey` from header and URL parts, but returning the service\n                // normally in the event a key could not be created.\n                let key = if let Some(key) = get_key(&parts) {\n                    key\n                } else {\n                    return Ok(resolve_service(service, Request::from_parts(parts, body)).await);\n                };\n\n                // Set the [`SerializedResponse::Start`] lock if the key does not exist in the cache\n                // returning whether the value was set\n                let lock_acquired = if let Ok(lock_acquired) = cache\n                    .set_if_not_exists(&key, &SerializedResponse::Start, expiry_starting())\n                    .await\n                {\n                    lock_acquired\n                } else {\n                    return Ok(StatusCode::INTERNAL_SERVER_ERROR.into_response());\n                };\n\n                // If the lock was not set, first check the cache for a `Finished` cache value. If\n                // it is instead `None` or the value is a `Start` lock, then enter a loop checking\n                // it every 200ms.\n                //\n                // If the loop times out, then reset the lock and proceed to resolve the service.\n                //\n                // If at any point the cache returns an `Err`, then return 500 response\n                if !lock_acquired {\n                    match cache.get::<SerializedResponse>(&key).await {\n                        Ok(Some(SerializedResponse::Finished {\n                            code,\n                            headers,\n                            body,\n                        })) => {\n                            return Ok(finished_serialized_response_to_response(\n                                code, headers, body,\n                            )\n                            .unwrap_or_else(|_| StatusCode::INTERNAL_SERVER_ERROR.into_response()))\n                        }\n\n                        Ok(Some(SerializedResponse::Start)) | Ok(None) => {\n                            if let Ok(Some(SerializedResponse::Finished {\n                                code,\n                                headers,\n                                body,\n                            })) = lock_loop(&cache, &key).await\n                            {\n                                return Ok(finished_serialized_response_to_response(\n                                    code, headers, body,\n                                )\n                                .unwrap_or_else(|_| {\n                                    StatusCode::INTERNAL_SERVER_ERROR.into_response()\n                                }));\n                            } else {\n                                // Set the lock if it returns `Ok(None)` and continue to resolve\n                                // as normal, but return 500 if the lock cannot be set\n                                if !matches!(\n                                    cache\n                                        .set_if_not_exists(\n                                            &key,\n                                            &SerializedResponse::Start,\n                                            expiry_starting(),\n                                        )\n                                        .await,\n                                    Ok(true)\n                                ) {\n                                    return Ok(StatusCode::INTERNAL_SERVER_ERROR.into_response());\n                                }\n                            }\n       <|fim_middle|>", "completion": "fn poll_ready(\n        &mut self,\n        cx: &mut std::task::Context<'_>,\n    ) -> std::task::Poll<Result<(), Self::Error>> {\n        self.service.poll_ready(cx)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/idempotency.rs", "node_type": "function_item", "line_range": [140, 145]}
{"prompt": "<|fim_prefix|> &url,\n            json!({\n                \"channels\": [\"test\"],\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(\n        out.ep.channels,\n        Some(EventChannelSet(HashSet::from([EventChannel(\n            \"test\".to_owned()\n        )])))\n    );\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n\n    // Test that channels may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"channels\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.channels, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n}\n\n#[allow(deprecated)]\n#[tokio::test]\nasync fn test_crud() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1: &str = \"v1EndpointCrudTestApp1\";\n    const APP_NAME_2: &str = \"v1EndpointCrudTestApp2\";\n\n    const EP_URI_APP_1_EP_1_VER_1: &str = \"http://v1EndpointCrudTestApp1Ep1Ver1.test/foo\";\n    const EP_URI_APP_1_EP_1_VER_2: &str = \"http://v1EndpointCrudTestApp1Ep1Ver2.test/\";\n    const EP_URI_APP_1_EP_2: &str = \"http://v1EndpointCrudTestApp1Ep2.test/\";\n    const EP_URI_APP_2_EP_1: &str = \"http://v1EndpointCrudTestApp2Ep1.test/\";\n    const EP_URI_APP_2_EP_2: &str = \"http://v1EndpointCrudTestApp2Ep2.test/\";\n\n    let app_1 = create_test_app(&client, APP_NAME_1).await.unwrap().id;\n    let app_2 = create_test_app(&client, APP_NAME_2).await.unwrap().id;\n\n    // CREATE\n    let app_1_ep_1 = create_test_endpoint(&client, &app_1, EP_URI_APP_1_EP_1_VER_1)\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_1.ep.url, EP_URI_APP_1_EP_1_VER_1.to_lowercase());\n    assert_eq!(app_1_ep_1.ep.version, 1);\n\n    let app_1_ep_2 = create_test_endpoint(&client, &app_1, EP_URI_APP_1_EP_2)\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_2.ep.url, EP_URI_APP_1_EP_2.to_lowercase());\n    assert_eq!(app_1_ep_2.ep.version, 1);\n\n    let app_2_ep_1 = create_test_endpoint(&client, &app_2, EP_URI_APP_2_EP_1)\n        .await\n        .unwrap();\n    assert_eq!(app_2_ep_1.ep.url, EP_URI_APP_2_EP_1.to_lowercase());\n    assert_eq!(app_2_ep_1.ep.version, 1);\n\n    let app_2_ep_2 = create_test_endpoint(&client, &app_2, EP_URI_APP_2_EP_2)\n        .await\n        .unwrap();\n    assert_eq!(app_2_ep_2.ep.url, EP_URI_APP_2_EP_2.to_lowercase());\n    assert_eq!(app_2_ep_2.ep.version, 1);\n\n    // READ\n\n    // Can read from correct app\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_1.id).await.unwrap(),\n        app_1_ep_1\n    );\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_2.id).await.unwrap(),\n        app_1_ep_2\n    );\n    assert_eq!(\n        get_endpoint(&client, &app_2, &app_2_ep_1.id).await.unwrap(),\n        app_2_ep_1\n    );\n    assert_eq!(\n        get_endpoint(&client, &app_2, &app_2_ep_2.id).await.unwrap(),\n        app_2_ep_2\n    );\n\n    // Can't read from incorrect app\n    get_endpoint_404(&client, &app_2, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_1, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_1, &app_2_ep_2.id)\n        .await\n        .unwrap();\n\n    // UPDATE\n    l<|fim_suffix|>    let app_1_ep_1: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_1}/endpoint/{app_1_ep_1_id}/\"),\n            endpoint_in(EP_URI_APP_1_EP_1_VER_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_1.ep.url, EP_URI_APP_1_EP_1_VER_2.to_lowercase());\n\n    // CONFIRM UPDATE\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_1_id).await.unwrap(),\n        app_1_ep_1\n    );\n\n    // Test that PUT with an invalid ID creates an endpoint\n    let app_1_ep_3: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_1}/endpoint/fake-id/\"),\n            endpoint_in(EP_URI_APP_1_EP_1_VER_2),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // LIST\n    let list_app_1: ListResponse<EndpointOut> = client\n        .get(&format!(\"api/v1/app/{app_1}/endpoint/\"), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list_app_1.data.len(), 3);\n    assert!(list_app_1.data.contains(&app_1_ep_1));\n    assert!(list_app_1.data.contains(&app_1_ep_2));\n    assert!(list_app_1.data.contains(&app_1_ep_3));\n\n    let list_app_2: ListResponse<EndpointOut> = client\n        .get(&format!(\"api/v1/app/{}/endpoint/\", &app_2), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list_app_2.data.len(), 2);\n    assert!(list_app_2.data.contains(&app_2_ep_1));\n    assert!(list_app_2.data.contains(&app_2_ep_2));\n\n    // DELETE\n    delete_endpoint(&client, &app_1, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_1, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_2, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_2, &app_2_ep_2.id)\n        .await\n        .unwrap();\n\n    // CONFIRM DELETION\n    get_endpoint_404(&client, &app_1, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_1, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_2_ep_2.id)\n        .await\n        .unwrap();\n\n    let mut ep_with_metadata = endpoint_in(\"https://somewhere.beyond.the.c\");\n    ep_with_metadata.metadata = metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#);\n    let ep = post_endpoint(&client, &app_1, ep_with_metadata)\n        .await\n        .unwrap();\n    assert_eq!(ep.metadata, metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#));\n\n    let ep_alias = get_endpoint(&client, &app_1, &ep.id).await.unwrap();\n    assert_eq!(\n        ep_alias.metadata,\n        metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#)\n    );\n\n    // Test that metadata may be unset\n    let ep_alias2: EndpointOut = client\n        .patch(\n            &format!(\"api/v1/app/{app_1}/endpoint/{}/\", ep.id),\n            json!({\n                \"metadata\": {},\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(ep_alias2.metadata, metadata(r#\"{}\"#));\n}\n\n#[tokio::test]\nasync fn test_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"App1\").await.unwrap().id;\n    common_test_list::<EndpointOut, EndpointIn>(\n        &client,\n        &format!(\"api/v1/app/{app_id}/endpoint/\"),\n        |i| endpoint_in(&format!(\"https://localhost/{i}\")),\n        false,\n        true,\n    )\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_endpoint_list_ordering() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"App1\").await.unwrap().id;\n\n    for i in 0..5 {\n        create_test_endpoint(&client, &app_id, &format!(\"https://test.url/{i}\"))\n            .await\n            .unwrap();\n        // Sleep to account for ksuid 4ms resolution\n        tokio::time::sleep(Duration::from_millis(5)).await;\n    }\n\n    let first_list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/?limit=2\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    <|fim_middle|>", "completion": "let app_1_ep_1_id = app_1_ep_1.id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [521, 521]}
{"prompt": "<|fim_prefix|>use std::time::{Duration, Instant};\n\nuse anyhow::{Context, Result};\nuse dialoguer::Input;\nuse reqwest::Client;\nuse serde::Deserialize;\n\nuse crate::{config, config::Config};\n\npub async fn prompt(_cfg: &Config) -> Result<()> {\n    print!(\"Welcome to the Svix CLI!\\n\\n\");\n\n    let selections = &[\"Login in dashboard.svix.com\", \"Input token manually\"];\n    let selection = dialoguer::Select::new()\n        .with_prompt(\"How would you like to authenticate?\")\n        .items(selections)\n        .default(0)\n        .interact()?;\n\n    let auth_token = if selection == 0 {\n        dashboard_login().await?\n    } else {\n        Input::new()\n            .with_prompt(\"Auth Token\")\n            .validate_with({\n                move |input: &String| -> Result<()> {\n                    if !input.trim().is_empty() {\n                        Ok(())\n                    } else {\n                        Err(anyhow::anyhow!(\"auth token cannot be empty\"))\n                    }\n                }\n            })\n            .interact_text()?\n            .trim()\n            .to_string()\n    };\n\n    // Load from disk and update the prompted fields.\n    // There are other fields (not prompted for) related to \"relay\" for the `listen` command\n    // that we'd rather not wipe out if `login` is invoked.\n    let mut cfg = Config::load()?;\n    cfg.auth_token = Some(auth_token);\n    let fp = config::get_config_file_path()?;\n    if let Err(e) = cfg.save_to_disk(&fp) {\n        eprintln!(\"\\n{e:#}\\n\");\n        anyhow::bail!(\n            \"Failed to configure the Svix CLI, please try again or try setting your auth \\\n             token manually `SVIX_AUTH_TOKEN` environment variable.\"\n        );\n    }\n\n    println!(\n        \"All Set! Your config has been written to `{}`\",\n        fp.display()\n    );\n    println!(\n        \"Type `{} --help` to print the Svix CLI documentation!\",\n        crate::BIN_NAME\n    );\n    Ok(())\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct CliStartLoginSessionOut {\n    session_id: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct AuthTokenOut {\n    token: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct DiscoverySessionOut {\n    pub region: String,\n}\n\nconst DASHBOARD_URL: &str = \"https://dashboard.svix.com\";\nconst LOGIN_SERVER_URL: &str = \"https://api.svix.com\";\n\npub async fn dashboard_login() -> Result<String> {\n    let client = reqwest::Client::new();\n\n    let start_session = client\n        .post(format!(\"{LOGIN_SERVER_URL}/dashboard/cli/login/start\"))\n        .send()\n        .await\n        .context(\"Failed to get session ID. Could not connect to server.\")?\n        .json::<CliStartLoginSessionOut>()\n        .await\n        .context(\"Failed to get session ID. Invalid response.\")?;\n\n    let session_id = start_session.session_id;\n    let code = &session_id[0..4].to_uppercase();\n\n    <|fim_suffix|>\n\n    println!(\"\\nPlease approve the login in your browser, then return here.\");\n    println!(\"Verification code: \\x1b[32m{code}\\x1b[0m\\n\");\n\n    if let Err(e) = open::that(&url) {\n        eprintln!(\"Failed to open browser: {e}\");\n        println!(\"Please manually open this URL in your browser: {url}\");\n    }\n\n    println!(\"Waiting for approval...\");\n\n    // First, poll the discovery endpoint to get the region\n    let discovery_poll_url = format!(\"{LOGIN_SERVER_URL}/dashboard/cli/login/discovery/complete\");\n    let discovery_data: DiscoverySessionOut =\n        poll_session(&client, &discovery_poll_url, &session_id).await?;\n\n    let region = discovery_data.region;\n    let region_server_url = format!(\"https://api.{region}.svix.com\");\n    let token_poll_url = format!(\"{region_server_url}/dashboard/cli/login/token/complete\");\n\n    // Then, poll the token endpoint to get the auth token\n    let token_data: AuthTokenOut = poll_session(&client, &token_poll_url, &session_id).await?;\n\n    println!(\"Authentication successful!\\n\");\n    Ok(token_data.token)\n}\n\nconst MAX_POLL_TIME: Duration = Duration::from_secs(5 * 60);\n\nasync fn poll_session<T>(client: &Client, poll_url: &str, session_id: &str) -> Result<T>\nwhere\n    T: for<'de> serde::Deserialize<'de>,\n{\n    let start_time: Instant = Instant::now();\n\n    while start_time.elapsed() < MAX_POLL_TIME {\n        let response = client\n            .post(poll_url)\n            .json(&serde_json::json!({ \"sessionId\": session_id }))\n            .send()\n            .await\n            .context(\"Failed to connect to authentication server\")?;\n\n        if response.status().is_success() {\n            return response\n                .json::<T>()\n                .await\n                .context(\"Failed to parse authentication data\");\n        } else if response.status() != reqwest::StatusCode::NOT_FOUND {\n            // Bail if session exists but has an error (is expired or something else)\n            let error_message = match response.json::<serde_json::Value>().await {\n                Ok(json) => json\n                    .get(\"detail\")\n                    .and_then(|d| d.as_str())\n                    .unwrap_or(\"Unknown error\")\n                    .to_string(),\n                Err(_) => \"Unknown error\".to_string(),\n            };\n\n            anyhow::bail!(\"Authentication failed: {error_message}\");\n        }\n\n        std::thread::sleep(std::time::Duration::from_secs(1));\n    }\n\n    anyhow::bail!(\"Authentication failed.\");\n}\n<|fim_middle|>", "completion": "let url = format!(\"{DASHBOARD_URL}/cli/login?sessionId={session_id}&code={code}\");", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/login.rs", "node_type": "let_declaration", "line_range": [100, 100]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::{Duration, Instant};\n\nuse sea_orm::{\n    ConnectionTrait, DatabaseConnection, DbErr, ExecResult, QueryResult, Statement,\n    TransactionTrait, UpdateResult,\n};\n\nuse crate::error::{Error, Result};\n\ntype DbResult<T> = std::result::Result<T, DbErr>;\n\nasync fn exec_without_timeout(pool: &DatabaseConnection, stmt: Statement) -> DbResult<ExecResult> {\n    let increase_timeout = Statement::from_string(\n        pool.get_database_backend(),\n        \"SET LOCAL statement_timeout=0;\",\n    );\n    let tx = pool.begin().await?;\n    let _ = tx.execute(increase_timeout).await?;\n    let res = tx.execute(stmt).await?;\n    tx.commit().await?;\n    Ok(res)\n}\nasync fn query_one_without_timeout(\n    pool: &DatabaseConnection,\n    stmt: Statement,\n) -> DbResult<Option<QueryResult>> {\n    let increase_timeout = Statement::from_string(\n        pool.get_database_backend(),\n        \"SET LOCAL statement_timeout=0;\",\n    );\n    let tx = pool.begin().await?;\n    let _ = tx.execute(increase_timeout).await?;\n    let res = tx.query_one(stmt).await?;\n    tx.commit().await?;\n    Ok(res)\n}\n\n/// Nullifies the payload column for expired messages,\n/// `limit` sets how many rows to update at a time.\npub async fn clean_expired_messages(\n    pool: &DatabaseConnection,\n    limit: u32,\n    enable_legacy_message_cleaner: bool,\n) -> DbResult<UpdateResult> {\n    // See the docs for [`has_message_payloads_pending_expiry`] for background on the legacy cleaner.\n    let legacy_row_count = if enable_legacy_message_cleaner {\n        let legacy_res = {\n            let legacy_stmt = Statement::from_sql_and_values(\n                pool.get_database_backend(),\n                r#\"\n        UPDATE message SET payload = NULL WHERE id IN (\n            SELECT id FROM message\n            WHERE\n                expiration <= now()\n                AND payload IS NOT NULL\n            LIMIT $1\n            FOR UPDATE SKIP LOCKED\n        )\n    \"#,\n                [limit.into()],\n            );\n\n            exec_without_timeout(pool, legacy_stmt).await?\n        };\n        legacy_res.rows_affected()\n    } else {\n        0\n    };\n\n    let stmt = Statement::from_sql_and_values(\n        pool.get_database_backend(),\n        r#\"\n        DELETE FROM messagecontent WHERE id = any(\n            array(\n                SELECT id FROM messagecontent\n                WHERE\n                    expiration <= now()\n                LIMIT $1\n                FOR UPDATE SKIP LOCKED\n            )\n        )\n    \"#,\n        [limit.into()],\n    );\n    let res = pool.execute(stmt).await?;\n\n    Ok(UpdateResult {\n        rows_affected: legacy_row_count + res.rows_affected(),\n    })\n}\n\n/// Checks to see if the message table has any non-null payloads requiring expiry.\n///\n/// ## Background\n///\n/// Initially payloads were modeled as a field in `message`, but later migrated to a separate\n/// table (`messagecontent`). In cases where there are no longer any payloads to expire in `message` we\n/// can avoid the expense of running the cleaner on the `message` table since all new messages should now be using\n/// `messagecontent`.\nasync fn has_message_payloads_pending_expiry(pool: &DatabaseConnection) -> Result<bool> {\n    query_one_without_timeout(\n        pool,\n        Statement::from_string(\n            pool.get_database_backend(),\n            r#\"SELECT EXISTS (SELECT 1 FROM message WHERE payload IS NOT NULL LIMIT 1)\"#,\n        ),\n    )\n    .await?\n    .ok_or_else(|| Error::generic(\"failed to check for message payloads\"))?\n    .try_get_by_index(0)\n    .map_err(|e| Error::generic(format_args!(\"failed to check for message payloads: {e}\")))\n}\n\n/// Polls the database for expired messages to nullify payloads for.\n///\n/// Uses a variable polling schedule, based on affected row counts each iteration of the loop.\npub async fn expired_message_cleaner_loop(pool: &DatabaseConnection) -> Result<()> {\n    let message_table_needs_cleaning = has_message_payloads_pending_expiry(pool).await?;\n    if !message_table_needs_cleaning {\n        tracing::info!(\"No payloads pending expiry found in `message` table. Skipping the cleaner for this table.\");\n    }\n\n    // When fewer rows than the batch size have been updated, take a nap for this long.\n    const IDLE: Duration = Duration::from_secs(60 * 60 * 12);\n    const ON_ERROR: Duration = Duration::from_secs(10);\n    const BATCH_SIZE: u32 = 5_000;\n    let mut sleep_time = None;\n    while !crate::is_shutting_down() {\n        if let Some(duration) = sleep_time {\n            i<|fim_suffix|>        }\n\n        let start = Instant::now();\n        match clean_expired_messages(pool, BATCH_SIZE, message_table_needs_cleaning).await {\n            Err(err) => {\n                tracing::error!(\"{}\", err);\n                sleep_time = Some(ON_ERROR);\n            }\n            Ok(UpdateResult { rows_affected }) => {\n                if rows_affected > 0 {\n                    tracing::debug!(elapsed =? start.elapsed(), \"expired {} payloads\", rows_affected);\n                }\n\n                sleep_time = if rows_affected < (BATCH_SIZE as _) {\n                    Some(IDLE)\n                } else {\n                    // When we see full batches, don't sleep at all.\n                    None\n                };\n            }\n        }\n    }\n\n    Ok(())\n}\n<|fim_middle|>", "completion": "if crate::shutting_down_token()\n                .run_until_cancelled_owned(tokio::time::sleep(duration))\n                .await\n                .is_none()\n            {\n                return Ok(());\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/expired_message_cleaner.rs", "node_type": "if_expression", "line_range": [133, 139]}
{"prompt": "<|fim_prefix|>use std::{sync::Arc, time::Duration};\n\nuse hyper_util::{client::legacy::Client as HyperClient, rt::TokioExecutor};\n\nuse crate::Configuration;\n\nconst CRATE_VERSION: &str = env!(\"CARGO_PKG_VERSION\");\n\npub struct SvixOptions {\n    pub debug: bool,\n\n    pub server_url: Option<String>,\n\n    /// Timeout for HTTP requests.\n    ///\n    /// The timeout is applied from when the request starts connecting until\n    /// the response body has finished. If set to `None`, requests never time\n    /// out.\n    ///\n    /// Default: 15 seconds.\n    pub timeout: Option<Duration>,\n\n    /// Number of retries\n    ///\n    /// The number of times the client will retry if a server-side error\n    /// or timeout is received.\n    ///\n    /// Default: 2\n    pub num_retries: Option<u32>,\n\n    /// Retry Schedule in milliseconds\n    ///\n    /// List of delays to wait before each retry attempt.\n    /// Takes precedence over `num_retries`.\n    pub retry_schedule: Option<Vec<Duration>>,\n\n    /// Proxy address.\n    ///\n    /// Currently `http://` and `https://` proxies using `HTTP CONNECT`, as well\n    /// as `socks5://` and `socks5h://` URLs are supported. The difference\n    /// between the last two is that DNS resolution also goes through the proxy\n    /// for `socks5h`, but not for `socks5`.\n    pub proxy_address: Option<String>,\n}\n\nimpl Default for SvixOptions {\n    fn default() -> Self {\n        Self {\n            debug: false,\n            server_url: None,\n            timeout: Some(Duration::from_secs(15)),\n            num_retries: None,\n            retry_schedule: None,\n            proxy_address: None,\n        }\n    }\n}\n\n/// Svix API client.\n#[derive(Clone)]\npub struct Svix {\n    pub(super) cfg: Arc<Configuration>,\n    server_url: Option<String>,\n}\n\nimpl Svix {\n    pub fn new(token: String, options: Option<SvixOptions>) -> Self {\n        let options = options.unwrap_or_default();\n\n        let cfg = Arc::new(Configuration {\n            user_agent: Some(format!(\"svix-libs/{CRATE_VERSION}/rust\")),\n            client: HyperClient::builder(TokioExecutor::new())\n                .build(crate::make_connector(options.proxy_address)),\n            timeout: options.timeout,\n            // These fields will be set by `with_token` below\n            base_path: String::new(),\n            bearer_access_token: None,\n            num_retries: options.num_retries.unwrap_or(2),\n            retry_schedule: options.retry_schedule,\n        });\n        let svix = Self {\n            cfg,\n            server_url: options.server_url,\n        };\n        svix.with_token(token)\n    }\n\n    /// Creates a new `Svix` API client with a different token,\n    /// re-using all of the settings and the Hyper client from\n    /// an existing `Svix` instance.\n    ///\n    /// This can be used to change the token without incurring\n    /// the cost of TLS initialization.\n    pub fn with_token(&self, token: String) -> Self {\n        let base_path = self.server_url.clone().unwrap_or_else(|| {\n            match token.split('.').next_back() {\n                Some(\"us\") => \"https://api.us.svix.com\",\n                Some(\"eu\") => \"https://api.eu.svix.com\",\n                Some(\"in\") => \"https://api.in.svix.com\",\n                Some(\"ca\") => \"https://api.ca.svix.com\",\n                Some(\"au\") => \"https://api.au.svix.com\",\n                _ => \"https://api.svix.com\",\n            }\n            .to_string()\n        });\n        let cfg = Arc::new(Configuration {\n            base_path,\n            user_agent: self.cfg.user_agent.clone(),\n            bearer_access_token: Some(token),\n            client: self.cfg.client.clone(),\n            timeout: self.cfg.timeout,\n            num_retries: self.cfg.num_retries,\n            retry_schedule: self.cfg.retry_schedule.clone(),\n        });\n\n        Self {\n            cfg,\n            server_url: self.server_url.clone(),\n        }\n    }\n\n    #[cfg(feature = \"svix_beta\")]\n    pub fn cfg(&self) -> &Configuration {\n        &self.cfg\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    <|fim_suffix|>\n\n    #[test]\n    fn test_future_send_sync() {\n        fn require_send_sync<T: Send + Sync>(_: T) {}\n\n        let svix = Svix::new(String::new(), None);\n        let message_api = svix.message();\n        let fut = message_api.expunge_content(String::new(), String::new());\n        require_send_sync(fut);\n    }\n}\n<|fim_middle|>", "completion": "use crate::api::Svix;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/client.rs", "node_type": "use_declaration", "line_range": [130, 130]}
{"prompt": "<|fim_prefix|>);\n\n    let secret3_key = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret3_key }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret3: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(secret3_key, secret3.key);\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret1, secret2, secret3] {\n        if let EndpointSecret::Symmetric(key) = &sec.key {\n            let sec = STANDARD.encode(key);\n            let wh = Webhook::new(&sec).unwrap();\n            wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_signing_symmetric_and_asymmetric() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Asymmetric key\n    let secret_2 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    // Long key\n    let secret_3 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n\n    let ep_in = EndpointIn {\n        url: Url::parse(&receiver.endpoint).unwrap(),\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\" }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Rotate back to symmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret_3.serialize_public_key() }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret_1, secret_2, secret_3] {\n        match sec {\n            EndpointSecret::Symmetric(key) => {\n                let sec = STANDARD.encode(key);\n                let wh = Webhook::new(&sec).unwrap();\n                wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n            }\n            EndpointSecret::Asymmetric(key) => {\n                let msg_id = last_headers.get(\"svix-id\").unwrap().to_str().unwrap();\n                let timestamp = last_headers\n                    .get(\"svix-timestamp\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                l<|fim_suffix|>                let to_sign = format!(\"{msg_id}.{timestamp}.{}\", &last_body);\n                let found =\n                    signatures\n                        .split(' ')\n                        .filter(|x| x.starts_with(\"v1a,\"))\n                        .any(|signature| {\n                            let sig: Signature = Signature::from_slice(\n                                STANDARD\n                                    .decode(&signature[\"v1a,\".len()..])\n                                    .unwrap()\n                                    .as_slice(),\n                            )\n                            .unwrap();\n                            key.0.pk.verify(to_sign.as_bytes(), &sig).is_ok()\n                        });\n                assert!(found);\n            }\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_config() {\n    let mut cfg = get_default_test_config();\n    cfg.default_signature_type = DefaultSignatureType::Ed25519;\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let key1 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key1.starts_with(\"whpk_\"));\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let key2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key2.starts_with(\"whpk_\"));\n}\n\n#[tokio::test]\nasync fn test_custom_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Long key\n    let secret_2 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n    // Asymmetric key\n    let secret_3 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    assert_eq!(\n        secret_3.serialize_public_key(),\n        \"whpk_1SiA4o9hyqTCpIqC5V9HUakiiaeACeqfZTInDBbOir4=\"\n    );\n\n    let mut ep_in = EndpointIn {\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    ep_in.key = Some(secret_2.clone());\n    let endp_2 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // We rotate the key after because it's easier than setting json! for everything\n    ep_in.key = Some(secret_2.clone());\n    let endp_3 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp_3.id),\n            json!({\n                \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\",\n            }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    for (secret, ep) in [(secret_1, endp_1), (secret_2, endp_2), (secret_3, endp_3)]<|fim_middle|>", "completion": "let signatures = last_headers\n                    .get(\"svix-signature\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1177, 1181]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Idempotency middleware for the Axum server.\n//!\n//! The middleware first looks up the given key for an existing response before\n//! routing to the given endpoint's function, and caches any such results\n//! such that subsequent requests to that endpoint with the same key will return\n//! the same response.\n//!\n//! Responses are cached for twelve hours by default.\n\nuse std::{collections::HashMap, convert::Infallible, future::Future, pin::Pin, time::Duration};\n\nuse axum::{\n    body::Body,\n    extract::Request,\n    http::StatusCode,\n    response::{IntoResponse, Response},\n};\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse blake2::{Blake2b512, Digest};\nuse http::request::Parts;\nuse http_body_util::BodyExt as _;\nuse serde::{Deserialize, Serialize};\nuse tower::Service;\n\nuse super::cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue};\nuse crate::error::Error;\n\n/// Returns the default expiry period for cached responses\nconst fn expiry_default() -> Duration {\n    Duration::from_secs(60 * 60 * 12)\n}\n\n/// Returns the default expiry period for the starting lock\nconst fn expiry_starting() -> Duration {\n    Duration::from_secs(5)\n}\n\n/// Returns the duration to sleep before retrying to find a [`SerializedResponse::Finished`] in the\n/// cache\nconst fn wait_duration() -> Duration {\n    Duration::from_millis(200)\n}\n\n/// The data structure containing all necessary components of a response ready to be (de)serialized\n/// from/into the cache\n#[derive(Deserialize, Serialize)]\nenum SerializedResponse {\n    Start,\n    Finished {\n        code: u16,\n        headers: Option<HashMap<String, Vec<u8>>>,\n        body: Option<Vec<u8>>,\n    },\n}\n\nkv_def!(IdempotencyKey, SerializedResponse);\n\nimpl IdempotencyKey {\n    fn new(auth_token: &str, key: &str, url: &str) -> IdempotencyKey {\n        let mut hasher = Blake2b512::new();\n\n        hasher.update(auth_token);\n        hasher.update(\":\");\n        hasher.update(key);\n        hasher.update(\":\");\n        hasher.update(url);\n\n        l<|fim_suffix|>        // FIXME: add (previously omitted) prefix: `SVIX_IDEMPOTENCY_CACHE`\n        IdempotencyKey(STANDARD.encode(res))\n    }\n}\n\n#[derive(thiserror::Error, Debug)]\npub enum ConversionToResponseError {\n    #[error(\"the status code is out of bounds\")]\n    StatusError(#[from] http::status::InvalidStatusCode),\n\n    #[error(\"a header name is invalid\")]\n    FromStr(#[from] http::header::InvalidHeaderName),\n    #[error(\"a header value is invalid\")]\n    InvalidHeaderValue(#[from] http::header::InvalidHeaderValue),\n}\n\n/// Will never error as long as Redis doesn't corrupt -- never use this with anything but values\n/// from Redis which were put in via the idempotency service from known good requests.\nfn finished_serialized_response_to_response(\n    code: u16,\n    headers: Option<HashMap<String, Vec<u8>>>,\n    body: Option<Vec<u8>>,\n) -> Result<Response, ConversionToResponseError> {\n    let mut out = body.unwrap_or_default().into_response();\n\n    let status = out.status_mut();\n    *status = code.try_into()?;\n\n    if let Some(resp_headers) = headers {\n        let headers = out.headers_mut();\n        *headers = resp_headers\n            .iter()\n            .map(|(k, v)| Ok((k.parse()?, http::HeaderValue::from_bytes(v)?)))\n            .collect::<Result<_, ConversionToResponseError>>()?;\n    }\n\n    Ok(out)\n}\n\nasync fn resolve_service<S>(mut service: S, req: Request) -> Response\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    match service.call(req).await {\n        Ok(res) => res.into_response(),\n        Err(e) => match e {},\n    }\n}\n\n/// The idempotency middleware itself -- used via the [`Router::layer`] method\n#[derive(Clone)]\npub struct IdempotencyService<S: Clone> {\n    pub cache: Cache,\n    pub service: S,\n}\n\nimpl<S> Service<Request> for IdempotencyService<S>\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    type Response = Response;\n    type Error = Infallible;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(\n        &mut self,\n        cx: &mut std::task::Context<'_>,\n    ) -> std::task::Poll<Result<(), Self::Error>> {\n        self.service.poll_ready(cx)\n    }\n\n    fn call(&mut self, req: Request) -> Self::Future {\n        let mut service = self.service.clone();\n        let cache = self.cache.clone();\n\n        if !cache.is_none() {\n            Box::pin(async move {\n                let (parts, body) = req.into_parts();\n\n                // If not a POST request, simply resolve the service as usual\n                if parts.method != http::Method::POST {\n                    return Ok(resolve_service(service, Request::from_parts(parts, body)).await);\n                }\n\n                // Retrieve `IdempotencyKey` from header and URL parts, but returning the service\n                // normally in the event a key could not be created.\n                let key = if let Some(key) = get_key(&parts) {\n                    key\n                } else {\n                    return Ok(resolve_service(service, Request::from_parts(parts, body)).await);\n                };\n\n                // Set the [`SerializedResponse::Start`] lock if the key does not exist in the cache\n                // returning whether the value was set\n                let lock_acquired = if let Ok(lock_acquired) = cache\n                    .set_if_not_exists(&key, &SerializedResponse::Start, expiry_starting())\n                    .await\n                {\n                    lock_acquired\n                } else {\n                    return Ok(StatusCode::INTERNAL_SERVER_ERROR.into_response());\n                };\n\n                // If the lock was not set, first check the cache for a `Finished` cache value. If\n                // it is instead `None` or the value is a `Start` lock, then enter a loop checking\n                // it every 200ms.\n                //\n                // If the loop times out, then reset the lock and proceed to resolve the service.\n                //\n                // If at any point the cache returns an `Err`, then return 500 response\n                if !lock_acquired {\n                    match cache.get::<SerializedResponse>(&key).await {\n                        Ok(Some(SerializedResponse::Finished {\n                            code,\n                            headers,\n                            body,\n                        })) => {\n                            return Ok(finished_serialized_response_to_response(\n                                code, headers, body,\n                            )\n                            .unwrap_or_else(|_| StatusCode::INTERNAL_SERVER_ERROR.into_response()))\n                        }\n\n                        Ok(Some(SerializedResponse::Start)) | Ok(None) => {\n                            if let Ok(Some(SerializedResponse::Finished {\n                                code,\n                                headers,\n                                body,\n                            })) = lock_loop(&cache, &key).await\n                            {\n                                return Ok(finished_serialized_response_to_response(\n                                    code, headers, body,\n                                )\n                                .unwrap_or_else(|_| {\n                                    StatusCode::INTERNAL_SERVER_ERROR.into_response()\n                                }));\n                            } else {\n                                // Set the lock if it returns `Ok(None)` and continue to resolve\n                                // as normal, but return 500 if the lock cannot be set\n                                if !matches!(\n                                    cache\n                                        .set_if_not_exists(\n                                            &key,\n                                            &SerializedResponse::Start,\n                                            expiry_starting(),\n            <|fim_middle|>", "completion": "let res = hasher.finalize();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/idempotency.rs", "node_type": "let_declaration", "line_range": [71, 71]}
{"prompt": "<|fim_prefix|>  let mut cfg = get_default_test_config();\n    cfg.encryption = Encryption::new([1; 32]);\n    let (client, jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading the existing secret works\n    assert_eq!(secret, secret2);\n\n    // Generate a new encrypted secret\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": secret }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading and saving works for encrypted\n    assert_eq!(secret, secret2);\n    jh.abort();\n\n    // Make sure we can't read it with the secret unset\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n    client\n        .get::<IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::INTERNAL_SERVER_ERROR,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_invalid_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_too_short = \"whsec_C2FVsBQIhrscChlQIM+b5sSYspob\".to_owned();\n    let secret_too_long =\n        \"whsec_V09IYXZUaFJoSnFobnpJQkpPMXdpdGFNWnJsRzAxdXZCeTVndVpwRmxSSXFsc0oyYzBTRWRUekJhYnlaZ0JSRGNPQ3BGZG1xYjFVVmRGQ3UK\"\n            .to_owned();\n    let invalid_prefix = \"hwsec_C2FVsBQIhrscChlQIM+b5sSYspob7oDazfgh\".to_owned();\n\n    for sec in [secret_too_short, secret_too_long, invalid_prefix] {\n        let _: IgnoredAny = client\n            .post(\n                &format!(\"api/v1/app/{app_id}/endpoint/\"),\n                json!({\n                    \"url\": \"http://www.example.com\",\n                    \"version\": 1,\n                    \"secret\": sec,\n                }),\n                StatusCode::UNPROCESSABLE_ENTITY,\n            )\n            .await\n            .unwrap();\n    }\n}\n\nfn new_message_attempt_at_time(\n    timestamp: DateTime<Utc>,\n    status: MessageStatus,\n    endp_id: &EndpointId,\n    msg_id: &MessageId,\n) -> messageattempt::ActiveModel {\n    messageattempt::ActiveModel {\n        endp_id: Set(endp_id.clone()),\n        msg_id: Set(msg_id.clone()),\n        id: Set(MessageAttemptId::new(timestamp.into(), None)),\n        status: Set(status),\n        created_at: Set(timestamp.into()),\n        url: Set(\"http://www.example.com\".into()),\n        response_status_code: Set(200),\n        response_duration_ms: Set(1000),\n        response: Set(\"{}\".into()),\n        trigger_type: Set(MessageAttemptTriggerType::Scheduled),\n\n        ..ActiveModelTrait::default()\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_stats() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp_id = create_test_endpoint(&client, &app_id, \"https://gabagool.deli\")\n        .await\n        .unwrap()\n        .id;\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 0);\n    assert_eq!(stats.sending, 0);\n\n    let last_msg_time = {\n        // Create the relevant Stats records manually, otherwise\n        // it's difficult to test exact state of messagedestinations.\n\n        let cfg = get_default_test_config();\n        let db = Arc::new(cfg);\n        let db = svix_server::db::init_db(&db).await;\n\n        l<|fim_suffix|>\n        let msg = message::ActiveModel {\n            app_id: Set(app_id.clone()),\n            org_id: Set(OrganizationId::new(None, None)),\n            expiration: Set(Utc::now().into()),\n            event_type: Set(EventTypeName(\"test.ing\".into())),\n            created_at: Set((now - chrono::Duration::minutes(65)).into()),\n            id: Set(MessageId::new(\n                (now - chrono::Duration::minutes(65)).into(),\n                None,\n            )),\n            ..message::ActiveModel::new()\n        }\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(60),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(45),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(30),\n            MessageStatus::Sending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap()\n        .created_at\n    };\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 2);\n    assert_eq!(stats.sending, 1);\n\n    let stats_filtered: EndpointStatsOut = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}&until={}\",\n                urlencoding::encode(&last_msg_time.to_rfc3339()),\n                urlencoding::encode(&Utc::now().to_rfc3339()),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats_filtered.fail, 0);\n    assert_eq!(stats_filtered.success, 0);\n    assert_eq!(stats_filtered.pending, 0);\n    assert_eq!(stats_filtered.sending, 1);\n\n    let _: IgnoredAny = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}\",\n                urlencoding::encode(&(Utc::now() - chrono::Duration::days(29)).to_rfc3339()),\n            ),\n            StatusCode::BAD_REQUEST,\n        )\n        .await\n        .unwrap();\n}\n\n/// We used to store the secret in the DB without a type marker, check loading those still works\n#[tokio::test]\nasync fn test_legacy_endpoint_secret() {\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let db = Arc::new(cfg);\n    let db = svix_server::db::init_db(&db).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_throwaway = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    let raw_key = STANDARD.decode(\"5gasBsSw3Nvf3ugNYVJIqnRVYPW7hPts\").unwrap();\n    let secret_1 = EndpointSecret::Symmetric(raw_key.clone());\n\n    let ep_in = EndpointIn {\n        key: Some(secret_throwaway.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Set the raw value to the database (like legacy)\n    db.execute(Statement::from_sql_and_values(\n        DatabaseBackend::Postgres,\n        \"UPDATE endpoint SET key = $1 WHERE id = $2\",\n        vec![raw_key.clone().into(), endp_1.id.clone().into()],\n    ))\n    .await\n    .unwrap();\n\n    let endp_1 = get_endpoint(&client, &app_id, &endp_1.id).await.unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let (secret, ep) = (secret_1, endp_1);\n    assert_eq!(\n        secret.serialize_public_key(),\n        client\n            .get::<EndpointSecretOutTest>(\n         <|fim_middle|>", "completion": "let now = Utc::now();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1487, 1487]}
{"prompt": "<|fim_prefix|>ient, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        &cfg.retry_schedule.len() + 1,\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, MessageStatus::Fail);\n        assert_eq!(i.response_status_code, 0);\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n}\n\n#[tokio::test]\nasync fn test_message_attempts_empty_retry_schedule() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![];\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let (status_code, msg_status, attempt_count) =\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None);\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(status_code);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, msg_status);\n        println!(\"{} {status_code}\", i.response_status_code);\n        assert_eq!(\n            i.response_status_code,\n            TryInto::<i16>::try_into(status_code.as_u16()).unwrap()\n        );\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_combined_before_after_filtering() {\n    let (client, _) = start_svix_server().await;\n\n    let app = create_test_app(&client, \"test_app\").await.unwrap();\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ep = create_test_endpoint(&client, &app.id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    // Send a first message\n    create_test_message(\n        &client,\n        &app.id,\n        serde_json::json!({\n            \"test\": 1,\n        }),\n    )\n    .await\n    .unwrap();\n\n    // Wait until attempt was made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let ts1 = chrono::Utc::now();\n\n    // Send another two messages\n    for i in 1..=2 {\n        create_test_message(\n            &client,\n            &app.id,\n            serde_json::json!({\n                \"test\": i + 1,\n            }),\n        )\n        .await\n        .unwrap();\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 3 {\n            anyhow::bail!(\"list len {}, not 3\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n    let ts2 = chrono::Utc::now();\n\n    // Send another three messages\n    for i in 1..=3 {\n        create_test_message(\n            &client,\n            &app.id,\n            serde_json::json!({\n                \"test\": i + 3,\n            }),\n        )\n        .await\n        .unwrap();\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        i<|fim_suffix|>\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // No timestamp-based filtering should yield all 6 messages\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{}/attempt/endpoint/{}/?limit=10\", app.id, ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 6);\n\n    // Limiting the time to the second batch should only yield those two messages\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/\\\n                 ?limit=10&before={ts2}&after={ts1}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // We got all the data there is for the filters we supplied..\n    assert!(out.done);\n    assert_eq!(out.data.len(), 2);\n\n    // .. but we can still iterate from here when loosening filters.\n    let prev_iter = out.prev_iterator.unwrap();\n    let iter = out.iterator.unwrap();\n\n    // Can get the older three messages via pagination\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/?iterator={prev_iter}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 3);\n\n    // Can get the earlier message via pagination\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/?iterator={iter}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 1);\n}\n\n#[tokio::test]\nasync fn test_pagination_by_endpoint() {\n    let (client, _jh) = start_svix_server().await;\n\n    // Setup six endpoints and six messages so there's a sufficient number to test pagination\n    let app = create_test_app(&client, \"app1\").await.unwrap();\n\n    let mut receivers = Vec::new();\n    for _ in 0..6 {\n        receivers.push(TestReceiver::start(StatusCode::OK));\n    }\n\n    let mut eps = Vec::new();\n    for receiver in &receivers {\n        eps.push(\n            create_test_endpoint(&client, &app.id, &receiver.endpoint)\n                .await\n                .unwrap(),\n        );\n    }\n\n    let mut messages = Vec::new();\n    for i in 1..=6usize {\n        messages.push(\n            async {\n                // the requests that depend on time (ie, `before` and `after`) can flake if too many\n                // messages are created too close together.\n                // This short sleep aims to separate them a little so we can get clean counts.\n                tokio::time::sleep(Duration::from_millis(10)).await;\n                create_test_message(\n                    &client,\n                    &app.id,\n                    serde_json::json!({\n                        \"test\": i,\n                    }),\n                )\n                .await\n                .unwrap()\n            }\n            .await,\n        );\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        for endp_id in eps.iter().map(|ep| &ep.id) {\n            let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{}/attempt/endpoint/{endp_id}/\", app.id),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if list.data.len() != 6 {\n                anyhow::bail!(\"list len {}, not 6\", list.data.len());\n            }\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // By endpoint\n    for ep in &eps {\n        let all_attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        <|fim_middle|>", "completion": "if list.data.len() != 6 {\n            anyhow::bail!(\"list len {}, not 6\", list.data.len());\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "if_expression", "line_range": [948, 950]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse chrono::Utc;\nuse sea_orm::{entity::prelude::*, sea_query::OnConflict, ActiveValue::Set, TryIntoModel};\n\nuse crate::{\n    core::types::{metadata::Metadata, EndpointId},\n    error,\n};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"endpointmetadata\")]\npub struct Model {\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: EndpointId,\n    pub created_at: DateTimeWithTimeZone,\n    pub updated_at: DateTimeWithTimeZone,\n    pub data: Metadata,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]\npub enum Relation {\n    #[sea_orm(\n        belongs_to = \"super::endpoint::Entity\",\n        from = \"Column::Id\",\n        to = \"super::endpoint::Column::Id\",\n        on_update = \"NoAction\",\n        on_delete = \"Restrict\"\n    )]\n    Endpoint,\n}\n\nimpl Related<super::endpoint::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Endpoint.def()\n    }\n}\n\n#[axum::async_trait]\nimpl ActiveModelBehavior for ActiveModel {\n    async fn before_save<C>(mut self, _db: &C, _insert: bool) -> Result<Self, DbErr>\n    where\n        C: ConnectionTrait,\n    {\n        self.updated_at = Set(Utc::now().into());\n        Ok(self)\n    }\n}\n\nimpl ActiveModel {\n    pub fn new(endp_id: EndpointId, metadata: impl Into<Option<Metadata>>) -> Self {\n        let id = Set(endp_id);\n        let data = Set(metadata.into().unwrap_or_default());\n        let timestamp = Utc::now();\n        Self {\n            id,\n            data,\n            created_at: Set(timestamp.into()),\n            updated_at: Set(timestamp.into()),\n        }\n    }\n\n    /// Upserts the record if it's new or updated, AND data is nonempty. Otherwise the record is\n    /// ignored or destroyed as appropriate.\n    p<|fim_suffix|>}\n\nimpl Entity {\n    pub fn upsert(am: ActiveModel) -> sea_orm::Insert<ActiveModel> {\n        Self::insert(am).on_conflict(\n            OnConflict::column(Column::Id)\n                .update_columns([Column::Data, Column::UpdatedAt])\n                .to_owned(),\n        )\n    }\n}\n<|fim_middle|>", "completion": "pub async fn upsert_or_delete(self, db: &impl ConnectionTrait) -> error::Result<Model> {\n        let data = self.data.clone().take().unwrap_or_default();\n\n        if data.is_empty() {\n            let model = self.clone().try_into_model()?;\n            self.delete(db).await?;\n            return Ok(model);\n        }\n\n        Ok(Entity::upsert(self).exec_with_returning(db).await?)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/endpointmetadata.rs", "node_type": "function_item", "line_range": [66, 76]}
{"prompt": "<|fim_prefix|>use std::{convert::Infallible, net::SocketAddr, sync::Arc, time::Duration};\n\nuse axum::{\n    extract::{FromRequestParts, Path, State},\n    http::{self, request},\n    routing::{get, post},\n    Router,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::{\n        api::{MessagePollerConsumerPollOptions, PollingEndpointMessageOut, Svix},\n        error::Error,\n    },\n    ForwardRequest, PollerInput, ReceiverOutput, TransformationConfig, TransformerInput,\n    TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\nuse tracing::instrument;\nuse types::{IntegrationId, IntegrationState, InternalState, SerializableRequest, Unvalidated};\n\nuse crate::{\n    config::{PollerInputOpts, PollerReceiverConfig, WebhookReceiverConfig},\n    webhook_receiver::types::SerializablePayload,\n};\n\nmod config;\nmod types;\nmod verification;\n\nfn router() -> Router<InternalState> {\n    Router::new()\n        .route(\n            \"/webhook/:integration_id\",\n            post(route).put(route).get(route).patch(route),\n        )\n        .route(\n            \"/webhook/:integration_id/\",\n            post(route).put(route).get(route).patch(route),\n        )\n        .route(\"/health\", get(health_handler))\n}\nstatic START_TIME: once_cell::sync::Lazy<std::time::Instant> =\n    once_cell::sync::Lazy::new(std::time::Instant::now);\n\nfn get_uptime_seconds() -> u64 {\n    START_TIME.elapsed().as_secs()\n}\n#[derive(serde::Serialize)]\nstruct HealthResponse {\n    pub status: &'static str,\n    pub version: &'static str,\n    pub uptime: u64,\n}\nasync fn health_handler() -> impl axum::response::IntoResponse {\n    let health_response = HealthResponse {\n        status: \"OK\",\n        version: env!(\"CARGO_PKG_VERSION\"),\n        uptime: get_uptime_seconds(),\n    };\n    axum::Json(health_response)\n}\npub async fn run(\n    listen_addr: SocketAddr,\n    routes: Vec<WebhookReceiverConfig>,\n    transformer_tx: TransformerTx,\n) -> std::io::Result<()> {\n    once_cell::sync::Lazy::force(&START_TIME);\n    let state = InternalState::from_receiver_configs(routes, transformer_tx)\n        .await\n        .map_err(std::io::Error::other)?;\n\n    let router = router().with_state(state);\n\n    tracing::info!(\"Listening on: {listen_addr}\");\n    let listener = tokio::net::TcpListener::bind(listen_addr).await.unwrap();\n    axum::serve(listener, router)\n        .await\n        .map_err(std::io::Error::other)\n}\n\nstruct WebhookIdHeader(Option<String>);\n\n#[async_trait]\nimpl<S> FromRequestParts<S> for WebhookIdHeader {\n    type Rejection = Infallible;\n\n    async fn from_request_parts(\n        parts: &mut request::Parts,\n        _: &S,\n    ) -> Result<Self, Self::Rejection> {\n        Ok(Self(\n            parts\n                .headers\n                .get(\"svix-id\")\n                .or_else(|| parts.headers.get(\"webhook-id\"))\n                .and_then(|val| Some(val.to_str().ok()?.to_owned())),\n        ))\n    }\n}\n\n#[instrument(\n    skip_all,\n    level = \"error\",\n    fields(\n        msg_id = _msg_id,\n        integration_id = integration_id.as_ref(),\n    )\n)]\nasync fn route(\n    Path(integration_id): Path<IntegrationId>,\n    WebhookIdHeader(_msg_id): WebhookIdHeader,\n    State(InternalState {\n        routes,\n        transformer_tx,\n    }): State<InternalState>,\n    req: SerializableRequest<Unvalidated>,\n) -> Result<http::StatusCode, http::StatusCode> {\n    let IntegrationState {\n        verifier,\n        output,\n        transformation,\n    } = routes\n        .get(&integration_id)\n        .ok_or(http::StatusCode::NOT_FOUND)?;\n\n    let req = req.validate(verifier).await.inspect_err(|code| {\n        tracing::warn!(\"validation failed: {code}\");\n    })?;\n\n    <|fim_suffix|>\n\n    handle(payload, Arc::clone(output)).await\n}\n\n// FIXME: Really odd return type - artifact of being extracted from the HTTP server\nasync fn handle(\n    payload: ForwardRequest,\n    output: Arc<Box<dyn ReceiverOutput>>,\n) -> Result<http::StatusCode, http::StatusCode> {\n    tracing::debug!(\"forwarding request\");\n    Ok(match output.handle(payload).await {\n        Ok(_) => http::StatusCode::NO_CONTENT,\n        Err(e) => {\n            tracing::error!(\"Error forwarding request: {}\", e);\n            http::StatusCode::INTERNAL_SERVER_ERROR\n        }\n    })\n}\n\n/// Figures out how to build a JSON object from the payload, optionally running it through a\n/// transformation.\n///\n/// WRT \"raw\" payloads, the return value here is going to be a JSON object regardless of whether\n/// or not the queue producer wants \"raw\" data.\n///\n/// When there's no transformation defined we therefore attempt to parse the body as json.\n/// When a transformation is defined, we branch to see if it expects string or json input.\n///\n/// For either case, we expect the value produced to match the schema of a [`ForwardRequest`].\nasync fn parse_payload(\n    payload: &SerializablePayload,\n    transformation: Option<&TransformationConfig>,\n    transformer_tx: TransformerTx,\n) -> Result<ForwardRequest, http::StatusCode> {\n    match transformation {\n        Some(xform) => {\n            let input = match xform.format() {\n                TransformerInputFormat::String => {\n                    TransformerInput::String(payload.as_string().map_err(|_| {\n                        tracing::error!(\"Unable to parse request body as string\");\n                        http::StatusCode::BAD_REQUEST\n                    })?)\n                }\n                TransformerInputFormat::Json => {\n                    TransformerInput::Json(payload.as_json().map_err(|_| {\n                        tracing::error!(\"Unable to parse request body as json\");\n                        http::StatusCode::BAD_REQUEST\n                    })?)\n                }\n            };\n            transform(input, xform.source().clone(), transformer_tx).await\n        }\n        // Keep the original payload as-is if there's no transformation specified, but stuff the\n        // whole thing into the payload field.\n        // The as_json() only gets us to `Value`, so we also need a `from_value` call to marshal\n        // into a [`ForwardRequest`] type.\n        None => Ok(ForwardRequest {\n            payload: payload.as_json().map_err(|_| {\n                tracing::error!(\"Unable to parse request body as json\");\n                http::StatusCode::BAD_REQUEST\n            })?,\n        }),\n    }\n}\n\n/// Attempts to run the payload through a js transformation.\nasync fn transform(\n    input: TransformerInput,\n    script: String,\n    tx: TransformerTx,\n) -> Result<ForwardRequest, http::StatusCode> {\n    let (job, callback) = TransformerJob::new(script, input);\n    if let Err(e) = tx.send(job) {\n        tracing::error!(\"transformations are not available: {}\", e);\n        return Err(http::StatusCode::INTERNAL_SERVER_ERROR);\n    }\n\n    match callback.await {\n        // This is the only \"good\" outcome giving a RHS value for the assignment.\n        // All other match arms should bail with a non-2xx status.\n        Ok(Ok(TransformerOutput::Object(obj))) => Ok(serde_json::from_value(\n            serde_json::Value::Object(obj),\n        )\n        .map_err(|e| {\n            tracing::error!(\"transformation produced invalid payload: {}\", e);\n            http::StatusCode::INTERNAL_SERVER_ERROR\n        })?),\n        Ok(Ok(TransformerOutput::Invalid)) => {\n            tracing::error!(\"transformation produced invalid payload\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n        _ => {\n            tracing::error!(\"transformation failed\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n    }\n}\n\nstruct SvixEventsPoller {\n    name: String,\n    input_opts: PollerInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n    output: Arc<Box<dyn ReceiverOutput>>,\n}\n\n#[async_trait]\nimpl PollerInput for SvixEventsPoller {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        run_inner(self).await\n    }\n}\n\nimpl PollerReceiverConfig {\n    pub async fn into_poller_input(\n        self,\n        transformer_tx: TransformerTx,\n    ) -> std::io::Result<Box<|fim_middle|>", "completion": "let payload = parse_payload(\n        req.payload(),\n        transformation.as_ref(),\n        transformer_tx.clone(),\n    )\n    .await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/mod.rs", "node_type": "let_declaration", "line_range": [130, 135]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse aide::axum::{\n    routing::{delete_with, get_with, post_with},\n    ApiRouter,\n};\nuse axum::{\n    extract::{Path, State},\n    Json,\n};\nuse chrono::{DateTime, Duration, Utc};\nuse futures::FutureExt;\nuse hyper::StatusCode;\nuse schemars::JsonSchema;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set, IntoActiveModel, TransactionTrait};\nuse serde::{Deserialize, Serialize};\nuse serde_json::value::RawValue;\nuse svix_server_derive::{aide_annotate, ModelIn, ModelOut};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::{\n        cache::Cache,\n        message_app::CreateMessageApp,\n        permissions,\n        types::{\n            ApplicationIdOrUid, EndpointId, EventChannel, EventChannelSet, EventTypeName,\n            EventTypeNameSet, MessageAttemptTriggerType, MessageId, MessageUid, OrganizationId,\n        },\n    },\n    db::models::{application, message, messagecontent},\n    error::{http_error_on_conflict, Error, HttpError, Result, ValidationErrorItem},\n    queue::{MessageTaskBatch, TaskQueueProducer},\n    v1::{\n        endpoints::application::{create_app_from_app_in, ApplicationIn},\n        utils::{\n            filter_and_paginate_time_limited, openapi_tag, validation_error, validation_errors,\n            ApplicationMsgPath, ApplicationPath, EventTypesQueryParams, JsonStatus, ListResponse,\n            ModelIn, ModelOut, PaginationDescending, PaginationLimit, ReversibleIterator,\n            ValidatedJson, ValidatedQuery,\n        },\n    },\n    AppState,\n};\n\npub fn validate_channels_msg(channels: &EventChannelSet) -> Result<(), ValidationError> {\n    let len = channels.0.len();\n    if !(1..=5).contains(&len) {\n        Err(validation_error(\n            Some(\"channels\"),\n            Some(\"Channels must have at least 1 and at most 5 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, Deserialize, Serialize)]\npub struct RawPayload(pub Box<RawValue>);\n\nimpl JsonSchema for RawPayload {\n    fn schema_name() -> String {\n        \"RawPayload\".to_string()\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        serde_json::Value::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\nimpl std::fmt::Display for RawPayload {\n    f<|fim_suffix|>}\n\nimpl Eq for RawPayload {}\n\nimpl PartialEq for RawPayload {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.get() == other.0.get()\n    }\n}\n\nimpl RawPayload {\n    pub fn from_string(val: String) -> serde_json::Result<Self> {\n        Ok(Self(RawValue::from_string(val)?))\n    }\n}\n\npub fn validate_raw_payload_is_object(payload: &RawPayload) -> Result<(), ValidationError> {\n    // Verify it's an object/map\n    if payload.0.get().starts_with('{') {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"payload\"),\n            Some(\"Payload must be an object.\"),\n        ))\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Serialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageIn {\n    /// Optional unique identifier for the message\n    #[validate]\n    #[serde(rename = \"eventId\", skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<MessageUid>,\n    #[validate]\n    pub event_type: EventTypeName,\n    #[validate(custom = \"validate_raw_payload_is_object\")]\n    #[serde(alias = \"payload\", alias = \"data\")]\n    #[schemars(example = \"example_payload\")]\n    pub payload: RawPayload,\n    /// List of free-form identifiers that endpoints can filter by\n    #[validate(custom = \"validate_channels_msg\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 5))]\n    pub channels: Option<EventChannelSet>,\n    #[validate(range(min = 5, max = 90))]\n    #[serde(default = \"default_90\")]\n    #[schemars(example = \"default_90\")]\n    pub payload_retention_period: i64,\n    #[serde(rename = \"transformationsParams\")]\n    #[schemars(skip)]\n    pub extra_params: Option<MessageInExtraParams>,\n\n    /// Optionally creates a new application alongside the message.\n    ///\n    /// If the application id or uid that is used in the path already exists,\n    /// this argument is ignored.\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub application: Option<ApplicationIn>,\n}\n\nimpl MessageIn {\n    fn payload(&self) -> Vec<u8> {\n        if let Some(params) = &self.extra_params {\n            if let Some(raw_payload) = &params.raw_payload {\n                return raw_payload.as_bytes().to_owned();\n            }\n        }\n\n        self.payload.0.get().as_bytes().to_owned()\n    }\n}\n\n#[derive(Clone, Debug, Default, PartialEq, Eq, Deserialize, Serialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageInExtraParams {\n    raw_payload: Option<String>,\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nfn example_payload() -> serde_json::Value {\n    serde_json::json!({\n        \"email\": \"test@example.com\",\n        \"username\": \"test_user\"\n    })\n}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for MessageIn {\n    type ActiveModel = message::ActiveModel;\n\n    fn update_model(self, model: &mut message::ActiveModel) {\n        let MessageIn {\n            uid,\n            event_type,\n            channels,\n            payload_retention_period,\n            ..\n        } = self;\n\n        let expiration = Utc::now() + Duration::days(payload_retention_period);\n\n        model.uid = Set(uid);\n        model.event_type = Set(event_type);\n        model.expiration = Set(expiration.with_timezone(&Utc).into());\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageOut {\n    /// Optional unique identifier for the message\n    #[serde(rename = \"eventId\")]\n    pub uid: Option<MessageUid>,\n    pub event_type: EventTypeName,\n    #[schemars(example = \"example_payload\")]\n    pub payload: RawPayload,\n    /// List of free-form identifiers that endpoints can filter by\n    #[schemars(length(min = 1, max = 5), example = \"example_channel_set\")]\n    pub channels: Option<EventChannelSet>,\n    pub id: MessageId,\n    #[serde(rename = \"timestamp\")]\n    pub created_at: DateTime<Utc>,\n}\n\nimpl MessageOut {\n    pub fn from_msg_and_payload(\n        model: message::Model,\n        content: Option<Vec<u8>>,\n        with_content: bool,\n    ) -> Self {\n        let payload = if with_content {\n            let payload = content\n                .and_then(|p| match serde_json::from_slice(&p) {\n                    Ok(v) => Some(v),\n                    Err(e) => {\n                        tracing::error!(\"Failed to parse content: {e}\");\n                        None\n                    }\n                })\n                .or(model.legacy_payload);\n            RawPayload::from_string(match payload {\n                Some(payload) => serde_json::to_string(&payload).expect(\"Can never fail\"),\n                None => r#\"{\"expired\":true}\"#.to_string(),\n            })\n            .expect(\"Can never fail\")\n        } else {\n            RawPayload::from_string(\"{}\".to_string()).expect(\"Can never fail\")\n        };\n\n        Self {\n            uid: model.uid,\n            event_type: model.event_type,\n            payload,\n            channels: model.channels,\n            id: model.id,\n            created_at: model.created_at.into(),\n        }\n    }\n}\n\nfn default_true() -> bool {\n    true\n}\n\nfn default_90() -> i64 {\n    90\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct ListMessagesQueryParams {\n    /// Filter response based on the channel\n    #[validate]\n    channel: Option<EventChannel>,\n    /// Only include items created before a certain date\n    before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    after: Option<DateTime<Utc>>,\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// List all of the application's mes<|fim_middle|>", "completion": "fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        f.write_str(self.0.get())\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/message.rs", "node_type": "function_item", "line_range": [77, 79]}
{"prompt": "<|fim_prefix|>       task.msg_id\n                        ))\n                    })?;\n                (\n                    msg,\n                    msg_content,\n                    task.force_endpoint,\n                    task.trigger_type,\n                    0,\n                    MessageStatus::Sending,\n                )\n            }\n        };\n\n    span.record(\"msg_id\", &msg.id.0);\n    span.record(\"app_id\", &msg.app_id.0);\n    span.record(\"org_id\", &msg.org_id.0);\n\n    let payload = msg_content\n        .and_then(|m| String::from_utf8(m.payload).ok())\n        .or_else(|| {\n            msg.legacy_payload\n                .take()\n                .and_then(|m| serde_json::to_string(&m).ok())\n        });\n\n    let Some(payload) = payload else {\n        tracing::warn!(\"Message payload is NULL; payload has most likely expired\");\n        return Ok(());\n    };\n\n    let Some(create_message_app) = CreateMessageApp::layered_fetch(\n        cache,\n        db,\n        None,\n        msg.org_id.clone(),\n        msg.app_id.clone(),\n        Duration::from_secs(30),\n    )\n    .await?\n    else {\n        tracing::info!(\"Application doesn't exist: {}\", &msg.app_id);\n        return Ok(());\n    };\n\n    let endpoints: Vec<CreateMessageEndpoint> = create_message_app\n        .filtered_endpoints(trigger_type, &msg.event_type, msg.channels.as_ref())\n        .iter()\n        .filter(|endpoint| match force_endpoint.as_ref() {\n            Some(endp_id) => endp_id == &endpoint.id,\n            None => true,\n        })\n        .cloned()\n        .collect();\n\n    let futures = endpoints.into_iter().map(|endpoint| {\n        let task = MessageTask {\n            msg_id: msg.id.clone(),\n            app_id: create_message_app.id.clone(),\n            endpoint_id: endpoint.id.clone(),\n            attempt_count,\n            trigger_type,\n        };\n\n        dispatch_message_task(\n            &worker_context,\n            &msg,\n            &create_message_app,\n            task,\n            &payload,\n            endpoint,\n            status,\n        )\n    });\n\n    let join = future::join_all(futures).await;\n\n    let errs: Vec<_> = join.iter().filter(|x| x.is_err()).collect();\n    if !errs.is_empty() {\n        return Err(Error::generic(format_args!(\n            \"Some dispatches failed unexpectedly: {errs:?}\",\n        )));\n    }\n\n    Ok(())\n}\n\npub static LAST_QUEUE_POLL: LazyLock<AtomicU64> = LazyLock::new(|| get_unix_timestamp().into());\n\nasync fn update_last_poll_time() {\n    LAST_QUEUE_POLL.swap(get_unix_timestamp(), Ordering::Relaxed);\n}\n\n/// Listens on the message queue for new tasks\n#[allow(clippy::too_many_arguments)]\npub async fn queue_handler(\n    cfg: &Configuration,\n    cache: Cache,\n    db: DatabaseConnection,\n    queue_tx: TaskQueueProducer,\n    mut queue_rx: TaskQueueConsumer,\n    op_webhook_sender: OperationalWebhookSender,\n) -> Result<()> {\n    let recv_deadline = Duration::from_secs(cfg.queue_max_poll_secs.into());\n\n    static NUM_WORKERS: AtomicUsize = AtomicUsize::new(0);\n\n    let task_limit = cfg.worker_max_tasks;\n    if task_limit == 0 {\n        tracing::info!(\"Worker concurrent task limit: unlimited\");\n    } else {\n        tracing::info!(\"Worker concurrent task limit: {}\", task_limit);\n    }\n\n    let webhook_client = WebhookClient::new(\n        cfg.whitelist_subnets.clone(),\n        Some(Arc::new(vec![\"backend\".to_owned()])),\n        cfg.dangerous_disable_tls_verification,\n        cfg.proxy_config.as_ref(),\n    );\n\n    tokio::spawn(\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_millis(500));\n            loop {\n                interval.tick().await;\n                let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                if num_workers > 0 {\n                    tracing::info!(\"{} active workers\", num_workers);\n                }\n            }\n        }\n        .instrument(tracing::error_span!(\n            \"worker_monitor\",\n            instance_id = tracing::field::Empty\n        )),\n    );\n\n    loop {\n        if task_limit > 0 {\n            let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n            i<|fim_suffix|>        }\n\n        if crate::is_shutting_down() {\n            tokio::join!(async move {\n                let mut interval = tokio::time::interval(Duration::from_millis(500));\n                loop {\n                    interval.tick().await;\n                    let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                    if num_workers > 0 {\n                        tracing::info!(\n                            \"{} active workers, waiting to shut down worker.\",\n                            num_workers\n                        );\n                    } else {\n                        tracing::info!(\"No active workers, shutting down worker.\");\n                        break;\n                    }\n                }\n            });\n            break;\n        }\n\n        match queue_rx.receive_all(recv_deadline).await {\n            Ok(batch) => {\n                for delivery in batch {\n                    let cfg = cfg.clone();\n                    let cache = cache.clone();\n                    let db = db.clone();\n                    let queue_tx = queue_tx.clone();\n                    let queue_task = delivery.task.clone();\n                    let op_webhook_sender = op_webhook_sender.clone();\n                    let webhook_client = webhook_client.clone();\n\n                    tokio::spawn(async move {\n                        NUM_WORKERS.fetch_add(1, Ordering::Relaxed);\n                        let worker_context = WorkerContext {\n                            cfg: &cfg,\n                            db: &db,\n                            cache: &cache,\n                            op_webhook_sender: &op_webhook_sender,\n                            queue_tx: &queue_tx,\n                            webhook_client: &webhook_client,\n                        };\n\n                        let queue_task =\n                            Arc::try_unwrap(queue_task).unwrap_or_else(|arc| (*arc).clone());\n                        if process_queue_task(worker_context, queue_task)\n                            .await\n                            .is_err()\n                        {\n                            if let Err(err) = delivery.nack().await {\n                                tracing::error!(\n                                    \"Error sending 'nack' to Redis after task execution error: {}\",\n                                    err\n                                );\n                            }\n                        } else if let Err(err) = delivery.ack().await {\n                            tracing::error!(\n                                \"Error sending 'ack' to Redis after successful task execution: {}\",\n                                err\n                            );\n                        }\n\n                        NUM_WORKERS.fetch_sub(1, Ordering::Relaxed);\n                    });\n                }\n            }\n            Err(err) => {\n                tracing::error!(\"Error receiving task: {:?}\", err);\n                sleep(tokio::time::Duration::from_millis(10)).await;\n            }\n        }\n\n        update_last_poll_time().await;\n    }\n\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests {\n    use std::collections::HashMap;\n\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use bytes::Bytes;\n    use ed25519_compact::Signature;\n\n    use super::{bytes_to_string, generate_msg_headers, sign_msg, CaseSensitiveHeaderMap};\n    use crate::core::{\n        cryptography::{AsymmetricKey, Encryption},\n        types::{BaseId, EndpointHeaders, EndpointSecret, EndpointSecretInternal, MessageId},\n    };\n\n    // [`generate_msg_headers`] tests\n    const TIMESTAMP: i64 = 1;\n    const WHITELABEL_HEADERS: bool = false;\n    const BODY: &str = \"{\\\"test\\\": \\\"body\\\"}\";\n    const ENDPOINT_SIGNING_KEYS: &[&EndpointSecretInternal] = &[];\n    const ENDPOINT_URL: &str = \"http://localhost:8071\";\n\n    /// Utility function that returns the default set of headers before configurable header are\n    /// accounted for\n    fn mock_headers() -> (CaseSensitiveHeaderMap, MessageId) {\n        let id = MessageId::new(None, None);\n\n        let signatures = sign_msg(\n           <|fim_middle|>", "completion": "if num_workers > task_limit.into() {\n                tokio::time::sleep(Duration::from_millis(100)).await;\n                continue;\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "if_expression", "line_range": [977, 980]}
{"prompt": "<|fim_prefix|>externally.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub struct EndpointSecretInternal {\n    marker: EndpointSecretMarker,\n\n    key: Vec<u8>,\n}\n\nimpl EndpointSecretInternal {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MINUS_ONE: usize = Self::KEY_SIZE - 1;\n\n    fn new(\n        encryption: &Encryption,\n        type_: EndpointSecretType,\n        key: &[u8],\n    ) -> crate::error::Result<Self> {\n        Ok(Self {\n            marker: EndpointSecretMarker {\n                type_,\n                encrypted: encryption.enabled(),\n            },\n            key: encryption.encrypt(key)?,\n        })\n    }\n\n    pub fn generate_symmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let buf: [u8; Self::KEY_SIZE] = rand::thread_rng().gen();\n        Self::new(encryption, EndpointSecretType::Hmac256, &buf)\n    }\n\n    pub fn generate_asymmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let key = AsymmetricKey::generate();\n        Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())\n    }\n\n    fn into_vec(mut self) -> Vec<u8> {\n        let marker: u8 = self.marker.to_u8();\n\n        let mut vec = vec![marker];\n        vec.append(&mut self.key);\n        vec\n    }\n\n    fn from_vec(v: Vec<u8>) -> crate::error::Result<Self> {\n        // Legacy had exact size\n        match v.len() {\n            0..=Self::KEY_SIZE_MINUS_ONE => Err(crate::error::Error::generic(\"Value too small\")),\n            Self::KEY_SIZE => Ok(Self {\n                marker: EndpointSecretMarker {\n                    type_: EndpointSecretType::Hmac256,\n                    encrypted: false,\n                },\n                key: v,\n            }),\n            _ => {\n                let marker = EndpointSecretMarker::from_u8(v[0])?;\n                Ok(Self {\n                    marker,\n                    key: v[1..].to_vec(),\n                })\n            }\n        }\n    }\n\n    pub fn into_endpoint_secret(\n        self,\n        encryption: &Encryption,\n    ) -> crate::error::Result<EndpointSecret> {\n        let key = self.key(encryption)?;\n        Ok(match self.type_() {\n            EndpointSecretType::Hmac256 => EndpointSecret::Symmetric(key),\n            EndpointSecretType::Ed25519 => {\n                EndpointSecret::Asymmetric(AsymmetricKey::from_slice(&key[..])?)\n            }\n        })\n    }\n\n    pub fn from_endpoint_secret(\n        endpoint_secret: EndpointSecret,\n        encryption: &Encryption,\n    ) -> crate::error::Result<Self> {\n        Ok(match endpoint_secret {\n            EndpointSecret::Symmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Hmac256, &key)?\n            }\n            EndpointSecret::Asymmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())?\n            }\n        })\n    }\n\n    pub fn sign(&self, encryption: &Encryption, bytes: &[u8]) -> Vec<u8> {\n        let key = self.key(encryption).unwrap();\n        // FIXME: remove unwrap\n        match self.marker.type_() {\n            EndpointSecretType::Hmac256 => hmac_sha256::HMAC::mac(bytes, key).to_vec(),\n            EndpointSecretType::Ed25519 => AsymmetricKey::from_slice(&key[..])\n                .unwrap()\n                .0\n                .sk\n                .sign(bytes, None)\n                .to_vec(),\n        }\n    }\n\n    fn key(&self, encryption: &Encryption) -> crate::error::Result<Vec<u8>> {\n        Ok(if self.marker.encrypted {\n            if encryption.enabled() {\n                encryption.decrypt(&self.key)?\n            } else {\n                return Err(crate::error::Error::generic(\n                    \"main_secret unset, can't decrypt key\",\n                ));\n            }\n        } else {\n            self.key.to_vec()\n        })\n    }\n\n    pub fn type_(&self) -> &EndpointSecretType {\n        self.marker.type_()\n    }\n}\n\ni<|fim_suffix|>\nimpl<'de> Deserialize<'de> for EndpointSecretInternal {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n\n        String::deserialize(deserializer).and_then(|string| {\n            // For backwards compat when loading from ExpiringSigningKeys. Going forward we just b64 it\n            if string.starts_with(EndpointSecretType::Hmac256.secret_prefix()) {\n                Ok(Self {\n                    marker: EndpointSecretMarker {\n                        type_: EndpointSecretType::Hmac256,\n                        encrypted: false,\n                    },\n                    key: string\n                        .get(EndpointSecretType::Hmac256.secret_prefix().len()..)\n                        .ok_or_else(|| Error::custom(\"invalid prefix\".to_string()))\n                        .and_then(|string| {\n                            STANDARD\n                                .decode(string)\n                                .map_err(|err| Error::custom(err.to_string()))\n                        })?,\n                })\n            } else {\n                let buf = STANDARD\n                    .decode(string)\n                    .map_err(|err| Error::custom(err.to_string()))?;\n                Self::from_vec(buf).map_err(|err| Error::custom(err.to_string()))\n            }\n        })\n    }\n}\n\nimpl From<EndpointSecretInternal> for sea_orm::Value {\n    fn from(v: EndpointSecretInternal) -> Self {\n        Self::Bytes(Some(Box::new(v.into_vec())))\n    }\n}\n\nimpl sea_orm::TryGetable for EndpointSecretInternal {\n    fn try_get_by<I: sea_orm::ColIdx>(\n        res: &sea_orm::QueryResult,\n        index: I,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get_by(res, index) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }\n\n    fn try_get(\n        res: &sea_orm::QueryResult,\n        pre: &str,\n        col: &str,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get(res, pre, col) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }\n}\n\nimpl sea_orm::sea_query::Nullable for EndpointSecretInternal {\n    fn null() -> sea_orm::Value {\n        sea_orm::Value::Bytes(None)\n    }\n}\n\nimpl sea_orm::sea_query::ValueType for EndpointSecretInternal {\n    fn try_from(v: sea_orm::Value) -> Result<Self, sea_orm::sea_query::ValueTypeErr> {\n        match v {\n            sea_orm::Value::Bytes(Some(x)) => {\n                EndpointSecretInternal::from_vec(*x).map_err(|_| sea_orm::sea_query::ValueTypeErr)\n            }\n            _ => Err(sea_orm::sea_query::ValueTypeErr),\n        }\n    }\n\n    fn type_name() -> String {\n        stringify!(EndpointSecretInternal).to_owned()\n    }\n\n    fn column_type() -> sea_orm::sea_query::ColumnType {\n        sea_orm::sea_query::ColumnType::Binary(\n            Self::KEY_SIZE\n                .try_into()\n                .expect(\"Key size is not more than u32::MAX\"),\n        )\n    }\n\n    fn array_type() -> sea_orm::sea_query::ArrayType {\n        sea_orm::sea_query::ArrayType::Bytes\n    }\n}\n\n/// The external representation of the endpoint secret.\n/// This one is used for serializing to and from customers.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum EndpointSecret {\n    Symmetric(Vec<u8>),\n    Asymmetric(AsymmetricKey),\n}\n\nimpl EndpointSecret {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MAX: usize = 75;\n\n    pub fn serialize_secret_key(&self) -> String {\n        match self {\n            Self::Symmetric(key) => {\n                format!(\n                    \"{}{}\",\n       <|fim_middle|>", "completion": "impl Serialize for EndpointSecretInternal {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        serializer.serialize_str(&STANDARD.encode(self.clone().into_vec()))\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "impl_item", "line_range": [833, 840]}
{"prompt": "<|fim_prefix|>    Ok(Ok(_)) => { /* nothing to do  */ }\n            // The outer Result is for the timeout, the inner is for if there was some other failure\n            // during `send`.\n            Ok(Err(_)) | Err(_) => {\n                anyhow::bail!(\"failed to complete handshake with Webhook Relay server: remote didn't accept start message\");\n            }\n        }\n\n        // The assumption is the very first message we get from the websocket reader will be the\n        // response to our `MessageOut::Start` but it could also be any number of control messages.\n        // Keep reading until we see a `MessageIn::Start` or give up after some attempts.\n        const MAX_ATTEMPTS: u8 = 10;\n        let mut attempts = 0;\n        let start_response = loop {\n            if attempts > MAX_ATTEMPTS {\n                anyhow::bail!(\"failed to complete handshake with Webhook Relay server: no response from remote\");\n            }\n            attempts += 1;\n\n            match tokio::time::timeout(SERVER_PING_PERIOD, ws_rx.next()).await {\n                Err(_timeout) => continue,\n                Ok(None) => {\n                    anyhow::bail!(\"no response from server for start message\");\n                }\n                Ok(Some(msg)) => {\n                    let data = match msg? {\n                        // Control messages.\n                        Message::Close(Some(CloseFrame { code, reason }))\n                            if code == Policy && reason == SOCKET_IN_USE_REASON =>\n                        {\n                            return Err(TokenInUse.into())\n                        }\n                        Message::Close(_) => {\n                            anyhow::bail!(\"Relay server refused connection\");\n                        }\n                        Message::Ping(_) | Message::Pong(_) | Message::Frame(_) => continue,\n\n                        // Messages that carry data we care to process.\n                        Message::Text(s) => s.into(),\n                        Message::Binary(bytes) => bytes,\n                    };\n\n                    match serde_json::from_slice::<MessageIn>(&data)? {\n                        // This is what we're waiting to see. A `MessageOut::Start` sent to the writer\n                        // should result in a `MessageInStart` coming back on the reader.\n                        MessageIn::Start { data, .. } => break data,\n                        MessageIn::Event { .. } => continue,\n                    };\n                }\n            }\n        };\n\n        if show_welcome_message {\n            printdoc!(\n                r#\"\n\n                Webhook Relay is now listening at:\n                {}\n\n                All requests on this endpoint will be forwarded to your local URL:\n                {}\n\n                View logs and debug information at:\n                {}\n\n                \"#,\n                receive_url(&start_response.token),\n                self.local_url,\n                view_url(&self.token),\n            );\n        } else {\n            // Shows that a reconnection attempt succeeded after some failing initial attempts.\n            println!(\"Connected!\");\n        }\n\n        set.spawn({\n            let local_url = self.local_url.clone();\n            let http_client = self.http_client.clone();\n            async move {\n                read_from_ws_loop(ws_rx, remote_tx, local_url.clone(), http_client.clone())\n                    .await\n                    .inspect_err(|e| eprintln!(\"read loop terminated: {e:#}\"))\n            }\n        });\n\n        set.spawn(async move {\n            send_to_ws_loop(remote_rx, ws_tx)\n                .await\n                .inspect_err(|e| eprintln!(\"write loop terminated: {e:#}\"))\n        });\n\n        // If any task terminates, trash the rest so we can reconnect.\n        if set.join_next().await.is_some() {\n            set.shutdown().await;\n        }\n\n        Ok(())\n    }\n}\n\npub async fn listen(\n    local_url: url::Url,\n    relay_token: String,\n    relay_debug_url: Option<&str>,\n    relay_disable_security: bool,\n    disable_tls_verification: bool,\n) -> Result<()> {\n    <|fim_suffix|>\n    let api_host = relay_debug_url.unwrap_or(DEFAULT_API_HOST);\n    let token = format!(\"c_{relay_token}\");\n\n    let websocket_url = format!(\"{scheme}://{api_host}/{API_PREFIX}/listen/\").parse()?;\n\n    let http_client = HttpClient::builder()\n        .danger_accept_invalid_certs(disable_tls_verification)\n        .build()?;\n\n    let mut client = Client {\n        token,\n        websocket_url,\n        local_url,\n        http_client,\n    };\n\n    const MAX_BACKOFF: Duration = Duration::from_millis(5000);\n    let backoff_schedule = [\n        Duration::ZERO,\n        Duration::from_millis(100),\n        Duration::from_millis(1000),\n        MAX_BACKOFF,\n    ];\n\n    let mut attempt_count = 0;\n    let mut last_attempt = Instant::now();\n\n    // We may ditch this token, generating a new one on the fly, depending on how the server\n    // responds when we connect.\n    let orig_token = client.token.clone();\n    loop {\n        // Any termination Ok or Err... try to reconnect.\n        let show_welcome_message = attempt_count == 0 || orig_token != client.token;\n\n        if let Err(e) = client.connect(show_welcome_message).await {\n            eprintln!(\"Failed to connect to Webhook Relay: {e:#}\");\n            if e.downcast_ref::<TokenInUse>().is_some() {\n                eprintln!(\"Generating a new token for this session.\");\n                client.token = {\n                    let relay_token = generate_token()?;\n                    format!(\"c_{relay_token}\")\n                };\n            }\n        } else {\n            eprintln!(\"Failed to connect to Webhook Relay\");\n        }\n\n        // Reset the backoff schedule if it's been a while since we've seen a disconnect.\n        if last_attempt.elapsed() > MAX_BACKOFF * 2 {\n            // N.b. attempt_count `0` is special because that's what prompts the printing of a\n            // welcome message in `Client::connect`.\n            // When we reset here, starting at `0` here will still avoid the\n            // re-print because we increment after selecting the sleep duration.\n            attempt_count = 0;\n        }\n\n        let backoff = *backoff_schedule.get(attempt_count).unwrap_or(&MAX_BACKOFF);\n        eprintln!(\"Reattempting connection in: {}ms\", backoff.as_millis());\n\n        attempt_count += 1;\n        last_attempt = Instant::now();\n\n        tokio::time::sleep(backoff).await;\n    }\n}\n\nfn receive_url(token: &str) -> String {\n    format!(\"https://play.svix.com/in/{token}/\")\n}\n\nfn view_url(token: &str) -> String {\n    format!(\"https://play.svix.com/view/{token}/\")\n}\n\ntype S = WebSocketStream<MaybeTlsStream<TcpStream>>;\n\nstruct WsConnection {\n    stream: S,\n}\n\nimpl WsConnection {\n    async fn new(websocket_url: &url::Url) -> Result<Self> {\n        let request = websocket_url.to_string().into_client_request()?;\n        let (stream, _resp) = connect_async(request)\n            .await\n            .inspect_err(|e| eprintln!(\"{e}\"))\n            .context(\"failed to connect to websocket server\")?;\n\n        Ok(Self { stream })\n    }\n}\n\nasync fn read_from_ws_loop(\n    mut rx: SplitStream<S>,\n    tx: UnboundedSender<MessageOut>,\n    local_url: url::Url,\n    client: HttpClient,\n) -> Result<()> {\n    // We expect to see roughly _at least one Ping_ in each `SERVER_PING_PERIOD`.\n    // Other messages may arrive ahead of this schedule.\n    // Tracking the time each message is received, we can know if the server has been quiet for too\n    // long, possibly requiring us to reconnect.\n    let mut last_msg = Instant::now();\n\n    loop {\n        const REMOTE_SERVER_CLOSED: &str = \"remote server closed connection\";\n\n        match tokio::time::timeout(SERVER_PING_PERIOD, rx.next()).await {\n            Err(_timeout_hit) => {\n                // Generous. 1.5x the ping frequency. If we go that long without\n                // seeing anything from the server, force a reconnect.\n                if last_msg.elapsed() > SERVER_PING_PERIOD + (SERVER_PING_PERIOD / 2) {\n                    anyhow::bail!(REMOTE_SERVER_CLOSED);\n                }\n            }\n            // Stream empty/closed\n            Ok(None) => break,\n      <|fim_middle|>", "completion": "let scheme = if relay_disable_security { \"ws\" } else { \"wss\" };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/mod.rs", "node_type": "let_declaration", "line_range": [220, 220]}
{"prompt": "<|fim_prefix|>// The location as a [`Vec`] of [`String`]s -- often in the form `[\"body\", \"field_name\"]`,\n    /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, detail: String) -> Self {\n        Self {\n            status,\n            body: HttpErrorBody::Standard(StandardHttpError { code, detail }),\n        }\n    }\n\n    pub fn bad_request(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::BAD_REQUEST,\n            code.unwrap_or_else(|| \"generic_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Generic error\".to_owned()),\n        )\n    }\n\n    pub fn not_found(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_FOUND,\n            code.unwrap_or_else(|| \"not_found\".to_owned()),\n            detail.unwrap_or_else(|| \"Entity not found\".to_owned()),\n        )\n    }\n\n    pub fn unauthorized(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::UNAUTHORIZED,\n            code.unwrap_or_else(|| \"authentication_failed\".to_owned()),\n            detail.unwrap_or_else(|| \"Incorrect authentication credentials.\".to_owned()),\n        )\n    }\n\n    pub fn permission_denied(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::FORBIDDEN,\n            code.unwrap_or_else(|| \"insufficient access\".to_owned()),\n            detail.unwrap_or_else(|| \"Insufficient access for the given operation.\".to_owned()),\n        )\n    }\n\n    pub fn conflict(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::CONFLICT,\n            code.unwrap_or_else(|| \"conflict\".to_owned()),\n            detail.unwrap_or_else(|| \"A conflict has occurred\".to_owned()),\n        )\n    }\n\n    pub fn unprocessable_entity(detail: Vec<ValidationErrorItem>) -> Self {\n        Self {\n            status: StatusCode::UNPROCESSABLE_ENTITY,\n            body: HttpErrorBody::Validation(ValidationHttpError { detail }),\n        }\n    }\n\n    pub fn internal_server_error(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::INTERNAL_SERVER_ERROR,\n            code.unwrap_or_else(|| \"server_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Internal Server Error\".to_owned()),\n        )\n    }\n\n    pub fn not_implemented(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_IMPLEMENTED,\n            code.unwrap_or_else(|| \"not_implemented\".to_owned()),\n            detail.unwrap_or_else(|| \"This API endpoint is not yet implemented.\".to_owned()),\n        )\n    }\n\n    pub fn too_large(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::PAYLOAD_TOO_LARGE,\n            code.unwrap_or_else(|| \"payload_too_large\".to_owned()),\n            detail.unwrap_or_else(|| \"Request payload is too large.\".to_owned()),\n        )\n    }\n}\n\nimpl From<HttpError> for Error {\n    fn from(err: HttpError) -> Error {\n        Error::http(err)\n    }\n}\n\nimpl fmt::Display for HttpError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match &self.body {\n            HttpErrorBody::Standard(StandardHttpError { code, detail }) => write!(\n                f,\n                \"status={} code=\\\"{code}\\\" detail=\\\"{detail}\\\"\",\n                self.status\n            ),\n\n            HttpErrorBody::Validation(ValidationHttpError { detail }) => {\n                write!(\n                    f,\n                    \"status={} detail={}\",\n                    self.status,\n                    serde_json::to_string(&detail)\n                        .unwrap_or_else(|e| format!(\"\\\"unserializable error for {e}\\\"\"))\n                )\n            }\n        }\n    }\n}\n\nimpl IntoResponse for HttpError {\n    fn into_response(self) -> Response {\n        (self.status, Json(self.body)).into_response()\n    }\n}\n\nimpl From<ErrorType> for Error {\n    fn from(typ: ErrorType) -> Self {\n        Self { trace: vec![], typ }\n    }\n}\n\n// FIXME - delete\nimpl From<crate::core::webhook_http_client::Error> for Error {\n    fn from(err: webhook_http_client::Error) -> Error {\n        match err {\n            webhook_http_client::Error::TimedOut => Self::timeout(err),\n            _ => Error::generic(err),\n        }\n    }\n}\n\n/// Utility function for Converting a [`DbErr`] into an [`Error`].\n///\n/// The error \"duplicate key value violates unique constraint\" is converted to\n/// an HTTP \"conflict\" error. This is to be used in `map_err` calls on\n/// creation/update of records.\npub fn http_error_on_conflict(db_err: DbErr) -> Error {\n    if is_conflict_err(&db_err) {\n        HttpError::conflict(None, None).into()\n    } else {\n        Error::database(db_err)\n    }\n}\n\npub fn is_conflict_err(db_err: &DbErr) -> bool {\n    use DbErr as E;\n    let rt_err = match db_err {\n        E::Exec(e) | E::Query(e) | E::Conn(e) => e,\n        // If sqlx ever extends this enum, I want a compile time error so we're forced to update this function.\n        // Hence we list out all the enumerations, rather than using a default match statement\n        E::TryIntoErr { .. }\n        | E::ConvertFromU64(_)\n        | E::UnpackInsertId\n        | E::UpdateGetPrimaryKey\n        | E::RecordNotFound(_)\n        | E::AttrNotSet(_)\n        | E::Custom(_)\n        | E::Type(_)\n        | E::Json(_)\n        | E::Migration(_)\n        | E::RecordNotInserted\n        | E::RecordNotUpdated\n        | E::ConnectionAcquire(_) => return false,\n    };\n\n    let sqlx_err = m<|fim_suffix|>\n\n    sqlx_err\n        .as_database_error()\n        .and_then(|e| e.code())\n        .filter(|code| code == \"23505\")\n        .is_some()\n}\n\npub fn is_timeout_error(db_err: &DbErr) -> bool {\n    let runtime_err = match &db_err {\n        DbErr::Conn(e) | DbErr::Exec(e) | DbErr::Query(e) => e,\n        _ => return false,\n    };\n\n    let sqlx_err = match runtime_err {\n        RuntimeErr::SqlxError(e) => e,\n        RuntimeErr::Internal(_) => return false,\n    };\n\n    match sqlx_err.as_database_error() {\n        // STUPID - no other good way to ID statement timeouts\n        Some(e) => e\n            .message()\n            .contains(\"canceling statement due to statement timeout\"),\n        None => false,\n    }\n}\n\n/// Returns true if the DbErr results from weirdness with a slow/long connection.\n/// This is distinct from [is_timeout_error], which reports whether the underlying\n/// query actually timed out on the pg side.\n///\n/// [is_connection_timeout_error] reports whether the connection to pg itself was slow\n/// for some reason.\npub fn is_connection_timeout_error(db_err: &DbErr) -> bool {\n    use DbErr as E;\n    let rt_err = match db_err {\n        E::ConnectionAcquire(_) | E::Conn(_) => return true,\n        E::Exec(e) | E::Query(e) => e.to_string(),\n\n        // If sqlx ever extends this enum, I want a compile time error so we're forced to update this function.\n        // Hence we list out all the enumerations, rather than using a default match statement\n        E::TryIntoErr { .. }\n        | E::ConvertFromU64(_)\n        | E::UnpackInsertId\n        | E::UpdateGetPrimaryKey\n        | E::RecordNotFound(_)\n        | E::AttrNotSet(_)\n        | E::Custom(_)\n        | E::Type(_)\n        | E::Json(_)\n        | E::Migration(_)\n        | E::RecordNotInserted\n        | E::RecordNotUpdated => return false,\n    };\n\n    const ERRORS: [&str; 3] = [\n        \"Connection pool timed out\",\n        \"Connection reset by peer\",\n        \"unexpected end of file\",\n    ];\n    for e in ERRORS {\n        if rt_err.contains(e) {\n            return true;\n        }\n    }\n\n    false\n}\n<|fim_middle|>", "completion": "match rt_err {\n        RuntimeErr::SqlxError(e) => e,\n        RuntimeErr::Internal(_) => return false,\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "match_expression", "line_range": [476, 479]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct StreamingStreamListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct StreamingStreamCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct StreamingStream<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> StreamingStream<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List of all the organization's streams.\n    pub async fn list(\n        &self,\n        options: Option<StreamingStreamListOptions>,\n    ) -> Result<ListResponseStreamOut> {\n        let StreamingStreamListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/stream\")\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Creates a new stream.\n    pub async fn create(\n        &self,\n        stream_in: StreamIn,\n        options: Option<StreamingStreamCreateOptions>,\n    ) -> Result<StreamOut> {\n        let StreamingStreamCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/stream\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(stream_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get a stream by id or uid.\n    pub async fn get(&self, stream_id: String) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Update a stream.\n    pub async fn update(&self, stream_id: String, stream_in: StreamIn) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_body_param(stream_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete a stream.\n    pub async fn delete(&self, stream_id: String) -> Result<()> {\n        crate::request::Request::new(http1::Method::DELETE, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .returns_nothing()\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Partially update a stream.\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub async fn patch(&self, stream_id: String, stream_patch: StreamPatch) -> Result<StreamOut> {\n        crate::request::Request::new(http1::Method::PATCH, \"/api/v1/stream/{stream_id}\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_body_param(stream_patch)\n            .execute(self.cfg)\n            .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/streaming_stream.rs", "node_type": "function_item", "line_range": [91, 97]}
{"prompt": "<|fim_prefix|>        visit_schema(&mut schema.json_schema)\n        }\n    }\n}\n\n/// Adds the `Idempotency-Key` header parameter to all `POST` operations in the schema.\nfn add_idempotency_to_post(openapi: &mut OpenApi) {\n    // The header's value can be any valid string\n    let string_schema = aide::gen::in_context(|ctx| String::json_schema(&mut ctx.schema));\n\n    let s = openapi::SchemaObject {\n        json_schema: string_schema,\n        external_docs: None,\n        example: None,\n    };\n\n    let idempotency_key_data = openapi::ParameterData {\n        name: \"idempotency-key\".to_string(),\n        description: Some(\"The request's idempotency key\".to_string()),\n        required: false,\n        deprecated: None,\n        format: openapi::ParameterSchemaOrContent::Schema(s),\n        example: None,\n        examples: indexmap::indexmap! {},\n        explode: None,\n        extensions: indexmap::indexmap! {},\n    };\n\n    if let Some(paths) = &mut openapi.paths {\n        for (_, op) in &mut paths.paths {\n            match op {\n                openapi::ReferenceOr::Reference { reference, .. } => {\n                    // References to operations should never appear in our\n                    // schema since all our operations are unique, and we\n                    // don't reference any 3rd party/external operations.\n                    tracing::warn!(\n                        \"Unexpected operation reference encountered in OpenAPI schema: {reference}\"\n                    );\n                }\n                openapi::ReferenceOr::Item(op) => {\n                    if let Some(post) = &mut op.post {\n                        post.parameters.push(ReferenceOr::Item(Parameter::Header {\n                            parameter_data: idempotency_key_data.clone(),\n                            style: openapi::HeaderStyle::Simple,\n                        }));\n                    }\n                }\n            }\n        }\n    }\n}\n\n/// Remove schemas from `components.schemas` of the spec which are under normal\n/// circumstances not referenced. At the moment these are struct schemas used\n/// by query parameters and path placeholders.\nfn remove_unneeded_schemas(openapi: &mut OpenApi) {\n    if let Some(components) = &mut openapi.components {\n        components.schemas.retain(|name, _| {\n            !(name.ends_with(\"Path\")\n                || name.ends_with(\"QueryParams\")\n                || name.starts_with(\"Pagination\"))\n        });\n    }\n}\n\n/// Replaces the `examples` property of a schema with a singular `example`\n/// property.\n/// OpenAPI <=3.0 used `example` as an extension, >=3.1 standardized `examples`.\nfn replace_multiple_examples(openapi: &mut OpenApi) {\n    let mut visitor = schemars::visit::SetSingleExample {\n        retain_examples: false,\n    };\n\n    if let Some(components) = &mut openapi.components {\n        for (_, schema_object) in &mut components.schemas {\n            visitor.visit_schema(&mut schema_object.json_schema);\n        }\n    }\n}\n\npub fn add_security_scheme(\n    api: aide::transform::TransformOpenApi<'_>,\n) -> aide::transform::TransformOpenApi<'_> {\n    api.security_scheme(\n        \"HTTPBearer\",\n        aide::openapi::SecurityScheme::Http {\n            scheme: \"bearer\".to_string(),\n            bearer_format: None,\n            description: Some(\"HTTP Bearer token passed in the `Authorization` header\".into()),\n            extensions: Default::default(),\n        },\n    )\n}\n\n/// Applies a list of hacks to the finished OpenAPI spec to make it usable with\n/// our tooling.\npub fn postprocess_spec(openapi: &mut OpenApi) {\n    let hacks = [\n        add_idempotency_to_post,\n        remove_unneeded_schemas,\n        replace_true_schemas,\n        replace_multiple_examples,\n    ];\n\n    for hack in hacks {\n        hack(openapi);\n    }\n}\n\n/// This module documents operational webhooks. To document one define a data\n/// struct first, use the [`webhook_event`] macro to generate a wrapping struct\n/// for it, then include it in the [`webhooks`] function call.\nmod webhooks {\n    use std::collections::HashMap;\n\n    use aide::openapi;\n    use schemars::JsonSchema;\n\n    <|fim_suffix|>\n\n    /// Documents the webhook specified by the type `T`.\n    fn document_webhook<T: Webhook>() -> (String, openapi::PathItem) {\n        let type_name = std::any::type_name::<T>()\n            .split(\"::\")\n            .last()\n            .expect(\"Last element of split can't be empty\")\n            .to_string();\n\n        let body_schema =\n            aide::gen::in_context(|ctx| ctx.schema.subschema_for::<T>().into_object());\n\n        let body_media = openapi::MediaType {\n            schema: Some(openapi::SchemaObject {\n                json_schema: body_schema.into(),\n                external_docs: None,\n                example: None,\n            }),\n            ..Default::default()\n        };\n\n        let body = openapi::RequestBody {\n            content: indexmap::indexmap! {\n                \"application/json\".to_string() => body_media,\n            },\n            ..Default::default()\n        };\n\n        let success_response = openapi::Response {\n            description:\n                \"Return any 2XX status to indicate that the data was received successfully\"\n                    .to_string(),\n            ..Default::default()\n        };\n\n        (\n            type_name.clone(),\n            openapi::PathItem {\n                post: Some(openapi::Operation {\n                    description: Some(T::description().to_string()),\n                    operation_id: Some(type_name.clone()),\n                    request_body: Some(openapi::ReferenceOr::Item(body)),\n                    responses: Some(openapi::Responses {\n                        responses: indexmap::indexmap! {\n                            openapi::StatusCode::Range(2) => openapi::ReferenceOr::Item(success_response),\n                        },\n                        ..Default::default()\n                    }),\n                    summary: Some(type_name),\n                    tags: vec![\"Webhooks\".to_string()],\n                    ..Default::default()\n                }),\n                ..Default::default()\n            },\n        )\n    }\n\n    trait Webhook: JsonSchema {\n        fn description() -> &'static str;\n    }\n\n    /// Generates a struct that wraps the webhook payload, adding the discriminant.\n    macro_rules! webhook_event {\n        ($name:ident, $payload_name:ident, $discriminant:expr, $description:literal) => {\n            #[allow(unused)]\n            struct $name {\n                data: $payload_name,\n                type_: String,\n            }\n\n            impl JsonSchema for $name {\n                fn schema_name() -> String {\n                    stringify!($name).to_string()\n                }\n\n                fn json_schema(\n                    gen: &mut schemars::gen::SchemaGenerator,\n                ) -> schemars::schema::Schema {\n                    #[derive(::schemars::JsonSchema)]\n                    #[allow(unused)]\n                    struct $name {\n                        data: $payload_name,\n                        #[serde(rename = \"type\")]\n                        type_: String,\n                    }\n\n                    let mut schema = $name::json_schema(gen).into_object();\n\n                    schema\n                        .object()\n                        .properties\n                        .get_mut(\"type\")\n                        .map(|type_property| {\n                            if let ::schemars::schema::Schema::Object(obj) = type_property {\n                                let discriminant_value =\n                                    serde_json::Value::String($discriminant.to_string());\n                                obj.enum_values = Some(vec![discriminant_value.clone()]);\n                                obj.metadata().default = Some(discriminant_value);\n                            }\n                        });\n                    schema.metadata().description = Some($description.to_string());\n\n                    schema.into()\n                }\n            }\n\n            impl Webhook for $name {\n                fn description() -> &'static str {\n                    $description\n                }\n            }\n        };\n    }\n\n    #[d<|fim_middle|>", "completion": "use crate::core::operational_webhooks::{\n        EndpointDisabledEventData, EndpointEvent, MessageAttemptEvent,\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/openapi.rs", "node_type": "use_declaration", "line_range": [270, 272]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct OrumIoConfig {\n    #[serde(rename = \"publicKey\")]\n    pub public_key: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl OrumIoConfig {\n    pub fn new(public_key: String) -> Self {\n        Self { public_key }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/orum_io_config.rs", "node_type": "impl_item", "line_range": [10, 14]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse aide::axum::{\n    routing::{delete_with, get_with, post_with},\n    ApiRouter,\n};\nuse axum::{\n    extract::{Path, State},\n    Json,\n};\nuse chrono::{DateTime, Duration, Utc};\nuse futures::FutureExt;\nuse hyper::StatusCode;\nuse schemars::JsonSchema;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set, IntoActiveModel, TransactionTrait};\nuse serde::{Deserialize, Serialize};\nuse serde_json::value::RawValue;\nuse svix_server_derive::{aide_annotate, ModelIn, ModelOut};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::{\n        cache::Cache,\n        message_app::CreateMessageApp,\n        permissions,\n        types::{\n            ApplicationIdOrUid, EndpointId, EventChannel, EventChannelSet, EventTypeName,\n            EventTypeNameSet, MessageAttemptTriggerType, MessageId, MessageUid, OrganizationId,\n        },\n    },\n    db::models::{application, message, messagecontent},\n    error::{http_error_on_conflict, Error, HttpError, Result, ValidationErrorItem},\n    queue::{MessageTaskBatch, TaskQueueProducer},\n    v1::{\n        endpoints::application::{create_app_from_app_in, ApplicationIn},\n        utils::{\n            filter_and_paginate_time_limited, openapi_tag, validation_error, validation_errors,\n            ApplicationMsgPath, ApplicationPath, EventTypesQueryParams, JsonStatus, ListResponse,\n            ModelIn, ModelOut, PaginationDescending, PaginationLimit, ReversibleIterator,\n            ValidatedJson, ValidatedQuery,\n        },\n    },\n    AppState,\n};\n\npub fn validate_channels_msg(channels: &EventChannelSet) -> Result<(), ValidationError> {\n    let len = channels.0.len();\n    if !(1..=5).contains(&len) {\n        Err(validation_error(\n            Some(\"channels\"),\n            Some(\"Channels must have at least 1 and at most 5 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, Deserialize, Serialize)]\npub struct RawPayload(pub Box<RawValue>);\n\nimpl JsonSchema for RawPayload {\n    fn schema_name() -> String {\n        \"RawPayload\".to_string()\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        serde_json::Value::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\nimpl std::fmt::Display for RawPayload {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        f.write_str(self.0.get())\n    }\n}\n\nimpl Eq for RawPayload {}\n\nimpl PartialEq for RawPayload {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.get() == other.0.get()\n    }\n}\n\nimpl RawPayload {\n    pub fn from_string(val: String) -> serde_json::Result<Self> {\n        Ok(Self(RawValue::from_string(val)?))\n    }\n}\n\np<|fim_suffix|>\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Serialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageIn {\n    /// Optional unique identifier for the message\n    #[validate]\n    #[serde(rename = \"eventId\", skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<MessageUid>,\n    #[validate]\n    pub event_type: EventTypeName,\n    #[validate(custom = \"validate_raw_payload_is_object\")]\n    #[serde(alias = \"payload\", alias = \"data\")]\n    #[schemars(example = \"example_payload\")]\n    pub payload: RawPayload,\n    /// List of free-form identifiers that endpoints can filter by\n    #[validate(custom = \"validate_channels_msg\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 5))]\n    pub channels: Option<EventChannelSet>,\n    #[validate(range(min = 5, max = 90))]\n    #[serde(default = \"default_90\")]\n    #[schemars(example = \"default_90\")]\n    pub payload_retention_period: i64,\n    #[serde(rename = \"transformationsParams\")]\n    #[schemars(skip)]\n    pub extra_params: Option<MessageInExtraParams>,\n\n    /// Optionally creates a new application alongside the message.\n    ///\n    /// If the application id or uid that is used in the path already exists,\n    /// this argument is ignored.\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub application: Option<ApplicationIn>,\n}\n\nimpl MessageIn {\n    fn payload(&self) -> Vec<u8> {\n        if let Some(params) = &self.extra_params {\n            if let Some(raw_payload) = &params.raw_payload {\n                return raw_payload.as_bytes().to_owned();\n            }\n        }\n\n        self.payload.0.get().as_bytes().to_owned()\n    }\n}\n\n#[derive(Clone, Debug, Default, PartialEq, Eq, Deserialize, Serialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageInExtraParams {\n    raw_payload: Option<String>,\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nfn example_payload() -> serde_json::Value {\n    serde_json::json!({\n        \"email\": \"test@example.com\",\n        \"username\": \"test_user\"\n    })\n}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for MessageIn {\n    type ActiveModel = message::ActiveModel;\n\n    fn update_model(self, model: &mut message::ActiveModel) {\n        let MessageIn {\n            uid,\n            event_type,\n            channels,\n            payload_retention_period,\n            ..\n        } = self;\n\n        let expiration = Utc::now() + Duration::days(payload_retention_period);\n\n        model.uid = Set(uid);\n        model.event_type = Set(event_type);\n        model.expiration = Set(expiration.with_timezone(&Utc).into());\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageOut {\n    /// Optional unique identifier for the message\n    #[serde(rename = \"eventId\")]\n    pub uid: Option<MessageUid>,\n    pub event_type: EventTypeName,\n    #[schemars(example = \"example_payload\")]\n    pub payload: RawPayload,\n    /// List of free-form identifiers that endpoints can filter by\n    #[schemars(length(min = 1, max = 5), example = \"example_channel_set\")]\n    pub channels: Option<EventChannelSet>,\n    pub id: MessageId,\n    #[serde(rename = \"timestamp\")]\n    pub created_at: DateTime<Utc>,\n}\n\nimpl MessageOut {\n    pub fn from_msg_and_payload(\n        model: message::Model,\n        content: Option<Vec<u8>>,\n        with_content: bool,\n    ) -> Self {\n        let payload = if with_content {\n            let payload = content\n                .and_then(|p| match serde_json::from_slice(&p) {\n                    Ok(v) => Some(v),\n                    Err(e) => {\n                        tracing::error!(\"Failed to parse content: {e}\");\n                        None\n                    }\n                })\n                .or(model.legacy_payload);\n            RawPayload::from_string(match payload {\n                Some(payload) => serde_json::to_string(&payload).expect(\"Can never fail\"),\n                None => r#\"{\"expired\":true}\"#.to_string(),\n            })\n            .expect(\"Can never fail\")\n        } else {\n            RawPayload::from_string(\"{}\".to_string()).expect(\"Can never fail\")\n        };\n\n        Self {\n            uid: model.uid,\n            event_type: model.event_type,\n            payload,\n            channels: model.channels,\n            id: model.id,\n            created_at: model.created_at.into(),\n        }\n    }\n}\n\nfn default_true() -> bool {\n    true\n}\n\nfn default_90() -> i64 {\n    90\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct ListMessagesQueryParams {\n    /// Filter response based on the channel\n    #[validate]\n    channel: Option<EventChannel>,\n    /// Only include items created before a certain date\n    before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    after: Option<DateTime<Utc>>,\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// List all of the application's messages.\n///\n/// The `before` parameter lets you filter all items created before a certain date and is ignored if an iterator is passed.\n/// The `after` parameter lets you filter all items created after a certain da<|fim_middle|>", "completion": "pub fn validate_raw_payload_is_object(payload: &RawPayload) -> Result<(), ValidationError> {\n    // Verify it's an object/map\n    if payload.0.get().starts_with('{') {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"payload\"),\n            Some(\"Payload must be an object.\"),\n        ))\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/message.rs", "node_type": "function_item", "line_range": [96, 106]}
{"prompt": "<|fim_prefix|>// Dead code is allowed because not everything is used in all of the tests\n#![allow(dead_code)]\n\nuse std::{\n    future::Future,\n    net::TcpListener,\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse anyhow::{Context, Result};\nuse axum::response::IntoResponse;\nuse http::HeaderMap;\nuse reqwest::{Client, RequestBuilder, StatusCode};\nuse serde::{de::DeserializeOwned, Serialize};\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::ConfigurationInner,\n    core::{\n        security::generate_org_token,\n        types::{BaseId, OrganizationId},\n    },\n    setup_tracing,\n};\nuse tokio::sync::mpsc;\nuse tracing::instrument::WithSubscriber;\n\npub mod common_calls;\n\n#[derive(Clone)]\npub struct TestClient {\n    base_uri: String,\n    auth_header: String,\n    client: Client,\n}\n\nimpl TestClient {\n    pub fn set_auth_header(&mut self, auth_header: String) {\n        self.auth_header = format!(\"Bearer {auth_header}\");\n    }\n}\n\nimpl TestClient {\n    pub fn new(base_uri: String, auth_token: &str) -> TestClient {\n        TestClient {\n            base_uri,\n            auth_header: format!(\"Bearer {auth_token}\"),\n            client: Client::new(),\n        }\n    }\n\n    fn build_uri(&self, endpoint: &str) -> String {\n        format!(\"{}/{endpoint}\", self.base_uri)\n    }\n\n    fn add_headers(&self, request: RequestBuilder) -> RequestBuilder {\n        request.header(\"Authorization\", &self.auth_header)\n    }\n\n    pub async fn get<O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        <|fim_suffix|>\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn get_without_response(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.get(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn post<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                resp.json()\n                    .await\n                    .context(\"error receiving/parsing response\")\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                Ok(())\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_with_idempotency<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        idempotency_key: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self\n            .add_headers(req)\n            .header(\"idempotency-key\", idempotency_key)\n            .json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json().await.context(\"error receiving/paring response\")\n    }\n\n    pub async fn put<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn put_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn delete(&self, endpoint: &str, expected_code: StatusCode) -> Result<()> {\n        let mut req = self.client.delete(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        if expected_code == StatusCode::NO_CONTENT {\n            let res_body = resp.text().await.context(\"error receiving response\")?;\n            anyhow::ensure!(res_body.is_empty());\n        }\n\n        Ok(())\n    }\n\n    pub async fn patch<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn patch_without_response<I: Serialize>(\n        &self,\n        endpoi<|fim_middle|>", "completion": "let mut req = self.client.get(self.build_uri(endpoint));", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/mod.rs", "node_type": "let_declaration", "line_range": [65, 65]}
{"prompt": "<|fim_prefix|> .collect();\n\n        if event_types.is_empty() {\n            Ok(Self(None))\n        } else {\n            let event_types = EventTypeNameSet(event_types);\n            event_types.validate().map_err(|e| {\n                HttpError::unprocessable_entity(validation_errors(vec![\"query\".to_owned()], e))\n            })?;\n            Ok(Self(Some(event_types)))\n        }\n    }\n}\n\nimpl OperationInput for EventTypesQueryParams {\n    fn operation_input(ctx: &mut aide::gen::GenContext, operation: &mut aide::openapi::Operation) {\n        // This struct must match what `EventTypesQuery` would be if we used a\n        // simple `#[derive(Deserialize)]` on it.\n        #[derive(JsonSchema)]\n        struct EventTypesQueryParams {\n            /// Filter response based on the event type\n            #[allow(unused)]\n            event_types: Option<EventTypeNameSet>,\n        }\n\n        Query::<EventTypesQueryParams>::operation_input(ctx, operation);\n    }\n}\n\npub async fn api_not_implemented() -> Result<()> {\n    Err(HttpError::not_implemented(None, None).into())\n}\n\npub fn validate_no_control_characters(str: &str) -> Result<(), ValidationError> {\n    let re = Regex::new(r\"[\\x00-\\x08]\").unwrap();\n    if re.is_match(str) {\n        return Err(validation_error(\n            Some(\"illegal_character\"),\n            Some(\"Control characters 0x00-0x08 not allowed.\"),\n        ));\n    }\n    Ok(())\n}\n\npub fn validate_no_control_characters_unrequired(\n    str: &UnrequiredField<String>,\n) -> Result<(), ValidationError> {\n    match str {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(str) => validate_no_control_characters(str),\n    }\n}\n\npub fn openapi_tag<T: AsRef<str>>(\n    tag: T,\n) -> impl Fn(TransformPathItem<'_>) -> TransformPathItem<'_> {\n    move |op| op.tag(tag.as_ref())\n}\n\npub fn openapi_desc<T: AsRef<str>>(\n    desc: T,\n) -> impl Fn(TransformOperation<'_>) -> TransformOperation<'_> {\n    move |op| op.description(desc.as_ref())\n}\n\npub fn get_unix_timestamp() -> u64 {\n    SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs()\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationPath {\n    pub app_id: ApplicationIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationEndpointPath {\n    pub app_id: ApplicationIdOrUid,\n    pub endpoint_id: EndpointIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgEndpointPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n    pub endpoint_id: EndpointIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgAttemptPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n    pub attempt_id: MessageAttemptId,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct EventTypeNamePath {\n    pub event_type_name: EventTypeName,\n}\n\n/// JsonStatus is a wrapper over `axum::extract::Json` as a handler output.\n///\n/// Setting the `STATUS` const parameter automatically sets the response\n/// status code, as well as inserting it into the aide documentation.\npub struct JsonStatus<const STATUS: u16, T: JsonSchema + Serialize>(pub T);\n\nimpl<const STATUS: u16, T: JsonSchema + Serialize> IntoResponse for JsonStatus<STATUS, T> {\n    fn into_response(self) -> axum::response::Response {\n        (\n            StatusCode::from_u16(STATUS).unwrap(),\n            axum::extract::Json(self.0),\n        )\n            .into_response()\n    }\n}\n\nimpl<const STATUS: u16, T: JsonSchema + Serialize> OperationOutput for JsonStatus<STATUS, T> {\n    type Inner = T;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        axum::extract::Json::<T>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        i<|fim_suffix|>    }\n}\n\n/// JsonStatusUpsert is a wrapper over `axum::extract::Json` as a handler\n/// output.\n///\n/// It is a special casing of `JsonStatus` for situations where a resource is\n/// either being updated or created within the same operation. In case of\n/// `Updated` HTTP 200 OK is returned, in case of `Created` HTTP 201 CREATED\n/// is returned.\npub enum JsonStatusUpsert<T: JsonSchema + Serialize> {\n    Updated(T),\n    Created(T),\n}\n\nimpl<T: JsonSchema + Serialize> IntoResponse for JsonStatusUpsert<T> {\n    fn into_response(self) -> axum::response::Response {\n        let (status, body) = match self {\n            JsonStatusUpsert::Updated(v) => (StatusCode::OK, v),\n            JsonStatusUpsert::Created(v) => (StatusCode::CREATED, v),\n        };\n        (status, axum::extract::Json(body)).into_response()\n    }\n}\n\nimpl<T: JsonSchema + Serialize> OperationOutput for JsonStatusUpsert<T> {\n    type Inner = T;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        axum::extract::Json::<T>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(resp) = Self::operation_response(ctx, operation) {\n            vec![\n                (Some(StatusCode::OK.into()), resp.clone()),\n                (Some(StatusCode::CREATED.into()), resp),\n            ]\n        } else {\n            vec![]\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{default_limit, validate_no_control_characters, validation_errors, Pagination};\n    use crate::{core::types::ApplicationUid, error::ValidationErrorItem};\n\n    #[derive(Debug, Validate)]\n    struct ValidationErrorTestStruct {\n        #[validate(range(min = 10, message = \"Below 10\"))]\n        a: u32,\n\n        #[validate]\n        b: ValidationErrorTestStructInner,\n\n        #[validate]\n        c: Vec<ValidationErrorTestStructInner>,\n    }\n\n    #[derive(Debug, Validate)]\n    struct ValidationErrorTestStructInner {\n        #[validate(range(max = 10, message = \"Above 10\"))]\n        inner: u8,\n    }\n\n    #[test]\n    fn test_validation_errors_fn() {\n        let valid = ValidationErrorTestStruct {\n            a: 11,\n            b: ValidationErrorTestStructInner { inner: 1 },\n            c: vec![\n                ValidationErrorTestStructInner { inner: 2 },\n                ValidationErrorTestStructInner { inner: 3 },\n            ],\n        };\n        let invalid = ValidationErrorTestStruct {\n            a: 9,\n            b: ValidationErrorTestStructInner { inner: 11 },\n            c: vec![\n                ValidationErrorTestStructInner { inner: 12 },\n                ValidationErrorTestStructInner { inner: 13 },\n            ],\n        };\n\n        assert_eq!(valid.validate(), Ok(()));\n\n        let errs = invalid.validate().unwrap_err();\n        let errs = validation_errors(vec![], errs);\n\n        assert_eq!(errs.len(), 4);\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"a\".to_owned()],\n            msg: \"Below 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"b\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"c\".to_owned(), \"[0]\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"c\".to_owned(), \"[1]\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n    }\n\n    #[test]\n    fn test_pagination_defaults() {\n        let p: Pagination<ApplicationUid> = serde_json::from_va<|fim_middle|>", "completion": "if let Some(resp) = Self::operation_response(ctx, operation) {\n            vec![(Some(STATUS), resp)]\n        } else {\n            vec![]\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "if_expression", "line_range": [805, 809]}
{"prompt": "<|fim_prefix|>fore` and `after` parameters let you filter all items created\n    /// before or after a certain date. These can be used alongside an\n    /// iterator to paginate over results within a certain window.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days'\n    /// worth of data relative to now or, if an iterator is provided, 90\n    /// days before/after the time indicated by the iterator ID. If you\n    /// require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    pub async fn list(\n        &self,\n        app_id: String,\n        options: Option<MessageListOptions>,\n    ) -> Result<ListResponseMessageOut> {\n        let MessageListOptions {\n            limit,\n            iterator,\n            channel,\n            before,\n            after,\n            with_content,\n            tag,\n            event_types,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/msg\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"channel\", channel)\n            .with_optional_query_param(\"before\", before)\n            .with_optional_query_param(\"after\", after)\n            .with_optional_query_param(\"with_content\", with_content)\n            .with_optional_query_param(\"tag\", tag)\n            .with_optional_query_param(\"event_types\", event_types)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Creates a new message and dispatches it to all of the application's\n    /// endpoints.\n    ///\n    /// The `eventId` is an optional custom unique ID. It's verified to be\n    /// unique only up to a day, after that no verification will be made. If\n    /// a message with the same `eventId` already exists for the application, a\n    /// 409 conflict error will be returned.\n    ///\n    /// The `eventType` indicates the type and schema of the event. All messages\n    /// of a certain `eventType` are expected to have the same schema. Endpoints\n    /// can choose to only listen to specific event types. Messages can also\n    /// have `channels`, which similar to event types let endpoints filter by\n    /// them. Unlike event types, messages can have multiple channels, and\n    /// channels don't imply a specific message content or schema.\n    ///\n    /// The `payload` property is the webhook's body (the actual webhook\n    /// message). Svix supports payload sizes of up to 1MiB, though it's\n    /// generally a good idea to keep webhook payloads small, probably no larger\n    /// than 40kb.\n    pub async fn create(\n        &self,\n        app_id: String,\n        message_in: MessageIn,\n        options: Option<MessageCreateOptions>,\n    ) -> Result<MessageOut> {\n        let MessageCreateOptions {\n            with_content,\n            idempotency_key,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/app/{app_id}/msg\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"with_content\", with_content)\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(message_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete all message payloads for the application.\n    ///\n    /// This operation is only available in the <a href=\"https://svix.com/pricing\" target=\"_blank\">Enterprise</a> plan.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"application.purge_content\",\n    ///   \"data\": {\n    ///     \"messagesPurged\": 150\n    ///   }\n    /// }\n    /// ```\n    pub async fn expunge_all_contents(\n        &self,\n        app_id: String,\n        options: Option<MessageExpungeAllContentsOptions>,\n    ) -> Result<ExpungeAllContentsOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/msg/expunge-all-contents\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get a message by its ID or eventID.\n    pub async fn get(\n        &self,\n        app_id: String,\n        msg_id: String,\n        options: Option<MessageGetOptions>,\n    ) -> Result<MessageOut> {\n        let MessageGetOptions { with_content } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/msg/{msg_id}\")\n            .with_path_param(\"app_id\", app_id)\n            .with_path_param(\"msg_id\", msg_id)\n            .with_optional_query_param(\"with_content\", with_content)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete the given message's payload.\n    ///\n    /// Useful in cases when a message was accidentally sent with sensitive\n    /// content. The message can't be replayed or resent once its payload\n    /// has been deleted or expired.\n    pub async fn expunge_content(&self, app_id: String, msg_id: String) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/app/{app_id}/msg/{msg_id}/content\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"msg_id\", msg_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    #[cfg(feature = \"svix_beta\")]\n    pub async fn events(\n        &self,\n        params: V1MessageEventsParams,\n    ) -> Result<crate::models::MessageEventsOut> {\n        let V1MessageEventsParams {\n            app_id,\n            limit,\n            iterator,\n            event_types,\n            channels,\n            after,\n        } = params;\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/events\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"event_types\", event_types)\n            .with_optional_query_param(\"channels\", channels)\n            .with_optional_query_param(\"after\", after)\n            .execute(self.cfg)\n            .await\n    }\n\n    #[cfg(feature = \"svix_beta\")]\n    pub async fn events_subscription(\n        &self,\n        params: V1MessageEventsSubscriptionParams,\n    ) -> Result<crate::models::MessageEventsOut> {\n        let V1MessageEventsSubscriptionParams {\n            app_id,\n            subscription_id,\n            limit,\n            iterator,\n            event_types,\n            channels,\n            after,\n        } = params;\n\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/events/subscription/{subscription_id}\",\n        )\n        .with_path_param(\"app_id\", app_id.to_string())\n        .with_path_param(\"subscription_id\", subscription_id.to_string())\n        .with_optional_query_param(\"limit\", limit)\n        .with_optional_query_param(\"iterator\", iterator)\n        .with_optional_query_param(\"event_types\", event_types)\n        .with_optional_query_param(\"channels\", channels)\n        .with_optional_query_param(\"after\", after)\n        .execute(self.cfg)\n        .await\n    }\n}\n\n#[cfg(feature = \"svix_beta\")]\n#[derive(Clone, Debug)]\npub struct V1MessageEventsParams {\n    /// The app's ID or UID\n    pub app_id: String,\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n    /// Filter response based on the event type\n    pub event_types: Option<Vec<String>>,\n    /// Filter response based on the event type.\n    pub channels: Option<Vec<String>>,\n    pub after: Option<String>,\n}\n\n#[cfg(feature = \"svix_beta\")]\n#[derive(Clone, Debug)]\npub struct V1MessageEventsSubscriptionParams {\n    /// The app's ID or UID\n    pub app_id: String,\n    /// The esub's ID or UID\n    <|fim_middle|>", "completion": "let MessageExpungeAllContentsOptions { idempotency_key } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/message.rs", "node_type": "let_declaration", "line_range": [168, 168]}
{"prompt": "<|fim_prefix|>nId {\n    fn from(value: String) -> Self {\n        IntegrationId(value)\n    }\n}\n\nimpl From<&str> for IntegrationId {\n    fn from(value: &str) -> Self {\n        IntegrationId(value.to_string())\n    }\n}\n\nimpl AsRef<str> for IntegrationId {\n    fn as_ref(&self) -> &str {\n        &self.0\n    }\n}\n\n/// The [`IntegrationState`] is a struct which is only able to be created via conversion from a\n/// [`IntegrationConfig`]. This struct is what is associated with an `[IntegrationId`] internally\n/// after the configuration has been read.\n///\n/// What distinguishes it from the [`IntegrationConfig`] is that it contains the necessary members\n/// for validating and forwarding a webhook instead of just containing the definition of how to\n/// derive these necessary members.\n#[derive(Clone)]\npub struct IntegrationState {\n    pub verifier: Verifier,\n    pub output: Arc<Box<dyn ReceiverOutput>>,\n    pub transformation: Option<TransformationConfig>,\n}\n\n/// The [`RequestFromParts`] is a structure consisting of all relevant parts of the HTTP request to\n/// be validated by a [`Verifier`] implementor. This is to be immediately converted into the struct\n/// [`SerializableRequest<Unvalidated>`] via its [`FromRequest`] implementation.\n///\n/// NOTE: This struct is never to be used directly unless by proxy of the aforementioned impl of\n/// [`FromRequest`]. It's simply used as any easy way to implement [`FromRequest`] via a macro .\n#[derive(Clone, Debug, FromRequest)]\npub struct RequestFromParts {\n    headers: HeaderMap,\n    payload: Bytes,\n}\n\n/// A simple marker trait to denote the state of a [`SerializableRequest`]. The only way to publicly\n/// construct any [`SerializableRequest<Validated>`]s  is via the associated method on unvalidated\n/// request's, [`SerializableRequest<Unvalidated>::validate`].\npub trait RequestState {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Unvalidated;\nimpl RequestState for Unvalidated {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Validated;\nimpl RequestState for Validated {}\n\n/// This intermediary representation is necessary because it is preferable to serialize the headers\n/// and/or body as a [`String`] over bytes when dealing with some [`VerificationMethod`]s and some\n/// [`ForwardingMethod`]s. This struct represents both the headers and body as enums which allow for\n/// either textual representations or byte representations when [`Serialize`]d via [`serde`].\n///\n/// On trying to convert a [`Standard`] variant into a [`StringSerializable`] variant, HTTP headers\n/// will be represented textually if and only if they are completely ASCII, while any bodies will\n/// attempt to be read as UTF-8 before falling back to bytes.\n///\n/// NOTE: This conversion *should* be lazy. The [`String`] variant are only acceptable in a subset\n/// of all cases, so lazy-conversion will prevent needless conversion back and forth. You may check\n/// whether the conversion is required and/or helpful with [`VerificationMethod::want_string_rep`]\n/// or [`VerificationMethod::need_string_rep`] plus the [`ForwardingMethod`] equivalents.\n///\n/// The intended course of action is to attempt to convert to string-serializable variants of the\n/// header map and the body immediately if either of the aforementioned methods are true --  but\n/// only returning an [`Err`] response if it *needs* it. Then, if the validation is a success (see\n/// [`SerializableRequest<Unvalidated>::validate`] and a validated equivalent is returned, then the\n/// same checks are to be performed, but with the [`ForwardingMethod`] methods before being sent to\n/// the appropriate [`ForwardingMethod`] implementor.\n#[derive(Clone, Debug, Serialize)]\npub struct SerializableRequest<S: RequestState> {\n    headers: SerializableHeaderMap,\n    payload: SerializablePayload,\n\n    #[serde(skip)]\n    _pd: PhantomData<S>,\n}\n\nimpl<S: RequestState> SerializableRequest<S> {\n    pub fn headers(&self) -> &SerializableHeaderMap {\n        &self.headers\n    }\n\n    pub fn payload(&self) -> &SerializablePayload {\n        &self.payload\n    }\n}\n\nimpl From<RequestFromParts> for SerializableRequest<Unvalidated> {\n    <|fim_suffix|>\n}\n\n#[async_trait]\nimpl<S> FromRequest<S> for SerializableRequest<Unvalidated>\nwhere\n    S: Send + Sync,\n{\n    type Rejection = <RequestFromParts as FromRequest<S>>::Rejection;\n\n    async fn from_request(req: Request, state: &S) -> Result<Self, Self::Rejection> {\n        RequestFromParts::from_request(req, state)\n            .await\n            .map(Into::into)\n    }\n}\n\nimpl SerializableRequest<Unvalidated> {\n    /// Given a specific validator\n    pub async fn validate<V: VerificationMethod>(\n        mut self,\n        verifier: &V,\n    ) -> Result<SerializableRequest<Validated>, http::StatusCode> {\n        // Do relevant conversions to [`String`] representations if wanted/needed\n        match (verifier.want_string_rep(), verifier.need_string_rep()) {\n            // Needed\n            (true, true) | (false, true) => {\n                self.headers = self\n                    .headers\n                    .try_to_string()\n                    .map_err(|_| http::StatusCode::BAD_REQUEST)?;\n                self.payload = self\n                    .payload\n                    .try_to_string()\n                    .map_err(|_| http::StatusCode::BAD_REQUEST)?;\n            }\n\n            // Wanted, but not needed\n            (true, false) => {\n                self.headers = match self.headers.try_to_string() {\n                    Ok(h) => h,\n                    Err(h) => h,\n                };\n\n                self.payload = match self.payload.try_to_string() {\n                    Ok(p) => p,\n                    Err(p) => p,\n                };\n            }\n\n            // Not wanted\n            (false, false) => {}\n        };\n\n        // FIXME: No cloning\n        // Then actually use the [`VerificationMethod`] implementor.\n        match verifier.validate(self.clone()).await {\n            Ok(true) => Ok(SerializableRequest::<Validated> {\n                headers: self.headers,\n                payload: self.payload,\n\n                _pd: PhantomData,\n            }),\n\n            Ok(false) => {\n                // FIXME: Read config to know whether to log\n                Err(http::StatusCode::BAD_REQUEST)\n            }\n\n            Err(e) => {\n                tracing::error!(\"Error validating request: {}\", e);\n                Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n            }\n        }\n    }\n}\n\n#[derive(Clone, Debug)]\npub enum SerializableHeaderMap {\n    Standard(HeaderMap),\n    StringSerializable(HashMap<String, String>),\n}\n\nimpl<'a> IntoIterator for &'a SerializableHeaderMap {\n    type Item = (&'a str, &'a [u8]);\n    type IntoIter = SerializableHeaderMapIter<'a>;\n\n    fn into_iter(self) -> Self::IntoIter {\n        match self {\n            SerializableHeaderMap::Standard(hm) => SerializableHeaderMapIter::HeaderMap(hm.iter()),\n            SerializableHeaderMap::StringSerializable(hm) => {\n                SerializableHeaderMapIter::HashMap(hm.iter())\n            }\n        }\n    }\n}\n\nimpl SerializableHeaderMap {\n    pub fn try_to_string(self) -> Result<Self, Self> {\n        match self {\n            Self::Standard(header_map) => Ok(Self::StringSerializable(\n                header_map\n                    .iter()\n                    .map(|(name, value)| Ok((name.as_str().to_owned(), value.to_str()?.to_owned())))\n                    .collect::<Result<HashMap<String, String>>>()\n                    .map_err(|_| Self::Standard(header_map))?,\n            )),\n            Self::StringSerializable(hash_map) => Ok(Self::StringSerializable(hash_map)),\n        }\n    }\n\n    pub fn len(&self) -> usize {\n        match self {\n            Self::Standard(m) => m.len(),\n            Self::StringSerializable(m) => m.len(),\n        }\n    }\n}\n\n/// Serialize is not implemented on [`HeaderMap`]s themselves, so custom serialization is required.\nimpl Serialize for SerializableHeaderMap {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            Self::Standard(header_map) => header_map\n                .iter()\n                .map(|(name, value)| (name.as_str().to_owned(), val<|fim_middle|>", "completion": "fn from(value: RequestFromParts) -> Self {\n        Self {\n            headers: SerializableHeaderMap::Standard(value.headers),\n            payload: SerializablePayload::Standard(value.payload.to_vec()),\n\n            _pd: PhantomData,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/types.rs", "node_type": "function_item", "line_range": [203, 210]}
{"prompt": "<|fim_prefix|>use deno_core::JsRuntime;\nuse serde_json::json;\nuse svix_bridge_types::{TransformerInput, TransformerOutput};\n\nuse super::{run_script_inner, validate_script};\n\nfn get_test_rt() -> JsRuntime {\n    JsRuntime::new(Default::default())\n}\n\n// Really just trying to figure out if the deno runtime is working the way I hope.\n#[test]\nfn test_happy_fn() {\n    let src = r#\"\n    function handler(input) {\n        return { \"x\": 123, ...input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({ \"y\": 456 }).into(), src).unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n            assert_eq!(v[\"y\"].as_i64(), Some(456));\n        }\n        TransformerOutput::Invalid => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\nfn test_invalid_output_bool() {\n    let src = r#\"\n    function handler(input) {\n        return false;\n    }\n    \"#\n    .to_string();\n\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    <|fim_suffix|>\n}\n\n#[test]\n// FIXME: serde decodes arrays with keys like \"0\", \"1\"... in this situation, failing the test.\n#[ignore]\nfn test_invalid_output_array() {\n    let src = r#\"\n    function handler(input) {\n        return [1, 2];\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => {\n            panic!(\"got unexpected return value\");\n        }\n    }\n}\n\n/// Receives a string input, parses as JSON in js, then returns the result back to rust.\n#[test]\nfn test_string_input() {\n    let src = r#\"\n    function handler(input) {\n        return JSON.parse(input);\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(r#\"{\"x\": 123}\"#)),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n/// Take the string input and just add it to a field in the returned object.\n/// The string should make it through, back to rust, as-is.\n#[test]\nfn test_string_input2() {\n    let src = r#\"\n    function handler(input) {\n        return { \"payload\": input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(\"Hello World\")),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"payload\"].as_str(), Some(\"Hello World\"));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n#[test]\nfn test_validate_script_bad_syntax_is_err() {\n    assert!(validate_script(\"let 123 = ';\").is_err());\n}\n\n#[test]\nfn test_validate_script_empty_handler_is_ok() {\n    assert!(validate_script(\"function handler() { }\").is_ok());\n}\n\n#[test]\nfn test_validate_script_arrow_fn_is_ok() {\n    assert!(validate_script(\"const handler = () => ({ a: 123 })\").is_ok());\n}\n\n/// Technically, this should be legal though the utility is questionable.\n#[test]\nfn test_validate_script_empty_is_ok() {\n    assert!(validate_script(\"\").is_ok());\n    assert!(validate_script(\"    \").is_ok());\n}\n<|fim_middle|>", "completion": "match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => panic!(\"got unexpected return value\"),\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/tests.rs", "node_type": "match_expression", "line_range": [42, 45]}
{"prompt": "<|fim_prefix|>use std::{\n    io::Write,\n    path::{Path, PathBuf},\n};\n\nuse anyhow::{Context as _, Result};\nuse figment::{\n    providers::{Env, Format, Toml},\n    Figment,\n};\nuse fs_err::{self as fs, File};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, Deserialize, Serialize)]\npub struct Config {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub auth_token: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    server_url: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    debug_url: Option<String>,\n\n    // Relay stuff relates to the `listen` command.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub relay_token: Option<String>,\n    #[serde(alias = \"relay_debug_url\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub relay_debug_hostname: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub relay_disable_security: Option<bool>,\n}\n\nfn create_config_file(path: &Path) -> Result<File> {\n    let dir = path\n        .parent()\n        .context(\"config file path must not be empty\")?;\n    fs::create_dir_all(dir)?;\n\n    let mut opts = File::options();\n    opts.create(true).truncate(true).write(true);\n\n    #[cfg(unix)]\n    {\n        use std::os::unix::fs::OpenOptionsExt;\n\n        const FILE_MODE: u32 = 0o600;\n        opts.options_mut().mode(FILE_MODE);\n    }\n\n    Ok(opts.open(path)?)\n}\n\nimpl Config {\n    pub fn load() -> Result<Config> {\n        let cfg_file = get_config_file_path()?;\n        let config: Config = Figment::new()\n            .merge(Toml::file(cfg_file))\n            .merge(Env::prefixed(\"SVIX_\"))\n            .extract()?;\n        Ok(config)\n    }\n\n    <|fim_suffix|>\n\n    /// Gives the `server_url` for a Svix client with fallback to the legacy `SVIX_DEBUG_URL` variable/config.\n    pub fn server_url(&self) -> Option<&str> {\n        match self.server_url.as_deref() {\n            Some(s) if s.trim().is_empty() => self.debug_url.as_deref(),\n            server_url @ Some(_) => server_url,\n            None => self.debug_url.as_deref(),\n        }\n    }\n}\n\nconst FILE_NAME: &str = \"config.toml\";\n\nfn get_folder() -> Result<PathBuf> {\n    Ok(dirs::config_dir()\n        .context(\"unable to find config path\")?\n        .join(\"svix\"))\n}\n\npub fn get_config_file_path() -> Result<PathBuf> {\n    Ok(get_folder()?.join(FILE_NAME))\n}\n<|fim_middle|>", "completion": "pub fn save_to_disk(&self, path: &Path) -> Result<()> {\n        let mut fh = create_config_file(path)?;\n        let source = &toml::to_string_pretty(self)?;\n        fh.write_all(source.as_bytes())?;\n        Ok(())\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/config.rs", "node_type": "function_item", "line_range": [63, 68]}
{"prompt": "<|fim_prefix|>TypeOut;\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::ConfigurationInner,\n    core::{\n        security::{generate_org_token, management_org_id},\n        types::{\n            metadata::Metadata, ApplicationId, ApplicationUid, BaseId, EndpointId, EndpointUid,\n            MessageAttemptId, MessageId, MessageUid, OrganizationId,\n        },\n    },\n    v1::endpoints::{\n        application::{ApplicationIn, ApplicationOut},\n        endpoint::{EndpointIn, EndpointOut, EndpointSecretRotateIn},\n    },\n};\n\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, default_test_endpoint,\n    },\n    get_default_test_config, TestClient, TestReceiver,\n};\n\n/// Sent when an endpoint has been automatically disabled after continuous failures.\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\n#[allow(dead_code)]\npub struct EndpointDisabledEvent {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub endpoint_id: EndpointId,\n    pub endpoint_uid: Option<EndpointUid>,\n    pub fail_since: DateTime<Utc>,\n}\n\n/// Sent when an endpoint is created, updated, or deleted\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointEvent {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub endpoint_id: EndpointId,\n    pub endpoint_uid: Option<EndpointUid>,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\n#[allow(dead_code)]\npub struct MessageAttempetLast {\n    pub id: MessageAttemptId,\n    pub response_status_code: i16,\n    pub timestamp: DateTime<Utc>,\n}\n\n/// Sent when a message delivery has failed (all of the retry attempts have been exhausted) as a\n/// \"message.attempt.exhausted\" type or after it's failed four times as a \"message.attempt.failing\"\n/// event.\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\n#[allow(dead_code)]\npub struct MessageAttemptEvent {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub msg_id: MessageId,\n    pub msg_event_id: Option<MessageUid>,\n    pub endpoint_id: EndpointId,\n    pub last_attempt: MessageAttempetLast,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\", content = \"data\")]\n#[allow(dead_code)]\npub enum OperationalWebhookTest {\n    #[serde(rename = \"endpoint.disabled\")]\n    EndpointDisabled(EndpointDisabledEvent),\n    #[serde(rename = \"endpoint.created\")]\n    EndpointCreated(EndpointEvent),\n    #[serde(rename = \"endpoint.updated\")]\n    EndpointUpdated(EndpointEvent),\n    #[serde(rename = \"endpoint.deleted\")]\n    EndpointDeleted(EndpointEvent),\n    #[serde(rename = \"message.attempt.exhausted\")]\n    MessageAttemptExhausted(MessageAttemptEvent),\n    #[serde(rename = \"message.attempt.failing\")]\n    MessageAttemptFailing(MessageAttemptEvent),\n}\n\n/// Operational webhooks are dispatched by a special organization, so this function returns two\n/// [`TestClient`]s, one with the Svix Management org token, and one with a random org token.\n///\n/// Additionally it returns the [`OrganizationId`] of the random  organization such that it may be\n/// included as an application of the of the Operational webhooks organization.\nfn start_svix_server_with_operational_webhooks(\n    mut cfg: ConfigurationInner,\n) -> (\n    TestClient,\n    TestClient,\n    OrganizationId,\n    tokio::task::JoinHandle<()>,\n) {\n    let op_webhook_jwt = generate_org_token(&cfg.jwt_signing_config, management_org_id()).unwrap();\n\n    let org_id = OrganizationId::new(None, None);\n    let regular_jwt = generate_org_token(&cfg.jwt_signing_config, org_id.clone()).unwrap();\n\n    let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n    // Could update this fn to take a tokio TcpListener instead, but that's a pretty large diff\n    // for very little benefit (since this is just test code anyways).\n    listener.set_nonblocking(true).unwrap();\n    let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n    let base_url = format!(\"http://{}\", listener.local_addr().unwrap());\n\n    cfg.operational_webhook_address = Some(base_url.clone());\n    l<|fim_suffix|>\n    let jh = tokio::spawn(svix_server::run_with_prefix(\n        Some(svix_ksuid::Ksuid::new(None, None).to_string()),\n        cfg,\n        Some(listener),\n    ));\n\n    (\n        TestClient::new(base_url.clone(), &regular_jwt),\n        TestClient::new(base_url, &op_webhook_jwt),\n        org_id,\n        jh,\n    )\n}\n\n#[tokio::test]\nasync fn test_endpoint_create_update_and_delete() {\n    let cfg = get_default_test_config();\n\n    let (client_regular, client_op, org_id, _jh) = start_svix_server_with_operational_webhooks(cfg);\n\n    // Setup operational webhook Application and Endpoint\n    let op_webhook_app: ApplicationOut = client_op\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name: \"TestOperationalWebhookApplication\".to_owned(),\n                rate_limit: None,\n                uid: Some(ApplicationUid(org_id.to_string())),\n                metadata: Metadata::default(),\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let _op_webhook_endp: EndpointOut = client_op\n        .post(\n            &format!(\"api/v1/app/{}/endpoint/\", op_webhook_app.id),\n            EndpointIn {\n                description: \"TestOperationalWebhookEndpoint\".to_owned(),\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Setup regular Application and Endpoint\n    let regular_app = create_test_app(&client_regular, \"TestOperationalWebhookApplicationRegular\")\n        .await\n        .unwrap();\n    let regular_endp = create_test_endpoint(&client_regular, &regular_app.id, \"http://junk.url\")\n        .await\n        .unwrap();\n\n    let op_webhook_out = receiver.data_recv.recv().await.unwrap();\n    assert_eq!(\n        op_webhook_out.get(\"type\").unwrap().as_str().unwrap(),\n        \"endpoint.created\"\n    );\n    let op_webhook_out: OperationalWebhookTest = serde_json::from_value(op_webhook_out).unwrap();\n\n    match op_webhook_out {\n        OperationalWebhookTest::EndpointCreated(EndpointEvent {\n            app_id,\n            app_uid,\n            endpoint_id,\n            endpoint_uid,\n        }) => {\n            assert_eq!(app_id, regular_app.id);\n            assert_eq!(app_uid, regular_app.uid);\n            assert_eq!(endpoint_id, regular_endp.id);\n            assert_eq!(endpoint_uid, regular_endp.ep.uid);\n        }\n        _ => panic!(\"Got wrong type\"),\n    };\n\n    // Update endpoint\n    let regular_endp: EndpointOut = client_regular\n        .put(\n            &format!(\n                \"api/v1/app/{}/endpoint/{}/\",\n                regular_app.id, regular_endp.id\n            ),\n            EndpointIn {\n                description: \"Updated description\".to_owned(),\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    let op_webhook_out = receiver.data_recv.recv().await.unwrap();\n    assert_eq!(\n        op_webhook_out.get(\"type\").unwrap().as_str().unwrap(),\n        \"endpoint.updated\"\n    );\n    let op_webhook_out: OperationalWebhookTest = serde_json::from_value(op_webhook_out).unwrap();\n\n    match op_webhook_out {\n        OperationalWebhookTest::EndpointUpdated(EndpointEvent {\n            app_id,\n            app_uid,\n            endpoint_id,\n            endpoint_uid,\n        }) => {\n            assert_eq!(app_id, regular_app.id);\n            assert_eq!(app_uid, regular_app.uid);\n            assert_eq!(endpoint_id, regular_endp.id);\n            assert_eq!(endpoint_uid, regular_endp.ep.uid);\n        }\n        _ => panic!(\"Got wrong type\"),\n    };\n\n    // Rotate secrets\n    client_regular\n        .post_without_response(\n            &format!(\n                \"api/v1/app/{}/endpoint/{}/secret/rotate/\",\n                regular_app.id, regular_endp.id\n            ),\n            EndpointSecretRotateIn::default(),\n            StatusCode::N<|fim_middle|>", "completion": "let cfg = Arc::new(cfg);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_operational_webhooks.rs", "node_type": "let_declaration", "line_range": [125, 125]}
{"prompt": "<|fim_prefix|>et delayed = \"{test}_delay_delayed\";\n        let lock = \"{test}_delay_delayed_lock\";\n        let dlq = \"{test}_delay_delayed_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(500);\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt1 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test1\".to_owned()),\n            app_id: ApplicationId(\"test1\".to_owned()),\n            endpoint_id: EndpointId(\"test1\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Scheduled,\n            attempt_count: 0,\n        });\n        let mt2 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n\n        p.send(&mt1, Some(Duration::from_millis(2000)))\n            .await\n            .unwrap();\n        p.send(&mt2, None).await.unwrap();\n\n        let recv2 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv2.task, mt2);\n        recv2.ack().await.unwrap();\n\n        let recv1 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv1.task, mt1);\n        recv1.ack().await.unwrap();\n    }\n\n    fn to_redis_key(id: &str, task: &QueueTask) -> String {\n        format!(\"{id}|{}\", serde_json::to_string(task).unwrap())\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_migrations() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        // Test queue name constants\n        let v1_main = \"{test}_migrations_main_v1\";\n        let v2_main = \"{test}_migrations_main_v2\";\n        let v3_main = \"{test}_migrations_main_v3\";\n\n        let v1_processing = \"{test}_migrations_processing_v1\";\n        let v2_processing = \"{test}_migrations_processing_v2\";\n        // v3_processing is the stream pending queue for v3_main\n\n        let v1_delayed = \"{test}_migrations_delayed_v1\";\n        let v2_delayed = \"{test}_migrations_delayed_v2\";\n        let v2_delayed_lock = \"{test}_migrations_delayed_lock_v2\";\n        // v3_delayed doesn not yet exist\n\n        {\n            let mut conn = pool.get().await.unwrap();\n\n            // Clear test keys\n            let _: () = conn\n                .del(&[\n                    v1_main,\n                    v2_main,\n                    v3_main,\n                    v1_processing,\n                    v2_processing,\n                    v1_delayed,\n                    v2_delayed,\n                ])\n                .await\n                .unwrap();\n\n            // Add v3 consumer group\n            let _: () = conn\n                .xgroup_create_mkstream(v3_main, super::WORKERS_GROUP, 0i8)\n                .await\n                .unwrap();\n\n            // Add v1 data\n            for num in 1..=10 {\n                let _: () = conn\n                    .rpush(\n                        v1_main,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            for num in 11..=15 {\n                let _: () = conn\n                    .zadd(\n                        v1_delayed,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                        Utc::now().timestamp() + 2,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // Move the first five of v1_main to v1_processing\n            for _ in 0..5 {\n                <|fim_suffix|>\n            }\n\n            // v1 to v2\n            migrate_list(&mut conn, v1_main, v2_main).await.unwrap();\n            migrate_list(&mut conn, v1_processing, v2_processing)\n                .await\n                .unwrap();\n            migrate_sset(&mut conn, v1_delayed, v2_delayed)\n                .await\n                .unwrap();\n\n            // v2 to v3\n            migrate_list_to_stream(&mut conn, v2_main, v3_main)\n                .await\n                .unwrap();\n            migrate_list_to_stream(&mut conn, v2_processing, v3_main)\n                .await\n                .unwrap();\n        }\n\n        // Read\n        let (_p, mut c) = new_pair_inner(\n            &cfg,\n            Duration::from_secs(5),\n            \"\",\n            v3_main,\n            v2_delayed,\n            v2_delayed_lock,\n            \"dlq-bruh\",\n        )\n        .await;\n\n        // 2 second delay on the delayed and pending queue is inserted after main queue, so first\n        // the 6-10 should appear, then 1-5, then 11-15\n\n        let mut items = c.receive_all(TEST_RECV_DEADLINE).await.unwrap();\n        while items.len() < 15 {\n            let more_tasks = c.receive_all(TEST_RECV_DEADLINE).await.unwrap();\n            assert!(!more_tasks.is_empty(), \"failed to receive all the tasks\");\n            items.extend(more_tasks);\n        }\n\n        let mut items = items.into_iter();\n        for num in 6..=10 {\n            let recv = items.next().unwrap();\n            assert_eq!(\n                &*recv.task,\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                    app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                })\n            );\n            recv.ack().await.unwrap();\n        }\n        for num in 1..=5 {\n            let recv = items.next().unwrap();\n            assert_eq!(\n                &*recv.task,\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                    app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                })\n            );\n            recv.ack().await.unwrap();\n        }\n        for num in 11..=15 {\n            let recv = items.next().unwrap();\n            assert_eq!(\n                &*recv.task,\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                    app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                })\n            );\n            recv.ack().await.unwrap();\n        }\n\n        if items.len() != 0 {\n            panic!(\"received more than the expected number of tasks, rest: {items:?}\");\n        }\n    }\n}\n<|fim_middle|>", "completion": "let _: () = conn\n                    .blmove(\n                        v1_main,\n                        v1_processing,\n                        Direction::Left,\n                        Direction::Right,\n                        0.0,\n                    )\n                    .await\n                    .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [796, 805]}
{"prompt": "<|fim_prefix|>\n      type: \"webhook\"\n      path_id: \"sqs\"\n      verification:\n        type: \"none\"\n    output:\n      # Note that the SQS forwarder requires credentials to be set as environment vars:\n      # - `AWS_DEFAULT_REGION`\n      # - `AWS_ACCESS_KEY_ID`\n      # - `AWS_SECRET_ACCESS_KEY`\n      type: \"sqs\"\n      queue_dsn: \"https://example.aws.com/my-queue\"\n\n  - name: \"forward-to-kafka-example\"\n    input:\n      type: \"webhook\"\n      path_id: \"kafka\"\n      verification:\n        type: \"none\"\n    output:\n      type: \"kafka\"\n      kafka_bootstrap_brokers: \"localhost:9094\"\n      kafka_topic: \"foobar\"\n      # Other valid values: \"plaintext\", \"ssl\"\n      kafka_security_protocol: \"sasl_ssl\"\n      # Only for SASL\n      kafka_sasl_username: \"user\"\n      kafka_sasl_password: \"pass\"\n\n\"#;\n\n#[test]\nfn test_sender_parses_ok() {\n    let conf: Result<WebhookSenderConfig, _> = serde_yaml::from_str(\n        r#\"\nname: \"from-rabbit-local-to-svix\"\ninput:\n    type: \"rabbitmq\"\n    queue_name: \"local\"\n    uri: \"amqp://example.com/%2f\"\ntransformation: |\n    handler = (x) => ({ appId: \"app_1234\", message: { eventType: \"foo.bar\", payload: x }})\noutput:\n    type: \"svix\"\n    token: \"XXXX\"\n    \"#,\n    );\n    conf.unwrap();\n}\n\n#[test]\nfn test_senders_parses_ok() {\n    let conf: Result<Vec<WebhookSenderConfig>, _> = serde_yaml::from_str(\n        r#\"\n\n- name: \"from-rabbit-local-to-svix\"\n  input:\n    type: \"rabbitmq\"\n    queue_name: \"local\"\n    uri: \"amqp://example.com/%2f\"\n  # Implicit json transformation\n  transformation: |\n    handler = (x) => ({ appId: \"app_1234\", message: { eventType: \"foo.bar\", payload: x }})\n  output:\n    type: \"svix\"\n    token: \"XXXX\"\n- name: \"from-SQS-to-svix\"\n  input:\n    type: \"sqs\"\n    queue_dsn: \"http://sqs.example.com/foo/bar\"\n  # Explicit string transformation\n  transformation:\n    format: string\n    src: |\n        function handler(x) {\n            return { appId: \"app_1234\", message: { eventType: \"foo.bar\", payload: x }}\n        }\n  output:\n    type: \"svix\"\n    token: \"YYYY\"\n\"#,\n    );\n    let conf = conf.unwrap();\n    assert_eq!(conf.len(), 2);\n}\n\n#[test]\nfn test_omnibus_parses_ok() {\n    let conf: Result<Config, _> = serde_yaml::from_str(OMNIBUS);\n    conf.unwrap();\n}\n\n#[test]\nfn test_empty() {\n    let conf: Config = serde_yaml::from_str(\"\").unwrap();\n    assert!(conf.senders.is_empty());\n    assert!(conf.receivers.is_empty());\n    assert_eq!(conf.http_listen_address, \"0.0.0.0:5000\".parse().unwrap());\n    assert!(conf.opentelemetry.is_none());\n    assert!(matches!(conf.log_format, LogFormat::Default));\n    assert!(matches!(conf.log_level, LogLevel::Info));\n}\n\n/// Don't particularly care about the parsed specifics here.\n/// This is more about making sure the examples we have in the repo actually parse.\n#[test]\nfn test_receivers_example() {\n    let fp = concat!(\n        env!(\"CARGO_MANIFEST_DIR\"),\n        \"/../svix-bridge.example.receivers.yaml\"\n    );\n    let conf: Config = serde_yaml::from_slice(&std::fs::read(fp).unwrap()).unwrap();\n    assert!(conf.senders.is_empty());\n    assert!(!conf.receivers.is_empty());\n}\n\n/// Don't particularly care about the parsed specifics here.\n/// This is more about making sure the examples we have in the repo actually parse.\n#[test]\nfn test_senders_example() {\n    let fp = concat!(\n        env!(\"CARGO_MANIFEST_DIR\"),\n        \"/../svix-bridge.example.senders.yaml\"\n    );\n    let conf: Config = serde_yaml::from_slice(&std::fs::read(fp).unwrap()).unwrap();\n    assert!(!conf.senders.is_empty());\n    assert!(conf.receivers.is_empty());\n}\n\n#[test]\nfn test_variable_substitution_missing_vars() {\n    let src = r#\"\n    opentelemetry:\n        address: \"${OTEL_ADDR}\"\n    \"#;\n    let vars = HashMap::new();\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n    let otel = cfg.opentelemetry.unwrap();\n    // when lookups in the vars map fail, the original token text is preserved.\n    assert_eq!(&otel.address, \"${OTEL_ADDR}\");\n}\n\n#[test]\nfn test_variable_substitution_available_vars() {\n    let src = r#\"\n    opentelemetry:\n        address: \"${OTEL_ADDR}\"\n        sample_ratio: ${OTEL_SAMPLE_RATIO}\n    \"#;\n    <|fim_suffix|>\n    vars.insert(\n        String::from(\"OTEL_ADDR\"),\n        String::from(\"http://127.0.0.1:8080\"),\n    );\n    vars.insert(String::from(\"OTEL_SAMPLE_RATIO\"), String::from(\"0.25\"));\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n    // when lookups succeed, the token should be replaced.\n    let otel = cfg.opentelemetry.unwrap();\n    assert_eq!(&otel.address, \"http://127.0.0.1:8080\");\n    assert_eq!(otel.sample_ratio, Some(0.25));\n}\n\n#[test]\nfn test_variable_substitution_braces_optional() {\n    let src = r#\"\n    opentelemetry:\n        # Formerly failing to use ${} notation means the port number would not be substituted.\n        # Today, it works. Test that it continues to.\n        address: \"${OTEL_SCHEME}://${OTEL_HOST}:$OTEL_PORT\"\n    \"#;\n    let mut vars = HashMap::new();\n    vars.insert(String::from(\"OTEL_SCHEME\"), String::from(\"https\"));\n    vars.insert(String::from(\"OTEL_HOST\"), String::from(\"127.0.0.1\"));\n    vars.insert(String::from(\"OTEL_PORT\"), String::from(\"9999\"));\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n    // when lookups succeed, the token should be replaced.\n    let otel = cfg.opentelemetry.unwrap();\n    // Not the user-intended outcome, but it simplifies the parsing requirements.\n    assert_eq!(&otel.address, \"https://127.0.0.1:9999\");\n}\n\n#[test]\nfn test_variable_substitution_missing_numeric_var_is_err() {\n    // Unfortunate side-effect of templating yaml.\n    //\n    // If the variable is missing, usually you've got three options:\n    // - retain the token text that failed the lookup (envsubst-rs does this)\n    // - replace the token with an empty string (the CLI `envsubst` does this)\n    // - mark it an error (neither do this, but we can if we roll our own impl)\n    //\n    // For yaml, the field typings are heavily/poorly inferred so for an optional float like\n    // `sample_ratio` an empty string would parse as a `None`, which could be a bad fallback since\n    // otel considers this a 1.0 ratio (send everything).\n    //\n    // For this specific case, retaining the token text produces an error, which happens to be useful.\n    // For fields that happen to be strings anyway, errors may show up later (after the config parsing).\n    // Ex: using `${QUEUE_NAME}` in a rabbit sender input will surface in logs as an error when we\n    // try to connect: \"no such queue '${QUEUE_NAME}'\".\n\n    let src = r#\"\n    opentelemetry:\n        address: \"${OTEL_ADDR}\"\n        # This var will be missing, causing the template token to\n        # be retained causing a parse failure :(\n        sample_ratio: ${OTEL_SAMPLE_RATIO}\n    \"#;\n    let vars = HashMap::new();\n    let err = Config::from_src(src, Some(&vars)).err().unwrap();\n    let want = \"Failed to parse config: opentelemetry.sample_ratio: invalid type: \\\n                    string \\\"${OTEL_SAMPLE_RATIO}\\\", expected f64 at line 6 column 23\";\n    assert_eq!(want, err.to_string());\n}\n\n/// This is probably a given, but we should expect a single variable can be referenced multiple\n/// times within the config.\n/// The concrete use case: auth tokens.\n#[test]\nfn test_variable_substitution_repeated_lookups() {\n    let src = r#\"\n    senders:\n      - name: \"rabbitmq-1\"\n        input:\n          type: \"rabbitmq\"\n          uri: \"${RABBIT_URI}\"\n          queue_name: \"${QUEUE_NAME_1}\"\n        output:\n          type: \"svix\"\n          token: \"${SVIX_TOKEN}\"\n      - name: \"rabbitmq-2\"\n        input:\n          type: \"rabbitmq\"\n          uri: \"${RABBIT_URI}\"\n          queue_name: \"${QUEUE_NAME_2}\"\n        output:\n          type: \"svix\"\n          token: \"${SVIX_TOKEN}\"\n    \"#;\n    let mut vars = HashMap::new();\n    vars.insert(\n        String::from(\"RABBIT_URI\"),\n        String::from(\"amqp://guest:guest@localhost:5672/%2f\"),\n    );\n    vars.insert(String::from(\"QUEUE_NAME_1\"), String::from(\"one\"));\n    vars.insert(String::from(\"QUEUE_NAME_2\"), String::from(\"two\"));\n    vars.insert(String::from(\"SVIX_TOKEN\"), String::from(\"x\"));\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n\n    if let WebhookSenderConfig {\n        input:\n            SenderInputOpts::Queu<|fim_middle|>", "completion": "let mut vars = HashMap::new();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/config/tests.rs", "node_type": "let_declaration", "line_range": [405, 405]}
{"prompt": "<|fim_prefix|>   VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub fn new() -> Self {\n        Self {\n            method: None,\n            uri: None,\n            accept: None,\n            user_agent: None,\n            headers: None,\n            header_names: None,\n            body: None,\n            version: None,\n            timeout: None,\n            content_type: None,\n            basic_auth: None,\n        }\n    }\n\n    pub fn method(mut self, method: Method) -> Self {\n        self.method = Some(method);\n        self\n    }\n\n    pub fn uri(mut self, uri: url::Url) -> Self {\n        let basic_auth = if uri.password().is_some() || !uri.username().is_empty() {\n            let username = decode_or_log(uri.username());\n            let password = uri.password().map(decode_or_log).unwrap_or_default();\n\n            Some(\n                Authorization::basic(&username, &password)\n                    .0\n                    .encode()\n                    .as_bytes()\n                    .to_vec(),\n            )\n        } else {\n            None\n        };\n        self.basic_auth = basic_auth;\n\n        let uri =\n            Uri::from_str(uri.as_str()).expect(\"If it's a valid url::Url, it's also a valid Uri\");\n        self.uri = Some(uri);\n        self\n    }\n\n    pub fn uri_str(self, uri: &str) -> Result<Self, url::ParseError> {\n        let uri = url::Url::from_str(uri)?;\n        Ok(self.uri(uri))\n    }\n\n    fn build_headers(\n        headers: CaseSensitiveHeaderMap,\n    ) -> (hyper::HeaderMap, hyper::ext::HeaderCaseMap) {\n        let mut hdr_map = hyper::HeaderMap::with_capacity(headers.len());\n        let mut case_sensitive_hdrs: hyper::HeaderMap<Bytes> =\n            hyper::HeaderMap::with_capacity(headers.len());\n        for (k, v) in headers.into_iter() {\n            match HeaderName::from_str(&k) {\n                Ok(key) => {\n                    hdr_map.insert(key.clone(), v);\n                    case_sensitive_hdrs.insert(key, Bytes::copy_from_slice(k.as_bytes()));\n                }\n                Err(e) => {\n                    tracing::error!(\"Failed to parse header {} {}\", k, e);\n                }\n            }\n        }\n        (hdr_map, case_sensitive_hdrs.into())\n    }\n\n    pub fn headers(mut self, headers: CaseSensitiveHeaderMap) -> Self {\n        let (hdrs, case_map) = Self::build_headers(headers);\n        self.headers = Some(hdrs);\n        self.header_names = Some(case_map);\n        self\n    }\n\n    pub fn body(mut self, body: Vec<u8>, content_type: HeaderValue) -> Self {\n        self.body = Some(body);\n        self.content_type = Some(content_type);\n        self\n    }\n\n    pub fn json_body<T: Serialize>(self, body: T) -> Result<Self, serde_json::Error> {\n        let body = serde_json::to_vec(&body)?;\n        Ok(self.body(body, HeaderValue::from_static(\"application/json\")))\n    }\n\n    pub fn version(mut self, version: Version) -> Self {\n        self.version = Some(version);\n        self\n    }\n\n    pub fn timeout(mut self, timeout: Duration) -> Self {\n        self.timeout = Some(timeout);\n        self\n    }\n\n    pub fn user_agent(mut self, user_agent: HeaderValue) -> Self {\n        self.user_agent = Some(user_agent);\n        self\n    }\n}\n\nimpl Default for RequestBuilder {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl RequestBuilder {\n    fn validate(&self) -> Result<(), RequestBuildError> {\n        let mut errs: Vec<BuildError> = Vec::new();\n        if self.uri.is_none() {\n            errs.push(BuildError::UriMissing);\n        }\n        if self.version.is_none() {\n            errs.push(BuildError::VersionMissing);\n        }\n\n        if !errs.is_empty() {\n            Err(RequestBuildError(errs))\n        } else {\n            Ok(())\n        }\n    }\n\n    pub fn build(self) -> Result<Request, RequestBuildError> {\n        self.validate()?;\n\n        let custom_headers = self.headers.unwrap_or_default();\n\n        <|fim_suffix|>\n        let authority = uri.authority().expect(\"Missing authority\");\n        let host = match authority.port() {\n            Some(port) => HeaderValue::from_str(&format!(\"{}:{port}\", authority.host())),\n            None => HeaderValue::from_str(authority.host()),\n        }\n        .unwrap();\n\n        let mut headers = HeaderMap::with_capacity(3 + custom_headers.len());\n\n        // Ensure that host header is first -- even though this is technically\n        // not required by HTTP spec, some clients fail if it's not first:\n        headers.insert(http::header::HOST, host);\n        headers.insert(\n            http::header::ACCEPT,\n            self.accept.unwrap_or(HeaderValue::from_static(\"*/*\")),\n        );\n        headers.insert(\n            http::header::CONTENT_TYPE,\n            self.content_type\n                .unwrap_or(HeaderValue::from_static(\"application/json\")),\n        );\n\n        headers.extend(custom_headers);\n\n        if let Some(user_agent) = self.user_agent {\n            headers.insert(http::header::USER_AGENT, user_agent);\n        }\n\n        if let Some(auth_header) = self.basic_auth {\n            if !headers.contains_key(http::header::AUTHORIZATION) {\n                headers.insert(\n                    http::header::AUTHORIZATION,\n                    HeaderValue::from_bytes(&auth_header).unwrap(),\n                );\n            }\n        }\n\n        Ok(Request {\n            method: self.method.unwrap_or(Method::POST),\n            uri,\n            headers,\n            header_names: self.header_names,\n            body: self.body,\n            timeout: self.timeout,\n            version: self.version.unwrap(),\n        })\n    }\n}\n\n/// HTTP connector that blocks outgoing requests to private IPs with support\n/// for HTTPS and optionally proxying via SOCKS5 or HTTP(S).\n#[derive(Clone)]\nenum SvixHttpsConnector {\n    Regular(HttpsConnector<NonLocalHttpConnector>),\n    Socks5Proxy {\n        proxy: HttpsConnector<SocksV5<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n    HttpProxy {\n        proxy: HttpsConnector<Tunnel<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n}\n\nimpl SvixHttpsConnector {\n    fn new(\n        inner: NonLocalHttpConnector,\n        proxy_cfg: Option<&ProxyConfig>,\n        disable_tls_verification: bool,\n    ) -> Result<Self, Box<dyn std::error::Error>> {\n        let https =\n            HttpsConnector::with_connector(inner.clone(), ssl_builder(disable_tls_verification))?;\n\n        let matcher = |proxy_url: String, noproxy: Option<ProxyBypassCfg>| -> Arc<Matcher> {\n            let mut matcher = Matcher::builder().all(proxy_url);\n            if let Some(noproxy) = noproxy {\n                matcher = matcher.no(noproxy.0);\n            }\n            Arc::new(matcher.build())\n        };\n\n        match proxy_cfg {\n            Some(proxy_cfg) => match proxy_cfg.addr.clone() {\n                ProxyAddr::Socks5(proxy_addr) => {\n                    let matcher = matcher(proxy_addr.to_string(), proxy_cfg.noproxy.clone());\n                    let socks = SocksV5::new(proxy_addr, inner).local_dns(true);\n                    let socks_https = HttpsConnector::with_connector(\n                        socks,\n                        ssl_builder(disable_tls_verification),\n                    )?;\n                    Ok(Self::Socks5Proxy {\n                        proxy: socks_https,\n                        bypass: https,\n                        matcher,\n                    })\n                }\n                ProxyAddr::Http(proxy_addr) => {\n                    let matcher = matcher(proxy_addr.to_string(), proxy_cfg.noproxy.clone());\n                    let tunnel = Tunnel::new(proxy_addr, inner);\n                    let tunnel_https = HttpsConnector::with_connector(\n                        tunnel,\n                        ssl_builder(disable_tls_verification),\n                    )?;\n                    Ok(Self::HttpProxy {\n                        proxy: tunnel_https,\n                <|fim_middle|>", "completion": "let uri = self.uri.unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [402, 402]}
{"prompt": "<|fim_prefix|>hannel>,\n    /// Only include items created before a certain date\n    before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    after: Option<DateTime<Utc>>,\n    /// When `true` attempt content is included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n// Applies filters common to [`list_attempts_by_endpoint`] and [`list_attempts_by_msg`]\nfn list_attempts_by_endpoint_or_message_filters(\n    mut query: Select<messageattempt::Entity>,\n    status: Option<MessageStatus>,\n    status_code_class: Option<StatusCodeClass>,\n    event_types: Option<EventTypeNameSet>,\n    channel: Option<EventChannel>,\n) -> Select<messageattempt::Entity> {\n    if let Some(status) = status {\n        query = query.filter(messageattempt::Column::Status.eq(status));\n    }\n\n    query = match status_code_class {\n        Some(StatusCodeClass::CodeNone) => {\n            query.filter(messageattempt::Column::ResponseStatusCode.between(0, 99))\n        }\n\n        Some(StatusCodeClass::Code1xx) => {\n            query.filter(messageattempt::Column::ResponseStatusCode.between(100, 199))\n        }\n\n        Some(StatusCodeClass::Code2xx) => {\n            query.filter(messageattempt::Column::ResponseStatusCode.between(200, 299))\n        }\n\n        Some(StatusCodeClass::Code3xx) => {\n            query.filter(messageattempt::Column::ResponseStatusCode.between(300, 399))\n        }\n\n        Some(StatusCodeClass::Code4xx) => {\n            query.filter(messageattempt::Column::ResponseStatusCode.between(400, 499))\n        }\n\n        Some(StatusCodeClass::Code5xx) => {\n            query.filter(messageattempt::Column::ResponseStatusCode.between(500, 599))\n        }\n\n        None => query,\n    };\n\n    // The event_types and channel filter require joining the associated message\n    if event_types.is_some() || channel.is_some() {\n        query = query.join_rev(\n            sea_orm::JoinType::InnerJoin,\n            message::Entity::belongs_to(messageattempt::Entity)\n                .from(message::Column::Id)\n                .to(messageattempt::Column::MsgId)\n                .into(),\n        );\n\n        if let Some(EventTypeNameSet(event_types)) = event_types {\n            query = query.filter(message::Column::EventType.is_in(event_types));\n        }\n\n        if let Some(channel) = channel {\n            // sea_orm evaluates the '$1' relative to the # of params in `Expr::cust_with_values`,\n            // NOT relative to the total number of params in the final query like you might expect.\n            // As such, this won't break if more $N params are added in earler/later\n            // `.filter` calls.\n            query = query.filter(Expr::cust_with_values(\"channels @> $1\", [channel.jsonb()]));\n        }\n    }\n\n    query\n}\n\n/// List attempts by endpoint id\n#[aide_annotate(op_id = \"v1.message-attempt.list-by-endpoint\")]\nasync fn list_attempts_by_endpoint(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<\n        PaginationDescending<ReversibleIterator<MessageAttemptId>>,\n    >,\n    ValidatedQuery(ListAttemptsByEndpointQueryParams {\n        status,\n        status_code_class,\n        channel,\n        before,\n        after,\n        with_content,\n    }): ValidatedQuery<ListAttemptsByEndpointQueryParams>,\n    EventTypesQueryParams(event_types): EventTypesQueryParams,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<ListResponse<MessageAttemptOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    // Confirm endpoint ID belongs to the given application\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let query = list_attempts_by_endpoint_or_message_filters(\n        messageattempt::Entity::secure_find_by_endpoint(endp.id),\n        status,\n        status_code_class,\n        event_types,\n        channel,\n    );\n\n    l<|fim_suffix|>\n    let out = query\n        .all(db)\n        .await?\n        .into_iter()\n        .map(|mut attempt| {\n            if !with_content {\n                \"{}\".clone_into(&mut attempt.response)\n            }\n\n            attempt\n        })\n        .map(Into::into)\n        .collect();\n\n    Ok(Json(MessageAttemptOut::list_response(\n        out,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\n/// Flattens in a [`ListAttemptsByEndpointOrMsgQueryParameters`] and adds one extra query parameter\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct ListAttemptsByMsgQueryParams {\n    /// Filter response based on the delivery status\n    status: Option<MessageStatus>,\n    /// Filter response based on the HTTP status code\n    status_code_class: Option<StatusCodeClass>,\n    /// Filter response based on the channel\n    #[validate]\n    channel: Option<EventChannel>,\n    /// Filter the attempts based on the attempted endpoint\n    #[validate]\n    endpoint_id: Option<EndpointIdOrUid>,\n    /// Only include items created before a certain date\n    before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    after: Option<DateTime<Utc>>,\n    /// When `true` attempt content is included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// List attempts by message id\n#[aide_annotate(op_id = \"v1.message-attempt.list-by-msg\")]\nasync fn list_attempts_by_msg(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<\n        PaginationDescending<ReversibleIterator<MessageAttemptId>>,\n    >,\n    ValidatedQuery(ListAttemptsByMsgQueryParams {\n        status,\n        status_code_class,\n        channel,\n        endpoint_id,\n        before,\n        after,\n        with_content,\n    }): ValidatedQuery<ListAttemptsByMsgQueryParams>,\n    Path(ApplicationMsgPath { msg_id, .. }): Path<ApplicationMsgPath>,\n    EventTypesQueryParams(event_types): EventTypesQueryParams,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<ListResponse<MessageAttemptOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    // Confirm message ID belongs to the given application\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id.clone(), msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut query = list_attempts_by_endpoint_or_message_filters(\n        messageattempt::Entity::secure_find_by_msg(msg.id),\n        status,\n        status_code_class,\n        event_types,\n        channel,\n    );\n\n    if let Some(endpoint_id) = endpoint_id {\n        // Ensure the endpoint ID/UID belongs to the given application\n        if let Some(endp) = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n            .one(db)\n            .await?\n        {\n            // And filter by its ID incase a UID was used\n            query = query.filter(messageattempt::Column::EndpId.eq(endp.id));\n        } else {\n            return Err(Error::http(HttpError::not_found(None, None)));\n        }\n    }\n\n    let (query, iter_direction) = filter_and_paginate_time_limited(\n        query,\n        messageattempt::Column::Id,\n        limit,\n        pagination.iterator,\n        before,\n        after,\n    );\n    let out = query\n        .all(db)\n        .await?\n        .into_iter()\n        .map(|mut attempt| {\n            if !with_content {\n                \"{}\".clone_into(&mut attempt.response)\n            }\n\n            attempt\n        })\n        .map(Into::into)\n        .collect();\n\n    Ok(Json(MessageAttemptOut::list_response(\n        out,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\n// A type combining information from [`messageattempt::Model`]s and [`endpoint::Model`]s to\n// output information on attempted destinations\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageEndpointOut {\n    #[serde(flatten)]\n    endpoint: super::endpoint::EndpointOutCommon,\n    pub id: EndpointId,\n    status: MessageStat<|fim_middle|>", "completion": "let (query, iter_direction) = filter_and_paginate_time_limited(\n        query,\n        messageattempt::Column::Id,\n        limit,\n        pagination.iterator,\n        before,\n        after,\n    );", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/attempt.rs", "node_type": "let_declaration", "line_range": [561, 568]}
{"prompt": "<|fim_prefix|>ding, 0);\n\n    let last_msg_time = {\n        // Create the relevant Stats records manually, otherwise\n        // it's difficult to test exact state of messagedestinations.\n\n        let cfg = get_default_test_config();\n        let db = Arc::new(cfg);\n        let db = svix_server::db::init_db(&db).await;\n\n        let now = Utc::now();\n\n        let msg = message::ActiveModel {\n            app_id: Set(app_id.clone()),\n            org_id: Set(OrganizationId::new(None, None)),\n            expiration: Set(Utc::now().into()),\n            event_type: Set(EventTypeName(\"test.ing\".into())),\n            created_at: Set((now - chrono::Duration::minutes(65)).into()),\n            id: Set(MessageId::new(\n                (now - chrono::Duration::minutes(65)).into(),\n                None,\n            )),\n            ..message::ActiveModel::new()\n        }\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(60),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(45),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(30),\n            MessageStatus::Sending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap()\n        .created_at\n    };\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 2);\n    assert_eq!(stats.sending, 1);\n\n    let stats_filtered: EndpointStatsOut = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}&until={}\",\n                urlencoding::encode(&last_msg_time.to_rfc3339()),\n                urlencoding::encode(&Utc::now().to_rfc3339()),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats_filtered.fail, 0);\n    assert_eq!(stats_filtered.success, 0);\n    assert_eq!(stats_filtered.pending, 0);\n    assert_eq!(stats_filtered.sending, 1);\n\n    let _: IgnoredAny = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}\",\n                urlencoding::encode(&(Utc::now() - chrono::Duration::days(29)).to_rfc3339()),\n            ),\n            StatusCode::BAD_REQUEST,\n        )\n        .await\n        .unwrap();\n}\n\n/// We used to store the secret in the DB without a type marker, check loading those still works\n#[tokio::test]\nasync fn test_legacy_endpoint_secret() {\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let db = Arc::new(cfg);\n    let db = svix_server::db::init_db(&db).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_throwaway = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    let raw_key = STANDARD.decode(\"5gasBsSw3Nvf3ugNYVJIqnRVYPW7hPts\").unwrap();\n    let secret_1 = EndpointSecret::Symmetric(raw_key.clone());\n\n    let ep_in = EndpointIn {\n        key: Some(secret_throwaway.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Set the raw value to the database (like legacy)\n    db.execute(Statement::from_sql_and_values(\n        DatabaseBackend::Postgres,\n        \"UPDATE endpoint SET key = $1 WHERE id = $2\",\n        vec![raw_key.clone().into(), endp_1.id.clone().into()],\n    ))\n    .await\n    .unwrap();\n\n    l<|fim_suffix|>\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let (secret, ep) = (secret_1, endp_1);\n    assert_eq!(\n        secret.serialize_public_key(),\n        client\n            .get::<EndpointSecretOutTest>(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                StatusCode::OK\n            )\n            .await\n            .unwrap()\n            .key\n    );\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_encryption_in_database() {\n    let mut cfg = get_default_test_config();\n    cfg.encryption = Encryption::new([1; 32]);\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let db = Arc::new(cfg);\n    let db = svix_server::db::init_db(&db).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret_encrypted: Option<QueryResult> = db\n        .query_one(Statement::from_sql_and_values(\n            DatabaseBackend::Postgres,\n            \"SELECT key FROM endpoint WHERE id = $1\",\n            vec![ep.id.clone().into()],\n        ))\n        .await\n        .unwrap();\n    let secret_encrypted: Vec<u8> = secret_encrypted.unwrap().try_get(\"\", \"key\").unwrap();\n\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret_clear: Option<QueryResult> = db\n        .query_one(Statement::from_sql_and_values(\n            DatabaseBackend::Postgres,\n            \"SELECT key FROM endpoint WHERE id = $1\",\n            vec![ep.id.clone().into()],\n        ))\n        .await\n        .unwrap();\n    let secret_clear: Vec<u8> = secret_clear.unwrap().try_get(\"\", \"key\").unwrap();\n\n    // Ensure that the length of the encrypted is much longer than the clear\n    assert!(secret_encrypted.len() > secret_clear.len() + 10);\n}\n\n#[tokio::test]\nasync fn test_endpoint_filter_events() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_empty_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"filterTypes\": [],\n    });\n\n    let ep_with_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"filterTypes\": [\"et1\"],\n    });\n\n    let ep_no_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n    });\n\n    let expected_et = EventTypeNameSet(HashSet::from([EventTypeName(\"et1\".to_owned())]));\n\n    let _ep_with_empty_events: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_empty_events,\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _ep_with_nonexistent_event: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"et1\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let ep_with_valid_event: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_with_valid_event.ep.event_types_ids.unwrap(), expected_et);\n\n    let ep_removed_events: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_valid_event.id),\n            ep_no_events.to_owned(),\n            StatusCode::OK,\n        )\n       <|fim_middle|>", "completion": "let endp_1 = get_endpoint(&client, &app_id, &endp_1.id).await.unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1615, 1615]}
{"prompt": "<|fim_prefix|>use std::{\n    str,\n    time::{Duration, Instant},\n};\n\nuse rdkafka::{\n    consumer::{CommitMode, Consumer as _},\n    error::KafkaError,\n    Message as _,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::api::{MessageCreateOptions, Svix},\n    CreateMessageRequest, JsObject, SenderInput, SenderOutputOpts, TransformationConfig,\n    TransformerInput, TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\nuse tokio::task::spawn_blocking;\n\nuse crate::{config::KafkaInputOpts, Error, Result};\n\npub struct KafkaConsumer {\n    name: String,\n    opts: KafkaInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n}\n\nimpl KafkaConsumer {\n    pub fn new(\n        name: String,\n        opts: KafkaInputOpts,\n        transformation: Option<TransformationConfig>,\n        output: SenderOutputOpts,\n    ) -> Result<Self> {\n        Ok(Self {\n            name,\n            transformation,\n            transformer_tx: None,\n            opts,\n            svix_client: match output {\n                SenderOutputOpts::Svix(output) => {\n                    Svix::new(output.token, output.options.map(Into::into))\n                }\n            },\n        })\n    }\n\n    #[tracing::instrument(skip_all)]\n    async fn process(&self, msg: &rdkafka::message::BorrowedMessage<'_>) -> Result<()> {\n        let payload = msg.payload().ok_or_else(|| Error::MissingPayload)?;\n        let payload = if let Some(transformation) = &self.transformation {\n            let input = match transformation.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload =\n                        serde_json::from_slice(payload).map_err(Error::Deserialization)?;\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    let raw_payload = str::from_utf8(payload).map_err(Error::NonUtf8Payload)?;\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n\n            let script = transformation.source().clone();\n            <|fim_suffix|>\n            serde_json::from_value(serde_json::Value::Object(object))\n                .map_err(Error::Deserialization)?\n        } else {\n            serde_json::from_slice(payload).map_err(Error::Deserialization)?\n        };\n\n        let CreateMessageRequest { app_id, message } = payload;\n\n        let KafkaInputOpts::Inner {\n            group_id, topic, ..\n        } = &self.opts;\n\n        let options = MessageCreateOptions {\n            with_content: None,\n            // If committing the message fails or the process crashes after posting the webhook but\n            // before committing, this makes sure that the next run of this fn with the same kafka\n            // message doesn't end up creating a duplicate webhook in svix.\n            idempotency_key: Some(format!(\n                \"svix_bridge_kafka_{group_id}_{topic}_{}\",\n                msg.offset()\n            )),\n        };\n\n        self.svix_client\n            .message()\n            .create(app_id, message, Some(options))\n            .await?;\n\n        Ok(())\n    }\n\n    async fn transform(&self, script: String, input: TransformerInput) -> Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx\n            .as_ref()\n            .ok_or_else(|| Error::transformation(\"transformations not configured\"))?\n            .send(job)\n            .map_err(|e| Error::transformation(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::transformation(\"transformation rx failed\"))\n            .and_then(|x| {\n                x.map_err(|_e| Error::transformation(\"transformation execution failed\"))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => Err(Error::transformation(\n                \"transformation produced unexpected value\",\n            )),\n        }\n    }\n\n    async fn run_inner(&self) -> Result<()> {\n        let opts = self.opts.clone();\n        // `ClientConfig::create` does blocking I/O.\n        // Same for subscribe, most likely.\n        let consumer = spawn_blocking(move || {\n            let KafkaInputOpts::Inner { topic, .. } = &opts;\n            let topic = topic.clone();\n\n            let consumer = opts.create_consumer()?;\n            tracing::debug!(\"Created StreamConsumer\");\n\n            consumer.subscribe(&[&topic])?;\n            tracing::debug!(topic, \"Subscribed\");\n\n            Ok::<_, KafkaError>(consumer)\n        })\n        .await\n        .expect(\"create_consumer task panicked\")?;\n\n        loop {\n            // It's fine to pull messages one-by-one without any buffering in our own code because\n            // rdkafka buffers messages internally through a background task / thread.\n            let msg = consumer.recv().await?;\n            tracing::debug!(\"Received a message\");\n\n            let mut process_error_count = 0;\n            while let Err(e) = self.process(&msg).await {\n                match e {\n                    // If the payload is invalid, log an error and continue.\n                    // It would fail the same way if retried.\n                    Error::MissingPayload\n                    | Error::Deserialization(_)\n                    | Error::NonUtf8Payload(_) => {\n                        tracing::error!(error = &e as &dyn std::error::Error, \"invalid payload\");\n                        break;\n                    }\n\n                    // If the error is (possibly) transient, retry a few times.\n                    // After that, bubble up the error so it's logged at error level.\n                    Error::Kafka(_) | Error::SvixClient(_) | Error::Transformation { .. } => {\n                        process_error_count += 1;\n                        if process_error_count >= 3 {\n                            return Err(e);\n                        }\n\n                        tracing::warn!(\n                            error = &e as &dyn std::error::Error,\n                            \"failed to process payload from kafka\"\n                        );\n\n                        // retry\n                    }\n                }\n            }\n\n            // FIXME(jplatte): Should we introduce logic to only commit every N messages to reduce\n            // back and forth on the Kafka connection / disk writes inside Kafka?\n            //\n            // Background: messages in Kafka are not committed individually, rather what this call\n            // does is update the stored stream position for the consumer group.\n            consumer.commit_message(&msg, CommitMode::Async)?;\n        }\n    }\n}\n\n#[async_trait]\nimpl SenderInput for KafkaConsumer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        let mut fails: u64 = 0;\n        let mut last_fail = Instant::now();\n\n        let KafkaInputOpts::Inner { topic, .. } = &self.opts;\n        tracing::info!(topic, \"Starting to listen for messages\");\n\n        loop {\n            if let Err(e) = self.run_inner().await {\n                tracing::error!(\"{e}\");\n            }\n\n            if last_fail.elapsed() > Duration::from_secs(10) {\n                // reset the fail count if we didn't have a hiccup in the past short while.\n                tracing::trace!(\"been a while since last fail, resetting count\");\n                fails = 0;\n            } else {\n                fails += 1;\n            }\n\n            last_fail = Instant::now();\n            tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n        }\n    }\n}\n<|fim_middle|>", "completion": "let object = self.transform(script, input).await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/input.rs", "node_type": "let_declaration", "line_range": [66, 66]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    borrow::Cow,\n    collections::HashSet,\n    error::Error as StdError,\n    ops::Deref,\n    sync::LazyLock,\n    time::{SystemTime, UNIX_EPOCH},\n};\n\nuse aide::{\n    transform::{TransformOperation, TransformPathItem},\n    OperationInput, OperationIo, OperationOutput,\n};\nuse axum::{\n    async_trait,\n    extract::{\n        rejection::{BytesRejection, FailedToBufferBody},\n        FromRequest, FromRequestParts, Query, Request,\n    },\n    response::IntoResponse,\n};\nuse chrono::{DateTime, Utc};\nuse http::{request::Parts, StatusCode};\nuse regex::Regex;\nuse schemars::JsonSchema;\nuse sea_orm::{ColumnTrait, QueryFilter, QueryOrder, QuerySelect};\nuse serde::{de::DeserializeOwned, Deserialize, Serialize};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::types::{\n        ApplicationIdOrUid, BaseId, EndpointIdOrUid, EventTypeName, EventTypeNameSet,\n        MessageAttemptId, MessageIdOrUid,\n    },\n    error::{Error, HttpError, Result, ValidationErrorItem},\n};\n\npub mod patch;\nuse patch::UnrequiredField;\n\nc<|fim_suffix|>\nconst PAGINATION_LIMIT_CAP_HARD: bool = true;\nconst PAGINATION_LIMIT_CAP_LIMIT: u64 = 250;\nstatic PAGINATION_LIMIT_ERROR: LazyLock<String> =\n    LazyLock::new(|| format!(\"Given limit must not exceed {PAGINATION_LIMIT_CAP_LIMIT}\"));\n\nstatic FUTURE_QUERY_LIMIT: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::hours(1));\nstatic LIMITED_QUERY_DURATION: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::days(90));\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct PaginationDescending<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct Pagination<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Clone, Debug, JsonSchema)]\n#[schemars(transparent)]\npub struct PaginationLimit(pub u64);\n\nimpl<'de> Deserialize<'de> for PaginationLimit {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let limit = u64::deserialize(deserializer)?;\n\n        // Want hard limits to stay the same so they can be validated\n        if !PAGINATION_LIMIT_CAP_HARD && limit > PAGINATION_LIMIT_CAP_LIMIT {\n            Ok(PaginationLimit(PAGINATION_LIMIT_CAP_LIMIT))\n        } else {\n            Ok(PaginationLimit(limit))\n        }\n    }\n}\n\nimpl Validate for PaginationLimit {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        let mut errs = validator::ValidationErrors::new();\n\n        if self.0 > PAGINATION_LIMIT_CAP_LIMIT {\n            errs.add(\n                \"limit\",\n                validation_error(Some(\"pagination\"), Some(&PAGINATION_LIMIT_ERROR)),\n            );\n        }\n\n        if errs.is_empty() {\n            Ok(())\n        } else {\n            Err(errs)\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum ReversibleIterator<T: Validate> {\n    /// Regular iteration - backwards in time.\n    Normal(T),\n    /// Reversed iteration - forwards in time.\n    Prev(T),\n}\n\nimpl<T: Validate> ReversibleIterator<T> {\n    pub(crate) fn direction(&self) -> IteratorDirection {\n        match self {\n            Self::Normal(_) => IteratorDirection::Normal,\n            Self::Prev(_) => IteratorDirection::Prev,\n        }\n    }\n}\n\nimpl<'de, T: 'static + Deserialize<'de> + Validate + From<String>> Deserialize<'de>\n    for ReversibleIterator<T>\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).map(|s| {\n            if let Some(s) = s.strip_prefix('-') {\n                ReversibleIterator::Prev(T::from(s.to_owned()))\n            } else {\n                ReversibleIterator::Normal(T::from(s))\n            }\n        })\n    }\n}\n\nimpl<T: Validate> Validate for ReversibleIterator<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            ReversibleIterator::Normal(val) => val.validate(),\n            ReversibleIterator::Prev(val) => val.validate(),\n        }\n    }\n}\n\nimpl<T: Validate + JsonSchema> JsonSchema for ReversibleIterator<T> {\n    fn schema_name() -> String {\n        format!(\"ReversibleIterator_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        T::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\n/// Applies sorting and filtration to a query from its iterator, sort column, and limit\n/// queries based on time\n/// Our rules for limiting queries are as follows\n///\n/// If `before` is passed:\n/// * lower limit on query is `before - LIMITED_QUERY_DURATION`\n/// * upper limit is `before`\n///\n/// If `after` is passed:\n/// * lower limit is `after`\n/// * upper limit is `now + FUTURE_QUERY_LIMIT`\n///\n/// If prev-iterator is passed:\n/// * lower limit is `prev-iterator`\n/// * upper limit is `prev-iterator + LIMITED_QUERY_DURATION`\n///\n/// If (normal) iterator is passed:\n/// * lower limit is `iterator - LIMITED_QUERY_DURATION`\n/// * upper limit is `iterator`\n///\n/// If no iterator is passed:\n/// * lower limit is `now() - LIMITED_QUERY_DURATION` if\n///   neither `before` nor `after` were passed\npub(crate) fn filter_and_paginate_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }\n\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket limit on future\n        .filter(sort_column.lt(I::start_<|fim_middle|>", "completion": "const fn default_limit() -> PaginationLimit {\n    PaginationLimit(50)\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "function_item", "line_range": [44, 46]}
{"prompt": "<|fim_prefix|>ng for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"sender input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n#[derive(Parser)]\npub struct Args {\n    #[arg(long, env = \"SVIX_BRIDGE_CFG_FILE\", help = \"Path to the config file.\")]\n    cfg_file: Option<PathBuf>,\n    #[arg(\n        long,\n        env = \"SVIX_BRIDGE_CFG\",\n        help = \"Config data as a string (instead of a file on disk).\",\n        conflicts_with = \"cfg_file\"\n    )]\n    cfg: Option<String>,\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let mut config_search_paths = vec![];\n\n    if let Some(fp) = args.cfg_file {\n        config_search_paths.push(fp)\n    } else {\n        for name in [\"svix-bridge.yaml\", \"svix-bridge.yml\", \"svix-bridge.json\"] {\n            config_search_paths.push(std::env::current_dir().expect(\"current dir\").join(name));\n        }\n    }\n\n    // Clap will ensure we have only one or the other (cfg and cfg_file can't be specified together).\n    let cfg_source = match args.cfg {\n        Some(cfg_source) => cfg_source,\n        None => {\n            let fp = config_search_paths\n                .into_iter()\n                .find(|x| x.exists())\n                .expect(\"config file path\");\n            std::fs::read_to_string(&fp).map_err(|e| {\n                <|fim_suffix|>\n                Error::other(format!(\"Failed to read {p}: {e}\"))\n            })\n        }?,\n    };\n\n    let vars = std::env::vars().collect();\n    let cfg = Config::from_src(&cfg_source, Some(vars).as_ref())?;\n    setup_tracing(&cfg);\n    let _metrics = setup_metrics(&cfg);\n    tracing::info!(\"starting\");\n\n    tokio::spawn(async move {\n        let mut interval = tokio::time::interval(Duration::from_secs(15));\n        let metrics = CommonMetrics::new(&opentelemetry::global::meter(\"svix.com\"));\n        match get_allocator_stat_mibs() {\n            Ok(mibs) => {\n                tracing::debug!(\"Common Metrics Collection: Started\");\n\n                loop {\n                    interval.tick().await;\n\n                    if let Ok(Some((allocated, resident))) = get_allocator_stats(true, &mibs) {\n                        metrics.record_mem_allocated(allocated as _);\n                        metrics.record_mem_resident(resident as _);\n                    }\n                }\n            }\n            Err(e) => tracing::error!(\"Unable to get allocator stats mibs: {e}\"),\n        }\n    });\n\n    let (xform_tx, mut xform_rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n\n    // XXX: this is a bit nasty, but might be okay to start.\n    // The nested spawns are needed to make sure we can saturate the\n    // threadpool (otherwise we'd run each job serially).\n    //\n    // Another approach would be to do what og-ingester did: give each plugin a clone of the\n    // `TpHandle`, but this would likely mean moving the runtime module over to the `-types` crate.\n    // I'd rather not do this, mostly to help keep things more unit test friendly; channels can\n    // help keep the coupling more loose, with less stateful baggage.\n    // Starting with this just to keep the JS executor stuff here in the binary.\n    tokio::spawn(async move {\n        tracing::info!(\n            \"Starting JS Transformation Workers: {}\",\n            cfg.transformation_worker_count\n        );\n\n        deno_core::JsRuntime::init_platform(None, false);\n        let pooler: runtime::JsPooler = runtime::JsPooler::new(cfg.transformation_worker_count);\n\n        while let Some(TransformerJob {\n            input,\n            script,\n            callback_tx,\n        }) = xform_rx.recv().await\n        {\n            let tp = pooler.clone();\n            tokio::spawn(async move {\n                let out = tp.run_script(input, script).await;\n                // FIXME: seeing this Err case come up during load testing.\n                //   Seems like we shouldn't be hitting this so easily while the process is not terminating.\n                //   Regularly there are group error log lines that show up right at the end of an\n                //   `oha` run, POSTing to receivers. Need to investigate why.\n                if callback_tx\n                    .send(out.map_err(|e| tracing::error!(\"{:?}\", e)))\n                    .is_err()\n                {\n                    // If the callback fails, the plugin is likely unwinding/dropping.\n                    // Not a whole lot we can do about that.\n                    tracing::error!(\"failed to send js output back to caller\");\n                }\n            });\n        }\n    });\n\n    let mut senders = Vec::with_capacity(cfg.senders.len());\n    for sc in cfg.senders {\n        let mut sender: Box<dyn SenderInput> = sc.try_into().map_err(Error::other)?;\n        sender.set_transformer(Some(xform_tx.clone()));\n        senders.push(sender);\n    }\n    if senders.is_empty() {\n        tracing::warn!(\"No senders configured.\")\n    }\n    let senders_fut = supervise_senders(senders);\n\n    if cfg.receivers.is_empty() {\n        tracing::warn!(\"No receivers configured.\")\n    }\n    let (webhook_receivers, poller_receivers): (\n        Vec<WebhookReceiverConfig>,\n        Vec<PollerReceiverConfig>,\n    ) = cfg\n        .receivers\n        .into_iter()\n        .partition_map(|either| match either {\n            EitherReceiver::Webhook(x) => Either::Left(x),\n            EitherReceiver::Poller(y) => Either::Right(y),\n        });\n\n    let webhook_receivers_fut =<|fim_middle|>", "completion": "let p = fp.into_os_string().into_string().expect(\"config file path\");", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "let_declaration", "line_range": [270, 270]}
{"prompt": "<|fim_prefix|>      receive_url(&start_response.token),\n                self.local_url,\n                view_url(&self.token),\n            );\n        } else {\n            // Shows that a reconnection attempt succeeded after some failing initial attempts.\n            println!(\"Connected!\");\n        }\n\n        set.spawn({\n            let local_url = self.local_url.clone();\n            let http_client = self.http_client.clone();\n            async move {\n                read_from_ws_loop(ws_rx, remote_tx, local_url.clone(), http_client.clone())\n                    .await\n                    .inspect_err(|e| eprintln!(\"read loop terminated: {e:#}\"))\n            }\n        });\n\n        set.spawn(async move {\n            send_to_ws_loop(remote_rx, ws_tx)\n                .await\n                .inspect_err(|e| eprintln!(\"write loop terminated: {e:#}\"))\n        });\n\n        // If any task terminates, trash the rest so we can reconnect.\n        if set.join_next().await.is_some() {\n            set.shutdown().await;\n        }\n\n        Ok(())\n    }\n}\n\npub async fn listen(\n    local_url: url::Url,\n    relay_token: String,\n    relay_debug_url: Option<&str>,\n    relay_disable_security: bool,\n    disable_tls_verification: bool,\n) -> Result<()> {\n    let scheme = if relay_disable_security { \"ws\" } else { \"wss\" };\n    let api_host = relay_debug_url.unwrap_or(DEFAULT_API_HOST);\n    let token = format!(\"c_{relay_token}\");\n\n    let websocket_url = format!(\"{scheme}://{api_host}/{API_PREFIX}/listen/\").parse()?;\n\n    let http_client = HttpClient::builder()\n        .danger_accept_invalid_certs(disable_tls_verification)\n        .build()?;\n\n    let mut client = Client {\n        token,\n        websocket_url,\n        local_url,\n        http_client,\n    };\n\n    const MAX_BACKOFF: Duration = Duration::from_millis(5000);\n    let backoff_schedule = [\n        Duration::ZERO,\n        Duration::from_millis(100),\n        Duration::from_millis(1000),\n        MAX_BACKOFF,\n    ];\n\n    let mut attempt_count = 0;\n    let mut last_attempt = Instant::now();\n\n    // We may ditch this token, generating a new one on the fly, depending on how the server\n    // responds when we connect.\n    let orig_token = client.token.clone();\n    loop {\n        // Any termination Ok or Err... try to reconnect.\n        let show_welcome_message = attempt_count == 0 || orig_token != client.token;\n\n        if let Err(e) = client.connect(show_welcome_message).await {\n            eprintln!(\"Failed to connect to Webhook Relay: {e:#}\");\n            if e.downcast_ref::<TokenInUse>().is_some() {\n                eprintln!(\"Generating a new token for this session.\");\n                client.token = {\n                    let relay_token = generate_token()?;\n                    format!(\"c_{relay_token}\")\n                };\n            }\n        } else {\n            eprintln!(\"Failed to connect to Webhook Relay\");\n        }\n\n        // Reset the backoff schedule if it's been a while since we've seen a disconnect.\n        if last_attempt.elapsed() > MAX_BACKOFF * 2 {\n            // N.b. attempt_count `0` is special because that's what prompts the printing of a\n            // welcome message in `Client::connect`.\n            // When we reset here, starting at `0` here will still avoid the\n            // re-print because we increment after selecting the sleep duration.\n            attempt_count = 0;\n        }\n\n        let backoff = *backoff_schedule.get(attempt_count).unwrap_or(&MAX_BACKOFF);\n        eprintln!(\"Reattempting connection in: {}ms\", backoff.as_millis());\n\n        attempt_count += 1;\n        last_attempt = Instant::now();\n\n        tokio::time::sleep(backoff).await;\n    }\n}\n\nfn receive_url(token: &str) -> String {\n    format!(\"https://play.svix.com/in/{token}/\")\n}\n\nfn view_url(token: &str) -> String {\n    format!(\"https://play.svix.com/view/{token}/\")\n}\n\ntype S = WebSocketStream<MaybeTlsStream<TcpStream>>;\n\nstruct WsConnection {\n    stream: S,\n}\n\nimpl WsConnection {\n    async fn new(websocket_url: &url::Url) -> Result<Self> {\n        let request = websocket_url.to_string().into_client_request()?;\n        <|fim_suffix|>\n\n        Ok(Self { stream })\n    }\n}\n\nasync fn read_from_ws_loop(\n    mut rx: SplitStream<S>,\n    tx: UnboundedSender<MessageOut>,\n    local_url: url::Url,\n    client: HttpClient,\n) -> Result<()> {\n    // We expect to see roughly _at least one Ping_ in each `SERVER_PING_PERIOD`.\n    // Other messages may arrive ahead of this schedule.\n    // Tracking the time each message is received, we can know if the server has been quiet for too\n    // long, possibly requiring us to reconnect.\n    let mut last_msg = Instant::now();\n\n    loop {\n        const REMOTE_SERVER_CLOSED: &str = \"remote server closed connection\";\n\n        match tokio::time::timeout(SERVER_PING_PERIOD, rx.next()).await {\n            Err(_timeout_hit) => {\n                // Generous. 1.5x the ping frequency. If we go that long without\n                // seeing anything from the server, force a reconnect.\n                if last_msg.elapsed() > SERVER_PING_PERIOD + (SERVER_PING_PERIOD / 2) {\n                    anyhow::bail!(REMOTE_SERVER_CLOSED);\n                }\n            }\n            // Stream empty/closed\n            Ok(None) => break,\n            Ok(Some(msg)) => {\n                last_msg = Instant::now();\n\n                let data = match msg? {\n                    // Control messages.\n                    Message::Close(_) => anyhow::bail!(REMOTE_SERVER_CLOSED),\n                    Message::Ping(_) | Message::Pong(_) | Message::Frame(_) => continue,\n\n                    // Messages that carry data we care to process.\n                    Message::Text(s) => s.into(),\n                    Message::Binary(bytes) => bytes,\n                };\n\n                handle_incoming_message(client.clone(), data, &local_url, tx.clone()).await;\n            }\n        }\n    }\n\n    Ok(())\n}\n\nasync fn send_to_ws_loop(\n    mut rx: UnboundedReceiver<MessageOut>,\n    mut tx: SplitSink<S, Message>,\n) -> Result<()> {\n    while let Some(msg) = rx.recv().await {\n        tokio::time::timeout(\n            WRITE_WAIT,\n            tx.send(Message::Binary(\n                serde_json::to_vec(&msg)\n                    .expect(\"trivial serialization\")\n                    .into(),\n            )),\n        )\n        .await?\n        .context(\"Websocket write timeout\")?;\n    }\n\n    Ok(())\n}\n\nasync fn make_local_request(\n    client: HttpClient,\n    url: &url::Url,\n    data: MessageInEvent,\n) -> Result<LocalServerResponse> {\n    let method = data.method.parse()?;\n    // FIXME: deprecation warning\n    #[allow(deprecated)]\n    let body = base64::decode(&data.body)?;\n    let mut headers = HeaderMap::with_capacity(data.headers.len());\n    for (k, v) in &data.headers {\n        // FIXME: there's a remark about the Go client freaking out if there's more than one host header set.\n        //   Do we care now that we're not using Go? TBD.\n        headers.insert(\n            HeaderName::try_from(k.as_str())?,\n            HeaderValue::try_from(v.as_str())?,\n        );\n    }\n    Ok(client\n        .request(method, url.clone())\n        .timeout(DEFAULT_TIMEOUT)\n        .body(body)\n        .headers(headers)\n        .send()\n        .await?)\n}\n\nfn format_resp_headers(headers: &HeaderMap) -> Result<HashMap<String, String>> {\n    let mut out = HashMap::new();\n    for (k, v) in headers {\n        out.insert(k.to_string(), v.to_str()?.to_string());\n    }\n    Ok(out)\n}\n\nasync fn handle_incoming_message(\n    client: HttpClient,\n    bytes: Bytes,\n    local_url: &url::Url,\n    tx: UnboundedSender<MessageOut>,\n) {\n    match serde_json::from_slice::<MessageIn>(&bytes) {\n        Ok(MessageIn::Event { data, .. }) => {\n            let msg_id = data.id.clone();\n            println!(\"<- Forwarding message id={msg_id} to: {local_url}\");\n            match make_local_request(client, local_url, data).await {\n                Err(err) => {\n                    eprintln!(\"Failed to make request to local server: \\n{err:#}\");\n                }\n                Ok(resp) => {\n                    if let Err(err) = process_response(msg_id, resp, tx).await {\n                        eprintln!(\"Failed to read response from local server: \\n{er<|fim_middle|>", "completion": "let (stream, _resp) = connect_async(request)\n            .await\n            .inspect_err(|e| eprintln!(\"{e}\"))\n            .context(\"failed to connect to websocket server\")?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/mod.rs", "node_type": "let_declaration", "line_range": [304, 307]}
{"prompt": "<|fim_prefix|>use omniqueue::{backends, DynConsumer, DynProducer};\nuse serde::Deserialize;\n\nuse crate::error::{Error, Result};\n\n#[derive(Debug, Default, Deserialize)]\npub struct RedisInputOpts {\n    pub dsn: String,\n    pub max_connections: u16,\n    #[serde(default = \"default_reinsert_on_nack\")]\n    pub reinsert_on_nack: bool,\n    pub queue_key: String,\n    pub delayed_queue_key: Option<String>,\n    pub consumer_group: String,\n    pub consumer_name: String,\n    #[serde(default = \"default_ack_deadline_ms\")]\n    pub ack_deadline_ms: i64,\n}\n\nfn default_reinsert_on_nack() -> bool {\n    true\n}\n\n#[derive(Clone, Debug, Deserialize)]\npub struct RedisOutputOpts {\n    pub dsn: String,\n    pub max_connections: u16,\n    pub queue_key: String,\n    pub delayed_queue_key: Option<String>,\n    #[serde(default = \"default_ack_deadline_ms\")]\n    pub ack_deadline_ms: i64,\n}\n\nfn default_ack_deadline_ms() -> i64 {\n    5_000\n}\n\npub async fn consumer(cfg: &RedisInputOpts) -> Result<DynConsumer> {\n    let delayed_queue_key = cfg\n        .delayed_queue_key\n        .clone()\n        .unwrap_or_else(|| format!(\"{}_delays\", cfg.queue_key));\n    <|fim_suffix|>\n\n    backends::RedisBackend::builder(backends::RedisConfig {\n        dsn: cfg.dsn.clone(),\n        max_connections: cfg.max_connections,\n        reinsert_on_nack: cfg.reinsert_on_nack,\n        queue_key: cfg.queue_key.clone(),\n        delayed_queue_key,\n        delayed_lock_key,\n        consumer_group: cfg.consumer_group.clone(),\n        consumer_name: cfg.consumer_name.clone(),\n        // FIXME: expose in config?\n        payload_key: \"payload\".to_string(),\n        ack_deadline_ms: cfg.ack_deadline_ms,\n        dlq_config: None,\n        sentinel_config: None,\n    })\n    .make_dynamic()\n    .build_consumer()\n    .await\n    .map_err(Error::from)\n}\npub async fn producer(cfg: &RedisOutputOpts) -> Result<DynProducer> {\n    let delayed_queue_key = cfg\n        .delayed_queue_key\n        .clone()\n        .unwrap_or_else(|| format!(\"{}_delays\", cfg.queue_key));\n    let delayed_lock_key = format!(\"{delayed_queue_key}_lock\");\n\n    backends::RedisBackend::builder(backends::RedisConfig {\n        dsn: cfg.dsn.clone(),\n        max_connections: cfg.max_connections,\n        queue_key: cfg.queue_key.clone(),\n        delayed_queue_key,\n        delayed_lock_key,\n        // FIXME: expose in config?\n        payload_key: \"payload\".to_string(),\n        // consumer stuff we don't care about.\n        reinsert_on_nack: false,\n        consumer_group: String::new(),\n        consumer_name: String::new(),\n        ack_deadline_ms: cfg.ack_deadline_ms,\n        dlq_config: None,\n        sentinel_config: None,\n    })\n    .make_dynamic()\n    .build_producer()\n    .await\n    .map_err(Error::from)\n}\n<|fim_middle|>", "completion": "let delayed_lock_key = format!(\"{delayed_queue_key}_lock\");", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/src/redis/mod.rs", "node_type": "let_declaration", "line_range": [43, 43]}
{"prompt": "<|fim_prefix|>s::JsonSchema;\nuse sea_orm::{ColumnTrait, QueryFilter, QueryOrder, QuerySelect};\nuse serde::{de::DeserializeOwned, Deserialize, Serialize};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::types::{\n        ApplicationIdOrUid, BaseId, EndpointIdOrUid, EventTypeName, EventTypeNameSet,\n        MessageAttemptId, MessageIdOrUid,\n    },\n    error::{Error, HttpError, Result, ValidationErrorItem},\n};\n\npub mod patch;\nuse patch::UnrequiredField;\n\nconst fn default_limit() -> PaginationLimit {\n    PaginationLimit(50)\n}\n\nconst PAGINATION_LIMIT_CAP_HARD: bool = true;\nconst PAGINATION_LIMIT_CAP_LIMIT: u64 = 250;\nstatic PAGINATION_LIMIT_ERROR: LazyLock<String> =\n    LazyLock::new(|| format!(\"Given limit must not exceed {PAGINATION_LIMIT_CAP_LIMIT}\"));\n\nstatic FUTURE_QUERY_LIMIT: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::hours(1));\nstatic LIMITED_QUERY_DURATION: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::days(90));\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct PaginationDescending<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct Pagination<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Clone, Debug, JsonSchema)]\n#[schemars(transparent)]\npub struct PaginationLimit(pub u64);\n\nimpl<'de> Deserialize<'de> for PaginationLimit {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let limit = u64::deserialize(deserializer)?;\n\n        // Want hard limits to stay the same so they can be validated\n        if !PAGINATION_LIMIT_CAP_HARD && limit > PAGINATION_LIMIT_CAP_LIMIT {\n            Ok(PaginationLimit(PAGINATION_LIMIT_CAP_LIMIT))\n        } else {\n            Ok(PaginationLimit(limit))\n        }\n    }\n}\n\nimpl Validate for PaginationLimit {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        let mut errs = validator::ValidationErrors::new();\n\n        if self.0 > PAGINATION_LIMIT_CAP_LIMIT {\n            errs.add(\n                \"limit\",\n                validation_error(Some(\"pagination\"), Some(&PAGINATION_LIMIT_ERROR)),\n            );\n        }\n\n        if errs.is_empty() {\n            Ok(())\n        } else {\n            Err(errs)\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum ReversibleIterator<T: Validate> {\n    /// Regular iteration - backwards in time.\n    Normal(T),\n    /// Reversed iteration - forwards in time.\n    Prev(T),\n}\n\nimpl<T: Validate> ReversibleIterator<T> {\n    pub(crate) fn direction(&self) -> IteratorDirection {\n        match self {\n            Self::Normal(_) => IteratorDirection::Normal,\n            Self::Prev(_) => IteratorDirection::Prev,\n        }\n    }\n}\n\nimpl<'de, T: 'static + Deserialize<'de> + Validate + From<String>> Deserialize<'de>\n    for ReversibleIterator<T>\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).map(|s| {\n            if let Some(s) = s.strip_prefix('-') {\n                ReversibleIterator::Prev(T::from(s.to_owned()))\n            } else {\n                ReversibleIterator::Normal(T::from(s))\n            }\n        })\n    }\n}\n\nimpl<T: Validate> Validate for ReversibleIterator<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            ReversibleIterator::Normal(val) => val.validate(),\n            ReversibleIterator::Prev(val) => val.validate(),\n        }\n    }\n}\n\ni<|fim_suffix|>\n/// Applies sorting and filtration to a query from its iterator, sort column, and limit\n/// queries based on time\n/// Our rules for limiting queries are as follows\n///\n/// If `before` is passed:\n/// * lower limit on query is `before - LIMITED_QUERY_DURATION`\n/// * upper limit is `before`\n///\n/// If `after` is passed:\n/// * lower limit is `after`\n/// * upper limit is `now + FUTURE_QUERY_LIMIT`\n///\n/// If prev-iterator is passed:\n/// * lower limit is `prev-iterator`\n/// * upper limit is `prev-iterator + LIMITED_QUERY_DURATION`\n///\n/// If (normal) iterator is passed:\n/// * lower limit is `iterator - LIMITED_QUERY_DURATION`\n/// * upper limit is `iterator`\n///\n/// If no iterator is passed:\n/// * lower limit is `now() - LIMITED_QUERY_DURATION` if\n///   neither `before` nor `after` were passed\npub(crate) fn filter_and_paginate_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }\n\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket limit on future\n        .filter(sort_column.lt(I::start_id(future_limit)));\n\n    (query, iter_direction)\n}\n\n/// Marker trait for any type that is used for iterating through results\n/// in the public API.\npub trait IdIterator: Validate + Into<sea_orm::Value> {}\n\nimpl<T: BaseId + Validate + Into<sea_orm::Value>> IdIterator for T {}\nimpl IdIterator for EventTypeName {}\n\npub fn apply_pagination<\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    C: ColumnTrait,\n    I: IdIterator,\n>(\n    query: Q,\n    sort_column: C,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    ordering: Ordering,\n) -> Q {\n    use Ordering::*;\n    use ReversibleIterator::*;\n\n    // Query for an extra element to be able to tell whether there's more\n    // data than the user requested.\n    let query = query.limit(limit + 1);\n\n    let iterator = if let Some(it) = iterator {\n        it\n    } else {\n        return match ordering {\n            Ascending => query.order_by_asc(sort_column),\n           <|fim_middle|>", "completion": "impl<T: Validate + JsonSchema> JsonSchema for ReversibleIterator<T> {\n    fn schema_name() -> String {\n        format!(\"ReversibleIterator_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        T::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "impl_item", "line_range": [164, 176]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ReplayIn {\n    pub since: String,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub until: Option<String>,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl ReplayIn {\n    pub fn new(since: String) -> Self {\n        Self { since, until: None }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/replay_in.rs", "node_type": "impl_item", "line_range": [12, 16]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse aide::axum::{\n    routing::{get, get_with},\n    ApiRouter,\n};\nuse axum::{extract::State, http::StatusCode, Json};\nuse sea_orm::{query::Statement, ConnectionTrait, DatabaseBackend};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::aide_annotate;\n\nuse crate::{\n    core::cache::{kv_def, CacheBehavior, CacheKey, CacheValue},\n    queue::QueueTask,\n    v1::utils::{openapi_tag, NoContent},\n    AppState,\n};\n\nasync fn ping() -> NoContent {\n    NoContent\n}\n\n#[derive(Debug, Deserialize, Serialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum HealthStatusVariant {\n    Ok,\n    Error,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthStatus {\n    status: HealthStatusVariant,\n    // TODO: information field\n}\n\nimpl HealthStatus {\n    pub fn new_ok() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Ok,\n        }\n    }\n\n    p<|fim_suffix|>\n    pub fn is_ok(&self) -> bool {\n        matches!(\n            self,\n            HealthStatus {\n                status: HealthStatusVariant::Ok,\n                ..\n            }\n        )\n    }\n}\nimpl<O, E> From<Result<O, E>> for HealthStatus {\n    fn from(res: Result<O, E>) -> Self {\n        match res {\n            Ok(_) => HealthStatus::new_ok(),\n            Err(_) => HealthStatus::new_error(),\n        }\n    }\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthReport {\n    database: HealthStatus,\n\n    queue: HealthStatus,\n    cache: HealthStatus,\n}\n\n#[derive(Deserialize, Serialize, Debug, PartialEq)]\nstruct HealthCheckCacheValue(());\nkv_def!(HealthCheckCacheKey, HealthCheckCacheValue);\n\n/// Verify the API server is up and running.\n#[aide_annotate(op_id = \"v1.health.get\")]\nasync fn health(\n    State(AppState {\n        ref db,\n        queue_tx,\n        cache,\n        ..\n    }): State<AppState>,\n) -> (StatusCode, Json<HealthReport>) {\n    // SELECT 1 FROM any table\n    let database: HealthStatus = db\n        .execute(Statement::from_string(\n            DatabaseBackend::Postgres,\n            \"SELECT 1\".to_owned(),\n        ))\n        .await\n        .into();\n\n    // Send a [`HealthCheck`] through the queue\n    let queue: HealthStatus = queue_tx.send(&QueueTask::HealthCheck, None).await.into();\n\n    // Set a cache value with an expiration to ensure it works\n    let cache: HealthStatus = cache\n        .set(\n            &HealthCheckCacheKey(\"health_check_value\".to_owned()),\n            &HealthCheckCacheValue(()),\n            // Expires after this time, so it won't pollute the DB\n            Duration::from_millis(100),\n        )\n        .await\n        .into();\n\n    let status = if database.is_ok() && queue.is_ok() && cache.is_ok() {\n        StatusCode::OK\n    } else {\n        StatusCode::INTERNAL_SERVER_ERROR\n    };\n\n    (\n        status,\n        Json(HealthReport {\n            database,\n            queue,\n            cache,\n        }),\n    )\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Health\");\n\n    ApiRouter::new()\n        .api_route(\"/health/ping\", get(ping).head(ping))\n        .api_route_with(\n            \"/health\",\n            get_with(health, |op| op.response::<204, ()>().with(health_operation))\n                .head_with(health, health_operation),\n            tag,\n        )\n}\n<|fim_middle|>", "completion": "pub fn new_error() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Error,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/health.rs", "node_type": "function_item", "line_range": [46, 50]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse super::{StreamingEventType, StreamingEvents, StreamingSink, StreamingStream};\nuse crate::{error::Result, models::*, Configuration};\n\npub struct Streaming<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> Streaming<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    pub fn event_type(&self) -> StreamingEventType<'a> {\n        StreamingEventType::new(self.cfg)\n    }\n\n    pub fn events(&self) -> StreamingEvents<'a> {\n        StreamingEvents::new(self.cfg)\n    }\n\n    pub fn sink(&self) -> StreamingSink<'a> {\n        StreamingSink::new(self.cfg)\n    }\n\n    pub fn stream(&self) -> StreamingStream<'a> {\n        StreamingStream::new(self.cfg)\n    }\n\n    /// Get the HTTP sink headers. Only valid for `http` or `otelTracing` sinks.\n    <|fim_suffix|>\n\n    /// Updates the Sink's headers. Only valid for `http` or `otelTracing`\n    /// sinks.\n    pub async fn sink_headers_patch(\n        &self,\n        stream_id: String,\n        sink_id: String,\n        http_sink_headers_patch_in: HttpSinkHeadersPatchIn,\n    ) -> Result<EndpointHeadersOut> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/headers\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .with_body_param(http_sink_headers_patch_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get the transformation code associated with this sink.\n    pub async fn sink_transformation_get(\n        &self,\n        stream_id: String,\n        sink_id: String,\n    ) -> Result<SinkTransformationOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/transformation\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub async fn sink_headers_get(\n        &self,\n        stream_id: String,\n        sink_id: String,\n    ) -> Result<EndpointHeadersOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/headers\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .execute(self.cfg)\n        .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/streaming.rs", "node_type": "function_item", "line_range": [31, 44]}
{"prompt": "<|fim_prefix|>&key,\n                &FailureCacheValue {\n                    first_failure_at: now,\n                },\n                // Failures are forgiven after double the `disable_in` `Duration` with the expiry of\n                // the Redis key\n                disable_in * 2,\n            )\n            .await\n            .map_err(Error::generic)?;\n\n        Ok(None)\n    }\n}\n\n/// Sign a message\nfn sign_msg(\n    main_secret: &Encryption,\n    timestamp: i64,\n    body: &str,\n    msg_id: &MessageId,\n    endpoint_signing_keys: &[&EndpointSecretInternal],\n) -> String {\n    let to_sign = format!(\"{msg_id}.{timestamp}.{body}\");\n\n    endpoint_signing_keys\n        .iter()\n        .format_with(\" \", |x, f| {\n            let sig = x.sign(main_secret, to_sign.as_bytes());\n            let version = match x.type_() {\n                EndpointSecretType::Hmac256 => \"v1\",\n                EndpointSecretType::Ed25519 => \"v1a\",\n            };\n\n            f(&format_args!(\"{version},{}\", STANDARD.encode(sig)))\n        })\n        .to_string()\n}\n\n/// Generates a set of headers for any one webhook event\nfn generate_msg_headers(\n    timestamp: i64,\n    msg_id: &MessageId,\n    signatures: String,\n    whitelabel_headers: bool,\n    configured_headers: Option<&EndpointHeaders>,\n    _endpoint_url: &str,\n) -> Result<CaseSensitiveHeaderMap> {\n    let mut headers = CaseSensitiveHeaderMap::new();\n    let id_hdr = msg_id\n        .0\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message id: {e:?}\")))?;\n    let timestamp = timestamp\n        .to_string()\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message timestamp: {e:?}\")))?;\n    let signatures_str = signatures\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message signatures: {e:?}\")))?;\n    if whitelabel_headers {\n        headers.insert(\"webhook-id\".to_owned(), id_hdr);\n        headers.insert(\"webhook-timestamp\".to_owned(), timestamp);\n        headers.insert(\"webhook-signature\".to_owned(), signatures_str);\n    } else {\n        headers.insert(\"svix-id\".to_owned(), id_hdr);\n        headers.insert(\"svix-timestamp\".to_owned(), timestamp);\n        headers.insert(\"svix-signature\".to_owned(), signatures_str);\n    }\n    headers.insert(\n        \"user-agent\".to_owned(),\n        USER_AGENT.to_string().parse().unwrap(),\n    );\n    headers.insert(\n        \"content-type\".to_owned(),\n        \"application/json\".parse().unwrap(),\n    );\n    if let Some(configured_headers) = configured_headers {\n        for (k, v) in &configured_headers.0 {\n            match v.parse() {\n                Ok(v) => {\n                    headers.insert(k.clone(), v);\n                }\n                Err(e) => {\n                    tracing::error!(\"Invalid HeaderValue {}: {}\", v, e);\n                }\n            }\n        }\n    }\n\n    Ok(headers)\n}\n\n#[derive(Clone)]\nstruct WorkerContext<'a> {\n    cfg: &'a Configuration,\n    cache: &'a Cache,\n    db: &'a DatabaseConnection,\n    queue_tx: &'a TaskQueueProducer,\n    op_webhook_sender: &'a OperationalWebhookSender,\n    webhook_client: &'a WebhookClient,\n}\n\nstruct FailedDispatch(messageattempt::ActiveModel, Error);\nstruct SuccessfulDispatch(messageattempt::ActiveModel);\n\n#[allow(clippy::large_enum_variant)]\nenum IncompleteDispatch {\n    Pending(PendingDispatch),\n    #[allow(dead_code)]\n    Failed(FailedDispatch),\n}\n\nstruct PendingDispatch {\n    method: http::Method,\n    url: String,\n    headers: CaseSensitiveHeaderMap,\n    payload: String,\n    request_timeout: u64,\n    created_at: DateTimeUtc,\n}\n\n// Clippy fails to compute the first variant's size, stating it as\n// \"at least 0 bytes\". They're actually very similar in size.\n#[allow(clippy::large_enum_variant)]\nenum CompletedDispatch {\n    Failed(FailedDispatch),\n    Successful(SuccessfulDispatch),\n}\n\n#[tracing::instrument(skip_all)]\nasync fn prepare_dispatch(\n    WorkerContext { cfg, .. }: &WorkerContext<'_>,\n    DispatchContext {\n        msg_task,\n        payload,\n        endp,\n        ..\n    }: DispatchContext<'_>,\n) -> Result<IncompleteDispatch> {\n    l<|fim_suffix|>\n    let headers = {\n        let keys = endp.valid_signing_keys();\n\n        let signatures = sign_msg(\n            &cfg.encryption,\n            attempt_created_at.timestamp(),\n            payload,\n            &msg_task.msg_id,\n            &keys,\n        );\n\n        generate_msg_headers(\n            attempt_created_at.timestamp(),\n            &msg_task.msg_id,\n            signatures,\n            cfg.whitelabel_headers,\n            endp.headers.as_ref(),\n            &endp.url,\n        )?\n    };\n\n    Ok(IncompleteDispatch::Pending(PendingDispatch {\n        method: http::Method::POST,\n        url: endp.url.clone(),\n        headers,\n        payload: payload.to_owned(),\n        request_timeout: cfg.worker_request_timeout as _,\n        created_at: attempt_created_at,\n    }))\n}\n\n#[tracing::instrument(skip_all)]\nasync fn make_http_call(\n    DispatchContext {\n        msg_task,\n        endp,\n        db_attempt_number,\n        ..\n    }: DispatchContext<'_>,\n    PendingDispatch {\n        method,\n        url,\n        headers,\n        payload,\n        request_timeout,\n        created_at,\n    }: PendingDispatch,\n    client: &WebhookClient,\n) -> Result<CompletedDispatch> {\n    let req = RequestBuilder::new()\n        .method(method)\n        .uri_str(&url)\n        .map_err(|e| Error::validation(format_args!(\"URL is invalid: {e:?}\")))?\n        .headers(headers)\n        .body(payload.into(), HeaderValue::from_static(\"application/json\"))\n        .version(Version::HTTP_11)\n        .timeout(Duration::from_secs(request_timeout))\n        .build()\n        .map_err(Error::generic)?;\n\n    let attempt = messageattempt::ActiveModel {\n        // Set both ID and created_at to the same timestamp\n        id: Set(MessageAttemptId::new(created_at.into(), None)),\n        created_at: Set(created_at.into()),\n        msg_id: Set(msg_task.msg_id.clone()),\n        endp_id: Set(endp.id.clone()),\n        msg_dest_id: Set(None),\n        url: Set(endp.url.clone()),\n        ended_at: Set(Some(Utc::now().into())),\n        trigger_type: Set(msg_task.trigger_type),\n        response_duration_ms: Set(0), // Default to 0, will be updated after the request\n        attempt_number: Set(db_attempt_number),\n        ..Default::default()\n    };\n\n    match client.execute(req).await {\n        Ok(res) => {\n            // Calculate the duration in milliseconds\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            let status_code = res.status().as_u16() as i16;\n            let status = if res.status().is_success() {\n                MessageStatus::Success\n            } else {\n                MessageStatus::Fail\n            };\n\n            let http_error = if !res.status().is_success() {\n                Some(WebhookClientError::FailureStatus(res.status()))\n            } else {\n                None\n            };\n\n            let body = match res.into_body().collect().await {\n                Ok(collected) => {\n                    let bytes = collected.to_bytes();\n                    if bytes.len() > RESPONSE_MAX_SIZE {\n                        bytes_to_string(bytes.slice(..RESPONSE_MAX_SIZE))\n                    } else {\n                        bytes_to_string(bytes)\n                    }\n                }\n                Err(err) => format!(\"Error reading response body: {err}\"),\n            };\n\n            let attempt = messageattempt::ActiveModel {\n                response_status_code: Set(status_code),\n                response: Set(body),\n                status: Set(status),\n                response_duration_ms: Set(duration_ms),\n                ..attempt\n            };\n\n            match http_error {\n                Some(err) => Ok(CompletedDispatch::Failed(FailedDispatch(\n                    attempt,\n                    Error::generic(err),\n                ))),\n                None => Ok(CompletedDispatch::Successful(SuccessfulDispatch(attempt))),\n            }\n        }\n        Err(err) => {\n            // For errors, we still calculate the duration\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            Ok(Completed<|fim_middle|>", "completion": "let attempt_created_at = Utc::now();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "let_declaration", "line_range": [290, 290]}
{"prompt": "<|fim_prefix|>// this file is @generated\n#![warn(unreachable_pub)]\n\nmod client;\nmod deprecated;\n\npub use self::client::{Svix, SvixOptions};\npub use crate::models::*;\n\nmod application;\nmod authentication;\nmod background_task;\nmod connector;\nmod endpoint;\nmod environment;\nmod event_type;\nmod ingest;\nmod ingest_endpoint;\nmod ingest_source;\nmod integration;\nmod message;\nmod message_attempt;\nmod message_poller;\nmod operational_webhook;\nmod operational_webhook_endpoint;\nmod statistics;\nmod streaming;\nmod streaming_event_type;\nmod streaming_events;\nmod streaming_sink;\nmod streaming_stream;\n\n#[cfg(feature = \"svix_beta\")]\npub use self::message::{V1MessageEventsParams, V1MessageEventsSubscriptionParams};\npub use self::{\n    application::{Application, ApplicationCreateOptions, ApplicationListOptions},\n    authentication::{\n        Authentication, AuthenticationAppPortalAccessOptions, AuthenticationExpireAllOptions,\n        AuthenticationLogoutOptions, AuthenticationRotateStreamPollerTokenOptions,\n        AuthenticationStreamPortalAccessOptions,\n    },\n    background_task::{BackgroundTask, BackgroundTaskListOptions},\n    connector::{Connector, ConnectorCreateOptions, ConnectorListOptions},\n    deprecated::*,\n    endpoint::{\n        Endpoint, EndpointCreateOptions, EndpointGetStatsOptions, EndpointListOptions,\n        EndpointRecoverOptions, EndpointReplayMissingOptions, EndpointRotateSecretOptions,\n        EndpointSendExampleOptions,\n    },\n    environment::{Environment, EnvironmentExportOptions, EnvironmentImportOptions},\n    event_type::{\n        EventType, EventTypeCreateOptions, EventTypeDeleteOptions, EventTypeImportOpenapiOptions,\n        EventTypeListOptions,\n    },\n    ingest::{Ingest, IngestDashboardOptions},\n    ingest_endpoint::{\n        IngestEndpoint, IngestEndpointCreateOptions, IngestEndpointListOptions,\n        IngestEndpointRotateSecretOptions,\n    },\n    ingest_source::{\n        IngestSource, IngestSourceCreateOptions, IngestSourceListOptions,\n        IngestSourceRotateTokenOptions,\n    },\n    integration::{\n        Integration, IntegrationCreateOptions, IntegrationListOptions, IntegrationRotateKeyOptions,\n    },\n    message::{\n        Message, MessageCreateOptions, MessageExpungeAllContentsOptions, MessageGetOptions,\n        MessageListOptions,\n    },\n    message_attempt::{\n        MessageAttempt, MessageAttemptListAttemptedDestinationsOptions,\n        MessageAttemptListAttemptedMessagesOptions, MessageAttemptListByEndpointOptions,\n        MessageAttemptListByMsgOptions, MessageAttemptResendOptions,\n    },\n    message_poller::{\n        MessagePoller, MessagePollerConsumerPollOptions, MessagePollerConsumerSeekOptions,\n        MessagePollerPollOptions,\n    },\n    operational_webhook::OperationalWebhook,\n    operational_webhook_endpoint::{\n        OperationalWebhookEndpoint, OperationalWebhookEndpointCreateOptions,\n        OperationalWebhookEndpointListOptions, OperationalWebhookEndpointRotateSecretOptions,\n    },\n    statistics::{Statistics, StatisticsAggregateAppStatsOptions},\n    streaming::Streaming,\n    streaming_event_type::{\n        StreamingEventType, StreamingEventTypeCreateOptions, StreamingEventTypeDeleteOptions,\n        StreamingEventTypeListOptions,\n    },\n    streaming_events::{StreamingEvents, StreamingEventsCreateOptions, StreamingEventsGetOptions},\n    streaming_sink::{\n        StreamingSink, StreamingSinkCreateOptions, StreamingSinkListOptions,\n        StreamingSinkRotateSecretOptions,\n    },\n    streaming_stream::{StreamingStream, StreamingStreamCreateOptions, StreamingStreamListOptions},\n};\n\nimpl Svix {\n    pub fn application(&self) -> Application<'_> {\n        Application::new(&self.cfg)\n    }\n\n    pub fn authentication(&self) -> Authentication<'_> {\n        Authentication::new(&self.cfg)\n    }\n\n    pub fn background_task(&self) -> BackgroundTask<'_> {\n        BackgroundTask::new(&self.cfg)\n    }\n\n    pub fn connector(&self) -> Connector<'_> {\n        Connector::new(&self.cfg)\n    }\n\n    pub fn endpoint(&self) -> Endpoint<'_> {\n        Endpoint::new(&self.cfg)\n    }\n\n    pub fn environment(&self) -> Environment<'_> {\n        Environment::new(&self.cfg)\n    }\n\n    pub fn event_type(&self) -> EventType<'_> {\n        EventType::new(&self.cfg)\n    }\n\n    pub fn ingest(&self) -> Ingest<'_> {\n        Ingest::new(&self.cfg)\n    }\n\n    pub fn integration(&self) -> Integration<'_> {\n        Integration::new(&self.cfg)\n    }\n\n    pub fn message(&self) -> Message<'_> {\n        Message::new(&self.cfg)\n    }\n\n    pub fn message_attempt(&self) -> MessageAttempt<'_> {\n        MessageAttempt::new(&self.cfg)\n    }\n\n    pub fn operational_webhook(&self) -> OperationalWebhook<'_> {\n        OperationalWebhook::new(&self.cfg)\n    }\n\n    <|fim_suffix|>\n\n    pub fn streaming(&self) -> Streaming<'_> {\n        Streaming::new(&self.cfg)\n    }\n\n    #[deprecated = \"Use .operational_webhook().endpoint() instead\"]\n    pub fn operational_webhook_endpoint(&self) -> OperationalWebhookEndpoint<'_> {\n        OperationalWebhookEndpoint::new(&self.cfg)\n    }\n}\n<|fim_middle|>", "completion": "pub fn statistics(&self) -> Statistics<'_> {\n        Statistics::new(&self.cfg)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/mod.rs", "node_type": "function_item", "line_range": [148, 150]}
{"prompt": "<|fim_prefix|>#}\"))\n            }\n        });\n\n        set.spawn(async move {\n            send_to_ws_loop(remote_rx, ws_tx)\n                .await\n                .inspect_err(|e| eprintln!(\"write loop terminated: {e:#}\"))\n        });\n\n        // If any task terminates, trash the rest so we can reconnect.\n        if set.join_next().await.is_some() {\n            set.shutdown().await;\n        }\n\n        Ok(())\n    }\n}\n\npub async fn listen(\n    local_url: url::Url,\n    relay_token: String,\n    relay_debug_url: Option<&str>,\n    relay_disable_security: bool,\n    disable_tls_verification: bool,\n) -> Result<()> {\n    let scheme = if relay_disable_security { \"ws\" } else { \"wss\" };\n    let api_host = relay_debug_url.unwrap_or(DEFAULT_API_HOST);\n    let token = format!(\"c_{relay_token}\");\n\n    let websocket_url = format!(\"{scheme}://{api_host}/{API_PREFIX}/listen/\").parse()?;\n\n    let http_client = HttpClient::builder()\n        .danger_accept_invalid_certs(disable_tls_verification)\n        .build()?;\n\n    let mut client = Client {\n        token,\n        websocket_url,\n        local_url,\n        http_client,\n    };\n\n    const MAX_BACKOFF: Duration = Duration::from_millis(5000);\n    let backoff_schedule = [\n        Duration::ZERO,\n        Duration::from_millis(100),\n        Duration::from_millis(1000),\n        MAX_BACKOFF,\n    ];\n\n    let mut attempt_count = 0;\n    let mut last_attempt = Instant::now();\n\n    // We may ditch this token, generating a new one on the fly, depending on how the server\n    // responds when we connect.\n    let orig_token = client.token.clone();\n    loop {\n        // Any termination Ok or Err... try to reconnect.\n        let show_welcome_message = attempt_count == 0 || orig_token != client.token;\n\n        if let Err(e) = client.connect(show_welcome_message).await {\n            eprintln!(\"Failed to connect to Webhook Relay: {e:#}\");\n            if e.downcast_ref::<TokenInUse>().is_some() {\n                eprintln!(\"Generating a new token for this session.\");\n                client.token = {\n                    let relay_token = generate_token()?;\n                    format!(\"c_{relay_token}\")\n                };\n            }\n        } else {\n            eprintln!(\"Failed to connect to Webhook Relay\");\n        }\n\n        // Reset the backoff schedule if it's been a while since we've seen a disconnect.\n        if last_attempt.elapsed() > MAX_BACKOFF * 2 {\n            // N.b. attempt_count `0` is special because that's what prompts the printing of a\n            // welcome message in `Client::connect`.\n            // When we reset here, starting at `0` here will still avoid the\n            // re-print because we increment after selecting the sleep duration.\n            attempt_count = 0;\n        }\n\n        let backoff = *backoff_schedule.get(attempt_count).unwrap_or(&MAX_BACKOFF);\n        eprintln!(\"Reattempting connection in: {}ms\", backoff.as_millis());\n\n        attempt_count += 1;\n        last_attempt = Instant::now();\n\n        tokio::time::sleep(backoff).await;\n    }\n}\n\nfn receive_url(token: &str) -> String {\n    format!(\"https://play.svix.com/in/{token}/\")\n}\n\nfn view_url(token: &str) -> String {\n    format!(\"https://play.svix.com/view/{token}/\")\n}\n\ntype S = WebSocketStream<MaybeTlsStream<TcpStream>>;\n\nstruct WsConnection {\n    stream: S,\n}\n\nimpl WsConnection {\n    async fn new(websocket_url: &url::Url) -> Result<Self> {\n        let request = websocket_url.to_string().into_client_request()?;\n        let (stream, _resp) = connect_async(request)\n            .await\n            .inspect_err(|e| eprintln!(\"{e}\"))\n            .context(\"failed to connect to websocket server\")?;\n\n        Ok(Self { stream })\n    }\n}\n\nasync fn read_from_ws_loop(\n    mut rx: SplitStream<S>,\n    tx: UnboundedSender<MessageOut>,\n    local_url: url::Url,\n    client: HttpClient,\n) -> Result<()> {\n    // We expect to see roughly _at least one Ping_ in each `SERVER_PING_PERIOD`.\n    // Other messages may arrive ahead of this schedule.\n    // Tracking the time each message is received, we can know if the server has been quiet for too\n    // long, possibly requiring us to reconnect.\n    let mut last_msg = Instant::now();\n\n    loop {\n        const REMOTE_SERVER_CLOSED: &str = \"remote server closed connection\";\n\n        match tokio::time::timeout(SERVER_PING_PERIOD, rx.next()).await {\n            Err(_timeout_hit) => {\n                // Generous. 1.5x the ping frequency. If we go that long without\n                // seeing anything from the server, force a reconnect.\n                if last_msg.elapsed() > SERVER_PING_PERIOD + (SERVER_PING_PERIOD / 2) {\n                    anyhow::bail!(REMOTE_SERVER_CLOSED);\n                }\n            }\n            // Stream empty/closed\n            Ok(None) => break,\n            Ok(Some(msg)) => {\n                last_msg = Instant::now();\n\n                let data = <|fim_suffix|>;\n\n                handle_incoming_message(client.clone(), data, &local_url, tx.clone()).await;\n            }\n        }\n    }\n\n    Ok(())\n}\n\nasync fn send_to_ws_loop(\n    mut rx: UnboundedReceiver<MessageOut>,\n    mut tx: SplitSink<S, Message>,\n) -> Result<()> {\n    while let Some(msg) = rx.recv().await {\n        tokio::time::timeout(\n            WRITE_WAIT,\n            tx.send(Message::Binary(\n                serde_json::to_vec(&msg)\n                    .expect(\"trivial serialization\")\n                    .into(),\n            )),\n        )\n        .await?\n        .context(\"Websocket write timeout\")?;\n    }\n\n    Ok(())\n}\n\nasync fn make_local_request(\n    client: HttpClient,\n    url: &url::Url,\n    data: MessageInEvent,\n) -> Result<LocalServerResponse> {\n    let method = data.method.parse()?;\n    // FIXME: deprecation warning\n    #[allow(deprecated)]\n    let body = base64::decode(&data.body)?;\n    let mut headers = HeaderMap::with_capacity(data.headers.len());\n    for (k, v) in &data.headers {\n        // FIXME: there's a remark about the Go client freaking out if there's more than one host header set.\n        //   Do we care now that we're not using Go? TBD.\n        headers.insert(\n            HeaderName::try_from(k.as_str())?,\n            HeaderValue::try_from(v.as_str())?,\n        );\n    }\n    Ok(client\n        .request(method, url.clone())\n        .timeout(DEFAULT_TIMEOUT)\n        .body(body)\n        .headers(headers)\n        .send()\n        .await?)\n}\n\nfn format_resp_headers(headers: &HeaderMap) -> Result<HashMap<String, String>> {\n    let mut out = HashMap::new();\n    for (k, v) in headers {\n        out.insert(k.to_string(), v.to_str()?.to_string());\n    }\n    Ok(out)\n}\n\nasync fn handle_incoming_message(\n    client: HttpClient,\n    bytes: Bytes,\n    local_url: &url::Url,\n    tx: UnboundedSender<MessageOut>,\n) {\n    match serde_json::from_slice::<MessageIn>(&bytes) {\n        Ok(MessageIn::Event { data, .. }) => {\n            let msg_id = data.id.clone();\n            println!(\"<- Forwarding message id={msg_id} to: {local_url}\");\n            match make_local_request(client, local_url, data).await {\n                Err(err) => {\n                    eprintln!(\"Failed to make request to local server: \\n{err:#}\");\n                }\n                Ok(resp) => {\n                    if let Err(err) = process_response(msg_id, resp, tx).await {\n                        eprintln!(\"Failed to read response from local server: \\n{err:#}\");\n                    }\n                }\n            }\n        }\n        Ok(MessageIn::Start { .. }) => { /* nothing to do */ }\n        Err(_err) => {\n            eprintln!(\"Received invalid webhook message... skipping\");\n        }\n    }\n}\n\nasync fn process_response(\n    id: String,\n    resp: LocalServerResponse,\n    tx: UnboundedSender<MessageOut>,\n) -> Result<()> {\n    let status = resp.status().as_u16();\n    let headers = format_resp_headers(resp.headers())?;\n    #[allow(deprecated)]\n    let body = base64::encode(resp.bytes().await?);\n    let msg = MessageOut::Event {\n        version: message::VERSION,\n        data: MessageOutEvent {\n            id,\n            body,\n            headers,\n            status,\n        },\n    };\n\n    println!(\"-> Received \\\"{status}\\\" response from local server, forwarding to webhook sender\");\n    Ok(tx.send(msg)?)\n}\n<|fim_middle|>", "completion": "match msg? {\n                    // Control messages.\n                    Message::Close(_) => anyhow::bail!(REMOTE_SERVER_CLOSED),\n                    Message::Ping(_) | Message::Pong(_) | Message::Frame(_) => continue,\n\n                    // Messages that carry data we care to process.\n                    Message::Text(s) => s.into(),\n                    Message::Binary(bytes) => bytes,\n                }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/mod.rs", "node_type": "match_expression", "line_range": [341, 349]}
{"prompt": "<|fim_prefix|>hanged while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({\n                \"metadata\": {\n                    \"foo\": \"bar\",\n                    \"bizz\": \"baz\",\n                },\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#), out.metadata);\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.rate_limit, None);\n}\n\n#[tokio::test]\nasync fn test_crud() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1_1: &str = \"v1ApplicationCrudTest11\";\n    const APP_NAME_1_2: &str = \"v1ApplicationCrudTest12\";\n    const APP_NAME_2_1: &str = \"v1ApplicationCrudTest21\";\n    const APP_NAME_2_2: &str = \"v1ApplicationCrudTest22\";\n\n    // CREATE\n    let app_1: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(APP_NAME_1_1),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_1.name, APP_NAME_1_1);\n\n    let app_2: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(APP_NAME_2_1),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_2.name, APP_NAME_2_1);\n\n    // READ\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::OK)\n            .await\n            .unwrap(),\n        app_1\n    );\n\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_2\n    );\n\n    // UPDATE\n    let app_1_id = app_1.id;\n    let app_1: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{app_1_id}/\"),\n            application_in(APP_NAME_1_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    let app_2_id = app_2.id;\n    let app_2: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{app_2_id}/\"),\n            application_in(APP_NAME_2_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // CONFIRM UPDATE\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{app_1_id}/\"), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_1\n    );\n\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{app_2_id}/\"), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_2\n    );\n\n    // DELETE\n    client\n        .delete(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n    client\n        .delete(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n\n    // CONFIRM DELETION\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n\n    let app: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            serde_json::json!({\n                \"name\": \"Apps all around\",\n            }),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(app.metadata, metadata(r#\"{}\"#));\n\n    let updated: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({\n                \"metadata\": {\n                    \"bizz\": \"bar\"\n                },\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(updated.metadata, metadata(r#\"{\"bizz\":\"bar\"}\"#));\n\n    l<|fim_suffix|>\n    assert_eq!(new_app.metadata, metadata(r#\"{\"foo\":\"bar\"}\"#));\n\n    let updated_metadata_app: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{}/\", new_app.id),\n            serde_json::json!({\n                \"name\": \"New Name\",\n                \"metadata\": {\n                    \"new\": \"data\"\n                },\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(updated_metadata_app.metadata, metadata(r#\"{\"new\":\"data\"}\"#));\n    assert_eq!(updated_metadata_app.name, \"New Name\");\n}\n\n#[tokio::test]\nasync fn test_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    common_test_list::<ApplicationOut, ApplicationIn>(\n        &client,\n        \"api/v1/app/\",\n        |i| application_in(&format!(\"App {i}\")),\n        true,\n        true,\n    )\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_uid() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name: \"App 1\".to_owned(),\n                uid: Some(ApplicationUid(\"app1\".to_owned())),\n                ..Default::default()\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_ne!(app.id.0, app.uid.unwrap().0);\n\n    // Can't create another app with the same uid twice\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name: \"App 1\".to_owned(),\n                uid: Some(ApplicationUid(\"app1\".to_owned())),\n                ..Default::default()\n            },\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Can't update an app to an existing uid (when we have no uid)\n    let app2: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name: \"App 2\".to_owned(),\n                ..Default::default()\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: IgnoredAny = client\n        .put(\n            &format!(\"api/v1/app/{}/\", app2.id),\n            ApplicationIn {\n                name: \"App 2\".to_owned(),\n                uid: Some(ApplicationUid(\"app1\".to_owned())),\n                ..Default::default()\n            },\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Can't update an app to an existing uid (when we have a uid)\n    let app2: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name: \"App 2\".to_owned(),\n                uid: Some(ApplicationUid(\"app2\".to_owned())),\n                ..Default::default()\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: IgnoredAny = client\n        .put(\n            &format!(\"api/v1/app/{}/\", app2.id),\n            ApplicationIn {\n                name: \"App 2\".to_owned(),\n                uid: Some(ApplicationUid(\"app1\".to_owned())),\n                ..Default::default()\n            },\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Delete app1\n    client\n        .delete(&format!(\"api/v1/app/{}/\", app.id), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n\n    // Update to a now deleted uid\n    let app2: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{}/\", app2.id),\n            ApplicationIn {\n                name: \"App 2\".to_owned(),\n                uid: Some(ApplicationUid(\"app1\".to_owned())),\n                ..Default::default()\n            },\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    client\n        .delete(\n            &format!(\"api/v1/app/{}/\", app2.uid.unwrap()),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Create an app with the same UID again (after it was deleted)\n    let app: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name<|fim_middle|>", "completion": "let new_app: ApplicationOut = client\n        .put(\n            \"api/v1/app/one_upserted_boi/\",\n            serde_json::json!({\n                \"name\": \"Apps for two\",\n                \"metadata\": {\n                    \"foo\": \"bar\"\n                },\n            }),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_application.rs", "node_type": "let_declaration", "line_range": [309, 321]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse axum::async_trait;\n\nuse super::{Cache, CacheBehavior, CacheKey, CacheValue, Result, StringCacheKey};\n\npub fn new() -> Cache {\n    tracing::warn!(\"Running with caching disabled will negatively affect performance. Idempotency is not supported without a cache.\");\n    NoCache {}.into()\n}\n\n#[derive(Clone)]\npub struct NoCache;\n\n#[async_trait]\nimpl CacheBehavior for NoCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get<T: CacheValue>(&self, _key: &T::Key) -> Result<Option<T>> {\n        Ok(None)\n    }\n\n    async fn get_raw(&self, _key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(None)\n    }\n\n    async fn get_string<T: StringCacheKey>(&self, _key: &T) -> Result<Option<String>> {\n        Ok(None)\n    }\n\n    async fn set<T: CacheValue>(&self, _key: &T::Key, _value: &T, _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_raw(&self, _key: &[u8], _value: &[u8], _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_string<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<()> {\n        Ok(())\n    }\n\n    async fn delete<T: CacheKey>(&self, _key: &T) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_if_not_exists<T: CacheValue>(\n        &self,\n        _key: &T::Key,\n        _value: &T,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n\n    a<|fim_suffix|>\n    async fn set_string_if_not_exists<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n}\n<|fim_middle|>", "completion": "async fn set_raw_if_not_exists(\n        &self,\n        _key: &[u8],\n        _value: &[u8],\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/none.rs", "node_type": "function_item", "line_range": [66, 73]}
{"prompt": "<|fim_prefix|>: None,\n    };\n\n    let create_message = create_message_inner(\n        db,\n        queue_tx,\n        cache,\n        false,\n        Some(endpoint.id),\n        msg_in,\n        app.org_id,\n        ApplicationIdOrUid(app.id.0),\n    )\n    .await?;\n\n    Ok(Json(create_message))\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Endpoint\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app/:app_id/endpoint\",\n            post_with(crud::create_endpoint, crud::create_endpoint_operation)\n                .get_with(crud::list_endpoints, crud::list_endpoints_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id\",\n            get_with(crud::get_endpoint, crud::get_endpoint_operation)\n                .put_with(crud::update_endpoint, crud::update_endpoint_operation)\n                .patch_with(crud::patch_endpoint, crud::patch_endpoint_operation)\n                .delete_with(crud::delete_endpoint, crud::delete_endpoint_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret\",\n            get_with(\n                secrets::get_endpoint_secret,\n                secrets::get_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret/rotate\",\n            post_with(\n                secrets::rotate_endpoint_secret,\n                secrets::rotate_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/stats\",\n            get_with(endpoint_stats, endpoint_stats_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/send-example\",\n            post_with(send_example, send_example_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/recover\",\n            post_with(\n                recovery::recover_failed_webhooks,\n                recovery::recover_failed_webhooks_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/headers\",\n            get_with(\n                headers::get_endpoint_headers,\n                headers::get_endpoint_headers_operation,\n            )\n            .patch_with(\n                headers::patch_endpoint_headers,\n                headers::patch_endpoint_headers_operation,\n            )\n            .put_with(\n                headers::update_endpoint_headers,\n                headers::update_endpoint_headers_operation,\n            ),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use std::collections::{HashMap, HashSet};\n\n    use reqwest::Url;\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{validate_url, EndpointHeadersOut, EndpointHeadersPatchIn, EndpointIn};\n    use crate::core::types::EndpointHeaders;\n\n    const URL_VALID: &str = \"https://www.example.com\";\n    const URL_INVALID: &str = \"invalid url\";\n    const VERSION_VALID: u16 = 1;\n    const VERSION_INVALID: u16 = 0;\n    const RATE_LIMIT_VALID: u16 = 1;\n    const RATE_LIMIT_INVALID: u16 = 0;\n    const EVENT_TYPES_INVALID: &[&str] = &[\"valid-event-type\", \"&&invalid-event-type\"];\n    const EVENT_TYPES_VALID: &[&str] = &[\"valid-event-type1\", \"valid-event-type2\"];\n    const EVENT_CHANNELS_INVALID: &[&str] = &[\"valid-event-channel\", \"&&invalid-event-channel\"];\n    const EVENT_CHANNELS_VALID: &[&str] = &[\"valid-event-channel1\", \"valid-event-channel2\"];\n    const ENDPOINT_ID_INVALID: &str = \"$$invalid-endpoint\";\n    const ENDPOINT_ID_VALID: &str = \"valid-endpoint\";\n\n    #[allow(deprecated)]\n    #[test]\n    fn test_endpoint_in_validation() {\n        let invalid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_INVALID,\n             \"url\": URL_VALID\n        }))\n        .unwrap();\n\n        let invalid_2: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"channels\": EVENT_CHANNELS_INVALID\n        }))\n        .unwrap();\n\n        let invalid_3: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_INVALID\n        }))\n        .unwrap();\n\n        let invalid_4: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"uid\": ENDPOINT_ID_INVALID\n        }))\n        .unwrap();\n\n        let invalid_5: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"filterTypes\": EVENT_TYPES_INVALID\n        }))\n        .unwrap();\n\n        let invalid_6: Result<EndpointIn, _> = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_INVALID\n        }));\n        assert!(invalid_6.is_err());\n\n        for e in [invalid_1, invalid_2, invalid_3, invalid_4, invalid_5] {\n            assert!(e.validate().is_err());\n        }\n\n        let valid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_1.validate().unwrap();\n\n        let valid_2: EndpointIn = serde_json::from_value(json!({\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_2.validate().unwrap();\n        assert_eq!(1, valid_2.version.unwrap());\n    }\n\n    #[test]\n    fn test_endpoint_headers_sensitive() {\n        let headers = EndpointHeaders(HashMap::from([\n            (\"foo\".to_string(), \"1\".to_string()),\n            (\"authorization\".to_string(), \"test\".to_string()),\n            (\"X-Auth-Token\".to_string(), \"test2\".to_string()),\n        ]));\n\n        let headers_out: EndpointHeadersOut = headers.into();\n\n        assert_eq!(\n            headers_out.headers,\n            HashMap::from([(\"foo\".to_string(), \"1\".to_string())])\n        );\n        assert_eq!(\n            headers_out.sensitive,\n            HashSet::from([\"authorization\".to_string(), \"X-Auth-Token\".to_string()])\n        );\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_in_validation() {\n        let headers_valid = HashMap::from([(\"x-valid\", \"1\")]);\n        let headers_invalid = HashMap::from([(\"x-invalid???\", \"1\")]);\n\n        let invalid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_invalid })).unwrap();\n        assert!(invalid.validate().is_err());\n\n        l<|fim_suffix|>        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_url_validation() {\n        let valid_https = Url::parse(\"https://test.url\").unwrap();\n        let valid_http = Url::parse(\"http://test.url\").unwrap();\n        let invalid_scheme = Url::parse(\"anythingelse://test.url\").unwrap();\n        let invalid_format = \"http://[:::1]\";\n\n        assert!(validate_url(&valid_https).is_ok());\n        assert!(validate_url(&valid_http).is_ok());\n        assert!(validate_url(&invalid_scheme).is_err());\n\n        let valid_https: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_https, \"version\": 1})).unwrap();\n        let valid_http: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_http, \"version\": 1})).unwrap();\n        let invalid_scheme: EndpointIn =\n            serde_json::from_value(json!({\"url\": invalid_scheme, \"version\": 1})).unwrap();\n        let invalid_format: Result<EndpointIn, _> =\n            serde_json::from_value(json!({\"url\": invalid_format, \"version\": 1}));\n\n        assert!(valid_https.validate().is_ok());\n        assert!(valid_http.validate().is_ok());\n        assert!(invalid_scheme.validate().is_err());\n        assert!(invalid_format.is_err());\n    }\n}\n<|fim_middle|>", "completion": "let valid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_valid })).unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "let_declaration", "line_range": [1008, 1009]}
{"prompt": "<|fim_prefix|>ent\n        .delete(&format!(\"api/v1/app/{id}/\"), StatusCode::NO_CONTENT)\n        .await\n}\n\n// Endpoint\n\npub fn default_test_endpoint() -> EndpointIn {\n    #[allow(deprecated)]\n    EndpointIn {\n        description: Default::default(),\n        rate_limit: Default::default(),\n        uid: Default::default(),\n        url: Url::parse(\"http://example.com\").unwrap(),\n        version: Some(1),\n        disabled: Default::default(),\n        event_types_ids: Default::default(),\n        channels: Default::default(),\n        key: Default::default(),\n        metadata: Default::default(),\n    }\n}\n\npub fn endpoint_in(url: &str) -> EndpointIn {\n    EndpointIn {\n        url: Url::parse(url).unwrap(),\n        ..default_test_endpoint()\n    }\n}\n\npub async fn create_test_endpoint(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    url: &str,\n) -> Result<EndpointOut> {\n    post_endpoint(client, app_id, endpoint_in(url)).await\n}\n\npub async fn post_endpoint(\n    client: &TestClient,\n    app_id: &str,\n    ep: EndpointIn,\n) -> Result<EndpointOut> {\n    client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep,\n            StatusCode::CREATED,\n        )\n        .await\n}\n\npub async fn put_endpoint(\n    client: &TestClient,\n    app_id: &str,\n    ep_id: &str,\n    ep: EndpointIn,\n) -> Result<EndpointOut> {\n    client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n            ep,\n            StatusCode::OK,\n        )\n        .await\n}\n\n// Message\n\npub fn message_in<T: Serialize>(event_type: &str, payload: T) -> Result<MessageIn> {\n    Ok(MessageIn {\n        event_type: EventTypeName(event_type.to_owned()),\n        payload: RawPayload::from_string(serde_json::to_string(&payload)?)?,\n        payload_retention_period: 5,\n        channels: None,\n        uid: None,\n        extra_params: None,\n        application: None,\n    })\n}\n\npub async fn create_test_message(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n) -> Result<MessageOut> {\n    client\n        .post(\n            &format!(\"api/v1/app/{}/msg/\", &app_id),\n            message_in(\"event.type\", payload)?,\n            StatusCode::ACCEPTED,\n        )\n        .await\n}\n\npub async fn create_test_msg_with(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n    event_type: &str,\n    channel: impl IntoIterator<Item = &str>,\n) -> MessageOut {\n    let channels: HashSet<EventChannel> = channel\n        .into_iter()\n        .map(|x| EventChannel(x.to_string()))\n        .collect();\n\n    let mut message_in = json!({\n        \"eventType\": event_type,\n        \"payload\": payload,\n        \"payloadRetentionPeriod\": 5,\n    });\n    if !channels.is_empty() {\n        message_in[\"channels\"] = json!(channels);\n    }\n\n    client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in,\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap()\n}\n\npub fn event_type_in(\n    name: &str,\n    schema: impl Into<Option<serde_json::Value>>,\n) -> Result<EventTypeIn> {\n    Ok(EventTypeIn {\n        name: EventTypeName(name.to_owned()),\n        description: \"test-event-description\".to_owned(),\n        deleted: false,\n        schemas: schema.into().map(|s| serde_json::from_value(s).unwrap()),\n        feature_flag: None,\n        deprecated: false,\n    })\n}\n\n// Common tests\npub async fn common_test_list<\n    ModelOut: DeserializeOwned + PartialEq + std::fmt::Debug,\n    ModelIn: Serialize,\n>(\n    client: &TestClient,\n    path: &str,\n    create_model: fn(usize) -> ModelIn,\n    sort_asc: bool,\n    supports_reverse: bool,\n) -> Result<()> {\n    let mut items = Vec::new();\n    for i in 0..10 {\n        let item: ModelOut = client\n            .post(path, create_model(i), StatusCode::CREATED)\n            .await\n            .unwrap();\n        // Sleep for 5ms because KsuidMs has 4ms accuracy so things got out of order\n        tokio::time::sleep(Duration::from_millis(5)).await;\n        items.push(item);\n    }\n\n    let original_list = run_with_retries(|| async {\n        l<|fim_suffix|>\n        assert_eq!(list.data.len(), 10);\n\n        Ok(list)\n    })\n    .await\n    .unwrap();\n\n    if sort_asc {\n        for i in 0..10 {\n            assert_eq!(items.get(i), original_list.data.get(i));\n        }\n    } else {\n        for i in 0..10 {\n            assert_eq!(items.get(9 - i), original_list.data.get(i));\n        }\n    }\n\n    // Limit results\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=1\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 1);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=50\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=10\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=6\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 6);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=6&iterator={}\", list.iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 4);\n    assert!(list.done);\n\n    let prev = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=3&iterator={}\", list.prev_iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(prev.data.len(), 3);\n    assert_eq!(\n        prev.data.first().unwrap(),\n        original_list.data.get(3).unwrap()\n    );\n\n    let _list = client\n        .get::<IgnoredAny>(\n            &format!(\"{path}?limit=6&iterator=BAD-$$$ITERATOR\"),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    if supports_reverse {\n        let opposite_order = if sort_asc { \"descending\" } else { \"ascending\" };\n\n        let opposite_1 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\"{path}?limit=3&order={opposite_order}\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let opposite_2 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\n                    \"{path}?limit=3&order={opposite_order}&iterator={}\",\n                    opposite_1.iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let opposite_prev = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\n                    \"{path}?limit=3&order={opposite_order}&iterator={}\",\n                    opposite_2.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for i in 0..3 {\n            assert_eq!(original_list.data.get(9 - i), opposite_1.data.get(i));\n            assert_eq!(original_list.data.get(6 - i), opposite_2.data.get(i));\n            assert_eq!(original_list.data.get(9 - i), opposite_prev.data.get(i));\n        }\n    }\n\n    // Test limits -- ten models were created previously and the default hard/soft cap is 250 so\n    // 10..251 is the sane range here.\n    for i in 10..251 {\n        let _: ModelOut = client\n            .post(path, create_model(i), StatusCode::CREATED)\n            .await\n            .unwrap();\n    }\n\n    // If limits are hard, it will be a 422 UNPROCESSABLE_ENTITY response, otherwise it'll be capped\n    // to 250\n    if client\n        .get::<IgnoredAny>(\n            &format!(\"{path}?limit=300\"),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .is_ok()\n        || client\n            .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=300\"), StatusCode::OK)\n            .await\n            .unwrap()\n            .data\n            .len()\n            == 250\n    {\n    <|fim_middle|>", "completion": "let list = client\n            .get::<ListResponse<ModelOut>>(&format!(\"{path}?with_content=true\"), StatusCode::OK)\n            .await\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/common_calls.rs", "node_type": "let_declaration", "line_range": [213, 216]}
{"prompt": "<|fim_prefix|>event_types_ids_unrequired_nullable(\n    event_types_ids: &UnrequiredNullableField<EventTypeNameSet>,\n) -> Result<(), ValidationError> {\n    match event_types_ids {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(event_type_ids) => validate_event_types_ids(event_type_ids),\n    }\n}\n\npub fn validate_channels_endpoint(channels: &EventChannelSet) -> Result<(), ValidationError> {\n    let len = channels.0.len();\n    if !(1..=10).contains(&len) {\n        Err(validation_error(\n            Some(\"channels\"),\n            Some(\"Channels must have at least 1 and at most 10 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nfn validate_channels_endpoint_unrequired_nullable(\n    channels: &UnrequiredNullableField<EventChannelSet>,\n) -> Result<(), ValidationError> {\n    match channels {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(channels) => validate_channels_endpoint(channels),\n    }\n}\n\npub fn validate_url(url: &Url) -> Result<(), ValidationError> {\n    let scheme = url.scheme();\n    if scheme == \"https\" || scheme == \"http\" {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"url\"),\n            Some(\"Endpoint URL schemes must be http or https\"),\n        ))\n    }\n}\n\nfn validate_url_unrequired(val: &UnrequiredField<Url>) -> Result<(), ValidationError> {\n    match val {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(val) => validate_url(val),\n    }\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nfn example_endpoint_description() -> &'static str {\n    \"An example endpoint name\"\n}\n\nfn example_filter_types() -> Vec<&'static str> {\n    vec![\"user.signup\", \"user.deleted\"]\n}\n\nfn endpoint_disabled_default() -> bool {\n    false\n}\n\nfn example_endpoint_url() -> &'static str {\n    \"https://example.com/webhook/\"\n}\n\nfn example_endpoint_version() -> u16 {\n    1\n}\n\nfn default_endpoint_version() -> Option<u16> {\n    Some(1)\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointIn {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n    /// List of message channels this endpoint listens to (omit for all)\n    #[validate(custom = \"validate_channels_endpoint\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n\n    #[validate]\n    #[serde(default)]\n    #[serde(rename = \"secret\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub key: Option<EndpointSecret>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\nimpl EndpointIn {\n    p<|fim_suffix|>}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for EndpointIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    #[allow(deprecated)]\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointIn {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            key: _,\n            metadata: _,\n        } = self;\n\n        model.description = Set(description);\n        model.rate_limit = Set(rate_limit.map(|x| x.into()));\n        model.uid = Set(uid);\n        model.url = Set(url.into());\n        model.version = Set(version.unwrap_or(1).into());\n        model.disabled = Set(disabled);\n        model.event_types_ids = Set(event_types_ids);\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\nstruct EndpointUpdate {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n\n    /// List of message channels this endpoint listens to (omit for all)\n    #[validate(custom = \"validate_channels_endpoint\")]\n    #[validate]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\nimpl ModelIn for EndpointUpdate {\n    type ActiveModel = endpoint::ActiveModel;\n\n    #[allow(deprecated)]\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointUpdate {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            metadata: _,\n        } = self;\n\n        model.description = Set(description);\n        model.rate_limit = Set(rate_limit.map(|x| x.into()));\n        model.uid = Set(uid);\n        model.url = Set(url.into());\n        model.version = Set(version.unwrap_or(1).into());\n        model.disabled = Set(disabled);\n        model.event_types_ids = Set(event_types_ids);\n        model.channels = Set(channels);\n    }\n}\n\nimpl EndpointUpdate {\n    #[allow(deprecated)]\n    pub fn into_in_with_default_key(self) -> EndpointIn {\n        let EndpointUpdate {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            metadata,\n        } = self;\n\n        EndpointIn {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            metadata,\n\n            key: None,\n        }\n    }\n}\n\n#[derive(Clone, Debug, Default, Serialize, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct E<|fim_middle|>", "completion": "pub fn key_take_or_generate(\n        &mut self,\n        encryption: &Encryption,\n        sig_type: &DefaultSignatureType,\n    ) -> error::Result<EndpointSecretInternal> {\n        if let Some(key) = self.key.take() {\n            EndpointSecretInternal::from_endpoint_secret(key, encryption)\n        } else {\n            generate_secret(encryption, sig_type)\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "function_item", "line_range": [197, 207]}
{"prompt": "<|fim_prefix|>tance_type = Some(SingleOrVec::Single(Box::new(InstanceType::Object)));\n                }\n\n                obj.object.as_mut().map(visit_object_validation);\n            }\n        }\n    }\n\n    fn visit_object_validation(obj: &mut Box<ObjectValidation>) {\n        if let Some(additional_props) = &mut obj.additional_properties {\n            visit_schema(additional_props)\n        }\n        for (_, schema) in &mut obj.properties {\n            visit_schema(schema)\n        }\n    }\n\n    if let Some(components) = &mut openapi.components {\n        for (_, schema) in &mut components.schemas {\n            visit_schema(&mut schema.json_schema)\n        }\n    }\n}\n\n/// Adds the `Idempotency-Key` header parameter to all `POST` operations in the schema.\nfn add_idempotency_to_post(openapi: &mut OpenApi) {\n    // The header's value can be any valid string\n    let string_schema = aide::gen::in_context(|ctx| String::json_schema(&mut ctx.schema));\n\n    let s = openapi::SchemaObject {\n        json_schema: string_schema,\n        external_docs: None,\n        example: None,\n    };\n\n    let idempotency_key_data = openapi::ParameterData {\n        name: \"idempotency-key\".to_string(),\n        description: Some(\"The request's idempotency key\".to_string()),\n        required: false,\n        deprecated: None,\n        format: openapi::ParameterSchemaOrContent::Schema(s),\n        example: None,\n        examples: indexmap::indexmap! {},\n        explode: None,\n        extensions: indexmap::indexmap! {},\n    };\n\n    if let Some(paths) = &mut openapi.paths {\n        for (_, op) in &mut paths.paths {\n            match op {\n                openapi::ReferenceOr::Reference { reference, .. } => {\n                    // References to operations should never appear in our\n                    // schema since all our operations are unique, and we\n                    // don't reference any 3rd party/external operations.\n                    tracing::warn!(\n                        \"Unexpected operation reference encountered in OpenAPI schema: {reference}\"\n                    );\n                }\n                openapi::ReferenceOr::Item(op) => {\n                    if let Some(post) = &mut op.post {\n                        post.parameters.push(ReferenceOr::Item(Parameter::Header {\n                            parameter_data: idempotency_key_data.clone(),\n                            style: openapi::HeaderStyle::Simple,\n                        }));\n                    }\n                }\n            }\n        }\n    }\n}\n\n/// Remove schemas from `components.schemas` of the spec which are under normal\n/// circumstances not referenced. At the moment these are struct schemas used\n/// by query parameters and path placeholders.\nfn remove_unneeded_schemas(openapi: &mut OpenApi) {\n    if let Some(components) = &mut openapi.components {\n        components.schemas.retain(|name, _| {\n            !(name.ends_with(\"Path\")\n                || name.ends_with(\"QueryParams\")\n                || name.starts_with(\"Pagination\"))\n        });\n    }\n}\n\n/// Replaces the `examples` property of a schema with a singular `example`\n/// property.\n/// OpenAPI <=3.0 used `example` as an extension, >=3.1 standardized `examples`.\nfn replace_multiple_examples(openapi: &mut OpenApi) {\n    let mut visitor = schemars::visit::SetSingleExample {\n        retain_examples: false,\n    };\n\n    if let Some(components) = &mut openapi.components {\n        for (_, schema_object) in &mut components.schemas {\n            visitor.visit_schema(&mut schema_object.json_schema);\n        }\n    }\n}\n\npub fn add_security_scheme(\n    api: aide::transform::TransformOpenApi<'_>,\n) -> aide::transform::TransformOpenApi<'_> {\n    api.security_scheme(\n        \"HTTPBearer\",\n        aide::openapi::SecurityScheme::Http {\n            scheme: \"bearer\".to_string(),\n            bearer_format: None,\n            description: Some(\"HTTP Bearer token passed in the `Authorization` header\".into()),\n            extensions: Default::default(),\n        },\n    )\n}\n\n/// Applies a list of hacks to the finished OpenAPI spec to make it usable with\n/// our tooling.\n<|fim_suffix|>\n\n/// This module documents operational webhooks. To document one define a data\n/// struct first, use the [`webhook_event`] macro to generate a wrapping struct\n/// for it, then include it in the [`webhooks`] function call.\nmod webhooks {\n    use std::collections::HashMap;\n\n    use aide::openapi;\n    use schemars::JsonSchema;\n\n    use crate::core::operational_webhooks::{\n        EndpointDisabledEventData, EndpointEvent, MessageAttemptEvent,\n    };\n\n    /// Documents the webhook specified by the type `T`.\n    fn document_webhook<T: Webhook>() -> (String, openapi::PathItem) {\n        let type_name = std::any::type_name::<T>()\n            .split(\"::\")\n            .last()\n            .expect(\"Last element of split can't be empty\")\n            .to_string();\n\n        let body_schema =\n            aide::gen::in_context(|ctx| ctx.schema.subschema_for::<T>().into_object());\n\n        let body_media = openapi::MediaType {\n            schema: Some(openapi::SchemaObject {\n                json_schema: body_schema.into(),\n                external_docs: None,\n                example: None,\n            }),\n            ..Default::default()\n        };\n\n        let body = openapi::RequestBody {\n            content: indexmap::indexmap! {\n                \"application/json\".to_string() => body_media,\n            },\n            ..Default::default()\n        };\n\n        let success_response = openapi::Response {\n            description:\n                \"Return any 2XX status to indicate that the data was received successfully\"\n                    .to_string(),\n            ..Default::default()\n        };\n\n        (\n            type_name.clone(),\n            openapi::PathItem {\n                post: Some(openapi::Operation {\n                    description: Some(T::description().to_string()),\n                    operation_id: Some(type_name.clone()),\n                    request_body: Some(openapi::ReferenceOr::Item(body)),\n                    responses: Some(openapi::Responses {\n                        responses: indexmap::indexmap! {\n                            openapi::StatusCode::Range(2) => openapi::ReferenceOr::Item(success_response),\n                        },\n                        ..Default::default()\n                    }),\n                    summary: Some(type_name),\n                    tags: vec![\"Webhooks\".to_string()],\n                    ..Default::default()\n                }),\n                ..Default::default()\n            },\n        )\n    }\n\n    trait Webhook: JsonSchema {\n        fn description() -> &'static str;\n    }\n\n    /// Generates a struct that wraps the webhook payload, adding the discriminant.\n    macro_rules! webhook_event {\n        ($name:ident, $payload_name:ident, $discriminant:expr, $description:literal) => {\n            #[allow(unused)]\n            struct $name {\n                data: $payload_name,\n                type_: String,\n            }\n\n            impl JsonSchema for $name {\n                fn schema_name() -> String {\n                    stringify!($name).to_string()\n                }\n\n                fn json_schema(\n                    gen: &mut schemars::gen::SchemaGenerator,\n                ) -> schemars::schema::Schema {\n                    #[derive(::schemars::JsonSchema)]\n                    #[allow(unused)]\n                    struct $name {\n                        data: $payload_name,\n                        #[serde(rename = \"type\")]\n                        type_: String,\n                    }\n\n                    let mut schema = $name::json_schema(gen).into_object();\n\n                    schema\n                        .object()\n                        .properties\n                        .get_mut(\"type\")\n                        .map(|type_property| {\n                            if let ::schemars::schema::Schema::Object(obj) = type_property {\n                                let discriminant_value =\n                                    serde_json::Value::String($discriminant.to_string());\n                                obj.enum_values = Some(vec![discriminant_value.clone()]);\n                     <|fim_middle|>", "completion": "pub fn postprocess_spec(openapi: &mut OpenApi) {\n    let hacks = [\n        add_idempotency_to_post,\n        remove_unneeded_schemas,\n        replace_true_schemas,\n        replace_multiple_examples,\n    ];\n\n    for hack in hacks {\n        hack(openapi);\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/openapi.rs", "node_type": "function_item", "line_range": [248, 259]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse chrono::Utc;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set, Order, QueryOrder, QuerySelect};\n\nuse crate::core::types::{\n    BaseId, EndpointId, MessageAttemptId, MessageAttemptTriggerType, MessageEndpointId, MessageId,\n    MessageStatus,\n};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"messageattempt\")]\npub struct Model {\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: MessageAttemptId,\n    pub created_at: DateTimeWithTimeZone,\n    pub msg_id: MessageId,\n    pub msg_dest_id: Option<MessageEndpointId>,\n    pub endp_id: EndpointId,\n    pub url: String,\n    pub status: MessageStatus,\n    pub response_status_code: i16,\n    #[sea_orm(column_type = \"Text\")]\n    pub response: String,\n    pub ended_at: Option<DateTimeWithTimeZone>,\n    pub trigger_type: MessageAttemptTriggerType,\n    /// Response duration in milliseconds\n    pub response_duration_ms: i64,\n    pub next_attempt: Option<DateTimeWithTimeZone>,\n    pub attempt_number: i16,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]\npub enum Relation {\n    #[sea_orm(\n        belongs_to = \"super::message::Entity\",\n        from = \"Column::MsgId\",\n        to = \"super::message::Column::Id\",\n        on_update = \"NoAction\",\n        on_delete = \"Restrict\"\n    )]\n    Message,\n}\n\nimpl Related<super::message::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Message.def()\n    }\n}\n\nimpl ActiveModelBehavior for ActiveModel {\n    fn new() -> Self {\n        let timestamp = Utc::now();\n        Self {\n            id: Set(MessageAttemptId::new(timestamp.into(), None)),\n            created_at: Set(timestamp.into()),\n            ..ActiveModelTrait::default()\n        }\n    }\n}\n\nimpl Entity {\n    pub fn secure_find_by_msg(msg_id: MessageId) -> Select<Entity> {\n        Self::find().filter(Column::MsgId.eq(msg_id))\n    }\n\n    pub fn secure_find_by_endpoint(endp_id: EndpointId) -> Select<Entity> {\n        Self::find().filter(Column::EndpId.eq(endp_id))\n    }\n}\n\npub trait Query: QuerySelect + QueryFilter + QueryOrder + Sized {\n    fn after_id(self, id: MessageAttemptId) -> Self {\n        self.filter(Column::Id.gte(id))\n    }\n\n    fn after_id_exclusive(self, id: MessageAttemptId) -> Self {\n        self.filter(Column::Id.gt(id))\n    }\n\n    f<|fim_suffix|>\n    fn after(self, t: DateTimeUtc) -> Self {\n        self.after_id(MessageAttemptId::start_id(t))\n    }\n\n    fn before(self, t: DateTimeUtc) -> Self {\n        self.before_id(MessageAttemptId::start_id(t))\n    }\n\n    fn with_status(self, status: MessageStatus) -> Self {\n        self.filter(Column::Status.eq(status))\n    }\n\n    fn oldest_first(self) -> Self {\n        self.order_by(Column::Id, Order::Asc)\n    }\n\n    /// Only return the last attempt per message (every attempt will be part of a separate message)\n    fn latest_per_msg(self) -> Self {\n        self.distinct_on([Column::MsgId])\n            .order_by_desc(Column::MsgId)\n            .order_by_desc(Column::Id)\n    }\n}\n\nimpl Query for Select<Entity> {}\n<|fim_middle|>", "completion": "fn before_id(self, id: MessageAttemptId) -> Self {\n        self.filter(Column::Id.lte(id))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/messageattempt.rs", "node_type": "function_item", "line_range": [82, 84]}
{"prompt": "<|fim_prefix|>use std::{collections::HashSet, time::Duration};\n\nuse chrono::{DateTime, FixedOffset, Utc};\nuse sea_orm::{DatabaseConnection, DatabaseTransaction, TransactionTrait};\nuse serde::{Deserialize, Serialize};\n\n<|fim_suffix|>\nuse crate::{\n    core::{\n        cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue},\n        types::{\n            ApplicationId, ApplicationUid, EndpointHeaders, EndpointId, EndpointSecretInternal,\n            EventChannelSet, EventTypeNameSet, ExpiringSigningKeys, MessageAttemptTriggerType,\n            OrganizationId,\n        },\n    },\n    db::models::{application, endpoint},\n    error::{Error, Result},\n};\n\n/// The information cached during the creation of a message. Includes a [`Vec`] of all endpoints\n/// associated with the given application and organization ID.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageApp {\n    pub id: ApplicationId,\n    pub uid: Option<ApplicationUid>,\n    pub org_id: OrganizationId,\n    pub rate_limit: Option<u16>,\n    endpoints: Vec<CreateMessageEndpoint>,\n    deleted: bool,\n}\n\nimpl CreateMessageApp {\n    /// Fetch all requisite information for creating a [`CreateMessageApp`] from the PostgreSQL\n    /// database\n    async fn fetch_from_pg_by_model(\n        db: &DatabaseTransaction,\n        app: application::Model,\n    ) -> Result<CreateMessageApp> {\n        let endpoints = endpoint::Entity::secure_find(app.id.clone())\n            .all(db)\n            .await?\n            .into_iter()\n            .map(TryInto::try_into)\n            .collect::<Result<Vec<_>>>()?;\n\n        Ok(CreateMessageApp {\n            id: app.id,\n            uid: app.uid,\n            org_id: app.org_id,\n            rate_limit: app\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Application rate limit out of bounds\"))?,\n            endpoints,\n            deleted: app.deleted,\n        })\n    }\n\n    /// Fetches all information for creating a [`CreateMessageApp`] from the Redis cache if it\n    /// exists or from PostgreSQL otherwise. If the RedisCache is Some, but does not contain the\n    /// requisite information, fetch it from PostgreSQL and insert the data into the cache.\n    pub async fn layered_fetch(\n        cache: &Cache,\n        pg: &DatabaseConnection,\n        app: Option<application::Model>,\n        org_id: OrganizationId,\n        app_id: ApplicationId,\n        ttl: Duration,\n    ) -> Result<Option<CreateMessageApp>> {\n        let cache_key = AppEndpointKey::new(&org_id, &app_id);\n\n        // First check Redis\n        if let Ok(Some(cma)) = cache.get::<CreateMessageApp>(&cache_key).await {\n            if cma.deleted {\n                return Ok(None);\n            } else {\n                return Ok(Some(cma));\n            }\n        }\n\n        // Then check PostgreSQL\n        let db = pg.begin().await?;\n        // Fetch the [`application::Model`] either given or from the ID\n        let app = if let Some(app) = app {\n            app\n        } else if let Some(app) = application::Entity::secure_find_by_id(org_id, app_id)\n            .one(&db)\n            .await?\n        {\n            app\n        } else {\n            return Ok(None);\n        };\n\n        // Fetch the actual [`CreateMessageApp`]\n        let out = Self::fetch_from_pg_by_model(&db, app).await?;\n\n        // Insert it into Redis\n        let _ = cache.set(&cache_key, &out, ttl).await;\n\n        if out.deleted {\n            return Ok(None);\n        }\n\n        Ok(Some(out))\n    }\n\n    pub fn filtered_endpoints(\n        &self,\n        trigger_type: MessageAttemptTriggerType,\n        event_type: &EventTypeName,\n        channels: Option<&EventChannelSet>,\n    ) -> Vec<CreateMessageEndpoint> {\n        self.endpoints\n            .iter()\n            .filter(|endpoint| {\n                // No disabled or deleted endpoints ever\n                !endpoint.disabled && !endpoint.deleted\n                    // Manual attempt types go through regardless\n                    && (trigger_type == MessageAttemptTriggerType::Manual\n                        || (\n                            // If an endpoint has event types and it matches ours, or has no event types\n                            endpoint\n                                .event_types_ids\n                                .as_ref()\n                                .map(|x| x.0.contains(event_type))\n                                .unwrap_or(true)\n                            // If an endpoint has no channels accept all messages, otherwise only if their channels overlap.\n                            // A message with no channels doesn't match an endpoint with channels.\n                            && endpoint\n                                .channels\n                                .as_ref()\n                                .map(|x| {\n                                    !x.0.is_disjoint(\n                                        channels.map(|x| &x.0).unwrap_or(&HashSet::new()),\n                                    )\n                                })\n                                .unwrap_or(true)\n                        ))\n            })\n            .cloned()\n            .collect()\n    }\n}\n\n/// The information for each individual endpoint cached with the creation of a message.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageEndpoint {\n    pub id: EndpointId,\n    pub url: String,\n    pub key: EndpointSecretInternal,\n    pub event_types_ids: Option<EventTypeNameSet>,\n    pub channels: Option<EventChannelSet>,\n    pub rate_limit: Option<u16>,\n    // Same type as the `DateTimeWithTimeZone from SeaORM used in the endpoint model\n    pub first_failure_at: Option<DateTime<FixedOffset>>,\n    pub headers: Option<EndpointHeaders>,\n    pub disabled: bool,\n    pub deleted: bool,\n    // outside of this module, valid_signing_keys should be used instead\n    old_signing_keys: Option<ExpiringSigningKeys>,\n}\n\nimpl CreateMessageEndpoint {\n    pub fn valid_signing_keys(&self) -> Vec<&EndpointSecretInternal> {\n        match self.old_signing_keys {\n            Some(ref old_keys) => std::iter::once(&self.key)\n                .chain(\n                    old_keys\n                        .0\n                        .iter()\n                        .filter(|x| x.expiration > Utc::now())\n                        .map(|x| &x.key),\n                )\n                .collect(),\n            None => vec![&self.key],\n        }\n    }\n}\n\nimpl TryFrom<endpoint::Model> for CreateMessageEndpoint {\n    type Error = Error;\n\n    fn try_from(m: endpoint::Model) -> Result<CreateMessageEndpoint> {\n        Ok(CreateMessageEndpoint {\n            id: m.id,\n            url: m.url,\n            key: m.key,\n            old_signing_keys: m.old_keys,\n            event_types_ids: m.event_types_ids,\n            channels: m.channels,\n            rate_limit: m\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Endpoint rate limit out of bounds\"))?,\n            first_failure_at: m.first_failure_at,\n            headers: m.headers,\n            disabled: m.disabled,\n            deleted: m.deleted,\n        })\n    }\n}\n\nkv_def!(AppEndpointKey, CreateMessageApp);\nimpl AppEndpointKey {\n    // FIXME: Rewrite doc comment when AppEndpointValue members are known\n    /// Returns a key for fetching all cached endpoints for a given organization and application.\n    pub fn new(org: &OrganizationId, app: &ApplicationId) -> AppEndpointKey {\n        AppEndpointKey(format!(\"SVIX_CACHE_APP_v3_{org}_{app}\"))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use chrono::Utc;\n\n    use super::CreateMessageEndpoint;\n    use crate::core::{\n        cryptography::Encryption,\n        types::{\n            EndpointId, EndpointSecret, EndpointSecretInternal, ExpiringSigningKey,\n            ExpiringSigningKeys,\n        },\n    };\n\n    #[test]\n    fn test_valid_signing_keys() {\n        let key = EndpointSecretInternal::from_endpoint_secret(\n            EndpointSecret::Symmetric(STANDARD.decode(\"MfKQ9r8GKYqrTwjUPD8ILPZIo2LaLaSw\").unwrap()),\n            &Encr<|fim_middle|>", "completion": "use super::types::EventTypeName;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/message_app.rs", "node_type": "use_declaration", "line_range": [7, 7]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\nuse reqwest::StatusCode;\nuse serde::de::IgnoredAny;\nuse svix_server::{\n    cfg::CacheType,\n    core::{\n        security::generate_org_token,\n        types::{ApplicationUid, BaseId, OrganizationId},\n    },\n    v1::endpoints::application::{ApplicationIn, ApplicationOut},\n};\n\nuse crate::utils::{\n    common_calls::{application_in, common_test_list, metadata},\n    get_default_test_config, start_svix_server,\n};\n\n// NOTE: PATCHing must be tested exhaustively as if any of the boilerplate is missed then the\n// operation could fail. This should probably be made into a macro if at all possible.\n#[tokio::test]\nasync fn test_patch() {\n    let (client, _jh) = start_svix_server().await;\n\n    l<|fim_suffix|>\n    // Test that PUT with an invalid ID creates an application\n    let _: ApplicationOut = client\n        .put(\n            \"api/v1/app/fake-id/\",\n            application_in(\"first_name\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that name may be set while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json! ({\n                \"name\": \"second_name\"\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert change was made when later fetched\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.name, \"second_name\".to_owned());\n    // Assert that no other field was changed\n    assert_eq!(out.rate_limit, None);\n    assert_eq!(out.uid, None);\n    assert_eq!(out.metadata, metadata(\"{}\"));\n\n    // Test that rate_limit may be set while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json! ({\n                \"rateLimit\": 1,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.rate_limit, Some(1));\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.uid, None);\n\n    // Test that rate_limit may be unset while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({ \"rateLimit\": null }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.rate_limit, None);\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.uid, None);\n\n    // Test that uid may be set while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json! ({\n                \"uid\": \"test_uid\"\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.uid, Some(ApplicationUid(\"test_uid\".to_owned())));\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.rate_limit, None);\n\n    // Test that uid may be unset while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({ \"uid\": null }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.uid, None);\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.rate_limit, None);\n\n    // Test that metadata may be changed while the rest are omitted\n    let _: ApplicationOut = client\n        .patch(\n            &format!(\"api/v1/app/{}/\", app.id),\n            serde_json::json!({\n                \"metadata\": {\n                    \"foo\": \"bar\",\n                    \"bizz\": \"baz\",\n                },\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app.id), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#), out.metadata);\n    // Assert that no other field was changed\n    assert_eq!(out.name, \"second_name\".to_owned());\n    assert_eq!(out.rate_limit, None);\n}\n\n#[tokio::test]\nasync fn test_crud() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1_1: &str = \"v1ApplicationCrudTest11\";\n    const APP_NAME_1_2: &str = \"v1ApplicationCrudTest12\";\n    const APP_NAME_2_1: &str = \"v1ApplicationCrudTest21\";\n    const APP_NAME_2_2: &str = \"v1ApplicationCrudTest22\";\n\n    // CREATE\n    let app_1: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(APP_NAME_1_1),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_1.name, APP_NAME_1_1);\n\n    let app_2: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(APP_NAME_2_1),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_2.name, APP_NAME_2_1);\n\n    // READ\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::OK)\n            .await\n            .unwrap(),\n        app_1\n    );\n\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_2\n    );\n\n    // UPDATE\n    let app_1_id = app_1.id;\n    let app_1: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{app_1_id}/\"),\n            application_in(APP_NAME_1_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    let app_2_id = app_2.id;\n    let app_2: ApplicationOut = client\n        .put(\n            &format!(\"api/v1/app/{app_2_id}/\"),\n            application_in(APP_NAME_2_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // CONFIRM UPDATE\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{app_1_id}/\"), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_1\n    );\n\n    assert_eq!(\n        client\n            .get::<ApplicationOut>(&format!(\"api/v1/app/{app_2_id}/\"), StatusCode::OK,)\n            .await\n            .unwrap(),\n        app_2\n    );\n\n    // DELETE\n    client\n        .delete(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n    client\n        .delete(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n\n    // CONFIRM DELETION\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{}/\", app_1.id), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{}/\", app_2.id), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n\n    let app: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            serde_json::json!({\n                \"name\": \"Apps all around\",\n            }),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(app.metadata, metadata(r#\"{}\"#));\n\n    let updated: ApplicationOut = client\n        .patch(\n            &format!<|fim_middle|>", "completion": "let app: ApplicationOut = client\n        .post(\n            \"api/v1/app/\",\n            application_in(\"first_name\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_application.rs", "node_type": "let_declaration", "line_range": [25, 32]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct OrumIoConfigOut {\n    #[serde(rename = \"publicKey\")]\n    pub public_key: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl OrumIoConfigOut {\n    pub fn new(public_key: String) -> Self {\n        Self { public_key }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/orum_io_config_out.rs", "node_type": "impl_item", "line_range": [10, 14]}
{"prompt": "<|fim_prefix|>thod::GET,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Update an endpoint.\n    pub async fn update(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_update: EndpointUpdate,\n    ) -> Result<EndpointOut> {\n        crate::request::Request::new(\n            http1::Method::PUT,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_update)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Delete an endpoint.\n    pub async fn delete(&self, app_id: String, endpoint_id: String) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Partially update an endpoint.\n    pub async fn patch(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_patch: EndpointPatch,\n    ) -> Result<EndpointOut> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_patch)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get the additional headers to be sent with the webhook.\n    pub async fn get_headers(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n    ) -> Result<EndpointHeadersOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/headers\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Set the additional headers to be sent with the webhook.\n    pub async fn update_headers(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_headers_in: EndpointHeadersIn,\n    ) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::PUT,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/headers\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_headers_in)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Partially set the additional headers to be sent with the webhook.\n    pub async fn patch_headers(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_headers_patch_in: EndpointHeadersPatchIn,\n    ) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/headers\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_body_param(endpoint_headers_patch_in)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Resend all failed messages since a given time.\n    ///\n    /// Messages that were sent successfully, even if failed initially, are not\n    /// resent.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"endpoint.recover\",\n    ///   \"data\": {\n    ///     \"messagesSent\": 2\n    ///   }\n    /// }\n    /// ```\n    pub async fn recover(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        recover_in: RecoverIn,\n        options: Option<EndpointRecoverOptions>,\n    ) -> Result<RecoverOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/recover\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .with_body_param(recover_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Replays messages to the endpoint.\n    ///\n    /// Only messages that were created after `since` will be sent.\n    /// Messages that were previously sent to the endpoint are not resent.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"endpoint.replay\",\n    ///   \"data\": {\n    ///     \"messagesSent\": 2\n    ///   }\n    /// }\n    /// ```\n    pub async fn replay_missing(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        replay_in: ReplayIn,\n        options: Option<EndpointReplayMissingOptions>,\n    ) -> Result<ReplayOut> {\n        let EndpointReplayMissingOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/replay-missing\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .with_body_param(replay_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get the endpoint's signing secret.\n    ///\n    /// This is used to verify the authenticity of the webhook.\n    /// For more information please refer to [the consuming webhooks docs](https://docs.svix.com/consuming-webhooks/).\n    pub async fn get_secret(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n    ) -> Result<EndpointSecretOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/secret\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Rotates the endpoint's signing secret.\n    ///\n    /// The previous secret will remain valid for the next 24 hours.\n    pub async fn rotate_secret(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        endpoint_secret_rotate_in: EndpointSecretRotateIn,\n        options: Option<EndpointRotateSecretOptions>,\n    ) -> Result<()> {\n        let EndpointRotateSecretOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/secret/rotate\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .with_body_param(endpoint_secret_rotate_in)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Send an example message for an event.\n    pub async fn send_example(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        event_example_in: EventExampleIn,\n        options: Option<EndpointSendExampleOptions>,\n    ) -> Result<MessageOut> {\n        let EndpointSendExampleOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/send-example\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .with_body_param(event_example_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get basic statistics for the endpoint.\n    pub async fn get_stats(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n<|fim_middle|>", "completion": "let EndpointRecoverOptions { idempotency_key } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/endpoint.rs", "node_type": "let_declaration", "line_range": [242, 242]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Tests related to the [`CreateMessageApp`] and the [`CreateMessageEndpoint`]s structs.\nuse std::time::Duration;\n\nuse http::StatusCode;\nuse serde::de::IgnoredAny;\nuse svix_server::{\n    cfg::CacheBackend,\n    core::{\n        cache::{self, CacheBehavior},\n        message_app::AppEndpointKey,\n        types::{BaseId, OrganizationId},\n    },\n    redis::RedisManager,\n};\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, create_test_message, message_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id, TestReceiver,\n};\n\n/// Ensures that a deleted application returns `None` when using [`layered_fetch`]\n#[tokio::test]\nasync fn test_app_deletion() {\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading Configuration\");\n    let org_id = OrganizationId::new(None, None);\n    let (client, _jh) =\n        start_svix_server_with_cfg_and_org_id(&get_default_test_config(), org_id.clone()).await;\n\n    // Cannot run test using an in-memory cache as we can't invalidate a key from within the test.\n    // Ie. the Redis backends all share the same memory when a cache is created in this test. The\n    // same is not true for an in-memory map.\n    if matches!(cfg.cache_backend(), CacheBackend::Memory) {\n        return;\n    }\n\n    let mut test_receiver = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let app_id = create_test_app(&client, \"TestAppDeletion\")\n        .await\n        .unwrap()\n        .id;\n    let _ = create_test_endpoint(&client, &app_id, &test_receiver.endpoint)\n        .await\n        .unwrap();\n\n    let payload = serde_json::json!({\"test\": \"value\"});\n\n    create_test_message(&client, &app_id, payload.clone())\n        .await\n        .unwrap();\n\n    assert_eq!(\n        tokio::time::timeout(Duration::from_millis(250), test_receiver.data_recv.recv()).await,\n        Ok(Some(payload.clone()))\n    );\n\n    client\n        .delete(&format!(\"api/v1/app/{app_id}/\"), StatusCode::NO_CONTENT)\n        .await\n        .unwrap();\n\n    // Delete the cached [`CreateMessageApp`] here instead of waiting 30s for it to expire\n    let cache = match cfg.cache_backend() {\n        CacheBackend::None => cache::none::new(),\n        CacheBackend::Redis(_)\n        | CacheBackend::RedisCluster(_)\n        | CacheBackend::RedisSentinel(_, _) => {\n            let mgr = RedisManager::from_cache_backend(&cfg.cache_backend()).await;\n            cache::redis::new(mgr)\n        }\n\n        // Cannot use memory cache for this test. See the above check.\n        CacheBackend::Memory => unreachable!(),\n    };\n\n    cache\n        .delete(&AppEndpointKey::new(&org_id, &app_id))\n        .await\n        .unwrap();\n\n    // Assert message creation return a 404 with a deleted application\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(\"test.event\", payload).unwrap(),\n            StatusCode::NOT_FOUND,\n        )\n        .await\n        .unwrap();\n\n    // And assert that no message is sent even if it 404s\n    assert!(\n        tokio::time::timeout(Duration::from_millis(250), test_receiver.data_recv.recv())\n            .await\n            .is_err()\n    );\n}\n\n#[tokio::test]\nasync fn test_endp_deletion() {\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading Configuration\");\n    let org_id = OrganizationId::new(None, None);\n    let (client, _jh) =\n        start_svix_server_with_cfg_and_org_id(&get_default_test_config(), org_id.clone()).await;\n\n    // Cannot run test using an in-memory cache as we can't invalidate a key from within the test.\n    // Ie. the Redis backends all share the same memory when a cache is created in this test. The\n    // same is not true for an in-memory map.\n    if matches!(cfg.cache_backend(), CacheBackend::Memory) {\n        return;\n    }\n\n    let mut test_receiver = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let app_id = create_test_app(&client, \"TestAppDeletion\")\n        .await\n        .unwrap()\n        .id;\n    l<|fim_suffix|>\n    let payload = serde_json::json!({\"test\": \"value\"});\n\n    create_test_message(&client, &app_id, payload.clone())\n        .await\n        .unwrap();\n\n    assert_eq!(\n        tokio::time::timeout(Duration::from_millis(250), test_receiver.data_recv.recv()).await,\n        Ok(Some(payload.clone()))\n    );\n\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Delete the cached [`CreateMessageApp`] here instead of waiting 30s for it to expire\n    let cache = match cfg.cache_backend() {\n        CacheBackend::None => cache::none::new(),\n        CacheBackend::Redis(_)\n        | CacheBackend::RedisCluster(_)\n        | CacheBackend::RedisSentinel(_, _) => {\n            let mgr = RedisManager::from_cache_backend(&cfg.cache_backend()).await;\n            cache::redis::new(mgr)\n        }\n\n        // Cannot use memory cache for this test. See the above check.\n        CacheBackend::Memory => unreachable!(),\n    };\n\n    cache\n        .delete(&AppEndpointKey::new(&org_id, &app_id))\n        .await\n        .unwrap();\n\n    // Assert message creation return a 202 with a deleted endpoint\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(\"test.event\", payload).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    // But assert that no message is sent as the endpoint no longer exists\n    assert!(\n        tokio::time::timeout(Duration::from_millis(250), test_receiver.data_recv.recv())\n            .await\n            .is_err()\n    );\n}\n<|fim_middle|>", "completion": "let endp_id = create_test_endpoint(&client, &app_id, &test_receiver.endpoint)\n        .await\n        .unwrap()\n        .id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/message_app.rs", "node_type": "let_declaration", "line_range": [124, 127]}
{"prompt": "<|fim_prefix|>\n    desc: T,\n) -> impl Fn(TransformOperation<'_>) -> TransformOperation<'_> {\n    move |op| op.description(desc.as_ref())\n}\n\npub fn get_unix_timestamp() -> u64 {\n    SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs()\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationPath {\n    pub app_id: ApplicationIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationEndpointPath {\n    pub app_id: ApplicationIdOrUid,\n    pub endpoint_id: EndpointIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgEndpointPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n    pub endpoint_id: EndpointIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgAttemptPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n    pub attempt_id: MessageAttemptId,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct EventTypeNamePath {\n    pub event_type_name: EventTypeName,\n}\n\n/// JsonStatus is a wrapper over `axum::extract::Json` as a handler output.\n///\n/// Setting the `STATUS` const parameter automatically sets the response\n/// status code, as well as inserting it into the aide documentation.\npub struct JsonStatus<const STATUS: u16, T: JsonSchema + Serialize>(pub T);\n\nimpl<const STATUS: u16, T: JsonSchema + Serialize> IntoResponse for JsonStatus<STATUS, T> {\n    fn into_response(self) -> axum::response::Response {\n        (\n            StatusCode::from_u16(STATUS).unwrap(),\n            axum::extract::Json(self.0),\n        )\n            .into_response()\n    }\n}\n\nimpl<const STATUS: u16, T: JsonSchema + Serialize> OperationOutput for JsonStatus<STATUS, T> {\n    type Inner = T;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        axum::extract::Json::<T>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(resp) = Self::operation_response(ctx, operation) {\n            vec![(Some(STATUS), resp)]\n        } else {\n            vec![]\n        }\n    }\n}\n\n/// JsonStatusUpsert is a wrapper over `axum::extract::Json` as a handler\n/// output.\n///\n/// It is a special casing of `JsonStatus` for situations where a resource is\n/// either being updated or created within the same operation. In case of\n/// `Updated` HTTP 200 OK is returned, in case of `Created` HTTP 201 CREATED\n/// is returned.\npub enum JsonStatusUpsert<T: JsonSchema + Serialize> {\n    Updated(T),\n    Created(T),\n}\n\nimpl<T: JsonSchema + Serialize> IntoResponse for JsonStatusUpsert<T> {\n    fn into_response(self) -> axum::response::Response {\n        let (status, body) = match self {\n            JsonStatusUpsert::Updated(v) => (StatusCode::OK, v),\n            JsonStatusUpsert::Created(v) => (StatusCode::CREATED, v),\n        };\n        (status, axum::extract::Json(body)).into_response()\n    }\n}\n\nimpl<T: JsonSchema + Serialize> OperationOutput for JsonStatusUpsert<T> {\n    type Inner = T;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        axum::extract::Json::<T>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(resp) = Self::operation_response(ctx, operation) {\n            vec![\n                (Some(StatusCode::OK.into()), resp.clone()),\n                (Some(StatusCode::CREATED.into()), resp),\n            ]\n        } else {\n            vec![]\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    u<|fim_suffix|>    use crate::{core::types::ApplicationUid, error::ValidationErrorItem};\n\n    #[derive(Debug, Validate)]\n    struct ValidationErrorTestStruct {\n        #[validate(range(min = 10, message = \"Below 10\"))]\n        a: u32,\n\n        #[validate]\n        b: ValidationErrorTestStructInner,\n\n        #[validate]\n        c: Vec<ValidationErrorTestStructInner>,\n    }\n\n    #[derive(Debug, Validate)]\n    struct ValidationErrorTestStructInner {\n        #[validate(range(max = 10, message = \"Above 10\"))]\n        inner: u8,\n    }\n\n    #[test]\n    fn test_validation_errors_fn() {\n        let valid = ValidationErrorTestStruct {\n            a: 11,\n            b: ValidationErrorTestStructInner { inner: 1 },\n            c: vec![\n                ValidationErrorTestStructInner { inner: 2 },\n                ValidationErrorTestStructInner { inner: 3 },\n            ],\n        };\n        let invalid = ValidationErrorTestStruct {\n            a: 9,\n            b: ValidationErrorTestStructInner { inner: 11 },\n            c: vec![\n                ValidationErrorTestStructInner { inner: 12 },\n                ValidationErrorTestStructInner { inner: 13 },\n            ],\n        };\n\n        assert_eq!(valid.validate(), Ok(()));\n\n        let errs = invalid.validate().unwrap_err();\n        let errs = validation_errors(vec![], errs);\n\n        assert_eq!(errs.len(), 4);\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"a\".to_owned()],\n            msg: \"Below 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"b\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"c\".to_owned(), \"[0]\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"c\".to_owned(), \"[1]\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n    }\n\n    #[test]\n    fn test_pagination_defaults() {\n        let p: Pagination<ApplicationUid> = serde_json::from_value(json!({})).unwrap();\n        assert_eq!(p.limit.0, default_limit().0);\n    }\n\n    #[test]\n    fn test_pagination_validation() {\n        let p: Pagination<ApplicationUid> =\n            serde_json::from_value(json!({\"iterator\": \"$$invalid-appuid\"})).unwrap();\n        assert!(p.validate().is_err());\n\n        let p: Pagination<ApplicationUid> =\n            serde_json::from_value(json!({ \"iterator\": \"valid-appuid\"})).unwrap();\n        p.validate().unwrap();\n    }\n\n    #[derive(Debug, serde::Deserialize, PartialEq)]\n    struct TestPaginationDeserializationStruct {\n        iterator: super::ReversibleIterator<crate::core::types::MessageId>,\n    }\n\n    #[test]\n    fn test_pagination_deserialization() {\n        let a = serde_json::json!({\"iterator\": \"msg_274DTsX0wVTSLvo91QopQgZrjDV\"});\n        let b = serde_json::json!({\"iterator\": \"-msg_274DTsX0wVTSLvo91QopQgZrjDV\"});\n\n        assert_eq!(\n            serde_json::from_value::<TestPaginationDeserializationStruct>(a).unwrap(),\n            TestPaginationDeserializationStruct {\n                iterator: super::ReversibleIterator::Normal(crate::core::types::MessageId(\n                    \"msg_274DTsX0wVTSLvo91QopQgZrjDV\".to_owned()\n                ))\n            }\n        );\n        assert_eq!(\n            serde_json::from_value::<TestPaginationDeserializationStruct>(b).unwrap(),\n            TestPaginationDeserializationStruct {\n                iterator: super::ReversibleIterator::Prev(crate::core::types::MessageId(\n                    \"msg_274DTsX0wVTSLvo91QopQgZrjDV\".to_owned()\n                ))\n            }\n        );\n    }\n\n    #[test]\n    fn test_validate_no_control_characters() {\n        let a = \"A good string\";\n        let b = \"A\\u{0000} bad string\";\n\n        assert!(validate_no_control_characters(a)<|fim_middle|>", "completion": "use super::{default_limit, validate_no_control_characters, validation_errors, Pagination};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "use_declaration", "line_range": [865, 865]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    collections::{HashMap, HashSet},\n    sync::Arc,\n    time::Duration,\n};\n\nuse anyhow::Result;\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chrono::{DateTime, Utc};\nuse ed25519_compact::Signature;\nuse reqwest::{StatusCode, Url};\nuse sea_orm::{\n    ActiveModelBehavior, ActiveModelTrait, ConnectionTrait, DatabaseBackend, QueryResult, Set,\n    Statement,\n};\nuse serde::{de::IgnoredAny, Deserialize};\nuse serde_json::json;\nuse svix::webhooks::Webhook;\nuse svix_server::{\n    cfg::DefaultSignatureType,\n    core::{\n        cryptography::{AsymmetricKey, Encryption},\n        types::{\n            ApplicationId, BaseId, EndpointHeaders, EndpointHeadersPatch, EndpointId,\n            EndpointSecret, EndpointSecretInternal, EndpointUid, EventChannel, EventChannelSet,\n            EventTypeName, EventTypeNameSet, ExpiringSigningKeys, MessageAttemptId,\n            MessageAttemptTriggerType, MessageId, MessageStatus, OrganizationId,\n        },\n    },\n    db::models::{message, messageattempt},\n    v1::{\n        endpoints::{\n            endpoint::{\n                EndpointHeadersIn, EndpointHeadersOut, EndpointHeadersPatchIn, EndpointIn,\n                EndpointOut, EndpointSecretOut, EndpointStatsOut,\n            },\n            event_type::EventTypeOut,\n            message::MessageOut,\n        },\n        utils::ListResponse,\n    },\n};\n\nuse crate::utils::{\n    common_calls::{\n        common_test_list, create_test_app, create_test_endpoint, create_test_message,\n        default_test_endpoint, delete_test_app, endpoint_in, event_type_in,\n        get_msg_attempt_list_and_assert_count, metadata, post_endpoint, put_endpoint,\n        recover_webhooks,\n    },\n    get_default_test_config, start_svix_server, start_svix_server_with_cfg,\n    start_svix_server_with_cfg_and_org_id, TestClient, TestReceiver,\n};\n\nasync fn get_endpoint(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    ep_id: &str,\n) -> Result<EndpointOut> {\n    client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n}\n\nasync fn get_endpoint_404(client: &TestClient, app_id: &str, ep_id: &str) -> Result<IgnoredAny> {\n    client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n            StatusCode::NOT_FOUND,\n        )\n        .await\n}\n\na<|fim_suffix|>\n#[allow(deprecated)]\n#[tokio::test]\nasync fn test_create() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app = create_test_app(&client, \"v1EndpointPatchTestApp\")\n        .await\n        .unwrap()\n        .id;\n    let ep = post_endpoint(\n        &client,\n        &app,\n        EndpointIn {\n            url: Url::parse(\"http://example.com\").unwrap(),\n            version: None,\n            ..default_test_endpoint()\n        },\n    )\n    .await\n    .unwrap()\n    .id;\n\n    let url = format!(\"api/v1/app/{app}/endpoint/{ep}/\");\n\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://example.com/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n}\n\n#[allow(deprecated)]\n#[tokio::test]\nasync fn test_patch() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app = create_test_app(&client, \"v1EndpointPatchTestApp\")\n        .await\n        .unwrap()\n        .id;\n    let ep = create_test_endpoint(&client, &app, \"http://bad.url\")\n        .await\n        .unwrap()\n        .id;\n\n    let url = format!(\"api/v1/app/{app}/endpoint/{ep}/\");\n\n    // Test that the description may be set\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"description\": \"test\"\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    // Assert that no other changes were made\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the rate limit may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"rateLimit\": 1 }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.rate_limit, Some(1));\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the rate limit may be unset\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"rateLimit\": null,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.rate_limit, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the UID may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"uid\": \"some\" }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.uid, Some(EndpointUid(\"some\".to_owned())));\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test the UID may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"uid\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.uid, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the URL may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"url\": \"http://bad.url2\" }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the version may be set\n    let _: En<|fim_middle|>", "completion": "async fn delete_endpoint(client: &TestClient, app_id: &ApplicationId, ep_id: &str) -> Result<()> {\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "function_item", "line_range": [80, 87]}
{"prompt": "<|fim_prefix|> assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/1\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/2\");\n}\n\n/// Tests that there is at most one endpoint with a single UID for all endpoints associated with\n/// any application\n#[tokio::test]\nasync fn test_uid() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1: &str = \"v1EndpointUidTestApp1\";\n    const APP_NAME_2: &str = \"v1EndpointUidTestApp2\";\n\n    const EP_URI_APP_1_EP_1: &str = \"http://v1EndpointUidTestApp1Ep1.test\";\n    const EP_URI_APP_1_EP_2: &str = \"http://v1EndpointUidTestApp1Ep2.test\";\n    const EP_URI_APP_2: &str = \"http://v1EndpointUidTestApp2Ep1.test\";\n\n    const DUPLICATE_UID: &str = \"test_uid\";\n\n    // Same App\n\n    // Double Create -- on creation, it should return an error if identical UIDs are used for\n    // endpoints in the same app\n    let app_id = create_test_app(&client, APP_NAME_1).await.unwrap().id;\n    let uid = EndpointUid(DUPLICATE_UID.to_owned());\n\n    let mut ep_1 = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1.uid = Some(uid.clone());\n\n    let mut ep_2 = endpoint_in(EP_URI_APP_1_EP_2);\n    ep_2.uid = Some(uid.clone());\n\n    let ep_1 = post_endpoint(&client, &app_id, ep_1).await.unwrap();\n\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_2,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Update One to Existing -- on update it should return an error if attempting to change\n    // the UID to that of an existing endpoint associated with the same app\n    let ep_2 = create_test_endpoint(&client, &app_id, EP_URI_APP_1_EP_2)\n        .await\n        .unwrap();\n\n    let mut ep_2_with_duplicate_uid = endpoint_in(EP_URI_APP_1_EP_2);\n    ep_2_with_duplicate_uid.uid = Some(uid.clone());\n\n    client\n        .put::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_2.id),\n            ep_2_with_duplicate_uid,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Update One to Identical -- however it should not return an error if updating the\n    // existing endpoint to one with the same UID\n    let mut ep_1_with_duplicate_id = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1_with_duplicate_id.uid = Some(uid.clone());\n\n    let ep_1_updated = client\n        .put::<_, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_1.id),\n            ep_1_with_duplicate_id,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(ep_1.id, ep_1_updated.id);\n    assert_eq!(ep_1.ep.uid, ep_1_updated.ep.uid);\n\n    // Delete One then Create One -- UIDs may be reused after deletion\n    delete_endpoint(&client, &app_id, &ep_1.id).await.unwrap();\n    delete_endpoint(&client, &app_id, &ep_2.id).await.unwrap();\n\n    let mut ep_1 = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1.uid = Some(uid.clone());\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{}/endpoint/\", &app_id),\n            ep_1,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    delete_test_app(&client, app_id).await.unwrap();\n\n    // Different App -- however if they are associated with different applications, identical\n    // UIDs are valid\n    let app_1 = create_test_app(&client, APP_NAME_1).await.unwrap().id;\n    let app_2 = create_test_app(&client, APP_NAME_2).await.unwrap().id;\n\n    let mut ep_1 = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1.uid = Some(uid.clone());\n\n    let mut ep_2 = endpoint_in(EP_URI_APP_2);\n    ep_2.uid = Some(uid.clone());\n\n    let _ = post_endpoint(&client, &app_1, ep_1).await.unwrap();\n    let _ = post_endpoint(&client, &app_2, ep_2).await.unwrap();\n}\n\n// Simply tests that upon rotating an endpoint secret that it differs from the prior one\n#[tokio::test]\nasync fn test_endpoint_secret_get_and_rotation() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME: &str = \"v1EndpointSecretRotationTestApp\";\n    const EP_URI: &str = \"http://v1EndpointSecretRotationTestEp.test\";\n\n    l<|fim_suffix|>\n    let ep = create_test_endpoint(&client, &app_id, EP_URI)\n        .await\n        .unwrap();\n\n    let former_secret: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    assert_ne!(\n        former_secret,\n        client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                StatusCode::OK\n            )\n            .await\n            .unwrap()\n    );\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            &former_secret,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        former_secret,\n        client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                StatusCode::OK\n            )\n            .await\n            .unwrap()\n    );\n}\n\n#[tokio::test]\nasync fn test_recovery_should_fail_if_start_time_too_old() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let _: serde_json::Value = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/recover/\"),\n            json!({ \"since\": Utc::now() - chrono::Duration::weeks(3) }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_recovery_expected_retry_counts() {\n    let mut cfg = get_default_test_config();\n\n    cfg.retry_schedule = (0..2).map(|_| Duration::from_millis(1)).collect();\n\n    // total attempts for a failed message should be 1 (first attempt) + length of retry_schedule:\n    let base_attempt_cnt = 1 + &cfg.retry_schedule.len();\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let before_msg = Utc::now();\n\n    let msg = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt)\n        .await\n        .unwrap();\n\n    tokio::time::sleep(Duration::from_millis(10)).await;\n    let after_msg = Utc::now();\n\n    // recovery time after msg -- should be no additional attempts\n    recover_webhooks(\n        &client,\n        after_msg,\n        &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/recover/\"),\n    )\n    .await;\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt)\n        .await\n        .unwrap();\n\n    // recovery time before msg -- should be 1 additional attempt\n    recover_webhooks(\n        &client,\n        before_msg,\n        &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/recover/\"),\n    )\n    .await;\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt + 1)\n        .await\n        .unwrap();\n\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_max() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp_id = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap()\n        .id;\n\n    for _ in 0..ExpiringSigningKeys::MAX_OLD_KEYS {\n        client\n            .post_without_res<|fim_middle|>", "completion": "let app_id = create_test_app(&client, APP_NAME).await.unwrap().id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [854, 854]}
{"prompt": "<|fim_prefix|>l std::fmt::Display for RequestBuildError {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        let mut iter = self.0.iter();\n\n        f.write_str(\"Build failed\")?;\n\n        if let Some(first) = iter.next() {\n            write!(f, \": {first}\")?;\n\n            for err in iter {\n                write!(f, \"; {err}\")?;\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Error)]\npub enum BuildError {\n    #[error(\"uri missing\")]\n    UriMissing,\n    #[error(\"version missing\")]\n    VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub fn new() -> Self {\n        Self {\n            method: None,\n            uri: None,\n            accept: None,\n            user_agent: None,\n            headers: None,\n            header_names: None,\n            body: None,\n            version: None,\n            timeout: None,\n            content_type: None,\n            basic_auth: None,\n        }\n    }\n\n    pub fn method(mut self, method: Method) -> Self {\n        self.method = Some(method);\n        self\n    }\n\n    pub fn uri(mut self, uri: url::Url) -> Self {\n        let basic_auth = if uri.password().is_some() || !uri.username().is_empty() {\n            let username = decode_or_log(uri.username());\n            let password = uri.password().map(decode_or_log).unwrap_or_default();\n\n            Some(\n                Authorization::basic(&username, &password)\n                    .0\n                    .encode()\n                    .as_bytes()\n                    .to_vec(),\n            )\n        } else {\n            None\n        };\n        self.basic_auth = basic_auth;\n\n        let uri =\n            Uri::from_str(uri.as_str()).expect(\"If it's a valid url::Url, it's also a valid Uri\");\n        self.uri = Some(uri);\n        self\n    }\n\n    pub fn uri_str(self, uri: &str) -> Result<Self, url::ParseError> {\n        let uri = url::Url::from_str(uri)?;\n        Ok(self.uri(uri))\n    }\n\n    fn build_headers(\n        headers: CaseSensitiveHeaderMap,\n    ) -> (hyper::HeaderMap, hyper::ext::HeaderCaseMap) {\n        let mut hdr_map = hyper::HeaderMap::with_capacity(headers.len());\n        let mut case_sensitive_hdrs: hyper::HeaderMap<Bytes> =\n            hyper::HeaderMap::with_capacity(headers.len());\n        for (k, v) in headers.into_iter() {\n            match HeaderName::from_str(&k) {\n                Ok(key) => {\n                    hdr_map.insert(key.clone(), v);\n                    case_sensitive_hdrs.insert(key, Bytes::copy_from_slice(k.as_bytes()));\n                }\n                Err(e) => {\n                    tracing::error!(\"Failed to parse header {} {}\", k, e);\n                }\n            }\n        }\n        (hdr_map, case_sensitive_hdrs.into())\n    }\n\n    pub fn headers(mut self, headers: CaseSensitiveHeaderMap) -> Self {\n        let (hdrs, case_map) = Self::build_headers(headers);\n        self.headers = Some(hdrs);\n        self.header_names = Some(case_map);\n        self\n    }\n\n    pub fn body(mut self, body: Vec<u8>, content_type: HeaderValue) -> Self {\n        self.body = Some(body);\n        self.content_type = Some(content_type);\n        self\n    }\n\n    pub fn json_body<T: Serialize>(self, body: T) -> Result<Self, serde_json::Error> {\n        let body = serde_json::to_vec(&body)?;\n        Ok(self.body(body, HeaderValue::from_static(\"application/json\")))\n    }\n\n    pub fn version(mut self, version: Version) -> Self {\n        self.version = Some(version);\n        self\n    }\n\n    pub fn timeout(mut self, timeout: Duration) -> Self {\n        self.timeout = Some(timeout);\n        self\n    }\n\n    pub fn user_agent(mut self, user_agent: HeaderValue) -> Self {\n        self.user_agent = Some(user_agent);\n        self\n    }\n}\n\nimpl Default for RequestBuilder {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl RequestBuilder {\n    fn validate(&self) -> Result<(), RequestBuildError> {\n        <|fim_suffix|>\n        if self.uri.is_none() {\n            errs.push(BuildError::UriMissing);\n        }\n        if self.version.is_none() {\n            errs.push(BuildError::VersionMissing);\n        }\n\n        if !errs.is_empty() {\n            Err(RequestBuildError(errs))\n        } else {\n            Ok(())\n        }\n    }\n\n    pub fn build(self) -> Result<Request, RequestBuildError> {\n        self.validate()?;\n\n        let custom_headers = self.headers.unwrap_or_default();\n\n        let uri = self.uri.unwrap();\n        let authority = uri.authority().expect(\"Missing authority\");\n        let host = match authority.port() {\n            Some(port) => HeaderValue::from_str(&format!(\"{}:{port}\", authority.host())),\n            None => HeaderValue::from_str(authority.host()),\n        }\n        .unwrap();\n\n        let mut headers = HeaderMap::with_capacity(3 + custom_headers.len());\n\n        // Ensure that host header is first -- even though this is technically\n        // not required by HTTP spec, some clients fail if it's not first:\n        headers.insert(http::header::HOST, host);\n        headers.insert(\n            http::header::ACCEPT,\n            self.accept.unwrap_or(HeaderValue::from_static(\"*/*\")),\n        );\n        headers.insert(\n            http::header::CONTENT_TYPE,\n            self.content_type\n                .unwrap_or(HeaderValue::from_static(\"application/json\")),\n        );\n\n        headers.extend(custom_headers);\n\n        if let Some(user_agent) = self.user_agent {\n            headers.insert(http::header::USER_AGENT, user_agent);\n        }\n\n        if let Some(auth_header) = self.basic_auth {\n            if !headers.contains_key(http::header::AUTHORIZATION) {\n                headers.insert(\n                    http::header::AUTHORIZATION,\n                    HeaderValue::from_bytes(&auth_header).unwrap(),\n                );\n            }\n        }\n\n        Ok(Request {\n            method: self.method.unwrap_or(Method::POST),\n            uri,\n            headers,\n            header_names: self.header_names,\n            body: self.body,\n            timeout: self.timeout,\n            version: self.version.unwrap(),\n        })\n    }\n}\n\n/// HTTP connector that blocks outgoing requests to private IPs with support\n/// for HTTPS and optionally proxying via SOCKS5 or HTTP(S).\n#[derive(Clone)]\nenum SvixHttpsConnector {\n    Regular(HttpsConnector<NonLocalHttpConnector>),\n    Socks5Proxy {\n        proxy: HttpsConnector<SocksV5<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n    HttpProxy {\n        proxy: HttpsConnector<Tunnel<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n}\n\nimpl SvixHttpsConnector {\n    fn new(\n        inner: NonLocalHttpConnector,\n        proxy_cfg: Option<&ProxyConfig>,\n        disable_tls_verification: bool,\n    ) -> Result<Self, Box<dyn std::error::Error>> {\n        let https =\n            HttpsConnector::with_connector(inner.clone(), ssl_builder(disable_tls_verification))?;\n\n        let matcher = |proxy_url: String, noproxy: Option<ProxyBypassCfg>| -> Arc<Matcher> {\n            let mut matcher = Matcher::builder().all(proxy_url);\n            if let Some(noproxy) = noproxy {\n                matcher = matcher.no(noproxy.0);\n            }\n            Arc::new(matcher.build())\n        };\n\n        match proxy_cfg {\n            Some(proxy_cfg) => match proxy_cfg.addr.clone() {\n                ProxyAddr::Socks5(proxy_addr) => {\n                    let matcher = matcher(proxy_addr.to_string(), proxy_cfg.noproxy.clone());\n                    let socks = SocksV5::new(proxy_addr, inner).local_dns(true);\n                    let socks_https = HttpsConnector::with_connector(\n                        socks,\n                        ssl_builder(disable_tls_verification),\n                    )?;\n                    Ok(Self::Socks5Proxy {\n                        proxy: socks_https,\n                        bypass: https,\n                        matcher,\n                    })\n                <|fim_middle|>", "completion": "let mut errs: Vec<BuildError> = Vec::new();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [382, 382]}
{"prompt": "<|fim_prefix|>ut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(secret3_key, secret3.key);\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret1, secret2, secret3] {\n        if let EndpointSecret::Symmetric(key) = &sec.key {\n            let sec = STANDARD.encode(key);\n            let wh = Webhook::new(&sec).unwrap();\n            wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_signing_symmetric_and_asymmetric() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Asymmetric key\n    let secret_2 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    // Long key\n    let secret_3 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n\n    let ep_in = EndpointIn {\n        url: Url::parse(&receiver.endpoint).unwrap(),\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\" }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Rotate back to symmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret_3.serialize_public_key() }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret_1, secret_2, secret_3] {\n        match sec {\n            EndpointSecret::Symmetric(key) => {\n                let sec = STANDARD.encode(key);\n                let wh = Webhook::new(&sec).unwrap();\n                wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n            }\n            EndpointSecret::Asymmetric(key) => {\n                let msg_id = last_headers.get(\"svix-id\").unwrap().to_str().unwrap();\n                let timestamp = last_headers\n                    .get(\"svix-timestamp\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let signatures = last_headers\n                    .get(\"svix-signature\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let to_sign = format!(\"{msg_id}.{timestamp}.{}\", &last_body);\n                let found =\n                    signatures\n                        .split(' ')\n                        .filter(|x| x.starts_with(\"v1a,\"))\n                        .any(|signature| {\n                            l<|fim_suffix|>                            key.0.pk.verify(to_sign.as_bytes(), &sig).is_ok()\n                        });\n                assert!(found);\n            }\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_config() {\n    let mut cfg = get_default_test_config();\n    cfg.default_signature_type = DefaultSignatureType::Ed25519;\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let key1 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key1.starts_with(\"whpk_\"));\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let key2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key2.starts_with(\"whpk_\"));\n}\n\n#[tokio::test]\nasync fn test_custom_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Long key\n    let secret_2 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n    // Asymmetric key\n    let secret_3 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    assert_eq!(\n        secret_3.serialize_public_key(),\n        \"whpk_1SiA4o9hyqTCpIqC5V9HUakiiaeACeqfZTInDBbOir4=\"\n    );\n\n    let mut ep_in = EndpointIn {\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    ep_in.key = Some(secret_2.clone());\n    let endp_2 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // We rotate the key after because it's easier than setting json! for everything\n    ep_in.key = Some(secret_2.clone());\n    let endp_3 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp_3.id),\n            json!({\n                \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\",\n            }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    for (secret, ep) in [(secret_1, endp_1), (secret_2, endp_2), (secret_3, endp_3)] {\n        assert_eq!(\n            secret.serialize_public_key(),\n            client\n                .get::<EndpointSecretOutTest>(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                    StatusCode::OK\n                )\n                .await\n                .unwrap()\n                .key\n        );\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_encryption() {\n    let org_id = OrganizationId::new(None, None);\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let cfg = get_default_test_config();\n    let (client, jh<|fim_middle|>", "completion": "let sig: Signature = Signature::from_slice(\n                                STANDARD\n                                    .decode(&signature[\"v1a,\".len()..])\n                                    .unwrap()\n                                    .as_slice(),\n                            )\n                            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1188, 1194]}
{"prompt": "<|fim_prefix|>use std::{num::NonZeroUsize, str::FromStr};\n\nuse anyhow::Result;\nuse deadpool::unmanaged::Pool;\nuse deno_ast::{MediaType, ParseParams};\nuse deno_core::{\n    serde_v8, url,\n    v8::{self},\n    JsRuntime,\n};\nuse svix_bridge_types::{JsObject, TransformerInput, TransformerOutput};\nuse tokio::sync::oneshot;\n\nstruct Executor {\n    tx: std::sync::mpsc::Sender<Job>,\n    _handle: std::thread::JoinHandle<()>,\n}\n\nimpl Default for Executor {\n    fn default() -> Self {\n        let (tx, rx) = std::sync::mpsc::channel::<Job>();\n        let _handle = std::thread::spawn(move || {\n            let mut runtime = JsRuntime::new(Default::default());\n            for Job { input, script, cb } in rx {\n                <|fim_suffix|>\n                if cb.send(ret).is_err() {\n                    tracing::error!(\"failed to send script output to caller\");\n                }\n            }\n        });\n        Self { tx, _handle }\n    }\n}\n\ntype Callback = oneshot::Sender<Result<TransformerOutput>>;\n\nstruct Job {\n    input: TransformerInput,\n    script: String,\n    cb: Callback,\n}\n\nimpl Executor {\n    async fn execute(\n        &mut self,\n        input: TransformerInput,\n        script: String,\n    ) -> Result<TransformerOutput> {\n        let (tx, rx) = oneshot::channel();\n        self.tx.send(Job {\n            input,\n            script,\n            cb: tx,\n        })?;\n        rx.await?\n    }\n}\n\n#[derive(Clone)]\npub struct JsPooler {\n    executors: Pool<Executor>,\n}\n\nimpl JsPooler {\n    pub fn new(pool_size: NonZeroUsize) -> Self {\n        let pool_size = pool_size.get();\n        let mut items = Vec::with_capacity(pool_size);\n        for _ in 0..pool_size {\n            items.push(Executor::default());\n        }\n        Self {\n            executors: Pool::from(items),\n        }\n    }\n\n    pub async fn run_script(\n        &self,\n        input: TransformerInput,\n        script: String,\n    ) -> Result<TransformerOutput> {\n        let pool = self.executors.clone();\n        let mut executor = pool.get().await;\n\n        executor\n            .as_mut()\n            .map_err(|e| anyhow::anyhow!(\"{e:?}\"))?\n            .execute(input, script)\n            .await\n    }\n}\n\n/// Checks that the input parses as valid JavaScript, giving the parser's error back on failure.\npub fn validate_script(src: &str) -> Result<()> {\n    Ok(deno_ast::parse_script(ParseParams {\n        specifier: url::Url::from_str(\"file:///x.js\").expect(\"static string\"),\n        text: src.into(),\n        media_type: MediaType::JavaScript,\n        capture_tokens: false,\n        scope_analysis: false,\n        maybe_syntax: None,\n    })\n    .map(|_| ())?)\n}\n\nfn run_script_inner(\n    runtime: &mut JsRuntime,\n    input: TransformerInput,\n    script: String,\n) -> Result<TransformerOutput> {\n    let input = serde_json::to_string(&input)?;\n    let res = runtime.execute_script(\n        \"<anon>\",\n        format!(\n            // Wrap the user script, and invocation of `handler`, in a self-calling closure.\n            // The hope is we'll prevent the globals space from being polluted call after call.\n            r#\"\n    (function () {{\n        {script}\n        return handler({input});\n    }})()\n    \"#,\n        ),\n    );\n    match res {\n        Ok(global) => {\n            let scope = &mut runtime.handle_scope();\n            let local = v8::Local::new(scope, global);\n            match serde_v8::from_v8::<JsObject>(scope, local) {\n                Ok(v) => Ok(TransformerOutput::Object(v)),\n                Err(e @ serde_v8::Error::ExpectedObject(_)) => {\n                    tracing::error!(\"{e}\");\n                    Ok(TransformerOutput::Invalid)\n                }\n                Err(e) => {\n                    tracing::error!(\"{e}\");\n                    Err(e)?\n                }\n            }\n        }\n        Err(err) => Err(anyhow::format_err!(\"Evaling error: {:?}\", err)),\n    }\n}\n\n#[cfg(test)]\nmod tests;\n<|fim_middle|>", "completion": "let ret = run_script_inner(&mut runtime, input, script);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/mod.rs", "node_type": "let_declaration", "line_range": [25, 25]}
{"prompt": "<|fim_prefix|>ect(management_org_id());\n\n    signing_config.generate(claims).map_err(Error::generic)\n}\n\npub fn generate_app_token(\n    keys: &JwtSigningConfig,\n    org_id: OrganizationId,\n    app_id: ApplicationId,\n    feature_flags: FeatureFlagSet,\n) -> Result<String> {\n    let claims = Claims::with_custom_claims(\n        CustomClaim {\n            organization: Some(org_id.0),\n            feature_flags,\n        },\n        Duration::from_hours(24 * 28),\n    )\n    .with_issuer(JWT_ISSUER)\n    .with_subject(app_id.0);\n\n    keys.generate(claims).map_err(Error::generic)\n}\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum JwtSigningConfig {\n    /// Variants that specify both key and algorithm to use\n    Advanced(JWTAlgorithm),\n    /// The variant used when the algorithm is not specified, defaults to HS256\n    Default {\n        #[serde(deserialize_with = \"deserialize_hs256\")]\n        jwt_secret: HS256Key,\n    },\n}\n\n/// A wrapper for the available JWT signing algorithms exposed by `jwt-simple`\n#[derive(Deserialize)]\n#[serde(tag = \"jwt_algorithm\", content = \"jwt_secret\")]\npub enum JWTAlgorithm {\n    #[serde(deserialize_with = \"deserialize_hs256\")]\n    HS256(HS256Key),\n    #[serde(deserialize_with = \"deserialize_hs384\")]\n    HS384(HS384Key),\n    #[serde(deserialize_with = \"deserialize_hs512\")]\n    HS512(HS512Key),\n    #[serde(deserialize_with = \"deserialize_rs256\")]\n    RS256(RS256),\n    #[serde(deserialize_with = \"deserialize_rs384\")]\n    RS384(RS384),\n    #[serde(deserialize_with = \"deserialize_rs512\")]\n    RS512(RS512),\n    #[serde(deserialize_with = \"deserialize_eddsa\")]\n    EdDSA(EdDSA),\n}\n\npub enum RS256 {\n    Public(RS256PublicKey),\n    Pair(Box<RS256KeyPair>),\n}\n\npub enum RS384 {\n    Public(RS384PublicKey),\n    Pair(Box<RS384KeyPair>),\n}\n\npub enum RS512 {\n    Public(RS512PublicKey),\n    Pair(Box<RS512KeyPair>),\n}\n\npub enum EdDSA {\n    Public(Ed25519PublicKey),\n    Pair(Box<Ed25519KeyPair>),\n}\n\nimpl JwtSigningConfig {\n    pub fn generate(&self, claims: JWTClaims<CustomClaim>) -> Result<String, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.authenticate(claims),\n                JWTAlgorithm::HS384(key) => key.authenticate(claims),\n                JWTAlgorithm::HS512(key) => key.authenticate(claims),\n                JWTAlgorithm::RS256(kind) => match kind {\n                    RS256::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS256::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS384::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS512::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::EdDSA(kind) => match kind {\n                    EdDSA::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    EdDSA::Pair(key) => key.sign(claims),\n                },\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.authenticate(claims),\n        }\n    }\n\n    pub fn verify_token(\n        &self,\n        token: &str,\n        options: Option<VerificationOptions>,\n    ) -> Result<JWTClaims<CustomClaim>, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS384(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS512(key) => key.verify_token(token, options),\n                JWTAlgorithm::RS256(kind) => match kind {\n                    RS256::Public(key) => key.verify_token(token, options),\n                    RS256::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(key) => key.verify_token(token, options),\n                    RS384::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(key) => key.verify_token(token, options),\n                    RS512::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::EdDSA(kind) => match kind {\n                    EdDSA::Public(key) => key.verify_token(token, options),\n                    EdDSA::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.verify_token(token, options),\n        }\n    }\n}\n\nimpl Debug for JwtSigningConfig {\n    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"{}\",\n            match self {\n                JwtSigningConfig::Advanced(a) => {\n                    match a {\n                        JWTAlgorithm::HS256(_) => \"HS256\",\n                        JWTAlgorithm::HS384(_) => \"HS384\",\n                        JWTAlgorithm::HS512(_) => \"HS512\",\n                        JWTAlgorithm::RS256(_) => \"RS256\",\n                        JWTAlgorithm::RS384(_) => \"RS384\",\n                        JWTAlgorithm::RS512(_) => \"RS512\",\n                        JWTAlgorithm::EdDSA(_) => \"EdDSA\",\n                    }\n                }\n                JwtSigningConfig::Default { .. } => {\n                    \"HS256\"\n                }\n            }\n        )\n    }\n}\n\nfn deserialize_hs256<'de, D>(deserializer: D) -> Result<HS256Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS256Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs384<'de, D>(deserializer: D) -> Result<HS384Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS384Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs512<'de, D>(deserializer: D) -> Result<HS512Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS512Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_rs256<'de, D>(deserializer: D) -> Result<RS256, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS256KeyPair::from_pem(&key) {\n        Ok(RS256::Pair(Box::new(pair)))\n    } else i<|fim_suffix|>}\n\nfn deserialize_rs384<'de, D>(deserializer: D) -> Result<RS384, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS384KeyPair::from_pem(&key) {\n        Ok(RS384::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS384PublicKey::from_pem(&key) {\n        Ok(RS384::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_rs512<'de, D>(deserializer: D) -> Result<RS512, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS512KeyPair::from_pem(&key) {\n        Ok(RS512::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS512PublicKey::from_pem(&key) {\n        Ok(RS512::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_eddsa<'de, D>(deserializer: D) -> Result<EdDSA, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = Ed25519KeyPair::from_pem(&key) {\n        Ok(EdDSA::Pair(Box::new(pair)))\n    } else if let Ok(public) = Ed25519PublicKey::from_pem(&key) {\n        Ok(EdDSA::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n<|fim_middle|>", "completion": "if let Ok(public) = RS256PublicKey::from_pem(&key) {\n        Ok(RS256::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/security.rs", "node_type": "if_expression", "line_range": [392, 396]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct AirwallexConfigOut {}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl AirwallexConfigOut {\n    pub fn new() -> Self {\n        Self {}\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/airwallex_config_out.rs", "node_type": "impl_item", "line_range": [7, 11]}
{"prompt": "<|fim_prefix|>  method: Method,\n    uri: Uri,\n    headers: HeaderMap,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    timeout: Option<Duration>,\n    version: Version,\n}\n\npub struct RequestBuilder {\n    method: Option<Method>,\n    uri: Option<Uri>,\n    accept: Option<HeaderValue>,\n    user_agent: Option<HeaderValue>,\n    headers: Option<HeaderMap>,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    version: Option<Version>,\n    timeout: Option<Duration>,\n    basic_auth: Option<Vec<u8>>,\n\n    // Derived from body\n    content_type: Option<HeaderValue>,\n}\n\n#[derive(Debug)]\npub struct RequestBuildError(pub Vec<BuildError>);\n\nimpl std::fmt::Display for RequestBuildError {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        let mut iter = self.0.iter();\n\n        f.write_str(\"Build failed\")?;\n\n        if let Some(first) = iter.next() {\n            write!(f, \": {first}\")?;\n\n            for err in iter {\n                write!(f, \"; {err}\")?;\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Error)]\npub enum BuildError {\n    #[error(\"uri missing\")]\n    UriMissing,\n    #[error(\"version missing\")]\n    VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub fn new() -> Self {\n        Self {\n            method: None,\n            uri: None,\n            accept: None,\n            user_agent: None,\n            headers: None,\n            header_names: None,\n            body: None,\n            version: None,\n            timeout: None,\n            content_type: None,\n            basic_auth: None,\n        }\n    }\n\n    pub fn method(mut self, method: Method) -> Self {\n        self.method = Some(method);\n        self\n    }\n\n    pub fn uri(mut self, uri: url::Url) -> Self {\n        let basic_auth = if uri.password().is_some() || !uri.username().is_empty() {\n            let username = decode_or_log(uri.username());\n            let password = uri.password().map(decode_or_log).unwrap_or_default();\n\n            Some(\n                Authorization::basic(&username, &password)\n                    .0\n                    .encode()\n                    .as_bytes()\n                    .to_vec(),\n            )\n        } else {\n            None\n        };\n        self.basic_auth = basic_auth;\n\n        let uri =\n            Uri::from_str(uri.as_str()).expect(\"If it's a valid url::Url, it's also a valid Uri\");\n        self.uri = Some(uri);\n        self\n    }\n\n    pub fn uri_str(self, uri: &str) -> Result<Self, url::ParseError> {\n        let uri = url::Url::from_str(uri)?;\n        Ok(self.uri(uri))\n    }\n\n    fn build_headers(\n        headers: CaseSensitiveHeaderMap,\n    ) -> (hyper::HeaderMap, hyper::ext::HeaderCaseMap) {\n        let mut hdr_map = hyper::HeaderMap::with_capacity(headers.len());\n        let mut case_sensitive_hdrs: hyper::HeaderMap<Bytes> =\n            hyper::HeaderMap::with_capacity(headers.len());\n        for (k, v) in headers.into_iter() {\n            match HeaderName::from_str(&k) {\n                Ok(key) => {\n                    hdr_map.insert(key.clone(), v);\n                    case_sensitive_hdrs.insert(key, Bytes::copy_from_slice(k.as_bytes()));\n                }\n                Err(e) => {\n                    tracing::error!(\"Failed to parse header {} {}\", k, e);\n                }\n            }\n        }\n        (hdr_map, case_sensitive_hdrs.into())\n    }\n\n    pub fn headers(mut self, headers: CaseSensitiveHeaderMap) -> Self {\n        let (hdrs, case_map) = Self::build_headers(headers);\n        self.headers = Some(hdrs);\n        self.header_names = Some(case_map);\n        self\n    }\n\n    pub fn body(mut self, body: Vec<u8>, content_type: HeaderValue) -> Self {\n        self.body = Some(body);\n        self.content_type = Some(content_type);\n        self\n    }\n\n    pub fn json_body<T: Serialize>(self, body: T) -> Result<Self, serde_json::Error> {\n        <|fim_suffix|>\n        Ok(self.body(body, HeaderValue::from_static(\"application/json\")))\n    }\n\n    pub fn version(mut self, version: Version) -> Self {\n        self.version = Some(version);\n        self\n    }\n\n    pub fn timeout(mut self, timeout: Duration) -> Self {\n        self.timeout = Some(timeout);\n        self\n    }\n\n    pub fn user_agent(mut self, user_agent: HeaderValue) -> Self {\n        self.user_agent = Some(user_agent);\n        self\n    }\n}\n\nimpl Default for RequestBuilder {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl RequestBuilder {\n    fn validate(&self) -> Result<(), RequestBuildError> {\n        let mut errs: Vec<BuildError> = Vec::new();\n        if self.uri.is_none() {\n            errs.push(BuildError::UriMissing);\n        }\n        if self.version.is_none() {\n            errs.push(BuildError::VersionMissing);\n        }\n\n        if !errs.is_empty() {\n            Err(RequestBuildError(errs))\n        } else {\n            Ok(())\n        }\n    }\n\n    pub fn build(self) -> Result<Request, RequestBuildError> {\n        self.validate()?;\n\n        let custom_headers = self.headers.unwrap_or_default();\n\n        let uri = self.uri.unwrap();\n        let authority = uri.authority().expect(\"Missing authority\");\n        let host = match authority.port() {\n            Some(port) => HeaderValue::from_str(&format!(\"{}:{port}\", authority.host())),\n            None => HeaderValue::from_str(authority.host()),\n        }\n        .unwrap();\n\n        let mut headers = HeaderMap::with_capacity(3 + custom_headers.len());\n\n        // Ensure that host header is first -- even though this is technically\n        // not required by HTTP spec, some clients fail if it's not first:\n        headers.insert(http::header::HOST, host);\n        headers.insert(\n            http::header::ACCEPT,\n            self.accept.unwrap_or(HeaderValue::from_static(\"*/*\")),\n        );\n        headers.insert(\n            http::header::CONTENT_TYPE,\n            self.content_type\n                .unwrap_or(HeaderValue::from_static(\"application/json\")),\n        );\n\n        headers.extend(custom_headers);\n\n        if let Some(user_agent) = self.user_agent {\n            headers.insert(http::header::USER_AGENT, user_agent);\n        }\n\n        if let Some(auth_header) = self.basic_auth {\n            if !headers.contains_key(http::header::AUTHORIZATION) {\n                headers.insert(\n                    http::header::AUTHORIZATION,\n                    HeaderValue::from_bytes(&auth_header).unwrap(),\n                );\n            }\n        }\n\n        Ok(Request {\n            method: self.method.unwrap_or(Method::POST),\n            uri,\n            headers,\n            header_names: self.header_names,\n            body: self.body,\n            timeout: self.timeout,\n            version: self.version.unwrap(),\n        })\n    }\n}\n\n/// HTTP connector that blocks outgoing requests to private IPs with support\n/// for HTTPS and optionally proxying via SOCKS5 or HTTP(S).\n#[derive(Clone)]\nenum SvixHttpsConnector {\n    Regular(HttpsConnector<NonLocalHttpConnector>),\n    Socks5Proxy {\n        proxy: HttpsConnector<SocksV5<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n    HttpProxy {\n        proxy: HttpsConnector<Tunnel<NonLocalHttpConnector>>,\n        bypass: HttpsConnector<NonLocalHttpConnector>,\n        matcher: Arc<Matcher>,\n    },\n}\n\nimpl SvixHttpsConnector {\n    fn new(\n        inner: NonLocalHttpConnector,\n        proxy_cfg: Option<&ProxyConfig>,\n        disable_tls_verification: bool,\n    ) -> Result<Self, Box<dyn std::error::Error>> {\n        let https =\n            HttpsConnector::with_connector(inner.clone(), ssl_builder(disable_tls_verification))?;\n\n        let matcher = |proxy_url: String, noproxy: Option<ProxyBypassCfg>| -> Arc<Matcher> {\n            let mut matcher = Matcher::builder().all(proxy_url);\n            if let Some(noproxy) = noproxy {\n                matcher = matcher.no(noproxy.0);\n            }\n            Arc::new(matcher.build())\n        };\n\n        match proxy_cfg {\n       <|fim_middle|>", "completion": "let body = serde_json::to_vec(&body)?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [354, 354]}
{"prompt": "<|fim_prefix|>\n\n        let _: () = pipe.query_async(conn).await?;\n    }\n}\n\nasync fn migrate_v1_to_v2_queues(conn: &mut RedisConnection<'_>) -> Result<()> {\n    migrate_list(conn, LEGACY_V1_MAIN, LEGACY_V2_MAIN).await?;\n    migrate_list(conn, LEGACY_V1_PROCESSING, LEGACY_V2_PROCESSING).await?;\n    migrate_sset(conn, LEGACY_V1_DELAYED, DELAYED).await?;\n\n    Ok(())\n}\n\nasync fn migrate_list(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    let batch_size = 1000;\n    loop {\n        // Checking for old messages from queue\n        let legacy_keys: Vec<String> = conn\n            .lpop(legacy_queue, NonZeroUsize::new(batch_size))\n            .await?;\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n        let _: () = conn.rpush(queue, legacy_keys).await?;\n    }\n}\n\nasync fn migrate_sset(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    let batch_size = 1000;\n    loop {\n        // Checking for old messages from LEGACY_DELAYED\n        let legacy_keys: Vec<(String, f64)> = conn.zpopmin(legacy_queue, batch_size).await?;\n\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n        let legacy_keys: Vec<(f64, String)> =\n            legacy_keys.into_iter().map(|(x, y)| (y, x)).collect();\n\n        let _: () = conn.zadd_multiple(queue, &legacy_keys).await?;\n    }\n}\n\n#[cfg(test)]\npub mod tests {\n    use std::time::Duration;\n\n    use chrono::Utc;\n    use redis::{streams::StreamReadReply, AsyncCommands as _, Direction};\n    use tokio::time::timeout;\n\n    use super::{migrate_list, migrate_list_to_stream, migrate_sset, new_pair_inner};\n    use crate::{\n        cfg::Configuration,\n        core::types::{ApplicationId, EndpointId, MessageAttemptTriggerType, MessageId},\n        queue::{MessageTask, QueueTask},\n        redis::RedisManager,\n    };\n\n    const TEST_RECV_DEADLINE: Duration = Duration::from_secs(5);\n\n    async fn get_pool(cfg: &Configuration) -> RedisManager {\n        RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await\n    }\n\n    #[tokio::test]\n    // run with `cargo test -- --ignored redis` only when redis is up and configured\n    #[ignore]\n    async fn test_migrate_list() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n        let mut pool = pool.get().await.unwrap();\n\n        const TEST_QUEUE: &str = \"{queue}_svix_test_queue_list\";\n        const TEST_LEGACY: &str = \"svix_queue_test_list\";\n\n        let v = \"test-value\";\n\n        // delete test queues first, just in case:\n        let _: () = pool.del(TEST_QUEUE).await.unwrap();\n        let _: () = pool.del(TEST_LEGACY).await.unwrap();\n\n        let _: () = pool.rpush(TEST_LEGACY, v).await.unwrap();\n\n        let should_be_none: Option<String> = pool.lpop(TEST_QUEUE, None).await.unwrap();\n        assert_eq!(should_be_none, None);\n\n        migrate_list(&mut pool, TEST_LEGACY, TEST_QUEUE)\n            .await\n            .unwrap();\n\n        let test_key: Option<String> = pool.lpop(TEST_QUEUE, None).await.unwrap();\n\n        assert_eq!(test_key.unwrap(), v);\n\n        let should_be_none: Option<String> = pool.lpop(TEST_LEGACY, None).await.unwrap();\n        assert_eq!(should_be_none, None);\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_migrate_sset() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n        let mut pool = pool.get().await.unwrap();\n\n        const TEST_QUEUE: &str = \"{queue}_svix_test_queue_sset\";\n        const TEST_LEGACY: &str = \"svix_queue_test_sset\";\n\n        let v = \"test-value\";\n\n        // delete test queues first, just in case:\n        let _: () = pool.del(TEST_QUEUE).await.unwrap();\n        let _: () = pool.del(TEST_LEGACY).await.unwrap();\n\n        <|fim_suffix|>\n\n        let should_be_none: Vec<(String, i32)> = pool.zpopmin(TEST_QUEUE, 1).await.unwrap();\n        assert_eq!(should_be_none, vec![]);\n\n        migrate_sset(&mut pool, TEST_LEGACY, TEST_QUEUE)\n            .await\n            .unwrap();\n\n        let test_key: Vec<(String, i32)> = pool.zpopmin(TEST_QUEUE, 1).await.unwrap();\n\n        assert_eq!(test_key.first().unwrap().0, v);\n\n        let should_be_none: Vec<(String, i32)> = pool.zpopmin(TEST_LEGACY, 1).await.unwrap();\n        assert_eq!(should_be_none, vec![]);\n    }\n\n    async fn cleanup(pool: &RedisManager, q1: &str, q2: &str, q3: &str) {\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n        let _: () = conn.del(&[q1, q2, q3]).await.unwrap();\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_idle_period() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        let main_queue = \"{test}_idle_period\";\n        let delayed = \"{test}_idle_period_delayed\";\n        let lock = \"{test}_idle_period_delayed_lock\";\n        let dlq = \"{test}_dlq\";\n\n        let delay = Duration::from_millis(100);\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test\".to_owned()),\n            app_id: ApplicationId(\"test\".to_owned()),\n            endpoint_id: EndpointId(\"test\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n        p.send(&mt, None).await.unwrap();\n\n        let recv = timeout(Duration::from_secs(5), c.receive_all(TEST_RECV_DEADLINE))\n            .await\n            .expect(\"`c.receive()` has timed out\");\n        assert_eq!(*recv.unwrap()[0].task, mt);\n\n        tokio::time::sleep(delay).await;\n\n        let recv = timeout(Duration::from_secs(1), c.receive_all(TEST_RECV_DEADLINE))\n            .await\n            .expect(\"`c.receive()` has timed out\");\n        let recv = recv.unwrap().pop().unwrap();\n        assert_eq!(*recv.task, mt);\n        // Acknowledge so the queue isn't further polluted\n        recv.ack().await.unwrap();\n\n        // And assert that the task has been deleted\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n        assert!(conn\n            .xread::<_, _, StreamReadReply>(&[main_queue], &[0])\n            .await\n            .unwrap()\n            .keys\n            .is_empty());\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_ack() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        let main_queue = \"{test}_ack\";\n        let delayed = \"{test}_ack_delayed\";\n        let lock = \"{test}_ack_delayed_lock\";\n        let dlq = \"{test}_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(100);\n\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n        p.send(&mt, None).await.unwrap();\n\n        let recv = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv.task, mt);\n        recv.ack().await.unwrap();\n\n        if let Ok(recv) = timeout(delay, c.receive_all(TEST_RECV_DEADLINE)).await {\n            panic!(\"Received unexpected QueueTask {:?}\", recv.unwrap()[0].task);\n        }\n\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n        // And assert that the task<|fim_middle|>", "completion": "let _: () = pool.zadd(TEST_LEGACY, v, 1isize).await.unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [477, 477]}
{"prompt": "<|fim_prefix|>/*\n * Svix API\n *\n * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)\n *\n * The version of the OpenAPI document: 1.1.1\n *\n * Generated by: https://openapi-generator.tech\n */\n\n#[allow(unused_imports)]\n<|fim_suffix|>\n#[allow(unused_imports)]\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]\npub struct HttpErrorOut {\n    #[serde(rename = \"code\")]\n    pub code: String,\n    #[serde(rename = \"detail\")]\n    pub detail: String,\n}\n\nimpl HttpErrorOut {\n    pub fn new(code: String, detail: String) -> HttpErrorOut {\n        HttpErrorOut { code, detail }\n    }\n}\n<|fim_middle|>", "completion": "use crate::models;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/http_error_out.rs", "node_type": "use_declaration", "line_range": [12, 12]}
{"prompt": "<|fim_prefix|>       disable_tls_verification: bool,\n    ) -> Result<Self, Box<dyn std::error::Error>> {\n        let https =\n            HttpsConnector::with_connector(inner.clone(), ssl_builder(disable_tls_verification))?;\n\n        let matcher = |proxy_url: String, noproxy: Option<ProxyBypassCfg>| -> Arc<Matcher> {\n            let mut matcher = Matcher::builder().all(proxy_url);\n            if let Some(noproxy) = noproxy {\n                matcher = matcher.no(noproxy.0);\n            }\n            Arc::new(matcher.build())\n        };\n\n        match proxy_cfg {\n            Some(proxy_cfg) => match proxy_cfg.addr.clone() {\n                ProxyAddr::Socks5(proxy_addr) => {\n                    let matcher = matcher(proxy_addr.to_string(), proxy_cfg.noproxy.clone());\n                    let socks = SocksV5::new(proxy_addr, inner).local_dns(true);\n                    let socks_https = HttpsConnector::with_connector(\n                        socks,\n                        ssl_builder(disable_tls_verification),\n                    )?;\n                    Ok(Self::Socks5Proxy {\n                        proxy: socks_https,\n                        bypass: https,\n                        matcher,\n                    })\n                }\n                ProxyAddr::Http(proxy_addr) => {\n                    let matcher = matcher(proxy_addr.to_string(), proxy_cfg.noproxy.clone());\n                    let tunnel = Tunnel::new(proxy_addr, inner);\n                    let tunnel_https = HttpsConnector::with_connector(\n                        tunnel,\n                        ssl_builder(disable_tls_verification),\n                    )?;\n                    Ok(Self::HttpProxy {\n                        proxy: tunnel_https,\n                        bypass: https,\n                        matcher,\n                    })\n                }\n            },\n            None => Ok(Self::Regular(https)),\n        }\n    }\n}\n\nimpl Service<Uri> for SvixHttpsConnector {\n    type Response = MaybeHttpsStream<TokioIo<TcpStream>>;\n    type Error = Box<dyn std::error::Error + Send + Sync>;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(\n        &mut self,\n        cx: &mut std::task::Context<'_>,\n    ) -> std::task::Poll<Result<(), Self::Error>> {\n        match self {\n            Self::Regular(inner) => inner.poll_ready(cx),\n            Self::Socks5Proxy { proxy, bypass, .. } => {\n                ready!(proxy.poll_ready(cx)?);\n                ready!(bypass.poll_ready(cx)?);\n                Poll::Ready(Ok(()))\n            }\n            Self::HttpProxy { proxy, bypass, .. } => {\n                ready!(proxy.poll_ready(cx)?);\n                ready!(bypass.poll_ready(cx)?);\n                Poll::Ready(Ok(()))\n            }\n        }\n    }\n\n    fn call(&mut self, req: Uri) -> Self::Future {\n        match self {\n            Self::Regular(inner) => Box::pin(inner.call(req)),\n            Self::Socks5Proxy {\n                proxy,\n                bypass,\n                matcher,\n                ..\n            } => match matcher.intercept(&req) {\n                Some(_) => Box::pin(proxy.call(req)),\n                None => Box::pin(bypass.call(req)),\n            },\n            Self::HttpProxy {\n                proxy,\n                bypass,\n                matcher,\n                ..\n            } => match matcher.intercept(&req) {\n                Some(_) => Box::pin(proxy.call(req)),\n                None => Box::pin(bypass.call(req)),\n            },\n        }\n    }\n}\n\n/// A plain-HTTP connector that blocks outgoing requests to private IPs.\ntype NonLocalHttpConnector = HttpConnector<NonLocalDnsResolver>;\n\n/// A DNS resolver that produces an error for names that resolve to private IPs.\n///\n/// Specific private subnets or domain names may be whitelisted.\n#[derive(Clone, Debug)]\nstruct NonLocalDnsResolver {\n    state: Arc<Mutex<DnsState>>,\n    whitelist_nets: Arc<Vec<IpNet>>,\n    whitelist_names: Arc<Vec<String>>,\n}\n\n#[derive(Clone, Debug)]\nenum DnsState {\n    Init,\n    Ready(Arc<TokioResolver>),\n}\n\nimpl NonLocalDnsResolver {\n    <|fim_suffix|>\n}\n\nimpl Service<Name> for NonLocalDnsResolver {\n    type Response = SocketAddrs;\n    type Error = Error;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(&mut self, _cx: &mut std::task::Context<'_>) -> Poll<Result<(), Self::Error>> {\n        Poll::Ready(Ok(()))\n    }\n\n    fn call(&mut self, name: Name) -> Self::Future {\n        let resolver = self.clone();\n        let whitelist_nets = self.whitelist_nets.clone();\n        let whitelist_names = self.whitelist_names.clone();\n\n        Box::pin(async move {\n            let mut lock = resolver.state.lock().await;\n\n            let resolver = match &*lock {\n                DnsState::Init => {\n                    let resolver = new_resolver().await?;\n                    *lock = DnsState::Ready(resolver.clone());\n                    resolver\n                }\n\n                DnsState::Ready(resolver) => resolver.clone(),\n            };\n\n            drop(lock);\n\n            let whitelisted_name = whitelist_names\n                .iter()\n                .any(|whitelisted| whitelisted == name.as_str());\n\n            let lookup = resolver.lookup_ip(name.as_str()).await?;\n\n            if lookup.iter().all(|ip| {\n                !is_allowed(ip)\n                    && !whitelist_nets.iter().any(|subnet| subnet.contains(&ip))\n                    && !whitelisted_name\n            }) {\n                Err(Error::BlockedIp)\n            } else {\n                Ok(SocketAddrs {\n                    iter: lookup.into_iter(),\n                    whitelist_nets,\n                    whitelisted_name,\n                })\n            }\n        })\n    }\n}\n\npub struct SocketAddrs {\n    iter: LookupIpIntoIter,\n    whitelist_nets: Arc<Vec<IpNet>>,\n    whitelisted_name: bool,\n}\n\nimpl Iterator for SocketAddrs {\n    type Item = SocketAddr;\n\n    fn next(&mut self) -> Option<Self::Item> {\n        loop {\n            match self.iter.next() {\n                Some(ip_addr) => {\n                    if is_allowed(ip_addr)\n                        || self\n                            .whitelist_nets\n                            .iter()\n                            .any(|subnet| subnet.contains(&ip_addr))\n                        || self.whitelisted_name\n                    {\n                        return Some(SocketAddr::from((ip_addr, 0)));\n                    }\n                }\n\n                None => return None,\n            }\n        }\n    }\n}\n\nasync fn new_resolver() -> Result<Arc<TokioResolver>, ResolveError> {\n    Ok(Arc::new(Resolver::builder_tokio()?.build()))\n}\n\nfn is_allowed(addr: IpAddr) -> bool {\n    match addr.to_canonical() {\n        IpAddr::V4(addr) => {\n            !addr.is_private()\n                && !addr.is_loopback()\n                && !addr.is_link_local()\n                && !addr.is_broadcast()\n                && !addr.is_documentation()\n                && !is_shared(addr)\n                && !is_reserved(addr)\n                && !is_benchmarking(addr)\n                && !starts_with_zero(addr)\n        }\n        IpAddr::V6(addr) => {\n            !addr.is_multicast()\n                && !addr.is_loopback()\n                && !is_unicast_link_local(addr)\n                && !is_unique_local(addr)\n                && !addr.is_unspecified()\n                && !is_documentation_v6(addr)\n        }\n    }\n}\n\n/// Util functions copied from the unstable standard library near identically\nfn is_shared(addr: Ipv4Addr) -> bool {\n    addr.octets()[0] == 100 && (addr.octets()[1] & 0b1100_0000 == 0b0100_0000)\n}\n\nfn is_reserved(addr: Ipv4Addr) -> bool {\n    (addr.octets()[0] == 192 && addr.octets()[1] == 0 && addr.octets()[2] == 0)\n        || (addr.octets()[0] & 240 == 240 && !addr.is_broadcast())\n}\n\nfn is_benchmarking(addr: Ipv4Addr) -> bool {\n    addr.octets()[0] == 198 && (addr.octets()[1] & 0xfe) == 18\n}\n\nfn starts_with_zero(addr: Ipv4Addr) -> bool {\n    addr.octets()[0] == 0\n}\n\nfn is_unicast_link_local(addr: Ipv6Addr) -> bool {\n    (addr.segments()[0] & 0xffc0) == 0xfe80\n}\n\nfn is_unique_local(addr: Ipv6Addr) -> bool {\n    (addr.segments()[0] & 0xfe0<|fim_middle|>", "completion": "pub fn new(whitelist_nets: Arc<Vec<IpNet>>, whitelist_names: Arc<Vec<String>>) -> Self {\n        NonLocalDnsResolver {\n            state: Arc::new(Mutex::new(DnsState::Init)),\n            whitelist_nets,\n            whitelist_names,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "function_item", "line_range": [589, 595]}
{"prompt": "<|fim_prefix|>rtialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum MessageAttemptTriggerType {\n    Scheduled = 0,\n    Manual = 1,\n}\n\njsonschema_for_repr_enum! {\n    MessageAttemptTriggerType,\n    i16,\n    \"The reason an attempt was made:\\n- Scheduled = 0\\n- Manual = 1\",\n    Scheduled, Manual\n}\n\n#[repr(i16)]\n#[derive(Clone, Debug, Copy, PartialEq, IntoPrimitive, TryFromPrimitive, Hash, Eq)]\npub enum MessageStatus {\n    Success = 0,\n    Pending = 1,\n    Fail = 2,\n    Sending = 3,\n}\n\njsonschema_for_repr_enum! {\n    MessageStatus,\n    i16,\n    \"The sending status of the message:\\n- Success = 0\\n- Pending = 1\\n- Fail = 2\\n- Sending = 3\",\n    Success, Pending, Fail, Sending\n}\n\n#[derive(Clone, Debug, Copy, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub enum MessageStatusText {\n    Success,\n    Pending,\n    #[serde(alias = \"failed\")]\n    Fail,\n    Sending,\n}\n\nimpl From<MessageStatus> for MessageStatusText {\n    fn from(status: MessageStatus) -> Self {\n        match status {\n            MessageStatus::Success => Self::Success,\n            MessageStatus::Pending => Self::Pending,\n            MessageStatus::Fail => Self::Fail,\n            MessageStatus::Sending => Self::Sending,\n        }\n    }\n}\n\n#[repr(i16)]\n#[derive(Clone, Debug, Copy, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum StatusCodeClass {\n    CodeNone = 0,\n    Code1xx = 100,\n    Code2xx = 200,\n    Code3xx = 300,\n    Code4xx = 400,\n    Code5xx = 500,\n}\n\njsonschema_for_repr_enum! {\n    StatusCodeClass,\n    i16,\n    \"The different classes of HTTP status codes:\\n- CodeNone = 0\\n- Code1xx = 100\\n- Code2xx = 200\\n- Code3xx = 300\\n- Code4xx = 400\\n- Code5xx = 500\",\n    CodeNone, Code1xx, Code2xx, Code3xx, Code4xx, Code5xx\n}\n\nenum_wrapper!(MessageAttemptTriggerType);\nenum_wrapper!(MessageStatus);\nenum_wrapper!(StatusCodeClass);\n\n#[derive(Clone, Debug, Hash, Eq, PartialEq, Serialize)]\npub struct FeatureFlag(pub String);\n\ncommon_jsonschema_impl!(\n    FeatureFlag,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            min_length: None,\n            max_length: Some(256),\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"cool-new-feature\".to_string()),\n    }\n);\n\nstring_wrapper_impl!(FeatureFlag);\n\nimpl<'de> Deserialize<'de> for FeatureFlag {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).and_then(|s| {\n            validate_limited_str(&s).map_err(serde::de::Error::custom)?;\n            Ok(FeatureFlag(s))\n        })\n    }\n}\n\npub type FeatureFlagSet = HashSet<FeatureFlag>;\n\n#[cfg(test)]\nmod tests {\n    use std::collections::HashMap;\n\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use validator::Validate;\n\n    use super::{\n        validate_header_map, ApplicationId, ApplicationUid, EndpointHeaders, EndpointHeadersPatch,\n        EndpointSecret, EventChannel, EventTypeName,\n    };\n    use crate::core::cryptography::AsymmetricKey;\n\n    #[test]\n    fn test_id_validation() {\n        let app_id = ApplicationId(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_id.validate().unwrap();\n\n        let app_id = ApplicationId(\"badprefix_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_id.validate().is_err());\n\n        let app_uid = ApplicationUid(\"app_24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3xQhJnw8fSumZK\".to_owned());\n        app_uid.validate().unwrap();\n\n        // With a space\n        let app_uid = ApplicationUid(\"24NVKcPqNLXKu3 \".to_owned());\n        assert!(app_uid.validate().is_err());\n\n        // Check all allowed\n        let app_uid = ApplicationUid(\"azAZ09-_.\".to_owned());\n        app_uid.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let app_id = ApplicationId(long_str.clone());\n        assert!(app_id.validate().is_err());\n        l<|fim_suffix|>        assert!(app_uid.validate().is_err());\n\n        let empty_str: String = \"\".to_owned();\n        let app_id = ApplicationId(empty_str.clone());\n        assert!(app_id.validate().is_err());\n        let app_uid = ApplicationUid(empty_str);\n        assert!(app_uid.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_names_validation() {\n        // With a space\n        let evt_name = EventTypeName(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventTypeName(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let evt_name = EventTypeName(long_str);\n        assert!(evt_name.validate().is_err());\n\n        let empty_str = \"\".to_owned();\n        let evt_name = EventTypeName(empty_str);\n        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_event_channel_validation() {\n        // With a space\n        let evt_name = EventChannel(\"event \".to_owned());\n        assert!(evt_name.validate().is_err());\n\n        // Check all allowed\n        let evt_name = EventChannel(\"azAZ09-_.\".to_owned());\n        evt_name.validate().unwrap();\n\n        // Check length\n        let long_str: String = \"X\".repeat(300);\n        let evt_name = EventChannel(long_str);\n        assert!(evt_name.validate().is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), \"true\".to_owned()),\n            (\"also-valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        validate_header_map(&endpoint_headers.0).unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), \"true\".to_owned()),\n            (\"valid\".to_owned(), \"true\".to_owned()),\n        ]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), \"true\".to_owned())]);\n        let endpoint_headers = EndpointHeaders(hdr_map);\n        assert!(validate_header_map(&endpoint_headers.0).is_err());\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_validation() {\n        let hdr_map = HashMap::from([\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n            (\"also-valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        endpoint_headers.validate().unwrap();\n\n        let hdr_map = HashMap::from([\n            (\"invalid?\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([\n            (\"invalid\\0\".to_owned(), Some(\"true\".to_owned())),\n            (\"valid\".to_owned(), Some(\"true\".to_owned())),\n        ]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"User-Agent\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n\n        let hdr_map = HashMap::from([(\"X-Amz-\".to_string(), Some(\"true\".to_owned()))]);\n        let endpoint_headers = EndpointHeadersPatch(hdr_map);\n        assert!(endpoint_headers.validate().is_err());\n    }\n\n    #[test]\n    f<|fim_middle|>", "completion": "let app_uid = ApplicationUid(long_str);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "let_declaration", "line_range": [1457, 1457]}
{"prompt": "<|fim_prefix|>// Dead code is allowed because not everything is used in all of the tests\n#![allow(dead_code)]\n\n<|fim_suffix|>\n\nuse anyhow::{Context, Result};\nuse axum::response::IntoResponse;\nuse http::HeaderMap;\nuse reqwest::{Client, RequestBuilder, StatusCode};\nuse serde::{de::DeserializeOwned, Serialize};\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::ConfigurationInner,\n    core::{\n        security::generate_org_token,\n        types::{BaseId, OrganizationId},\n    },\n    setup_tracing,\n};\nuse tokio::sync::mpsc;\nuse tracing::instrument::WithSubscriber;\n\npub mod common_calls;\n\n#[derive(Clone)]\npub struct TestClient {\n    base_uri: String,\n    auth_header: String,\n    client: Client,\n}\n\nimpl TestClient {\n    pub fn set_auth_header(&mut self, auth_header: String) {\n        self.auth_header = format!(\"Bearer {auth_header}\");\n    }\n}\n\nimpl TestClient {\n    pub fn new(base_uri: String, auth_token: &str) -> TestClient {\n        TestClient {\n            base_uri,\n            auth_header: format!(\"Bearer {auth_token}\"),\n            client: Client::new(),\n        }\n    }\n\n    fn build_uri(&self, endpoint: &str) -> String {\n        format!(\"{}/{endpoint}\", self.base_uri)\n    }\n\n    fn add_headers(&self, request: RequestBuilder) -> RequestBuilder {\n        request.header(\"Authorization\", &self.auth_header)\n    }\n\n    pub async fn get<O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.get(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn get_without_response(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.get(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn post<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                resp.json()\n                    .await\n                    .context(\"error receiving/parsing response\")\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                Ok(())\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_with_idempotency<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        idempotency_key: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self\n            .add_headers(req)\n            .header(\"idempotency-key\", idempotency_key)\n            .json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json().await.context(\"error receiving/paring response\")\n    }\n\n    pub async fn put<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn put_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn delete(&self, endpoint: &str, expected_code: StatusCode) -> Result<()> {\n        let mut req = self.client.delete(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        if expected_code == StatusCode::NO_CONTENT {\n            let res_body = resp.text().await.context(\"error receiving response\")?;\n            anyhow::ensure!(res_body.is_empty());\n        }\n\n        Ok(())\n    }\n\n    pub async fn patch<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn patch_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expecte<|fim_middle|>", "completion": "use std::{\n    future::Future,\n    net::TcpListener,\n    sync::{Arc, Mutex},\n    time::Duration,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/mod.rs", "node_type": "use_declaration", "line_range": [4, 9]}
{"prompt": "<|fim_prefix|>     event_type_in(\"et1\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let ep_with_valid_event: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_with_valid_event.ep.event_types_ids.unwrap(), expected_et);\n\n    let ep_removed_events: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_valid_event.id),\n            ep_no_events.to_owned(),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(ep_removed_events.ep.event_types_ids.is_none());\n\n    let ep_removed_events = get_endpoint(&client, &app_id, &ep_removed_events.id)\n        .await\n        .unwrap();\n\n    assert!(ep_removed_events.ep.event_types_ids.is_none());\n\n    let ep_updated_events: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_valid_event.id),\n            ep_with_events.to_owned(),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_updated_events.ep.event_types_ids.unwrap(), expected_et);\n\n    let ep_updated_events: EndpointOut = get_endpoint(&client, &app_id, &ep_with_valid_event.id)\n        .await\n        .unwrap();\n\n    assert_eq!(ep_updated_events.ep.event_types_ids.unwrap(), expected_et);\n}\n\n#[tokio::test]\nasync fn test_endpoint_filter_channels() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    // Channels must not be empty:\n    let ep_empty_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"channels\": [],\n    });\n\n    let ep_with_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"channels\": [\"tag1\"],\n    });\n\n    let ep_without_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n    });\n\n    let expected_ec = EventChannelSet(HashSet::from([EventChannel(\"tag1\".to_owned())]));\n\n    let _ep_w_empty_channel: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_empty_channels,\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let ep_with_channel: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_channels.to_owned(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_with_channel.ep.channels.unwrap(), expected_ec);\n\n    let ep_with_deleted_channel: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_channel.id),\n            ep_without_channels,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(ep_with_deleted_channel.ep.channels.is_none());\n\n    // GET / assert channels empty\n    let ep_with_deleted_channel: EndpointOut = get_endpoint(&client, &app_id, &ep_with_channel.id)\n        .await\n        .unwrap();\n\n    assert!(ep_with_deleted_channel.ep.channels.is_none());\n\n    // Update with channels:\n    let updated_ep_with_channel: EndpointOut = client\n        .put(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{}/\",\n                ep_with_deleted_channel.id\n            ),\n            ep_with_channels,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(updated_ep_with_channel.ep.channels.unwrap(), expected_ec);\n\n    // GET / assert channels match\n    let updated_ep_with_channel: EndpointOut =\n        get_endpoint(&client, &app_id, &updated_ep_with_channel.id)\n            .await\n            .unwrap();\n\n    assert_eq!(updated_ep_with_channel.ep.channels.unwrap(), expected_ec);\n}\n\n#[tokio::test]\nasync fn test_rate_limit() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    l<|fim_suffix|>\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    assert_eq!(endp.ep.rate_limit.unwrap(), 100);\n\n    let endp = put_endpoint(\n        &client,\n        &app_id,\n        &endp.id,\n        EndpointIn {\n            rate_limit: None,\n            ..ep_in.clone()\n        },\n    )\n    .await\n    .unwrap();\n\n    assert!(endp.ep.rate_limit.is_none());\n\n    let endp = get_endpoint(&client, &app_id, &endp.id).await.unwrap();\n\n    assert!(endp.ep.rate_limit.is_none());\n}\n\n#[tokio::test]\nasync fn test_msg_event_types_filter() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    for et in [\n        event_type_in(\"et1\", None).unwrap(),\n        event_type_in(\"et2\", None).unwrap(),\n    ] {\n        let _: EventTypeOut = client\n            .post(\"api/v1/event-type/\", et, StatusCode::CREATED)\n            .await\n            .unwrap();\n    }\n\n    for event_types in [\n        Some(EventTypeNameSet(HashSet::from([EventTypeName(\n            \"et1\".to_owned(),\n        )]))),\n        Some(EventTypeNameSet(HashSet::from([\n            EventTypeName(\"et1\".to_owned()),\n            EventTypeName(\"et2\".to_owned()),\n        ]))),\n        None,\n    ] {\n        post_endpoint(\n            &client,\n            &app_id,\n            EndpointIn {\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                event_types_ids: event_types,\n                ..default_test_endpoint()\n            },\n        )\n        .await\n        .unwrap();\n    }\n\n    // Number of attempts should match based on event-types registered to endpoints\n    for (event_name, expected_count) in [\n        (EventTypeName(\"et1\".to_owned()), 3),\n        (EventTypeName(\"et2\".to_owned()), 2),\n    ] {\n        let msg: MessageOut = client\n            .post(\n                &format!(\"api/v1/app/{app_id}/msg/\"),\n                json!({\n                    \"eventType\": event_name,\n                    \"payload\": {},\n                    \"payloadRetentionPeriod\": 5,\n                }),\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        tokio::time::sleep(Duration::from_millis(10)).await;\n\n        let _list =\n            get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, expected_count)\n                .await\n                .unwrap();\n    }\n}\n\n#[tokio::test]\nasync fn test_msg_channels_filter() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ec = EventChannelSet(HashSet::from([EventChannel(\"tag1\".to_owned())]));\n\n    for channels in [Some(ec.clone()), None] {\n        let _endp = post_endpoint(\n            &client,\n            &app_id,\n            EndpointIn {\n                channels,\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n        )\n        .await\n        .unwrap();\n    }\n\n    for (channels, expected_count) in [(Some(&ec), 2), (None, 1)] {\n        let mut message_in = json!({\n            \"eventType\": \"et1\",\n            \"payload\": {},\n            \"payloadRetentionPeriod\": 5,\n        });\n        if let Some(channels) = channels {\n            message_in[\"channels\"] = json!(channels);\n        }\n\n        let msg: MessageOut = client\n            .post(\n                &format!(\"api/v1/app/{}/msg/\", &app_id),\n                message_in,\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        let _list =\n            get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, expected_count)\n                .await\n                .unwrap();\n\n        let msg: MessageOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(msg.channels.as_ref(), <|fim_middle|>", "completion": "let ep_in = EndpointIn {\n        rate_limit: Some(100),\n        ..default_test_endpoint()\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1880, 1883]}
{"prompt": "<|fim_prefix|>use aide::OperationInput;\nuse axum::{\n    async_trait,\n    extract::{FromRequestParts, Path},\n    http::request::Parts,\n};\n\nuse super::{\n    security::{permissions_from_bearer, AccessLevel, Permissions},\n    types::{ApplicationId, ApplicationIdOrUid, FeatureFlagSet, OrganizationId},\n};\nuse crate::{\n    db::models::{application, applicationmetadata},\n    error::{Error, HttpError, Result, Traceable},\n    AppState,\n};\n\npub struct ReadAll {\n    pub org_id: OrganizationId,\n    pub feature_flags: AllowedFeatureFlags,\n}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for ReadAll {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n        let org_id = permissions.org_id();\n        let feature_flags = match permissions.access_level {\n            AccessLevel::Organization(_) => AllowedFeatureFlags::All,\n            AccessLevel::Application(_, _) => AllowedFeatureFlags::Some(permissions.feature_flags),\n        };\n        Ok(Self {\n            org_id,\n            feature_flags,\n        })\n    }\n}\n\nimpl OperationInput for ReadAll {}\n\npub struct Organization {\n    pub org_id: OrganizationId,\n}\n\nimpl OperationInput for Organization {}\n\nimpl Permissions {\n    fn check_app_is_permitted(&self, app_id: &ApplicationId) -> Result<()> {\n        if let Some(ref permitted_app_id) = self.app_id() {\n            if permitted_app_id != app_id {\n                return Err(HttpError::not_found(None, None).into());\n            }\n        }\n        Ok(())\n    }\n}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for Organization {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n\n        let org_id = match permissions.access_level {\n            AccessLevel::Organization(org_id) => org_id,\n            _ => return Err(HttpError::permission_denied(None, None).into()),\n        };\n\n        Ok(Self { org_id })\n    }\n}\n\npub struct Application {\n    pub app: application::Model,\n}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for Application {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n\n        let Path(ApplicationPathParams { app_id }) =\n            Path::<ApplicationPathParams>::from_request_parts(parts, state).await?;\n        let app =\n            application::Entity::secure_find_by_id_or_uid(permissions.org_id(), app_id.to_owned())\n                .one(&state.db)\n                .await?\n                .ok_or_else(|| HttpError::not_found(None, None))?;\n\n        permissions.check_app_is_permitted(&app.id)?;\n\n        Ok(Self { app })\n    }\n}\n\nimpl OperationInput for Application {}\n\n// Organization level privileges, with the requested application\npub struct OrganizationWithApplication {\n    pub app: application::Model,\n}\n\nimpl OperationInput for OrganizationWithApplication {}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for OrganizationWithApplication {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let Organization { org_id } = Organization::from_request_parts(parts, state)\n            .await\n            .trace()?;\n\n        let Path(ApplicationPathParams { app_id }) =\n            Path::<ApplicationPathParams>::from_request_parts(parts, state).await?;\n        <|fim_suffix|>\n        Ok(OrganizationWithApplication { app })\n    }\n}\n\npub struct ApplicationWithMetadata {\n    pub app: application::Model,\n    pub metadata: applicationmetadata::Model,\n}\n\nimpl OperationInput for ApplicationWithMetadata {}\n\n#[async_trait]\nimpl FromRequestParts<AppState> for ApplicationWithMetadata {\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &AppState) -> Result<Self> {\n        let permissions = permissions_from_bearer(parts, state).await?;\n\n        let Path(ApplicationPathParams { app_id }) =\n            Path::<ApplicationPathParams>::from_request_parts(parts, state).await?;\n        let (app, metadata) = application::Model::fetch_with_metadata(\n            &state.db,\n            permissions.org_id(),\n            app_id.to_owned(),\n        )\n        .await\n        .trace()?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n        permissions.check_app_is_permitted(&app.id)?;\n\n        Ok(Self { app, metadata })\n    }\n}\n\n#[derive(serde::Deserialize)]\nstruct ApplicationPathParams {\n    app_id: ApplicationIdOrUid,\n}\n\n/// Denotes what features gated by feature flags the requester has access to.\npub enum AllowedFeatureFlags {\n    /// Requester has access to all features regardless of flags. No checking\n    /// of feature flags should be done for this request.\n    All,\n    /// Requester has access to a limited set of features. The set may be empty.\n    Some(FeatureFlagSet),\n}\n<|fim_middle|>", "completion": "let app = application::Entity::secure_find_by_id_or_uid(org_id, app_id.to_owned())\n            .one(&state.db)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/permissions.rs", "node_type": "let_declaration", "line_range": [121, 124]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for PATCH requests focused mostly around non-required field types.\n\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse validator::Validate;\n\n/// This is an enum that will wrap every nullable field for a PATCH request.\n///\n/// Nonnullable fields can be represented via an [`UnrequiredField`]. This\n/// differs from an [`Option`] in that it distinguishes null values and absent\n/// values such that an optional value in a model may be made `None` via\n/// PATCHing while allowing omitted fields to be skipped when updating.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredNullableField<T> {\n    #[default]\n    Absent,\n    None,\n    Some(T),\n}\n\n/// This enum is a non-nullable equivalent to [`UnrequiredNullableField`].\n///\n/// This is effectively an [`Option`] with the additional context that any field\n/// which uses this type is a member of a PATCH request model and that the field\n/// may be absent, meaning it is not to be updated. In comparison, [`Option`]s\n/// are used in other [`ModelIn`]s to define a field, that when absent, is\n/// `null`.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredField<T> {\n    #[default]\n    Absent,\n    Some(T),\n}\n\nimpl<T> UnrequiredNullableField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredNullableField::Absent)\n    }\n\n    p<|fim_suffix|>}\n\nimpl<T> UnrequiredField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredField<U> {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> From<Option<T>> for UnrequiredNullableField<T> {\n    fn from(opt: Option<T>) -> Self {\n        match opt {\n            Some(v) => UnrequiredNullableField::Some(v),\n            None => UnrequiredNullableField::None,\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredNullableField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n            UnrequiredNullableField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredField::Absent => Ok(()),\n            UnrequiredField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredNullableField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone + Copy> Copy for UnrequiredNullableField<T> {}\nimpl<T: Clone + Copy> Copy for UnrequiredField<T> {}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredNullableField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        Option::deserialize(deserializer).map(Into::into)\n    }\n}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        T::deserialize(deserializer).map(UnrequiredField::Some)\n    }\n}\n\nimpl<T> Serialize for UnrequiredNullableField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredNullableField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredNullableField must skip serializing if field is absent\",\n            )),\n            UnrequiredNullableField::None => serializer.serialize_none(),\n            UnrequiredNullableField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\nimpl<T> Serialize for UnrequiredField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredField must skip serializing if field is absent\",\n            )),\n            UnrequiredField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"Unrequired_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<T>()\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredNullableField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"UnrequiredNullable_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<Option<T>>()\n    }\n}\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`Option`] is `Some`, but\n/// performs no operation in the case it is `None`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The nullable equivalent which is used for [`UnrequiredNullableField`] is [`patch_field_nullable`].\nmacro_rules! patch_field_non_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n}\npub(crate) use patch_field_non_nullable;\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`UnrequiredNullableField`] is\n/// `Some` and unsetting the field when it is `None`, but performs no operation in the case it is\n///  `Absent`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The non-nullable equivalent which is used for [`Option`] is [`patch_field_non_nullable`].\nmacro_rules! patch_field_nullable {\n    ($model:ident, $member:ident) => {\n     <|fim_middle|>", "completion": "pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredNullableField<U> {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(f(v)),\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/patch.rs", "node_type": "function_item", "line_range": [49, 55]}
{"prompt": "<|fim_prefix|>use anyhow::Result;\nuse rand::distributions::{Distribution, Uniform};\n\nconst BASE62: &[u8; 62] = b\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\";\n\npub fn generate_token() -> Result<String> {\n    let between = Uniform::from(0..BASE62.len());\n    <|fim_suffix|>\n    let mut bytes = [0_u8; 27];\n    for b in &mut bytes {\n        *b = BASE62[between.sample(&mut rng)];\n    }\n    Ok(String::from_utf8(bytes.to_vec())?)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::generate_token;\n\n    #[test]\n    fn test_generate_token() {\n        let out1 = generate_token().unwrap();\n        let out2 = generate_token().unwrap();\n        let out3 = generate_token().unwrap();\n\n        // Sort of weak as far as assertions go, but this is the least we can expect, right?\n        assert_ne!(out1, out2, \"random tokens should be different\");\n        assert_ne!(out2, out3, \"random tokens should be different\");\n        assert_ne!(out3, out1, \"random tokens should be different\");\n    }\n}\n<|fim_middle|>", "completion": "let mut rng = rand::thread_rng();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/token.rs", "node_type": "let_declaration", "line_range": [8, 8]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Configuration-dependent queue tests. This will depend on the set environment variables as with\n//! the e2e tests such as to allow testing multiple queue backends via the test script.\n\nuse std::{str::FromStr, time::Duration};\n\nuse http::StatusCode;\nuse redis::AsyncCommands as _;\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::Configuration,\n    core::types::{\n        ApplicationId, BaseId, EndpointId, MessageAttemptTriggerType, MessageId, OrganizationId,\n    },\n    queue::{\n        new_pair, MessageTask, QueueTask, TaskQueueConsumer, TaskQueueDelivery, TaskQueueProducer,\n    },\n    redis::RedisManager,\n    v1::endpoints::message::MessageOut,\n};\nuse tokio::time::timeout;\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id_and_prefix,\n};\n\n// TODO: Don't copy this from the Redis queue test directly, place the fn somewhere both can access\na<|fim_suffix|>\nfn task_queue_delivery_to_u16(tqd: &TaskQueueDelivery) -> u16 {\n    match &*tqd.task {\n        QueueTask::HealthCheck => panic!(\"Health check in test\"),\n        QueueTask::MessageBatch(batch) => u16::from_str(batch.msg_id.as_str()).unwrap(),\n        QueueTask::MessageV1(task) => u16::from_str(task.msg_id.as_str()).unwrap(),\n    }\n}\n\nasync fn test_many_queue_consumers_inner(prefix: &str, delay: Option<Duration>) {\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n\n    // This test assumes an empty queue, so load Redis and delete the test key\n    {\n        let pool = get_pool(&cfg).await;\n        let mut conn = pool.get().await.unwrap();\n\n        let _: () = conn\n            .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n            .await\n            .unwrap();\n    }\n\n    // Make 20 producers and 20 consumers using the same configuration\n    let mut producers_and_consumers: Vec<(TaskQueueProducer, TaskQueueConsumer)> = Vec::new();\n    for _ in 0..20 {\n        producers_and_consumers.push(new_pair(&cfg, Some(prefix)).await);\n    }\n\n    // Add 200 test messagesÂ¹ with unique message IDs to each producer for a\n    // total of 4000 unique messages\n    //\n    // Â¹ it is important for this number to be no smaller than MAX_MESSAGES in\n    //   TaskQueueConsumer::receive_all\n    for (index, (p, _c)) in producers_and_consumers.iter().enumerate() {\n        for num in 0..200 {\n            p.send(\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"{}\", index * 200 + num)),\n                    app_id: ApplicationId(\"TestApplicationId\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointId\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                }),\n                delay,\n            )\n            .await\n            .unwrap();\n        }\n    }\n\n    let mut join_handles = Vec::new();\n    // Producers need to stay alive for the remainder of the test for in-memory queue which uses\n    // [`tokio::mpsc`]s, so add them to this [`Vec`]\n    let mut producers = Vec::new();\n\n    // Ensure that consumers run on separate OS threads and receive messages until 500ms has passed\n    // without any messages\n    for (p, mut c) in producers_and_consumers {\n        producers.push(p);\n        let handle = tokio::runtime::Handle::current();\n        join_handles.push(std::thread::spawn(move || {\n            handle.block_on(async move {\n                let mut out = Vec::new();\n                let mut read = 0;\n\n                while let Ok(recv) = timeout(\n                    Duration::from_secs(1),\n                    c.receive_all(Duration::from_secs(5)),\n                )\n                .await\n                {\n                    let recv = recv.unwrap();\n                    read += recv.len();\n                    for r in recv {\n                        out.push(task_queue_delivery_to_u16(&r));\n                        r.ack().await.unwrap();\n                    }\n                }\n\n                (out, read)\n            })\n        }));\n    }\n\n    // Create a Vec with all the threads' outputs\n    let mut out = Vec::new();\n    for jh in join_handles {\n        let (mut jh_out, read): (Vec<u16>, usize) = jh.join().unwrap();\n        out.append(&mut jh_out);\n\n        if read < 20 {\n            panic!(\"Consumer starved, only read {read} messages\");\n        }\n    }\n\n    // Sort it by the message ID\n    out.sort();\n\n    // Then assert that all the messages are there\n    assert_eq!(out.len(), 4000);\n    for (idx, &num) in out.iter().enumerate() {\n        assert_eq!(idx, num as usize);\n    }\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n// run with `cargo test -- --ignored redis` only when redis is up and configured\n#[ignore]\nasync fn test_many_queue_consumers() {\n    test_many_queue_consumers_inner(\"test_many_queue_consumers_\", None).await;\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n#[ignore]\nasync fn test_many_queue_consumers_delayed() {\n    test_many_queue_consumers_inner(\n        \"test_many_queue_consumers_delayed_\",\n        Some(Duration::from_millis(500)),\n    )\n    .await;\n}\n\n#[tokio::test]\n#[ignore]\nasync fn test_redis_streams_dlq() {\n    let mut cfg = get_default_test_config();\n    cfg.worker_enabled = false;\n    cfg.redis_pending_duration_secs = 1;\n\n    let cfg = std::sync::Arc::new(cfg);\n    let prefix = svix_ksuid::Ksuid::new(None, None).to_string();\n\n    let pool = get_pool(&cfg).await;\n    let mut conn = pool.get().await.unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n        .await\n        .unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_dlq\"))\n        .await\n        .unwrap();\n\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id_and_prefix(\n        &cfg,\n        OrganizationId::new(None, None),\n        prefix.clone(),\n    )\n    .await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let _message_1: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({\"test\": \"value\"})).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let (_p, mut c) = new_pair(&cfg, Some(&prefix)).await;\n\n    let wait_time = std::time::Duration::from_millis(1_500);\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n\n    tokio::time::sleep(wait_time).await;\n\n    // Redrive\n    client\n        .post_without_response(\n            \"/api/v1/admin/redrive-dlq\",\n            serde_json::Value::Null,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n}\n<|fim_middle|>", "completion": "async fn get_pool(cfg: &Configuration) -> RedisManager {\n    RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/redis_queue.rs", "node_type": "function_item", "line_range": [31, 33]}
{"prompt": "<|fim_prefix|>\npub struct RequestFromParts {\n    headers: HeaderMap,\n    payload: Bytes,\n}\n\n/// A simple marker trait to denote the state of a [`SerializableRequest`]. The only way to publicly\n/// construct any [`SerializableRequest<Validated>`]s  is via the associated method on unvalidated\n/// request's, [`SerializableRequest<Unvalidated>::validate`].\npub trait RequestState {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Unvalidated;\nimpl RequestState for Unvalidated {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Validated;\nimpl RequestState for Validated {}\n\n/// This intermediary representation is necessary because it is preferable to serialize the headers\n/// and/or body as a [`String`] over bytes when dealing with some [`VerificationMethod`]s and some\n/// [`ForwardingMethod`]s. This struct represents both the headers and body as enums which allow for\n/// either textual representations or byte representations when [`Serialize`]d via [`serde`].\n///\n/// On trying to convert a [`Standard`] variant into a [`StringSerializable`] variant, HTTP headers\n/// will be represented textually if and only if they are completely ASCII, while any bodies will\n/// attempt to be read as UTF-8 before falling back to bytes.\n///\n/// NOTE: This conversion *should* be lazy. The [`String`] variant are only acceptable in a subset\n/// of all cases, so lazy-conversion will prevent needless conversion back and forth. You may check\n/// whether the conversion is required and/or helpful with [`VerificationMethod::want_string_rep`]\n/// or [`VerificationMethod::need_string_rep`] plus the [`ForwardingMethod`] equivalents.\n///\n/// The intended course of action is to attempt to convert to string-serializable variants of the\n/// header map and the body immediately if either of the aforementioned methods are true --  but\n/// only returning an [`Err`] response if it *needs* it. Then, if the validation is a success (see\n/// [`SerializableRequest<Unvalidated>::validate`] and a validated equivalent is returned, then the\n/// same checks are to be performed, but with the [`ForwardingMethod`] methods before being sent to\n/// the appropriate [`ForwardingMethod`] implementor.\n#[derive(Clone, Debug, Serialize)]\npub struct SerializableRequest<S: RequestState> {\n    headers: SerializableHeaderMap,\n    payload: SerializablePayload,\n\n    #[serde(skip)]\n    _pd: PhantomData<S>,\n}\n\nimpl<S: RequestState> SerializableRequest<S> {\n    pub fn headers(&self) -> &SerializableHeaderMap {\n        &self.headers\n    }\n\n    pub fn payload(&self) -> &SerializablePayload {\n        &self.payload\n    }\n}\n\nimpl From<RequestFromParts> for SerializableRequest<Unvalidated> {\n    fn from(value: RequestFromParts) -> Self {\n        Self {\n            headers: SerializableHeaderMap::Standard(value.headers),\n            payload: SerializablePayload::Standard(value.payload.to_vec()),\n\n            _pd: PhantomData,\n        }\n    }\n}\n\n#[async_trait]\nimpl<S> FromRequest<S> for SerializableRequest<Unvalidated>\nwhere\n    S: Send + Sync,\n{\n    type Rejection = <RequestFromParts as FromRequest<S>>::Rejection;\n\n    async fn from_request(req: Request, state: &S) -> Result<Self, Self::Rejection> {\n        RequestFromParts::from_request(req, state)\n            .await\n            .map(Into::into)\n    }\n}\n\nimpl SerializableRequest<Unvalidated> {\n    /// Given a specific validator\n    pub async fn validate<V: VerificationMethod>(\n        mut self,\n        verifier: &V,\n    ) -> Result<SerializableRequest<Validated>, http::StatusCode> {\n        // Do relevant conversions to [`String`] representations if wanted/needed\n        match (verifier.want_string_rep(), verifier.need_string_rep()) {\n            // Needed\n            (true, true) | (false, true) => {\n                self.headers = self\n                    .headers\n                    .try_to_string()\n                    .map_err(|_| http::StatusCode::BAD_REQUEST)?;\n                self.payload = self\n                    .payload\n                    .try_to_string()\n                    .map_err(|_| http::StatusCode::BAD_REQUEST)?;\n            }\n\n            // Wanted, but not needed\n            (true, false) => {\n                self.headers = match self.headers.try_to_string() {\n                    Ok(h) => h,\n                    Err(h) => h,\n                };\n\n                self.payload = match self.payload.try_to_string() {\n                    Ok(p) => p,\n                    Err(p) => p,\n                };\n            }\n\n            // Not wanted\n            (false, false) => {}\n        };\n\n        // FIXME: No cloning\n        // Then actually use the [`VerificationMethod`] implementor.\n        match verifier.validate(self.clone()).await {\n            Ok(true) => Ok(SerializableRequest::<Validated> {\n                headers: self.headers,\n                payload: self.payload,\n\n                _pd: PhantomData,\n            }),\n\n            Ok(false) => {\n                // FIXME: Read config to know whether to log\n                Err(http::StatusCode::BAD_REQUEST)\n            }\n\n            Err(e) => {\n                tracing::error!(\"Error validating request: {}\", e);\n                Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n            }\n        }\n    }\n}\n\n#[derive(Clone, Debug)]\npub enum SerializableHeaderMap {\n    Standard(HeaderMap),\n    StringSerializable(HashMap<String, String>),\n}\n\n<|fim_suffix|>\n\nimpl SerializableHeaderMap {\n    pub fn try_to_string(self) -> Result<Self, Self> {\n        match self {\n            Self::Standard(header_map) => Ok(Self::StringSerializable(\n                header_map\n                    .iter()\n                    .map(|(name, value)| Ok((name.as_str().to_owned(), value.to_str()?.to_owned())))\n                    .collect::<Result<HashMap<String, String>>>()\n                    .map_err(|_| Self::Standard(header_map))?,\n            )),\n            Self::StringSerializable(hash_map) => Ok(Self::StringSerializable(hash_map)),\n        }\n    }\n\n    pub fn len(&self) -> usize {\n        match self {\n            Self::Standard(m) => m.len(),\n            Self::StringSerializable(m) => m.len(),\n        }\n    }\n}\n\n/// Serialize is not implemented on [`HeaderMap`]s themselves, so custom serialization is required.\nimpl Serialize for SerializableHeaderMap {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            Self::Standard(header_map) => header_map\n                .iter()\n                .map(|(name, value)| (name.as_str().to_owned(), value.as_bytes().to_vec()))\n                .collect::<HashMap<String, Vec<u8>>>()\n                .serialize(serializer),\n            Self::StringSerializable(hash_map) => hash_map.serialize(serializer),\n        }\n    }\n}\n\npub enum SerializableHeaderMapIter<'a> {\n    HeaderMap(http::header::Iter<'a, HeaderValue>),\n    HashMap(std::collections::hash_map::Iter<'a, String, String>),\n}\n\nimpl<'a> Iterator for SerializableHeaderMapIter<'a> {\n    type Item = (&'a str, &'a [u8]);\n\n    fn next(&mut self) -> Option<Self::Item> {\n        match self {\n            Self::HeaderMap(hm) => hm.next().map(|(k, v)| (k.as_str(), v.as_bytes())),\n            Self::HashMap(hm) => hm.next().map(|(k, v)| (k.as_str(), v.as_bytes())),\n        }\n    }\n}\n\n#[derive(Clone, Debug, Serialize)]\n#[serde(untagged)]\npub enum SerializablePayload {\n    Standard(Vec<u8>),\n    StringSerializable(String),\n}\n\nimpl SerializablePayload {\n    fn try_to_string(self) -> Result<Self, Self> {\n        match self {\n            Self::Standard(v) => Ok(Self::StringSerializable(\n                String::from_utf8(v).map_err(|e| Self::Standard(e.into_bytes()))?,\n            )),\n            Self::StringSerializable(s) => Ok(Self::StringSerializable(s)),\n        }\n    }\n\n    pub fn as_json(&self) -> Result<serde_json::Value> {\n        Ok(match self {\n            Self::Standard(v) => serde_json::from_slice(v)?,\n            Self::StringSerializable(s) => serde_json::from_str(s)?,\n        })\n    }\n\n    pub fn as_string(&self) -> Result<String> {\n        match self {\n            Self::Standard(v) => Ok(String::from_utf8(v.clone())?),\n            Self::StringSerializable(s) => Ok(s.clone()),\n        }\n    }\n}\n<|fim_middle|>", "completion": "impl<'a> IntoIterator for &'a SerializableHeaderMap {\n    type Item = (&'a str, &'a [u8]);\n    type IntoIter = SerializableHeaderMapIter<'a>;\n\n    fn into_iter(self) -> Self::IntoIter {\n        match self {\n            SerializableHeaderMap::Standard(hm) => SerializableHeaderMapIter::HeaderMap(hm.iter()),\n            SerializableHeaderMap::StringSerializable(hm) => {\n                SerializableHeaderMapIter::HashMap(hm.iter())\n            }\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/types.rs", "node_type": "impl_item", "line_range": [293, 305]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse super::OperationalWebhookEndpoint;\nuse crate::Configuration;\n\npub struct OperationalWebhook<'a> {\n    cfg: &'a Configuration,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl<'a> OperationalWebhook<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    pub fn endpoint(&self) -> OperationalWebhookEndpoint<'a> {\n        OperationalWebhookEndpoint::new(self.cfg)\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/operational_webhook.rs", "node_type": "impl_item", "line_range": [9, 17]}
{"prompt": "<|fim_prefix|>use opentelemetry::metrics::{Meter, ObservableGauge};\nuse redis::{streams::StreamPendingReply, AsyncCommands as _};\n\nuse super::init_metric;\nuse crate::{\n    error::{Error, Result},\n    redis::RedisManager,\n};\n\npub enum RedisQueueType<'a> {\n    Stream(&'a str),\n    StreamPending { stream: &'a str, group: &'a str },\n    List(&'a str),\n    SortedSet(&'a str),\n}\n\nimpl RedisQueueType<'_> {\n    pub async fn queue_depth(&self, redis: &RedisManager) -> Result<u64> {\n        let mut conn = redis.get().await?;\n        match self {\n            RedisQueueType::Stream(q) => conn\n                .xlen(q)\n                .await\n                .map_err(|e| Error::queue(format_args!(\"Failed to query queue depth: {e}\"))),\n            RedisQueueType::StreamPending { stream, group } => {\n                let reply: StreamPendingReply = conn.xpending(stream, group).await?;\n                Ok(reply.count() as _)\n            }\n            RedisQueueType::List(q) => conn\n                .llen(q)\n                .await\n                .map_err(|e| Error::queue(format_args!(\"Failed to query queue depth: {e}\"))),\n            RedisQueueType::SortedSet(q) => conn\n                .zcard(q)\n                .await\n                .map_err(|e| Error::queue(format_args!(\"Failed to query queue depth: {e}\"))),\n        }\n    }\n}\n\n#[derive(Clone)]\npub struct RedisQueueMetrics {\n    main_queue: Option<ObservableGauge<u64>>,\n    pending_queue: Option<ObservableGauge<u64>>,\n    delayed_queue: Option<ObservableGauge<u64>>,\n    deadletter_queue: Option<ObservableGauge<u64>>,\n}\n\nimpl RedisQueueMetrics {\n    pub fn new(meter: &Meter) -> Self {\n        let main_queue = init_metric(\n            meter\n                .u64_observable_gauge(\"svix.queue.depth_main\")\n                .try_init(),\n        );\n\n        let pending_queue = init_metric(\n            meter\n                .u64_observable_gauge(\"svix.queue.pending_msgs\")\n                .try_init(),\n        );\n\n        let delayed_queue = init_metric(\n            meter\n                .u64_observable_gauge(\"svix.queue.depth_delayed\")\n                .try_init(),\n        );\n\n        let deadletter_queue = init_metric(\n            meter\n                .u64_observable_gauge(\"svix.queue.depth_dlq\")\n                .try_init(),\n        );\n\n        Self {\n            main_queue,\n            pending_queue,\n            delayed_queue,\n            deadletter_queue,\n        }\n    }\n    pub async fn record(\n        &self,\n        redis: &RedisManager,\n        main_queue: &RedisQueueType<'_>,\n        pending_queue: &RedisQueueType<'_>,\n        delayed_queue: &RedisQueueType<'_>,\n        deadletter_queue: &RedisQueueType<'_>,\n    ) {\n        main_queue\n            .queue_depth(redis)\n            .await\n            .map(|d| self.record_main_queue_depth(d))\n            .unwrap_or_else(|e| {\n                tracing::warn!(\"Failed to record queue depth: {e}\");\n            });\n        pending_queue\n            .queue_depth(redis)\n            .await\n            .map(|d| self.record_pending_queue_depth(d))\n            .unwrap_or_else(|e| {\n                tracing::warn!(\"Failed to record queue depth: {e}\");\n            });\n        delayed_queue\n            .queue_depth(redis)\n            .await\n            .map(|d| self.record_delayed_queue_depth(d))\n            .unwrap_or_else(|e| {\n                tracing::warn!(\"Failed to record queue depth: {e}\");\n            });\n        deadletter_queue\n            .queue_depth(redis)\n            .await\n            .map(|d| self.record_deadletter_queue_depth(d))\n            .unwrap_or_else(|e| {\n                tracing::warn!(\"Failed to record queue depth: {e}\");\n            });\n    }\n\n    fn record_main_queue_depth(&self, value: u64) {\n        if let Some(recorder) = &self.main_queue {\n            recorder.observe(value, &[]);\n        }\n    }\n    <|fim_suffix|>\n    fn record_delayed_queue_depth(&self, value: u64) {\n        if let Some(recorder) = &self.delayed_queue {\n            recorder.observe(value, &[]);\n        }\n    }\n    fn record_deadletter_queue_depth(&self, value: u64) {\n        if let Some(recorder) = &self.deadletter_queue {\n            recorder.observe(value, &[]);\n        }\n    }\n}\n<|fim_middle|>", "completion": "fn record_pending_queue_depth(&self, value: u64) {\n        if let Some(recorder) = &self.pending_queue {\n            recorder.observe(value, &[]);\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/metrics/redis.rs", "node_type": "function_item", "line_range": [125, 129]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse js_option::JsOption;\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct EndpointPatch {\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub channels: JsOption<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub description: Option<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub disabled: Option<bool>,\n\n    #[serde(rename = \"filterTypes\")]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub filter_types: JsOption<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub metadata: Option<std::collections::HashMap<String, String>>,\n\n    #[serde(rename = \"rateLimit\")]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub rate_limit: JsOption<u16>,\n\n    /// The endpoint's verification secret.\n    ///\n    /// Format: `base64` encoded random bytes optionally prefixed with `whsec_`.\n    /// It is recommended to not set this and let the server generate the\n    /// secret.\n    #[deprecated]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub secret: JsOption<String>,\n\n    /// The Endpoint's UID.\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub uid: JsOption<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub url: Option<String>,\n\n    #[deprecated]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub version: Option<u16>,\n}\n\nimpl EndpointPatch {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new() -> Self {\n        #[allow(deprecated)]\n        Self {\n            channels: JsOption::Undefined,\n            description: None,\n            disabled: None,\n            filter_types: JsOption::Undefined,\n            metadata: None,\n            rate_limit: JsOption::Undefined,\n            secret: JsOption::Undefined,\n            uid: JsOption::Undefined,\n            url: None,\n            version: None,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/endpoint_patch.rs", "node_type": "function_item", "line_range": [49, 63]}
{"prompt": "<|fim_prefix|>//! Allocator stats are only available when we're using jemalloc, and jemalloc doesn't work on windows.\n//!\n//! 2 impls for the helper functions are therefore provided. One set that does nothing (for windows)\n//! and another that works in the non-windows world.\n//!\n//! Care should be taken to keep the signatures aligned between these two so the callsites can be\n//! used consistently regardless of whether jemalloc is in use or not.\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\npub use supported::*;\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\npub use unsupported::*;\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\nmod supported {\n    use std::sync::Arc;\n\n    use tikv_jemalloc_ctl::{epoch, stats};\n\n    pub struct AllocatorStatMibs {\n        epoch: tikv_jemalloc_ctl::epoch_mib,\n        allocated: stats::allocated_mib,\n        resident: stats::resident_mib,\n    }\n\n    pub fn get_allocator_stats(\n        bust_cache: bool,\n        mibs: &AllocatorStatMibs,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        if bust_cache {\n            // Stats are cached internally and advancing the epoch is a way to invalidate those caches.\n            mibs.epoch.advance()?;\n        }\n        <|fim_suffix|>\n        let resident = mibs.resident.read()?;\n        Ok(Some((allocated, resident)))\n    }\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        let e = epoch::mib()?;\n        let allocated = stats::allocated::mib()?;\n        let resident = stats::resident::mib()?;\n\n        Ok(Arc::new(AllocatorStatMibs {\n            epoch: e,\n            allocated,\n            resident,\n        }))\n    }\n}\n\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\nmod unsupported {\n    use std::sync::Arc;\n\n    use anyhow::anyhow;\n\n    pub struct AllocatorStatMibs;\n\n    pub fn get_allocator_stats(\n        _bust_cache: bool,\n        _mibs: Arc<AllocatorStatMibs>,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        Ok(None)\n    }\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        Err(anyhow!(\"metric collection is not supported\"))\n    }\n}\n<|fim_middle|>", "completion": "let allocated = mibs.allocated.read()?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/allocator.rs", "node_type": "let_declaration", "line_range": [34, 34]}
{"prompt": "<|fim_prefix|>        // Failures are forgiven after double the `disable_in` `Duration` with the expiry of\n                // the Redis key\n                disable_in * 2,\n            )\n            .await\n            .map_err(Error::generic)?;\n\n        Ok(None)\n    }\n}\n\n/// Sign a message\nfn sign_msg(\n    main_secret: &Encryption,\n    timestamp: i64,\n    body: &str,\n    msg_id: &MessageId,\n    endpoint_signing_keys: &[&EndpointSecretInternal],\n) -> String {\n    let to_sign = format!(\"{msg_id}.{timestamp}.{body}\");\n\n    endpoint_signing_keys\n        .iter()\n        .format_with(\" \", |x, f| {\n            let sig = x.sign(main_secret, to_sign.as_bytes());\n            let version = match x.type_() {\n                EndpointSecretType::Hmac256 => \"v1\",\n                EndpointSecretType::Ed25519 => \"v1a\",\n            };\n\n            f(&format_args!(\"{version},{}\", STANDARD.encode(sig)))\n        })\n        .to_string()\n}\n\n/// Generates a set of headers for any one webhook event\nfn generate_msg_headers(\n    timestamp: i64,\n    msg_id: &MessageId,\n    signatures: String,\n    whitelabel_headers: bool,\n    configured_headers: Option<&EndpointHeaders>,\n    _endpoint_url: &str,\n) -> Result<CaseSensitiveHeaderMap> {\n    let mut headers = CaseSensitiveHeaderMap::new();\n    let id_hdr = msg_id\n        .0\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message id: {e:?}\")))?;\n    let timestamp = timestamp\n        .to_string()\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message timestamp: {e:?}\")))?;\n    let signatures_str = signatures\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message signatures: {e:?}\")))?;\n    if whitelabel_headers {\n        headers.insert(\"webhook-id\".to_owned(), id_hdr);\n        headers.insert(\"webhook-timestamp\".to_owned(), timestamp);\n        headers.insert(\"webhook-signature\".to_owned(), signatures_str);\n    } else {\n        headers.insert(\"svix-id\".to_owned(), id_hdr);\n        headers.insert(\"svix-timestamp\".to_owned(), timestamp);\n        headers.insert(\"svix-signature\".to_owned(), signatures_str);\n    }\n    headers.insert(\n        \"user-agent\".to_owned(),\n        USER_AGENT.to_string().parse().unwrap(),\n    );\n    headers.insert(\n        \"content-type\".to_owned(),\n        \"application/json\".parse().unwrap(),\n    );\n    if let Some(configured_headers) = configured_headers {\n        for (k, v) in &configured_headers.0 {\n            match v.parse() {\n                Ok(v) => {\n                    headers.insert(k.clone(), v);\n                }\n                Err(e) => {\n                    tracing::error!(\"Invalid HeaderValue {}: {}\", v, e);\n                }\n            }\n        }\n    }\n\n    Ok(headers)\n}\n\n#[derive(Clone)]\nstruct WorkerContext<'a> {\n    cfg: &'a Configuration,\n    cache: &'a Cache,\n    db: &'a DatabaseConnection,\n    queue_tx: &'a TaskQueueProducer,\n    op_webhook_sender: &'a OperationalWebhookSender,\n    webhook_client: &'a WebhookClient,\n}\n\nstruct FailedDispatch(messageattempt::ActiveModel, Error);\nstruct SuccessfulDispatch(messageattempt::ActiveModel);\n\n#[allow(clippy::large_enum_variant)]\nenum IncompleteDispatch {\n    Pending(PendingDispatch),\n    #[allow(dead_code)]\n    Failed(FailedDispatch),\n}\n\nstruct PendingDispatch {\n    method: http::Method,\n    url: String,\n    headers: CaseSensitiveHeaderMap,\n    payload: String,\n    request_timeout: u64,\n    created_at: DateTimeUtc,\n}\n\n// Clippy fails to compute the first variant's size, stating it as\n// \"at least 0 bytes\". They're actually very similar in size.\n#[allow(clippy::large_enum_variant)]\nenum CompletedDispatch {\n    Failed(FailedDispatch),\n    Successful(SuccessfulDispatch),\n}\n\n#[tracing::instrument(skip_all)]\nasync fn prepare_dispatch(\n    WorkerContext { cfg, .. }: &WorkerContext<'_>,\n    DispatchContext {\n        msg_task,\n        payload,\n        endp,\n        ..\n    }: DispatchContext<'_>,\n) -> Result<IncompleteDispatch> {\n    let attempt_created_at = Utc::now();\n\n    let headers = {\n        let keys = endp.valid_signing_keys();\n\n        l<|fim_suffix|>\n        generate_msg_headers(\n            attempt_created_at.timestamp(),\n            &msg_task.msg_id,\n            signatures,\n            cfg.whitelabel_headers,\n            endp.headers.as_ref(),\n            &endp.url,\n        )?\n    };\n\n    Ok(IncompleteDispatch::Pending(PendingDispatch {\n        method: http::Method::POST,\n        url: endp.url.clone(),\n        headers,\n        payload: payload.to_owned(),\n        request_timeout: cfg.worker_request_timeout as _,\n        created_at: attempt_created_at,\n    }))\n}\n\n#[tracing::instrument(skip_all)]\nasync fn make_http_call(\n    DispatchContext {\n        msg_task,\n        endp,\n        db_attempt_number,\n        ..\n    }: DispatchContext<'_>,\n    PendingDispatch {\n        method,\n        url,\n        headers,\n        payload,\n        request_timeout,\n        created_at,\n    }: PendingDispatch,\n    client: &WebhookClient,\n) -> Result<CompletedDispatch> {\n    let req = RequestBuilder::new()\n        .method(method)\n        .uri_str(&url)\n        .map_err(|e| Error::validation(format_args!(\"URL is invalid: {e:?}\")))?\n        .headers(headers)\n        .body(payload.into(), HeaderValue::from_static(\"application/json\"))\n        .version(Version::HTTP_11)\n        .timeout(Duration::from_secs(request_timeout))\n        .build()\n        .map_err(Error::generic)?;\n\n    let attempt = messageattempt::ActiveModel {\n        // Set both ID and created_at to the same timestamp\n        id: Set(MessageAttemptId::new(created_at.into(), None)),\n        created_at: Set(created_at.into()),\n        msg_id: Set(msg_task.msg_id.clone()),\n        endp_id: Set(endp.id.clone()),\n        msg_dest_id: Set(None),\n        url: Set(endp.url.clone()),\n        ended_at: Set(Some(Utc::now().into())),\n        trigger_type: Set(msg_task.trigger_type),\n        response_duration_ms: Set(0), // Default to 0, will be updated after the request\n        attempt_number: Set(db_attempt_number),\n        ..Default::default()\n    };\n\n    match client.execute(req).await {\n        Ok(res) => {\n            // Calculate the duration in milliseconds\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            let status_code = res.status().as_u16() as i16;\n            let status = if res.status().is_success() {\n                MessageStatus::Success\n            } else {\n                MessageStatus::Fail\n            };\n\n            let http_error = if !res.status().is_success() {\n                Some(WebhookClientError::FailureStatus(res.status()))\n            } else {\n                None\n            };\n\n            let body = match res.into_body().collect().await {\n                Ok(collected) => {\n                    let bytes = collected.to_bytes();\n                    if bytes.len() > RESPONSE_MAX_SIZE {\n                        bytes_to_string(bytes.slice(..RESPONSE_MAX_SIZE))\n                    } else {\n                        bytes_to_string(bytes)\n                    }\n                }\n                Err(err) => format!(\"Error reading response body: {err}\"),\n            };\n\n            let attempt = messageattempt::ActiveModel {\n                response_status_code: Set(status_code),\n                response: Set(body),\n                status: Set(status),\n                response_duration_ms: Set(duration_ms),\n                ..attempt\n            };\n\n            match http_error {\n                Some(err) => Ok(CompletedDispatch::Failed(FailedDispatch(\n                    attempt,\n                    Error::generic(err),\n                ))),\n                None => Ok(CompletedDispatch::Successful(SuccessfulDispatch(attempt))),\n            }\n        }\n        Err(err) => {\n            // For errors, we still calculate the duration\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            Ok(CompletedDispatch::Failed(FailedDispatch(\n                messageattempt::ActiveModel {\n                    response_status_code: Set(0),\n                    response: Set(err.to_string()),\n                    status: Set(MessageStatus::Fail),\n                    re<|fim_middle|>", "completion": "let signatures = sign_msg(\n            &cfg.encryption,\n            attempt_created_at.timestamp(),\n            payload,\n            &msg_task.msg_id,\n            &keys,\n        );", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "let_declaration", "line_range": [295, 301]}
{"prompt": "<|fim_prefix|>use serde::Deserialize;\nuse svix_bridge_types::{\n    ReceiverOutput, SenderInput, SenderOutputOpts, TransformationConfig, TransformerInputFormat,\n};\n\nuse crate::sender_input::QueueSender;\npub use crate::{\n    gcp_pubsub::{GcpPubSubInputOpts, GcpPubSubOutputOpts},\n    rabbitmq::{RabbitMqInputOpts, RabbitMqOutputOpts},\n    receiver_output::QueueForwarder,\n    redis::{RedisInputOpts, RedisOutputOpts},\n    sqs::{SqsInputOpts, SqsOutputOpts},\n};\n\npub fn into_sender_input(\n    name: String,\n    input_opts: QueueInputOpts,\n    transformation: Option<TransformationConfig>,\n    output: SenderOutputOpts,\n) -> Result<Box<dyn SenderInput>, &'static str> {\n    // FIXME: see if this check is still needed. String transforms worked for the omniqueue redis receiver, I think?\n    if matches!(input_opts, QueueInputOpts::Redis(_))\n        && transformation\n            .as_ref()\n            .map(|t| t.format() != TransformerInputFormat::Json)\n            .unwrap_or_default()\n    {\n        return Err(\"redis only supports json formatted transformations\");\n    }\n\n    Ok(Box::new(QueueSender::new(\n        name,\n        input_opts,\n        transformation,\n        output,\n    )))\n}\n\npub async fn into_receiver_output(\n    name: String,\n    opts: QueueOutputOpts,\n    // Annoying to have to pass this, but certain backends (redis) only work with certain transformations (json).\n    transformation: Option<&TransformationConfig>,\n) -> Result<Box<dyn ReceiverOutput>, crate::Error> {\n    // FIXME: see if this check is still needed. String transforms worked for the omniqueue redis receiver, I think?\n    if matches!(opts, QueueOutputOpts::Redis(_))\n        && transformation\n            .as_ref()\n            .map(|t| t.format() != TransformerInputFormat::Json)\n            .unwrap_or_default()\n    {\n        return Err(crate::Error::Generic(\n            \"redis only supports json formatted transformations\".to_string(),\n        ));\n    }\n\n    let forwarder = QueueForwarder::from_receiver_output_opts(name, opts).await?;\n    Ok(Box::new(forwarder))\n}\n\n// TODO: feature flag the variants, thread the features down through to generic-queue\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum QueueInputOpts {\n    #[serde(rename = \"gcp-pubsub\")]\n    GcpPubSub(GcpPubSubInputOpts),\n    RabbitMQ(RabbitMqInputOpts),\n    Redis(RedisInputOpts),\n    Sqs(SqsInputOpts),\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[allow(clippy::large_enum_variant)] // Largest variant isn't _that_ big\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum QueueOutputOpts {\n    #[serde(rename = \"gcp-pubsub\")]\n    GcpPubSub(GcpPubSubOutputOpts),\n    RabbitMQ(RabbitMqOutputOpts),\n    Redis(RedisOutputOpts),\n    Sqs(SqsOutputOpts),\n}\n\n#[cfg(test)]\nmod tests {\n    use svix_bridge_types::{\n        SenderOutputOpts, SvixSenderOutputOpts, TransformationConfig, TransformerInputFormat,\n    };\n\n    use super::{into_receiver_output, into_sender_input};\n    use crate::{\n        config::{QueueInputOpts, QueueOutputOpts},\n        redis::{RedisInputOpts, RedisOutputOpts},\n    };\n\n    // FIXME: can't support raw payload access for redis because it requires JSON internally.\n    //   Revisit after `omniqueue` adoption.\n    #[test]\n    fn redis_sender_with_string_transformation_is_err() {\n        <|fim_suffix|>\n\n        let err = into_sender_input(\n            \"redis-with-string-transformation\".to_owned(),\n            input_opts,\n            Some(TransformationConfig::Explicit {\n                format: TransformerInputFormat::String,\n                src: String::new(),\n            }),\n            SenderOutputOpts::Svix(SvixSenderOutputOpts {\n                token: \"\".to_string(),\n                options: None,\n            }),\n        )\n        .err()\n        .expect(\"invalid config didn't result in error\");\n\n        assert_eq!(err, \"redis only supports json formatted transformations\")\n    }\n\n    // FIXME: can't support raw payload access for redis because it requires JSON internally.\n    //   Revisit after `omniqueue` adoption.\n    #[tokio::test]\n    async fn test_redis_receiver_string_transform_is_err() {\n        let redis_out = QueueOutputOpts::Redis(RedisOutputOpts {\n            dsn: \"\".to_string(),\n            max_connections: 0,\n            queue_key: \"\".to_string(),\n            delayed_queue_key: None,\n            ack_deadline_ms: 2_000,\n        });\n\n        // Explicit String fails\n        let res = into_receiver_output(\n            \"\".to_string(),\n            redis_out,\n            Some(TransformationConfig::Explicit {\n                src: String::new(),\n                format: TransformerInputFormat::String,\n            })\n            .as_ref(),\n        )\n        .await;\n        assert!(matches!(\n            res.err()\n                .expect(\"invalid config didn't result in error\"),\n            crate::error::Error::Generic(msg) if msg == \"redis only supports json formatted transformations\"\n        ));\n    }\n}\n<|fim_middle|>", "completion": "let input_opts = QueueInputOpts::Redis(RedisInputOpts {\n            dsn: \"\".to_string(),\n            max_connections: 0,\n            reinsert_on_nack: false,\n            queue_key: \"\".to_string(),\n            delayed_queue_key: None,\n            consumer_group: \"\".to_string(),\n            consumer_name: \"\".to_string(),\n            ack_deadline_ms: 2_000,\n        });", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/src/config.rs", "node_type": "let_declaration", "line_range": [99, 108]}
{"prompt": "<|fim_prefix|>uration::from_secs(60 * 60 * 6),\n                // 12 hours\n                Duration::from_secs(60 * 60 * 12),\n                // 24 hours\n                Duration::from_secs(60 * 60 * 24),\n            ];\n\n            run_migration_schedule(&delays, pool).await;\n        }\n    });\n\n    // Metrics task\n    tokio::spawn({\n        let pool = pool.clone();\n        let main_queue_name = main_queue_name.clone();\n        let delayed_queue_name = delayed_queue_name.clone();\n        let deadletter_queue_name = dlq_name.clone();\n\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_secs(1));\n            let main_queue = RedisQueueType::Stream(&main_queue_name);\n            let pending = RedisQueueType::StreamPending {\n                stream: &main_queue_name,\n                group: WORKERS_GROUP,\n            };\n            let delayed_queue = RedisQueueType::SortedSet(&delayed_queue_name);\n            let deadletter_queue = RedisQueueType::List(&deadletter_queue_name);\n            let metrics =\n                crate::metrics::RedisQueueMetrics::new(&opentelemetry::global::meter(\"svix.com\"));\n            loop {\n                interval.tick().await;\n                metrics\n                    .record(\n                        &pool,\n                        &main_queue,\n                        &pending,\n                        &delayed_queue,\n                        &deadletter_queue,\n                    )\n                    .await;\n            }\n        }\n    });\n\n    let config = RedisConfig {\n        dsn: dsn.to_owned(),\n        max_connections: cfg.redis_pool_max_size,\n        reinsert_on_nack: false, // TODO\n        queue_key: main_queue_name,\n        delayed_queue_key: delayed_queue_name,\n        delayed_lock_key: delayed_lock_name,\n        consumer_group: WORKERS_GROUP.to_owned(),\n        consumer_name: WORKER_CONSUMER.to_owned(),\n        payload_key: QUEUE_KV_KEY.to_owned(),\n        ack_deadline_ms: pending_duration,\n        dlq_config: Some(DeadLetterQueueConfig {\n            queue_key: dlq_name,\n            max_receives: 3,\n        }),\n        sentinel_config: cfg.redis_sentinel_cfg.clone().map(|c| c.into()),\n    };\n\n    match &cfg.queue_type {\n        QueueType::RedisCluster => {\n            let (producer, consumer) = RedisBackend::cluster_builder(config)\n                .build_pair()\n                .await\n                .expect(\"Error initializing redis-cluster queue\");\n\n            let producer = TaskQueueProducer::new(producer);\n            let consumer = TaskQueueConsumer::new(consumer);\n            (producer, consumer)\n        }\n        QueueType::RedisSentinel => {\n            let (producer, consumer) = RedisBackend::sentinel_builder(config)\n                .build_pair()\n                .await\n                .expect(\"Error initializing redis-cluster queue\");\n\n            let producer = TaskQueueProducer::new(producer);\n            let consumer = TaskQueueConsumer::new(consumer);\n            (producer, consumer)\n        }\n        QueueType::Redis => {\n            let (producer, consumer) = RedisBackend::builder(config)\n                .build_pair()\n                .await\n                .expect(\"Error initializing redis queue\");\n\n            let producer = TaskQueueProducer::new(producer);\n            let consumer = TaskQueueConsumer::new(consumer);\n            (producer, consumer)\n        }\n        _ => panic!(\"Unsupported backend!\"),\n    }\n}\n\nfn task_from_redis_key(key: &str) -> serde_json::Result<Arc<QueueTask>> {\n    // Get the first delimiter -> it has to have the |\n    let pos = key\n        .find('|')\n        .ok_or_else(|| serde::de::Error::custom(\"key must contain '|'\"))?;\n    serde_json::from_str(&key[pos + 1..])\n}\n\nasync fn migrate_v2_to_v3_queues(conn: &mut RedisConnection<'_>) -> Result<()> {\n    migrate_list_to_stream(conn, LEGACY_V2_MAIN, MAIN).await?;\n    migrate_list_to_stream(conn, LEGACY_V2_PROCESSING, MAIN).await?;\n\n    Ok(())\n}\n\nasync fn migrate_list_to_stream(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    <|fim_suffix|>\n    loop {\n        let legacy_keys: Vec<String> = conn\n            .lpop(legacy_queue, NonZeroUsize::new(batch_size))\n            .await?;\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n\n        let mut pipe = redis::pipe();\n        for key in legacy_keys {\n            let task = match task_from_redis_key(&key) {\n                Ok(t) => t,\n                Err(e) => {\n                    tracing::error!(error = &e as &dyn std::error::Error, \"Invalid legacy key\");\n                    continue;\n                }\n            };\n            let _ = pipe.xadd(\n                queue,\n                GENERATE_STREAM_ID,\n                &[(QUEUE_KV_KEY, serde_json::to_string(&task).unwrap())],\n            );\n        }\n\n        let _: () = pipe.query_async(conn).await?;\n    }\n}\n\nasync fn migrate_v1_to_v2_queues(conn: &mut RedisConnection<'_>) -> Result<()> {\n    migrate_list(conn, LEGACY_V1_MAIN, LEGACY_V2_MAIN).await?;\n    migrate_list(conn, LEGACY_V1_PROCESSING, LEGACY_V2_PROCESSING).await?;\n    migrate_sset(conn, LEGACY_V1_DELAYED, DELAYED).await?;\n\n    Ok(())\n}\n\nasync fn migrate_list(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    let batch_size = 1000;\n    loop {\n        // Checking for old messages from queue\n        let legacy_keys: Vec<String> = conn\n            .lpop(legacy_queue, NonZeroUsize::new(batch_size))\n            .await?;\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n        let _: () = conn.rpush(queue, legacy_keys).await?;\n    }\n}\n\nasync fn migrate_sset(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    let batch_size = 1000;\n    loop {\n        // Checking for old messages from LEGACY_DELAYED\n        let legacy_keys: Vec<(String, f64)> = conn.zpopmin(legacy_queue, batch_size).await?;\n\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n        let legacy_keys: Vec<(f64, String)> =\n            legacy_keys.into_iter().map(|(x, y)| (y, x)).collect();\n\n        let _: () = conn.zadd_multiple(queue, &legacy_keys).await?;\n    }\n}\n\n#[cfg(test)]\npub mod tests {\n    use std::time::Duration;\n\n    use chrono::Utc;\n    use redis::{streams::StreamReadReply, AsyncCommands as _, Direction};\n    use tokio::time::timeout;\n\n    use super::{migrate_list, migrate_list_to_stream, migrate_sset, new_pair_inner};\n    use crate::{\n        cfg::Configuration,\n        core::types::{ApplicationId, EndpointId, MessageAttemptTriggerType, MessageId},\n        queue::{MessageTask, QueueTask},\n        redis::RedisManager,\n    };\n\n    const TEST_RECV_DEADLINE: Duration = Duration::from_secs(5);\n\n    async fn get_pool(cfg: &Configuration) -> RedisManager {\n        RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await\n    }\n\n    #[tokio::test]\n    // run with `cargo test -- --ignored redis` only when redis is up and configured\n    #[ignore]\n    async fn test_migrate_list() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n        let mut pool = pool.get().await.unwrap();\n\n        const TEST_QUEUE: &str = \"{queue}_svix_test_queue_list\";\n        const TEST_LEGACY: &str = \"svix_queue_test_list\";\n\n        let v = \"test-value\";\n\n        // delete test queues first, just in case:\n        let _: () = pool.del(TEST_QUEUE).await.unwrap();\n        let _: () = pool.del(TEST_LEGACY).await.unwrap();\n\n        let _: () = pool.rpush(TEST_LEGACY, v).await.unwrap();\n\n        let should_be_none: Option<String> = pool.lpop(TEST_QUEUE, None).await.unwrap();\n        assert_eq!(should_be_none, None);\n\n        migrate_list<|fim_middle|>", "completion": "let batch_size = 1000;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [315, 315]}
{"prompt": "<|fim_prefix|>create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp_id = create_test_endpoint(&client, &app_id, \"https://gabagool.deli\")\n        .await\n        .unwrap()\n        .id;\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 0);\n    assert_eq!(stats.sending, 0);\n\n    let last_msg_time = {\n        // Create the relevant Stats records manually, otherwise\n        // it's difficult to test exact state of messagedestinations.\n\n        let cfg = get_default_test_config();\n        let db = Arc::new(cfg);\n        let db = svix_server::db::init_db(&db).await;\n\n        let now = Utc::now();\n\n        let msg = message::ActiveModel {\n            app_id: Set(app_id.clone()),\n            org_id: Set(OrganizationId::new(None, None)),\n            expiration: Set(Utc::now().into()),\n            event_type: Set(EventTypeName(\"test.ing\".into())),\n            created_at: Set((now - chrono::Duration::minutes(65)).into()),\n            id: Set(MessageId::new(\n                (now - chrono::Duration::minutes(65)).into(),\n                None,\n            )),\n            ..message::ActiveModel::new()\n        }\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(60),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(45),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(30),\n            MessageStatus::Sending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap()\n        .created_at\n    };\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 2);\n    assert_eq!(stats.sending, 1);\n\n    let stats_filtered: EndpointStatsOut = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}&until={}\",\n                urlencoding::encode(&last_msg_time.to_rfc3339()),\n                urlencoding::encode(&Utc::now().to_rfc3339()),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats_filtered.fail, 0);\n    assert_eq!(stats_filtered.success, 0);\n    assert_eq!(stats_filtered.pending, 0);\n    assert_eq!(stats_filtered.sending, 1);\n\n    let _: IgnoredAny = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}\",\n                urlencoding::encode(&(Utc::now() - chrono::Duration::days(29)).to_rfc3339()),\n            ),\n            StatusCode::BAD_REQUEST,\n        )\n        .await\n        .unwrap();\n}\n\n/// We used to store the secret in the DB without a type marker, check loading those still works\n#[tokio::test]\nasync fn test_legacy_endpoint_secret() {\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let db = Arc::new(cfg);\n    let db = svix_server::db::init_db(&db).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_throwaway = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    let raw_key = STANDARD.decode(\"5gasBsSw3Nvf3ugNYVJIqnRVYPW7hPts\").unwrap();\n    let secret_1 = EndpointSecret::Symmetric(raw_key.clone());\n\n    l<|fim_suffix|>\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Set the raw value to the database (like legacy)\n    db.execute(Statement::from_sql_and_values(\n        DatabaseBackend::Postgres,\n        \"UPDATE endpoint SET key = $1 WHERE id = $2\",\n        vec![raw_key.clone().into(), endp_1.id.clone().into()],\n    ))\n    .await\n    .unwrap();\n\n    let endp_1 = get_endpoint(&client, &app_id, &endp_1.id).await.unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let (secret, ep) = (secret_1, endp_1);\n    assert_eq!(\n        secret.serialize_public_key(),\n        client\n            .get::<EndpointSecretOutTest>(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                StatusCode::OK\n            )\n            .await\n            .unwrap()\n            .key\n    );\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_encryption_in_database() {\n    let mut cfg = get_default_test_config();\n    cfg.encryption = Encryption::new([1; 32]);\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let db = Arc::new(cfg);\n    let db = svix_server::db::init_db(&db).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret_encrypted: Option<QueryResult> = db\n        .query_one(Statement::from_sql_and_values(\n            DatabaseBackend::Postgres,\n            \"SELECT key FROM endpoint WHERE id = $1\",\n            vec![ep.id.clone().into()],\n        ))\n        .await\n        .unwrap();\n    let secret_encrypted: Vec<u8> = secret_encrypted.unwrap().try_get(\"\", \"key\").unwrap();\n\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret_clear: Option<QueryResult> = db\n        .query_one(Statement::from_sql_and_values(\n            DatabaseBackend::Postgres,\n            \"SELECT key FROM endpoint WHERE id = $1\",\n            vec![ep.id.clone().into()],\n        ))\n        .await\n        .unwrap();\n    let secret_clear: Vec<u8> = secret_clear.unwrap().try_get(\"\", \"key\").unwrap();\n\n    // Ensure that the length of the encrypted is much longer than the clear\n    assert!(secret_encrypted.len() > secret_clear.len() + 10);\n}\n\n#[tokio::test]\nasync fn test_endpoint_filter_events() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_empty_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"filterTypes\": [],\n    });\n\n    let ep_with_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"filterTypes\": [\"et1\"],\n    });\n\n    let ep_no_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n    });\n\n    let expected_et = EventTypeNameSet(HashSet::from([EventTypeName(\"et1\".to_owned())]));\n\n    let _ep_with_empty_events: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_empty_events,\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _ep_with_nonexistent_event: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"et1\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let ep_with_valid_event: EndpointOut = client\n        .post(\n            <|fim_middle|>", "completion": "let ep_in = EndpointIn {\n        key: Some(secret_throwaway.clone()),\n        ..default_test_endpoint()\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1597, 1600]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse reqwest::StatusCode;\nuse serde_json::json;\nuse svix_server::{\n    core::types::{EndpointUid, MessageStatus},\n    v1::{\n        endpoints::{\n            attempt::{EndpointMessageOut, MessageAttemptOut},\n            endpoint::{EndpointIn, EndpointOut},\n        },\n        utils::ListResponse,\n    },\n};\nuse wiremock::{matchers, Mock, MockServer, Respond, ResponseTemplate};\n\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, create_test_msg_with,\n        endpoint_in, get_msg_attempt_list_and_assert_count,\n    },\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n    TestReceiver,\n};\n\n#[tokio::test]\nasync fn test_expunge_attempt_response_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let sensitive_response_json = serde_json::json!({\"sensitive\":\"data\"});\n    let mut receiver = TestReceiver::start_with_body(\n        axum::http::StatusCode::OK,\n        axum::Json(sensitive_response_json.clone()),\n    );\n\n    let endpoint_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap()\n        .id;\n\n    receiver.data_recv.recv().await;\n\n    let attempt = run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endpoint_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        i<|fim_suffix|>        Ok(attempts.data[0].clone())\n    })\n    .await\n    .unwrap();\n\n    let attempt_response: serde_json::Value = serde_json::from_str(&attempt.response).unwrap();\n    assert_eq!(sensitive_response_json, attempt_response);\n\n    let attempt_id = &attempt.id;\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let attempt: MessageAttemptOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\"EXPUNGED\", &attempt.response);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"t<|fim_middle|>", "completion": "if attempts.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", attempts.data.len());\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "if_expression", "line_range": [64, 66]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct ApplicationListOptions {\n    /// Exclude applications that have no endpoints. Default is false.\n    pub exclude_apps_with_no_endpoints: Option<bool>,\n\n    /// Exclude applications that have only disabled endpoints. Default is\n    /// false.\n    pub exclude_apps_with_disabled_endpoints: Option<bool>,\n\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct ApplicationCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct Application<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> Application<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List of all the organization's applications.\n    pub async fn list(\n        &self,\n        options: Option<ApplicationListOptions>,\n    ) -> Result<ListResponseApplicationOut> {\n        let ApplicationListOptions {\n            exclude_apps_with_no_endpoints,\n            exclude_apps_with_disabled_endpoints,\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app\")\n            .with_optional_query_param(\n                \"exclude_apps_with_no_endpoints\",\n                exclude_apps_with_no_endpoints,\n            )\n            .with_optional_query_param(\n                \"exclude_apps_with_disabled_endpoints\",\n                exclude_apps_with_disabled_endpoints,\n            )\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create a new application.\n    pub async fn create(\n        &self,\n        application_in: ApplicationIn,\n        options: Option<ApplicationCreateOptions>,\n    ) -> Result<ApplicationOut> {\n        let ApplicationCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/app\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(application_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create the application with the given ID, or create a new one if it\n    /// doesn't exist yet.\n    pub async fn get_or_create(\n        &self,\n        application_in: ApplicationIn,\n        options: Option<ApplicationCreateOptions>,\n    ) -> Result<ApplicationOut> {\n        let ApplicationCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/app\")\n            .with_query_param(\"get_if_exists\", \"true\".to_owned())\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(application_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an application.\n    pub async fn get(&self, app_id: String) -> Result<ApplicationOut> {\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}\")\n            .with_path_param(\"app_id\", app_id)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Update an application.\n    pub async fn update(\n        &self,\n        app_id: String,\n        application_in: ApplicationIn,\n    ) -> Result<ApplicationOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/api/v1/app/{app_id}\")\n            .with_path_param(\"app_id\", app_id)\n            .with_body_param(application_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete an application.\n    pub async fn delete(&self, app_id: String) -> Result<()> {\n        crate::request::Request::new(http1::Method::DELETE, \"/api/v1/app/{app_id}\")\n            .with_path_param(\"app_id\", app_id)\n            .returns_nothing()\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Partially update an application.\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub async fn patch(\n        &self,\n        app_id: String,\n        application_patch: ApplicationPatch,\n    ) -> Result<ApplicationOut> {\n        crate::request::Request::new(http1::Method::PATCH, \"/api/v1/app/{app_id}\")\n            .with_path_param(\"app_id\", app_id)\n            .with_body_param(application_patch)\n            .execute(self.cfg)\n            .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/application.rs", "node_type": "function_item", "line_range": [129, 139]}
{"prompt": "<|fim_prefix|>rr) => Ok(CompletedDispatch::Failed(FailedDispatch(\n                    attempt,\n                    Error::generic(err),\n                ))),\n                None => Ok(CompletedDispatch::Successful(SuccessfulDispatch(attempt))),\n            }\n        }\n        Err(err) => {\n            // For errors, we still calculate the duration\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            Ok(CompletedDispatch::Failed(FailedDispatch(\n                messageattempt::ActiveModel {\n                    response_status_code: Set(0),\n                    response: Set(err.to_string()),\n                    status: Set(MessageStatus::Fail),\n                    response_duration_ms: Set(duration_ms),\n                    ..attempt\n                },\n                err.into(),\n            )))\n        }\n    }\n}\n\n#[tracing::instrument(skip_all, fields(response_code))]\nasync fn handle_successful_dispatch(\n    WorkerContext {\n        cache,\n        db,\n        op_webhook_sender,\n        ..\n    }: &WorkerContext<'_>,\n    DispatchContext {\n        org_id,\n        endp,\n        app_id,\n        app_uid,\n        msg_task,\n        msg_uid,\n        ..\n    }: DispatchContext<'_>,\n    SuccessfulDispatch(mut attempt): SuccessfulDispatch,\n) -> Result<()> {\n    attempt.ended_at = Set(Some(Utc::now().into()));\n    attempt.next_attempt = Set(None);\n    let attempt = attempt.insert(*db).await?;\n\n    process_endpoint_success(cache, app_id, org_id, endp).await?;\n\n    tracing::Span::current().record(\"response_code\", attempt.response_status_code);\n    tracing::info!(\"Webhook success.\");\n\n    if msg_task.attempt_count as usize >= OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER {\n        if let Err(e) = op_webhook_sender\n            .send_operational_webhook(\n                org_id,\n                OperationalWebhook::MessageAttemptRecovered(MessageAttemptEvent {\n                    app_id: app_id.clone(),\n                    app_uid: app_uid.cloned(),\n                    endpoint_id: msg_task.endpoint_id.clone(),\n                    msg_id: msg_task.msg_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await\n        {\n            tracing::error!(\n                \"Failed sending MessageAttemptRecovered Operational Webhook: {}\",\n                e\n            );\n        }\n    }\n\n    Ok(())\n}\n\nfn calculate_retry_delay(duration: Duration, err: &Error) -> Duration {\n    let duration = if matches!(err.typ, ErrorType::Timeout(_))\n        || matches!(err.typ, ErrorType::Http(HttpError { status, .. }) if status == StatusCode::TOO_MANY_REQUESTS)\n    {\n        std::cmp::max(duration, Duration::from_secs(OVERLOAD_PENALTY_SECS))\n    } else {\n        duration\n    };\n    // Apply jitter with a maximum variation of JITTER_DELTA\n    rand::thread_rng()\n        .gen_range(duration.mul_f32(1.0 - JITTER_DELTA)..=duration.mul_f32(1.0 + JITTER_DELTA))\n}\n\n#[tracing::instrument(skip_all, fields(response_code))]\nasync fn handle_failed_dispatch(\n    WorkerContext {\n        db,\n        cache,\n        op_webhook_sender,\n        cfg,\n        queue_tx,\n        ..\n    }: &WorkerContext<'_>,\n    DispatchContext {\n        org_id,\n        app_id,\n        app_uid,\n        msg_uid,\n        endp,\n        msg_task,\n        ..\n    }: DispatchContext<'_>,\n    FailedDispatch(mut attempt, err): FailedDispatch,\n) -> Result<()> {\n    attempt.ended_at = Set(Some(Utc::now().into()));\n\n    tracing::Span::current().record(\"response_code\", attempt.response_status_code.try_as_ref());\n    tracing::info!(\"Webhook failure.\");\n\n    let retry_schedule = &cfg.retry_schedule;\n\n    let attempt_count = msg_task.attempt_count as usize;\n    if msg_task.trigger_type == MessageAttemptTriggerType::Manual {\n        tracing::debug!(\"Manual retry failed\");\n        attempt.next_attempt = Set(None);\n        attempt.insert(*db).await?;\n        Ok(())\n    } else if attempt_count < retry_schedule.len() {\n        let retry_delay = calculate_retry_delay(retry_schedule[attempt_count], &err);\n        l<|fim_suffix|>\n        attempt.next_attempt = Set(Some(next_attempt_time.into()));\n        let attempt = attempt.insert(*db).await?;\n\n        tracing::debug!(\n            retry_delay = retry_delay.as_secs(),\n            \"Worker failure retrying for attempt {}: {} {} {}\",\n            attempt_count,\n            err,\n            &attempt.id,\n            &endp.id\n        );\n\n        if attempt_count == (OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER - 1) {\n            if let Err(e) = op_webhook_sender\n                .send_operational_webhook(\n                    org_id,\n                    OperationalWebhook::MessageAttemptFailing(MessageAttemptEvent {\n                        app_id: app_id.clone(),\n                        app_uid: app_uid.cloned(),\n                        endpoint_id: msg_task.endpoint_id.clone(),\n                        msg_id: msg_task.msg_id.clone(),\n                        msg_event_id: msg_uid.cloned(),\n                        last_attempt: attempt.into(),\n                    }),\n                )\n                .await\n            {\n                tracing::error!(\n                    \"Failed sending MessageAttemptFailing Operational Webhook: {}\",\n                    e\n                );\n            }\n        }\n        queue_tx\n            .send(\n                &QueueTask::MessageV1(MessageTask {\n                    attempt_count: msg_task.attempt_count + 1,\n                    ..msg_task.clone()\n                }),\n                Some(retry_delay),\n            )\n            .await?;\n\n        Ok(())\n    } else {\n        attempt.next_attempt = Set(None);\n        let attempt = attempt.insert(*db).await?;\n\n        tracing::debug!(\n            \"Worker failure attempts exhausted: {} {} {}\",\n            err,\n            &attempt.id,\n            &endp.id\n        );\n\n        // Send common operational webhook\n        op_webhook_sender\n            .send_operational_webhook(\n                org_id,\n                OperationalWebhook::MessageAttemptExhausted(MessageAttemptEvent {\n                    app_id: app_id.clone(),\n                    app_uid: app_uid.cloned(),\n                    endpoint_id: msg_task.endpoint_id.clone(),\n                    msg_id: msg_task.msg_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await?;\n\n        match process_endpoint_failure(\n            cache,\n            app_id,\n            org_id,\n            endp,\n            cfg.endpoint_failure_disable_after,\n        )\n        .await?\n        {\n            None => Ok(()),\n\n            Some(EndpointDisableInfo { first_failure_at }) => {\n                let endp = endpoint::Entity::secure_find_by_id(\n                    msg_task.app_id.clone(),\n                    msg_task.endpoint_id.clone(),\n                )\n                .one(*db)\n                .await?\n                .ok_or_else(|| {\n                    Error::generic(format_args!(\n                        \"Endpoint not found {app_id} {}\",\n                        &msg_task.endpoint_id\n                    ))\n                })?;\n\n                let endp = endpoint::ActiveModel {\n                    disabled: Set(true),\n                    first_failure_at: Set(Some(first_failure_at.into())),\n                    ..endp.into()\n                };\n                let _endp = endp.update(*db).await?;\n\n                // Send operational webhooks\n                op_webhook_sender\n                    .send_operational_webhook(\n                        org_id,\n                        OperationalWebhook::EndpointDisabled(EndpointDisabledEventData {\n                            app_id: app_id.clone(),\n                            app_uid: app_uid.cloned(),\n                            endpoint_id: msg_task.endpoint_id.clone(),\n                            // TODO:\n                            endpoint_uid: None,\n                            fail_since: first_failure_at,\n                        }),\n                    )\n                    .await\n            }\n        }\n    }\n}\n\n#[derive(Clone)]\nstruct D<|fim_middle|>", "completion": "let next_attempt_time =\n            Utc::now() + chrono::Duration::from_std(retry_delay).expect(\"Error parsing duration\");", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "let_declaration", "line_range": [533, 534]}
{"prompt": "<|fim_prefix|>_ISSUER)\n    .with_subject(management_org_id());\n\n    signing_config.generate(claims).map_err(Error::generic)\n}\n\npub fn generate_app_token(\n    keys: &JwtSigningConfig,\n    org_id: OrganizationId,\n    app_id: ApplicationId,\n    feature_flags: FeatureFlagSet,\n) -> Result<String> {\n    let claims = Claims::with_custom_claims(\n        CustomClaim {\n            organization: Some(org_id.0),\n            feature_flags,\n        },\n        Duration::from_hours(24 * 28),\n    )\n    .with_issuer(JWT_ISSUER)\n    .with_subject(app_id.0);\n\n    keys.generate(claims).map_err(Error::generic)\n}\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum JwtSigningConfig {\n    /// Variants that specify both key and algorithm to use\n    Advanced(JWTAlgorithm),\n    /// The variant used when the algorithm is not specified, defaults to HS256\n    Default {\n        #[serde(deserialize_with = \"deserialize_hs256\")]\n        jwt_secret: HS256Key,\n    },\n}\n\n/// A wrapper for the available JWT signing algorithms exposed by `jwt-simple`\n#[derive(Deserialize)]\n#[serde(tag = \"jwt_algorithm\", content = \"jwt_secret\")]\npub enum JWTAlgorithm {\n    #[serde(deserialize_with = \"deserialize_hs256\")]\n    HS256(HS256Key),\n    #[serde(deserialize_with = \"deserialize_hs384\")]\n    HS384(HS384Key),\n    #[serde(deserialize_with = \"deserialize_hs512\")]\n    HS512(HS512Key),\n    #[serde(deserialize_with = \"deserialize_rs256\")]\n    RS256(RS256),\n    #[serde(deserialize_with = \"deserialize_rs384\")]\n    RS384(RS384),\n    #[serde(deserialize_with = \"deserialize_rs512\")]\n    RS512(RS512),\n    #[serde(deserialize_with = \"deserialize_eddsa\")]\n    EdDSA(EdDSA),\n}\n\npub enum RS256 {\n    Public(RS256PublicKey),\n    Pair(Box<RS256KeyPair>),\n}\n\npub enum RS384 {\n    Public(RS384PublicKey),\n    Pair(Box<RS384KeyPair>),\n}\n\npub enum RS512 {\n    Public(RS512PublicKey),\n    Pair(Box<RS512KeyPair>),\n}\n\npub enum EdDSA {\n    Public(Ed25519PublicKey),\n    Pair(Box<Ed25519KeyPair>),\n}\n\nimpl JwtSigningConfig {\n    pub fn generate(&self, claims: JWTClaims<CustomClaim>) -> Result<String, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.authenticate(claims),\n                JWTAlgorithm::HS384(key) => key.authenticate(claims),\n                JWTAlgorithm::HS512(key) => key.authenticate(claims),\n                JWTAlgorithm::RS256(kind) => match kind {\n                    RS256::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS256::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS384::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS512::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::EdDSA(kind) => match kind {\n                    EdDSA::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    EdDSA::Pair(key) => key.sign(claims),\n                },\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.authenticate(claims),\n        }\n    }\n\n    pub fn verify_token(\n        &self,\n        token: &str,\n        options: Option<VerificationOptions>,\n    ) -> Result<JWTClaims<CustomClaim>, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS384(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS512(key) => key.verify_token(token, options),\n                JWTAlgorithm::RS256(kind) => match kind {\n                    RS256::Public(key) => key.verify_token(token, options),\n                    RS256::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(key) => key.verify_token(token, options),\n                    RS384::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(key) => key.verify_token(token, options),\n                    RS512::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::EdDSA(kind) => m<|fim_suffix|>\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.verify_token(token, options),\n        }\n    }\n}\n\nimpl Debug for JwtSigningConfig {\n    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"{}\",\n            match self {\n                JwtSigningConfig::Advanced(a) => {\n                    match a {\n                        JWTAlgorithm::HS256(_) => \"HS256\",\n                        JWTAlgorithm::HS384(_) => \"HS384\",\n                        JWTAlgorithm::HS512(_) => \"HS512\",\n                        JWTAlgorithm::RS256(_) => \"RS256\",\n                        JWTAlgorithm::RS384(_) => \"RS384\",\n                        JWTAlgorithm::RS512(_) => \"RS512\",\n                        JWTAlgorithm::EdDSA(_) => \"EdDSA\",\n                    }\n                }\n                JwtSigningConfig::Default { .. } => {\n                    \"HS256\"\n                }\n            }\n        )\n    }\n}\n\nfn deserialize_hs256<'de, D>(deserializer: D) -> Result<HS256Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS256Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs384<'de, D>(deserializer: D) -> Result<HS384Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS384Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs512<'de, D>(deserializer: D) -> Result<HS512Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS512Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_rs256<'de, D>(deserializer: D) -> Result<RS256, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS256KeyPair::from_pem(&key) {\n        Ok(RS256::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS256PublicKey::from_pem(&key) {\n        Ok(RS256::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_rs384<'de, D>(deserializer: D) -> Result<RS384, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS384KeyPair::from_pem(&key) {\n        Ok(RS384::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS384PublicKey::from_pem(&key) {\n        Ok(RS384::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_rs512<'de, D>(deserializer: D) -> Result<RS512, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS512KeyPair::from_pem(&key) {\n        Ok(RS512::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS512PublicKey::from_pem(&key) {\n        Ok(RS512::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_eddsa<'de, D>(deserializer: D) -> Result<EdDSA, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = Ed25519KeyPair::from_pem(&key) {\n        Ok(EdDSA::Pair(Box::new(pair)))\n    } else if let Ok(public) = Ed25519PublicKey::from_pem(&key) {\n        Ok(EdDSA::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n<|fim_middle|>", "completion": "match kind {\n                    EdDSA::Public(key) => key.verify_token(token, options),\n                    EdDSA::Pair(pair) => pair.public_key().verify_token(token, options),\n                }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/security.rs", "node_type": "match_expression", "line_range": [323, 326]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-Licensepub(crate) -Identifier: MIT\n\nuse std::{\n    collections::HashMap,\n    sync::{\n        atomic::{AtomicU64, AtomicUsize, Ordering},\n        Arc, LazyLock,\n    },\n    time::Duration,\n};\n\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chrono::Utc;\nuse futures::future;\nuse http::{HeaderValue, StatusCode, Version};\nuse http_body_util::BodyExt as _;\nu<|fim_suffix|>use rand::Rng;\nuse sea_orm::{\n    prelude::DateTimeUtc, ActiveModelTrait, ColumnTrait, DatabaseConnection, EntityTrait,\n    QueryFilter, QueryOrder, Set,\n};\nuse serde::{Deserialize, Serialize};\nuse tokio::time::sleep;\nuse tracing::Instrument;\n\nuse crate::{\n    cfg::Configuration,\n    core::{\n        cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue},\n        cryptography::Encryption,\n        message_app::{CreateMessageApp, CreateMessageEndpoint},\n        operational_webhooks::{\n            EndpointDisabledEventData, MessageAttemptEvent, OperationalWebhook,\n            OperationalWebhookSender,\n        },\n        types::{\n            ApplicationId, ApplicationUid, BaseId, EndpointHeaders, EndpointId,\n            EndpointSecretInternal, EndpointSecretType, MessageAttemptId,\n            MessageAttemptTriggerType, MessageId, MessageStatus, MessageUid, OrganizationId,\n        },\n        webhook_http_client::{Error as WebhookClientError, RequestBuilder, WebhookClient},\n    },\n    db::models::{endpoint, message, messageattempt, messagecontent, messagedestination},\n    error::{Error, ErrorType, HttpError, Result},\n    queue::{MessageTask, QueueTask, TaskQueueConsumer, TaskQueueProducer},\n    v1::utils::get_unix_timestamp,\n};\n\npub type CaseSensitiveHeaderMap = HashMap<String, HeaderValue>;\n\n// The maximum variation from the retry schedule when applying jitter to a resent webhook event in\n// percent deviation\nconst JITTER_DELTA: f32 = 0.2;\nconst OVERLOAD_PENALTY_SECS: u64 = 60;\n\nconst USER_AGENT: &str = concat!(\"Svix-Webhooks/\", env!(\"CARGO_PKG_VERSION\"));\n\n/// Send the MessageAttemptFailingEvent after exceeding this number of failed attempts\nconst OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER: usize = 4;\n\nconst RESPONSE_MAX_SIZE: usize = 20000;\n\n/// A simple struct noting the context of the wrapped [`DateTimeUtc`]. This struct is returned when\n/// you are to disable disable an endpoint. This is optionally returned by [`process_failure_cache`]\n/// which is to be called after all retry events are exhausted.\n#[repr(transparent)]\nstruct EndpointDisableInfo {\n    first_failure_at: DateTimeUtc,\n}\n\n/// The first_failure_at time is only stored in Postgres after the endpoint has been disabled.\n/// Otherwise, it is stored in the cache with an expiration.\n#[derive(Deserialize, Serialize)]\npub struct FailureCacheValue {\n    pub first_failure_at: DateTimeUtc,\n}\n\nkv_def!(FailureCacheKey, FailureCacheValue);\n\nimpl FailureCacheKey {\n    pub fn new(\n        org_id: &OrganizationId,\n        app_id: &ApplicationId,\n        endp_id: &EndpointId,\n    ) -> FailureCacheKey {\n        FailureCacheKey(format!(\"SVIX_FAILURE_CACHE_{org_id}_{app_id}_{endp_id}\"))\n    }\n}\n\n/// Called upon the successful dispatch of an endpoint. Simply clears the cache of a\n/// [`FailureCacheKey`]/[`FailureCacheValue`] pair associated with a given endpoint. This is such\n/// that an endpoint that was previously not responding is not disabled after responding again.\n///\n/// If the key value pair does not already exist in the cache, indicating that the endpoint never\n/// stopped responding, no operation is performed.\n#[tracing::instrument(skip_all)]\nasync fn process_endpoint_success(\n    cache: &Cache,\n    app_id: &ApplicationId,\n    org_id: &OrganizationId,\n    endp: &CreateMessageEndpoint,\n) -> Result<()> {\n    let key = FailureCacheKey::new(org_id, app_id, &endp.id);\n\n    cache.delete(&key).await.map_err(Error::cache)\n}\n\n/// Called upon endpoint failure. Returns whether to disable the endpoint based on the time of first\n/// failure stored in the cache.\n///\n/// If no failure has previously been reported, then now is cached as the time of first failure and\n/// the endpoint is not disabled.\n///\n/// If there has been a  previous failure, then it is compared to the configured grace period, where\n/// if there have been only failures within the grace period, then the endpoint is disabled.\n///\n/// All cache values are set with an expiration time greater that the grace period, so occasional\n/// failures will not cause an endpoint to be disabled.\n#[tracing::instrument(skip_all)]\nasync fn process_endpoint_failure(\n    cache: &Cache,\n    app_id: &ApplicationId,\n    org_id: &OrganizationId,\n    endp: &CreateMessageEndpoint,\n    disable_in: Duration,\n) -> Result<Option<EndpointDisableInfo>> {\n    let key = FailureCacheKey::new(org_id, app_id, &endp.id);\n    let now = Utc::now();\n\n    // If it already exists in the cache, see if the grace period has already elapsed\n    if let Some(FailureCacheValue { first_failure_at }) = cache\n        .get::<FailureCacheValue>(&key)\n        .await\n        .map_err(Error::generic)?\n    {\n        if now - first_failure_at\n            > chrono::Duration::from_std(disable_in).expect(\"Given `disable_in` is too large\")\n        {\n            Ok(Some(EndpointDisableInfo { first_failure_at }))\n        } else {\n            Ok(None)\n        }\n    }\n    // If it does not yet exist in the cache, set the first_failure_at value to now\n    else {\n        cache\n            .set(\n                &key,\n                &FailureCacheValue {\n                    first_failure_at: now,\n                },\n                // Failures are forgiven after double the `disable_in` `Duration` with the expiry of\n                // the Redis key\n                disable_in * 2,\n            )\n            .await\n            .map_err(Error::generic)?;\n\n        Ok(None)\n    }\n}\n\n/// Sign a message\nfn sign_msg(\n    main_secret: &Encryption,\n    timestamp: i64,\n    body: &str,\n    msg_id: &MessageId,\n    endpoint_signing_keys: &[&EndpointSecretInternal],\n) -> String {\n    let to_sign = format!(\"{msg_id}.{timestamp}.{body}\");\n\n    endpoint_signing_keys\n        .iter()\n        .format_with(\" \", |x, f| {\n            let sig = x.sign(main_secret, to_sign.as_bytes());\n            let version = match x.type_() {\n                EndpointSecretType::Hmac256 => \"v1\",\n                EndpointSecretType::Ed25519 => \"v1a\",\n            };\n\n            f(&format_args!(\"{version},{}\", STANDARD.encode(sig)))\n        })\n        .to_string()\n}\n\n/// Generates a set of headers for any one webhook event\nfn generate_msg_headers(\n    timestamp: i64,\n    msg_id: &MessageId,\n    signatures: String,\n    whitelabel_headers: bool,\n    configured_headers: Option<&EndpointHeaders>,\n    _endpoint_url: &str,\n) -> Result<CaseSensitiveHeaderMap> {\n    let mut headers = CaseSensitiveHeaderMap::new();\n    let id_hdr = msg_id\n        .0\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message id: {e:?}\")))?;\n    let timestamp = timestamp\n        .to_string()\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message timestamp: {e:?}\")))?;\n    let signatures_str = signatures\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message signatures: {e:?}\")))?;\n    if whitelabel_headers {\n        headers.insert(\"webhook-id\".to_owned(), id_hdr);\n        headers.insert(\"webhook-timestamp\".to_owned(), timestamp);\n        headers.insert(\"webhook-signature\".to_owned(), signatures_str);\n    } else {\n        headers.insert(\"svix-id\".to_owned(), id_hdr);\n        headers.insert(\"svix-timestamp\".to_owned(), timestamp);\n        headers.insert(\"svix-signature\".to_owned(), signatures_str);\n    }\n    headers.insert(\n        \"user-agent\".to_owned(),\n        USER_AGENT.to_string().parse().unwrap(),\n    );\n    headers.insert(\n        \"content-type\".to_owned(),\n        \"application/json\".parse().unwrap(),\n    );\n    if let Some(configured_headers) = configured_headers {\n        for (k, v) in &configured_headers.0 {\n            match v.parse() {\n                Ok(v) => {\n                    headers.insert(k.clone(), v);\n                }\n                Err(e) => {\n    <|fim_middle|>", "completion": "use itertools::Itertools;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "use_declaration", "line_range": [18, 18]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{string::FromUtf8Error, time::Duration};\n\nuse ::redis::RedisError;\nuse axum::async_trait;\nuse enum_dispatch::enum_dispatch;\nuse serde::{de::DeserializeOwned, Serialize};\n\nuse crate::core::retry::run_with_retries;\n\npub mod memory;\npub mod none;\npub mod redis;\n\n/// Errors internal to the cache\n#[derive(thiserror::Error, Debug)]\npub enum Error {\n    #[error(\"error deserializing Redis value\")]\n    Deserialization(#[from] serde_json::error::Error),\n\n    #[error(\"error deserializing Redis value\")]\n    DeserializationOther,\n\n    #[error(\"error deserializing byte array\")]\n    DeserializationBytes(#[from] FromUtf8Error),\n\n    #[error(\"Redis pool error: {0}\")]\n    Pool(#[from] bb8::RunError<RedisError>),\n\n    #[error(\"Redis database error: {0}\")]\n    Database(#[from] RedisError),\n\n    #[error(\"input error: {0}\")]\n    Input(String),\n}\ntype Result<T> = std::result::Result<T, Error>;\n\n/// A valid key value for the cache -- usually just a wrapper around a [`String`]\npub trait CacheKey: AsRef<str> + Send + Sync {}\n\n/// A cache key for setting/getting raw [`String`]s -- this is just a marker\n/// trait added in the `string_kv_def macro`\npub trait StringCacheKey: AsRef<str> + Send + Sync {}\n\n/// Any (de)serializable structure usable as a value in the cache -- it is associated with a\n/// given key type to ensure type checking on creation or reading of values from the cache\npub trait CacheValue: DeserializeOwned + Serialize + Send + Sync {\n    type Key: CacheKey;\n}\n\npub trait StringCacheValue: ToString + TryFrom<String> + Send + Sync {\n    type Key: CacheKey;\n}\n\n/// An inner macro which defines everything common to the below macro. Not really meant to be used,\n/// but it can't be made private or else it couldn't be used in the outer macro.\nmacro_rules! kv_def_inner {\n    ($key_id:ident, $val_struct:ident) => {\n        #[derive(Clone, Debug)]\n        pub struct $key_id(String);\n\n        impl AsRef<str> for $key_id {\n            fn as_ref(&self) -> &str {\n                &self.0\n            }\n        }\n\n        impl CacheValue for $val_struct {\n            type Key = $key_id;\n        }\n    };\n}\npub(crate) use kv_def_inner;\n\n/// A macro that creates a [`CacheKey`] and ties it to any value that implements\n/// [`DeserializeOwned`] and [`Serialize`]\nmacro_rules! kv_def {\n    ($key_id:ident, $val_struct:ident) => {\n        crate::core::cache::kv_def_inner!($key_id, $val_struct);\n\n        impl CacheKey for $key_id {}\n    };\n}\npub(crate) use kv_def;\n\n/// An inner macro which defines everything common to the below macro. Not really meant to be used,\n/// but it can't be made private or else it couldn't be used in the outer macro.\n#[allow(unused_macros)]\nmacro_rules! string_kv_def_inner {\n    ($key_id:ident) => {\n        #[derive(Clone, Debug)]\n        pub struct $key_id(String);\n\n        impl AsRef<str> for $key_id {\n            fn as_ref(&self) -> &str {\n                &self.0\n            }\n        }\n    };\n}\n#[allow(unused_imports)]\npub(crate) use string_kv_def_inner;\n\n#[cfg(test)]\nmacro_rules! string_kv_def {\n    ($key_id:ident) => {\n        crate::core::cache::string_kv_def_inner!($key_id);\n\n        impl crate::core::cache::StringCacheKey for $key_id {}\n        // so key can work w/ other methods, like delete:\n        impl crate::core::cache::CacheKey for $key_id {}\n    };\n}\n#[cfg(test)]\np<|fim_suffix|>\n#[derive(Clone)]\n#[enum_dispatch]\npub enum Cache {\n    Memory(memory::MemoryCache),\n    Redis(redis::RedisCache),\n    None(none::NoCache),\n}\n\nimpl Cache {\n    pub fn is_none(&self) -> bool {\n        matches!(*self, Cache::None(none::NoCache))\n    }\n}\n\nconst RETRY_SCHEDULE: &[Duration] = &[\n    Duration::from_millis(10),\n    Duration::from_millis(20),\n    Duration::from_millis(40),\n];\n\n#[async_trait]\n#[enum_dispatch(Cache)]\npub trait CacheBehavior: Sync + Send {\n    fn should_retry(&self, e: &Error) -> bool;\n    async fn get<T: CacheValue>(&self, key: &T::Key) -> Result<Option<T>> {\n        run_with_retries(\n            || async move {\n                self.get_raw(key.as_ref().as_bytes())\n                    .await?\n                    .map(|x| {\n                        String::from_utf8(x)\n                            .map_err(|e| e.into())\n                            .and_then(|json| serde_json::from_str(&json).map_err(|e| e.into()))\n                    })\n                    .transpose()\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>>;\n\n    async fn get_string<T: StringCacheKey>(&self, key: &T) -> Result<Option<String>> {\n        run_with_retries(\n            || async move {\n                self.get_raw(key.as_ref().as_bytes())\n                    .await?\n                    .map(|x| String::from_utf8(x).map_err(|e| e.into()))\n                    .transpose()\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set<T: CacheValue>(&self, key: &T::Key, value: &T, ttl: Duration) -> Result<()> {\n        run_with_retries(\n            || async move {\n                self.set_raw(\n                    key.as_ref().as_bytes(),\n                    serde_json::to_string(value)?.as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()>;\n\n    async fn set_string<T: StringCacheKey>(\n        &self,\n        key: &T,\n        value: &str,\n        ttl: Duration,\n    ) -> Result<()> {\n        run_with_retries(\n            || async move {\n                self.set_raw(key.as_ref().as_bytes(), value.to_string().as_bytes(), ttl)\n                    .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()>;\n\n    async fn set_if_not_exists<T: CacheValue>(\n        &self,\n        key: &T::Key,\n        value: &T,\n        ttl: Duration,\n    ) -> Result<bool> {\n        run_with_retries(\n            || async move {\n                self.set_raw_if_not_exists(\n                    key.as_ref().as_bytes(),\n                    serde_json::to_string(value)?.as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool>;\n\n    async fn set_string_if_not_exists<T: StringCacheKey>(\n        &self,\n        key: &T,\n        value: &str,\n        ttl: Duration,\n    ) -> Result<bool> {\n        run_with_retries(\n            || async move {\n                self.set_raw_if_not_exists(\n                    key.as_ref().as_bytes(),\n                    value.to_string().as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub(crate) use string_kv_def;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/mod.rs", "node_type": "use_declaration", "line_range": [117, 117]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::message_attempt_failing_event_data::MessageAttemptFailingEventData;\n\n/// Sent after a message has been failing for a few times.\n/// It's sent on the fourth failure. It complements `message.attempt.exhausted`\n/// which is sent after the last failure.\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct MessageAttemptFailingEvent {\n    pub data: MessageAttemptFailingEventData,\n\n    pub r#type: String,\n}\n\nimpl MessageAttemptFailingEvent {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(data: MessageAttemptFailingEventData, r#type: String) -> Self {\n        Self { data, r#type }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/message_attempt_failing_event.rs", "node_type": "function_item", "line_range": [17, 19]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\nuse super::operational_webhook_endpoint::OperationalWebhookEndpointArgs;\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct OperationalWebhookArgs {\n    #[command(subcommand)]\n    pub command: OperationalWebhookCommands,\n}\n\n#[derive(Subcommand)]\npub enum OperationalWebhookCommands {\n    Endpoint(OperationalWebhookEndpointArgs),\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl OperationalWebhookCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Endpoint(args) => {\n                args.command.exec(client, color_mode).await?;\n            }\n        }\n\n        Ok(())\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/operational_webhook.rs", "node_type": "impl_item", "line_range": [19, 33]}
{"prompt": "<|fim_prefix|>use std::{num::NonZeroUsize, str::FromStr};\n\nuse anyhow::Result;\nuse deadpool::unmanaged::Pool;\nuse deno_ast::{MediaType, ParseParams};\nuse deno_core::{\n    serde_v8, url,\n    v8::{self},\n    JsRuntime,\n};\nuse svix_bridge_types::{JsObject, TransformerInput, TransformerOutput};\nuse tokio::sync::oneshot;\n\nstruct Executor {\n    tx: std::sync::mpsc::Sender<Job>,\n    _handle: std::thread::JoinHandle<()>,\n}\n\nimpl Default for Executor {\n    fn default() -> Self {\n        let (tx, rx) = std::sync::mpsc::channel::<Job>();\n        let _handle = std::thread::spawn(move || {\n            let mut runtime = JsRuntime::new(Default::default());\n            for Job { input, script, cb } in rx {\n                let ret = run_script_inner(&mut runtime, input, script);\n                if cb.send(ret).is_err() {\n                    tracing::error!(\"failed to send script output to caller\");\n                }\n            }\n        });\n        Self { tx, _handle }\n    }\n}\n\ntype Callback = oneshot::Sender<Result<TransformerOutput>>;\n\nstruct Job {\n    input: TransformerInput,\n    script: String,\n    cb: Callback,\n}\n\nimpl Executor {\n    async fn execute(\n        &mut self,\n        input: TransformerInput,\n        script: String,\n    ) -> Result<TransformerOutput> {\n        let (tx, rx) = oneshot::channel();\n        self.tx.send(Job {\n            input,\n            script,\n            cb: tx,\n        })?;\n        rx.await?\n    }\n}\n\n#[derive(Clone)]\npub struct JsPooler {\n    executors: Pool<Executor>,\n}\n\nimpl JsPooler {\n    pub fn new(pool_size: NonZeroUsize) -> Self {\n        let pool_size = pool_size.get();\n        let mut items = Vec::with_capacity(pool_size);\n        for _ in 0..pool_size {\n            items.push(Executor::default());\n        }\n        Self {\n            executors: Pool::from(items),\n        }\n    }\n\n    pub async fn run_script(\n        &self,\n        input: TransformerInput,\n        script: String,\n    ) -> Result<TransformerOutput> {\n        let pool = self.executors.clone();\n        let mut executor = pool.get().await;\n\n        executor\n            .as_mut()\n            .map_err(|e| anyhow::anyhow!(\"{e:?}\"))?\n            .execute(input, script)\n            .await\n    }\n}\n\n/// Checks that the input parses as valid JavaScript, giving the parser's error back on failure.\npub fn validate_script(src: &str) -> Result<()> {\n    Ok(deno_ast::parse_script(ParseParams {\n        specifier: url::Url::from_str(\"file:///x.js\").expect(\"static string\"),\n        text: src.into(),\n        media_type: MediaType::JavaScript,\n        capture_tokens: false,\n        scope_analysis: false,\n        maybe_syntax: None,\n    })\n    .map(|_| ())?)\n}\n\nfn run_script_inner(\n    runtime: &mut JsRuntime,\n    input: TransformerInput,\n    script: String,\n) -> Result<TransformerOutput> {\n    let input = serde_json::to_string(&input)?;\n    let res = runtime.execute_script(\n        \"<anon>\",\n        format!(\n            // Wrap the user script, and invocation of `handler`, in a self-calling closure.\n            // The hope is we'll prevent the globals space from being polluted call after call.\n            r#\"\n    (function () {{\n        {script}\n        return handler({input});\n    }})()\n    \"#,\n        ),\n    );\n    match res {\n        Ok(global) => {\n            <|fim_suffix|>\n            let local = v8::Local::new(scope, global);\n            match serde_v8::from_v8::<JsObject>(scope, local) {\n                Ok(v) => Ok(TransformerOutput::Object(v)),\n                Err(e @ serde_v8::Error::ExpectedObject(_)) => {\n                    tracing::error!(\"{e}\");\n                    Ok(TransformerOutput::Invalid)\n                }\n                Err(e) => {\n                    tracing::error!(\"{e}\");\n                    Err(e)?\n                }\n            }\n        }\n        Err(err) => Err(anyhow::format_err!(\"Evaling error: {:?}\", err)),\n    }\n}\n\n#[cfg(test)]\nmod tests;\n<|fim_middle|>", "completion": "let scope = &mut runtime.handle_scope();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/mod.rs", "node_type": "let_declaration", "line_range": [126, 126]}
{"prompt": "<|fim_prefix|>\n\n#[derive(Debug)]\npub enum ErrorType {\n    /// A generic error\n    Generic(String),\n    /// Database error\n    Database(String),\n    /// Queue error\n    Queue(String),\n    /// Database error\n    Validation(String),\n    /// Any kind of HttpError\n    Http(HttpError),\n    /// Cache error\n    Cache(String),\n    /// Timeout error\n    Timeout(String),\n    /// Database timeout error\n    DbTimeout(String),\n    /// Connection timeout error\n    ConnectionTimeout(DbErr),\n    /// Conflict error\n    Conflict(DbErr),\n}\n\nimpl fmt::Display for ErrorType {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Generic(s) => s.fmt(f),\n            Self::Database(s) => s.fmt(f),\n            Self::Queue(s) => s.fmt(f),\n            Self::Validation(s) => s.fmt(f),\n            Self::Http(s) => s.fmt(f),\n            Self::Cache(s) => s.fmt(f),\n            Self::Timeout(s) => s.fmt(f),\n            Self::DbTimeout(s) => s.fmt(f),\n            Self::ConnectionTimeout(s) => s.fmt(f),\n            Self::Conflict(s) => s.fmt(f),\n        }\n    }\n}\n\nimpl From<HttpError> for ErrorType {\n    fn from(e: HttpError) -> Self {\n        Self::Http(e)\n    }\n}\n\n// Python generation relies on the title of this being `HttpError`\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HttpErrorOut\", title = \"HttpError\")]\npub struct StandardHttpError {\n    code: String,\n    detail: String,\n}\n\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HTTPValidationError\")]\npub struct ValidationHttpError {\n    detail: Vec<ValidationErrorItem>,\n}\n\n#[derive(Debug, Clone, Serialize)]\n#[serde(untagged)]\npub enum HttpErrorBody {\n    Standard(StandardHttpError),\n    Validation(ValidationHttpError),\n}\n\n#[derive(Debug, Clone, Serialize, PartialEq, Eq, JsonSchema)]\n/// Validation errors have their own schema to provide context for invalid requests eg. mismatched\n/// types and out of bounds values. There may be any number of these per 422 UNPROCESSABLE ENTITY\n/// error.\npub struct ValidationErrorItem {\n    /// The location as a [`Vec`] of [`String`]s -- often in the form `[\"body\", \"field_name\"]`,\n    /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, detail: String) -> Self {\n        Self {\n            status,\n            body: HttpErrorBody::Standard(StandardHttpError { code, detail }),\n        }\n    }\n\n    pub fn bad_request(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::BAD_REQUEST,\n            code.unwrap_or_else(|| \"generic_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Generic error\".to_owned()),\n        )\n    }\n\n    pub fn not_found(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_FOUND,\n            code.unwrap_or_else(|| \"not_found\".to_owned()),\n            detail.unwrap_or_else(|| \"Entity not found\".to_owned()),\n        )\n    }\n\n    pub fn unauthorized(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::UNAUTHORIZED,\n            code.unwrap_or_else(|| \"authentication_failed\".to_owned()),\n            detail.unwrap_or_else(|| \"Incorrect authentication credentials.\".to_owned()),\n        )\n    }\n\n    pub fn permission_denied(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::FORBIDDEN,\n            code.unwrap_or_else(|| \"insufficient access\".to_owned()),\n            detail.unwrap_or_else(|| \"Insufficient access for the given operation.\".to_owned()),\n        )\n    }\n\n    p<|fim_suffix|>\n    pub fn unprocessable_entity(detail: Vec<ValidationErrorItem>) -> Self {\n        Self {\n            status: StatusCode::UNPROCESSABLE_ENTITY,\n            body: HttpErrorBody::Validation(ValidationHttpError { detail }),\n        }\n    }\n\n    pub fn internal_server_error(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::INTERNAL_SERVER_ERROR,\n            code.unwrap_or_else(|| \"server_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Internal Server Error\".to_owned()),\n        )\n    }\n\n    pub fn not_implemented(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_IMPLEMENTED,\n            code.unwrap_or_else(|| \"not_implemented\".to_owned()),\n            detail.unwrap_or_else(|| \"This API endpoint is not yet implemented.\".to_owned()),\n        )\n    }\n\n    pub fn too_large(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::PAYLOAD_TOO_LARGE,\n            code.unwrap_or_else(|| \"payload_too_large\".to_owned()),\n            detail.unwrap_or_else(|| \"Request payload is too large.\".to_owned()),\n        )\n    }\n}\n\nimpl From<HttpError> for Error {\n    fn from(err: HttpError) -> Error {\n        Error::http(err)\n    }\n}\n\nimpl fmt::Display for HttpError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match &self.body {\n            HttpErrorBody::Standard(StandardHttpError { code, detail }) => write!(\n                f,\n                \"status={} code=\\\"{code}\\\" detail=\\\"{detail}\\\"\",\n                self.status\n            ),\n\n            HttpErrorBody::Validation(ValidationHttpError { detail }) => {\n                write!(\n                    f,\n                    \"status={} detail={}\",\n                    self.status,\n                    serde_json::to_string(&detail)\n                        .unwrap_or_else(|e| format!(\"\\\"unserializable error for {e}\\\"\"))\n                )\n            }\n        }\n    }\n}\n\nimpl IntoResponse for HttpError {\n    fn into_response(self) -> Response {\n        (self.status, Json(self.body)).into_response()\n    }\n}\n\nimpl From<ErrorType> for Error {\n    fn from(typ: ErrorType) -> Self {\n        Self { trace: vec![], typ }\n    }\n}\n\n// FIXME - delete\nimpl From<crate::core::webhook_http_client::Error> for Error {\n    fn from(err: webhook_http_client::Error) -> Error {\n        match err {\n            webhook_http_client::Error::TimedOut => Self::timeout(err),\n            _ => Error::generic(err),\n        }\n    }\n}\n\n/// Utility function for Converting a [`DbErr`] into an [`Error`].\n///\n/// The error \"duplicate key value violates unique constraint\" is converted to\n/// an HTTP \"conflict\" error. This is to be used in `map_err` calls on\n/// creation/update of records.\npub fn http_error_on_conflict(db_err: DbErr) -> Error {\n    if is_conflict_err(&db_err) {\n        HttpError::conflict(None, None).into()\n    } else {\n        Error::database(db_err)\n    }\n}\n\npub fn is_conflict_err(db_err: &DbErr) -> bool {\n    use DbErr as E;\n    let rt_err = match db_err {\n        E::Exec(e) | E::Query(e) | E::Conn(e) => e,\n        // If sqlx ever extends this enum, I want a compile time error so we're forced to update this function.\n        // Hence we list out all the enumerations, rather than using a default match statement\n        E::TryIntoErr { .. }\n        | E::ConvertFromU64(_)\n        | E::UnpackInsertId\n        | E::UpdateGetPrimaryKey\n        | E::RecordNotFound(_)\n        | E::AttrNotSet(_)\n        | E::Custom(_)\n        | E::Type(_)\n        | E::Json(_)\n        | E::Migration(_)\n        | E::RecordNotInserted\n        | E::RecordNotUpdated\n        | E::ConnectionAcquire(_) => return false,\n    };\n\n    let sqlx_err = match rt_err {\n        RuntimeErr::SqlxError(e) => e,\n        RuntimeErr::Internal(_) => return false,\n    };\n\n    sqlx_err\n        .as_database_error()\n        .and_then(|e| e.code())\n        .filter(|code| code == \"23505\")\n        .is_some()\n}\n\npub fn is_timeout_error(db_err: &DbErr) -> bool {\n    let run<|fim_middle|>", "completion": "pub fn conflict(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::CONFLICT,\n            code.unwrap_or_else(|| \"conflict\".to_owned()),\n            detail.unwrap_or_else(|| \"A conflict has occurred\".to_owned()),\n        )\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "function_item", "line_range": [352, 358]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::collections::HashMap;\n\nuse aide::axum::{\n    routing::{delete_with, get_with, post_with},\n    ApiRouter,\n};\nuse axum::{\n    extract::{Path, State},\n    Json,\n};\nuse chrono::{DateTime, Duration, Utc};\nuse hyper::StatusCode;\nuse schemars::JsonSchema;\nu<|fim_suffix|>use serde::{Deserialize, Deserializer, Serialize};\nuse serde_json::value::RawValue;\nuse svix_server_derive::{aide_annotate, ModelOut};\nuse validator::Validate;\n\nuse crate::{\n    core::{\n        permissions,\n        types::{\n            BaseId, EndpointId, EndpointIdOrUid, EventChannel, EventTypeNameSet, MessageAttemptId,\n            MessageAttemptTriggerType, MessageId, MessageStatus, MessageStatusText,\n            StatusCodeClass,\n        },\n    },\n    db::models::{\n        endpoint, message,\n        messageattempt::{self, Query},\n        messagecontent,\n    },\n    error::{Error, HttpError, Result},\n    queue::MessageTask,\n    v1::{\n        endpoints::message::MessageOut,\n        utils::{\n            filter_and_paginate_time_limited, openapi_tag, ApplicationEndpointPath,\n            ApplicationMsgAttemptPath, ApplicationMsgEndpointPath, ApplicationMsgPath,\n            EventTypesQueryParams, IteratorDirection, ListResponse, ModelOut, NoContentWithCode,\n            PaginationDescending, PaginationLimit, ReversibleIterator, ValidatedQuery,\n        },\n    },\n    AppState,\n};\n\nfn example_status_code() -> i16 {\n    200\n}\n\nfn example_endpoint_url() -> &'static str {\n    \"https://example.com/webhook/\"\n}\n\nfn example_attempt_response() -> &'static str {\n    \"{}\"\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageAttemptOut {\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: String,\n    #[schemars(example = \"example_attempt_response\")]\n    pub response: String,\n    #[schemars(example = \"example_status_code\")]\n    pub response_status_code: i16,\n    /// Response duration in milliseconds.\n    pub response_duration_ms: i64,\n    pub status: MessageStatus,\n    pub status_text: MessageStatusText,\n    pub trigger_type: MessageAttemptTriggerType,\n    pub msg_id: MessageId,\n    pub endpoint_id: EndpointId,\n\n    pub id: MessageAttemptId,\n\n    #[serde(rename = \"timestamp\")]\n    pub created_at: DateTime<Utc>,\n}\n\nimpl From<messageattempt::Model> for MessageAttemptOut {\n    fn from(model: messageattempt::Model) -> Self {\n        Self {\n            url: model.url,\n            response: model.response,\n            response_status_code: model.response_status_code,\n            response_duration_ms: model.response_duration_ms,\n            status: model.status,\n            status_text: model.status.into(),\n            trigger_type: model.trigger_type,\n            msg_id: model.msg_id,\n            endpoint_id: model.endp_id,\n\n            id: model.id,\n            created_at: model.created_at.into(),\n        }\n    }\n}\n\n/// A model containing information on a given message plus additional fields on the last attempt for\n/// that message.\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointMessageOut {\n    #[serde(flatten)]\n    pub msg: MessageOut,\n    pub status: MessageStatus,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub next_attempt: Option<DateTimeWithTimeZone>,\n}\n\nimpl ModelOut for EndpointMessageOut {\n    fn id_copy(&self) -> String {\n        self.msg.id.0.clone()\n    }\n}\n\nimpl EndpointMessageOut {\n    pub fn from_attempt_and_msg(\n        attempt: messageattempt::Model,\n        msg: message::Model,\n        msg_content: Option<Vec<u8>>,\n        with_content: bool,\n    ) -> EndpointMessageOut {\n        let status = if attempt.next_attempt.is_some() {\n            MessageStatus::Sending\n        } else {\n            attempt.status\n        };\n        EndpointMessageOut {\n            msg: MessageOut::from_msg_and_payload(msg, msg_content, with_content),\n            status,\n            next_attempt: attempt.next_attempt,\n        }\n    }\n}\n\n// XXX: only used in tests, so OK if it's a bit hacky\nimpl<'de> Deserialize<'de> for EndpointMessageOut {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: Deserializer<'de>,\n    {\n        let raw_value = Box::<RawValue>::deserialize(deserializer)?;\n        let rest: HashMap<&str, &RawValue> = serde_json::from_str(raw_value.get()).unwrap();\n        Ok(Self {\n            msg: serde_json::from_str(raw_value.get()).unwrap(),\n            status: serde_json::from_str(rest.get(\"status\").unwrap().get()).unwrap(),\n            next_attempt: rest\n                .get(\"next_attempt\")\n                .map(|x| serde_json::from_str(x.get()).unwrap()),\n        })\n    }\n}\n\n/// Additional parameters (besides pagination) in the query string for the \"List Attempted Messages\"\n/// endpoint.\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct ListAttemptedMessagesQueryParams {\n    /// Filter response based on the channel\n    #[validate]\n    channel: Option<EventChannel>,\n    /// Filter response based on the delivery status\n    status: Option<MessageStatus>,\n    /// Only include items created before a certain date\n    before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    after: Option<DateTime<Utc>>,\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\nfn default_true() -> bool {\n    true\n}\n\npub const FUTURE_QUERY_LIMIT: chrono::Duration = chrono::Duration::hours(1);\npub const LIMITED_QUERY_DURATION: chrono::Duration = chrono::Duration::days(90);\n\npub fn limit_messageattempt_join<Q: QuerySelect + QueryOrder + QueryFilter>(\n    mut query: Q,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n    now: DateTime<Utc>,\n) -> Q {\n    const SORT_COLUMN: messageattempt::Column = messageattempt::Column::Id;\n    if let Some(before) = before {\n        query = query\n            .filter(SORT_COLUMN.gt(MessageAttemptId::start_id(before - LIMITED_QUERY_DURATION)));\n        query = query\n            .filter(SORT_COLUMN.lt(MessageAttemptId::start_id(before + LIMITED_QUERY_DURATION)));\n    } else {\n        query =\n            query.filter(SORT_COLUMN.gt(MessageAttemptId::start_id(now - LIMITED_QUERY_DURATION)));\n    }\n    if let Some(after) = after {\n        query = query.filter(SORT_COLUMN.gt(MessageAttemptId::start_id(after)));\n        query =\n            query.filter(SORT_COLUMN.lt(MessageAttemptId::end_id(after + LIMITED_QUERY_DURATION)));\n    }\n\n    // blanket limit on future\n    query.filter(SORT_COLUMN.lt(MessageAttemptId::start_id(now + FUTURE_QUERY_LIMIT)))\n}\n\nfn limit_message_join<Q: QuerySelect + QueryOrder + QueryFilter>(\n    mut query: Q,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n    now: DateTime<Utc>,\n) -> Q {\n    const SORT_COLUMN: message::Column = message::Column::Id;\n    if let Some(before) = before {\n        query = query\n            .filter(SORT_COLUMN.gt(MessageId::start_id(before - LIMITED_QUERY_DURATION)))\n            .filter(SORT_COLUMN.lt(MessageId::end_id(before)));\n    } else {\n        query = query.filter(SORT_COLUMN.gt(MessageId::start_id(now - LIMITED_QUERY_DURATION)));\n    }\n    if let Some(after) = after {\n        query = query\n            .filter(SORT_COLUMN.lt(MessageId::end_id(after + LIMITED_QUERY_DURATION)))\n            .filter(SORT_COLUMN.gt(MessageId::start_id(after)));\n    }\n\n    // blanket limit on future\n    query.filter(SORT_COLUMN.lt(MessageId::start_id(now + FUTURE_QUERY_LIMIT)))\n}\n\n/// List messages for a particular endpoint. Additionally includes metadata about the latest message attempt.\n///\n/// The `before` parameter lets you filter all items created before a certain date and is ignored if an iterator is passed.\n#[aide_annotate(op_id = \"v1.message-attempt.list-attempted-messages\")]\nasync fn list_attempted_messages(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<PaginationDescending<ReversibleIterator<MessageId>>>,\n    Validate<|fim_middle|>", "completion": "use sea_orm::{entity::prelude::*, IntoActiveModel, QueryOrder, QuerySelect, QueryTrait};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/attempt.rs", "node_type": "use_declaration", "line_range": [17, 17]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n<|fim_suffix|>\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct BackgroundTaskOut {\n    pub data: serde_json::Value,\n\n    /// The QueueBackgroundTask's ID.\n    pub id: String,\n\n    pub status: BackgroundTaskStatus,\n\n    pub task: BackgroundTaskType,\n}\n\nimpl BackgroundTaskOut {\n    pub fn new(\n        data: serde_json::Value,\n        id: String,\n        status: BackgroundTaskStatus,\n        task: BackgroundTaskType,\n    ) -> Self {\n        Self {\n            data,\n            id,\n            status,\n            task,\n        }\n    }\n}\n<|fim_middle|>", "completion": "use super::{\n    background_task_status::BackgroundTaskStatus, background_task_type::BackgroundTaskType,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/background_task_out.rs", "node_type": "use_declaration", "line_range": [4, 6]}
{"prompt": "<|fim_prefix|>ndpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([(\"x-test-3\".to_owned(), \"4\".to_owned()),]),\n        recvd_headers.headers\n    );\n\n    let endp = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap();\n\n    for bad_hdr in [\n        \"content-length\",\n        \"some:thing\",\n        \"some\\u{0000}thing\",\n        \"svix-foo\",\n        \"x-svix-foo\",\n        \"x-amzn-foo\",\n    ] {\n        let _: IgnoredAny = client\n            .put(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                json!({\n                    \"headers\": { bad_hdr: \"123\" },\n                }),\n                StatusCode::UNPROCESSABLE_ENTITY,\n            )\n            .await\n            .unwrap();\n    }\n\n    let org_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };\n\n    let updated_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"3\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };\n\n    for hdrs in [&org_headers, &updated_headers] {\n        client\n            .put_without_response(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                hdrs,\n                StatusCode::NO_CONTENT,\n            )\n            .await\n            .unwrap();\n\n        let recvd_headers: EndpointHeadersOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(hdrs.headers.0, recvd_headers.headers);\n    }\n\n    let patched_headers_in = EndpointHeadersPatchIn {\n        headers: EndpointHeadersPatch(HashMap::from([\n            (\"x-test-3\".to_owned(), Some(\"4\".to_owned())),\n            (\"x-test-2\".to_owned(), None),\n        ])),\n    };\n\n    client\n        .patch_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            &patched_headers_in,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let recvd_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([\n            (\"x-test-1\".to_owned(), \"3\".to_owned()),\n            (\"x-test-3\".to_owned(), \"4\".to_owned()),\n        ]),\n        recvd_headers.headers\n    );\n\n    let redacted_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"authorization\".to_owned(), \"secret\".to_owned()),\n        ])),\n    };\n\n    client\n        .put_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            redacted_headers,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let recvd_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([(\"x-test-1\".to_owned(), \"1\".to_owned())]),\n        recvd_headers.headers\n    );\n\n    assert_eq!(\n        HashSet::from([\"authorization\".to_owned()]),\n        recvd_headers.sensitive\n    );\n}\n\n#[tokio::test]\nasync fn test_endpoint_headers_sending() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let endp = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };\n\n    client\n        .put_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            &headers,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n\n    for (k, v) in &headers.headers.0 {\n        assert_eq!(v, last_headers.get(k).unwrap().to_str().unwrap());\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_header_key_capitalization() {\n    l<|fim_suffix|>\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let endp = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([(\n            \"X-Api-Test\".to_owned(),\n            \"test-value\".to_owned(),\n        )])),\n    };\n\n    client\n        .put_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            &headers,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let retrieved_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    for k in headers.headers.0.keys() {\n        assert!(retrieved_headers.headers.contains_key(k));\n    }\n}\n\n/// Tests the endpoint_https_only configuration\n#[tokio::test]\nasync fn test_endpoint_https_only() {\n    let http_url = \"http://www.example.com\";\n    let https_url = \"https://www.example.com\";\n\n    // No https enforcement (default)\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"App 1\").await.unwrap().id;\n\n    assert!(!cfg.endpoint_https_only);\n\n    let _endpoint: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endpoint_in(https_url),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _endpoint: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endpoint_in(http_url),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Enforce https\n    let mut cfg = get_default_test_config();\n    cfg.endpoint_https_only = true;\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"App 1\").await.unwrap().id;\n\n    assert!(cfg.endpoint_https_only);\n\n    let _endpoint: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endpoint_in(https_url),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endpoint_in(http_url),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_send_example() {\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let app_id = create_test_app(&client, \"Test App\").await.unwrap().id;\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/send-example/\"),\n            json!({ \"eventType\": \"svix.ping\" }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    let msg = receiver.data_recv.recv().await.unwrap();\n    assert_eq!(msg, serde_json::json!({ \"success\": true }));\n}\n<|fim_middle|>", "completion": "let (client, _jk) = start_svix_server().await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [2244, 2244]}
{"prompt": "<|fim_prefix|>use opentelemetry::metrics::{Meter, ObservableGauge};\n\nfn init_metric<T, E: std::fmt::Display>(result: Result<T, E>) -> Option<T> {\n    match result {\n        Ok(t) => Some(t),\n        Err(e) => {\n            tracing::error!(\"Failed to initialize metric: {}\", e);\n            None\n        }\n    }\n}\n\n#[derive(Clone)]\npub struct CommonMetrics {\n    mem_allocated_recorder: Option<ObservableGauge<u64>>,\n    mem_resident_recorder: Option<ObservableGauge<u64>>,\n}\n\nimpl CommonMetrics {\n    pub fn new(meter: &Meter) -> Self {\n        let mem_resident_recorder =\n            init_metric(meter.u64_observable_gauge(\"svix.mem_resident\").try_init());\n        let mem_allocated_recorder =\n            init_metric(meter.u64_observable_gauge(\"svix.mem_allocated\").try_init());\n\n        Self {\n            mem_allocated_recorder,\n            mem_resident_recorder,\n        }\n    }\n\n    pub fn record_mem_allocated(&self, value: u64) {\n        <|fim_suffix|>\n    }\n\n    pub fn record_mem_resident(&self, value: u64) {\n        if let Some(ref recorder) = self.mem_resident_recorder {\n            recorder.observe(value, &[]);\n        }\n    }\n}\n<|fim_middle|>", "completion": "if let Some(ref recorder) = self.mem_allocated_recorder {\n            recorder.observe(value, &[]);\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/metrics.rs", "node_type": "if_expression", "line_range": [33, 35]}
{"prompt": "<|fim_prefix|>use deno_core::JsRuntime;\nuse serde_json::json;\nuse svix_bridge_types::{TransformerInput, TransformerOutput};\n\nuse super::{run_script_inner, validate_script};\n\nfn get_test_rt() -> JsRuntime {\n    JsRuntime::new(Default::default())\n}\n\n// Really just trying to figure out if the deno runtime is working the way I hope.\n#[test]\nfn test_happy_fn() {\n    let src = r#\"\n    function handler(input) {\n        return { \"x\": 123, ...input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({ \"y\": 456 }).into(), src).unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n            assert_eq!(v[\"y\"].as_i64(), Some(456));\n        }\n        TransformerOutput::Invalid => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\nfn test_invalid_output_bool() {\n    <|fim_suffix|>\n\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\n// FIXME: serde decodes arrays with keys like \"0\", \"1\"... in this situation, failing the test.\n#[ignore]\nfn test_invalid_output_array() {\n    let src = r#\"\n    function handler(input) {\n        return [1, 2];\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => {\n            panic!(\"got unexpected return value\");\n        }\n    }\n}\n\n/// Receives a string input, parses as JSON in js, then returns the result back to rust.\n#[test]\nfn test_string_input() {\n    let src = r#\"\n    function handler(input) {\n        return JSON.parse(input);\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(r#\"{\"x\": 123}\"#)),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n/// Take the string input and just add it to a field in the returned object.\n/// The string should make it through, back to rust, as-is.\n#[test]\nfn test_string_input2() {\n    let src = r#\"\n    function handler(input) {\n        return { \"payload\": input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(\"Hello World\")),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"payload\"].as_str(), Some(\"Hello World\"));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n#[test]\nfn test_validate_script_bad_syntax_is_err() {\n    assert!(validate_script(\"let 123 = ';\").is_err());\n}\n\n#[test]\nfn test_validate_script_empty_handler_is_ok() {\n    assert!(validate_script(\"function handler() { }\").is_ok());\n}\n\n#[test]\nfn test_validate_script_arrow_fn_is_ok() {\n    assert!(validate_script(\"const handler = () => ({ a: 123 })\").is_ok());\n}\n\n/// Technically, this should be legal though the utility is questionable.\n#[test]\nfn test_validate_script_empty_is_ok() {\n    assert!(validate_script(\"\").is_ok());\n    assert!(validate_script(\"    \").is_ok());\n}\n<|fim_middle|>", "completion": "let src = r#\"\n    function handler(input) {\n        return false;\n    }\n    \"#\n    .to_string();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/tests.rs", "node_type": "let_declaration", "line_range": [33, 38]}
{"prompt": "<|fim_prefix|>dpoint_ids,\n        vec![endp_1.id.clone(), endp_2.id.clone(), endp_3.id.clone()]\n    );\n\n    // Delete one endpoint\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", endp_2.id),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Should still see all 3 endpoints (including deleted)\n    let destinations_after_delete: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(destinations_after_delete.data.len(), 3);\n\n    let first_page: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/?limit=2\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(first_page.data.len(), 2);\n    let endpoint_ids: Vec<_> = first_page.data.iter().map(|d| d.id.clone()).collect();\n    assert_eq!(endpoint_ids, vec![endp_1.id.clone(), endp_2.id.clone()]);\n\n    let next_iter = first_page.iterator.unwrap().clone();\n    let second_page: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/?limit=2&iterator={next_iter}\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(second_page.data.len(), 1);\n    let endpoint_ids: Vec<_> = second_page.data.iter().map(|d| d.id.clone()).collect();\n    assert_eq!(endpoint_ids, vec![endp_3.id.clone()]);\n\n    // Test backward pagination\n    let all_destinations: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    let last_id = &all_destinations.data.last().unwrap().id;\n\n    let prev_page: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/?limit=1&iterator=-{last_id}\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(prev_page.data.len(), 1);\n    let endpoint_ids: Vec<_> = prev_page.data.iter().map(|d| &d.id).collect();\n    assert_eq!(endpoint_ids, vec![&endp_2.id]);\n\n    let prev_iter = prev_page.prev_iterator.unwrap();\n    let prev_page: ListResponse<MessageEndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/endpoint/?limit=1&iterator={prev_iter}\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(prev_page.data.len(), 1);\n    let endpoint_ids: Vec<_> = prev_page.data.iter().map(|d| &d.id).collect();\n    assert_eq!(endpoint_ids, vec![&endp_1.id]);\n\n    receiver_1.jh.abort();\n    receiver_2.jh.abort();\n    receiver_3.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_list_attempts_by_endpoint() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1AttemptListAttemptsByEndpointTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n    let endp_id_2 = create_test_endpoint(&client, &app_id, &receiver_2.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"user.exploded\",\n        [\"obits\"],\n    )\n    .await;\n\n    // And wait at most one second for all attempts to be processed\n    run_with_retries(|| async {\n        for endp_id in [endp_id_1.clone(), endp_id_2.clone()] {\n            l<|fim_suffix|>\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_1: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_1}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    let list_2: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    for list in [list_1, list_2] {\n        let message_ids: Vec<_> = list.data.into_iter().map(|amo| amo.msg_id).collect();\n        assert!(message_ids.contains(&msg_1.id));\n        assert!(message_ids.contains(&msg_2.id));\n        assert!(message_ids.contains(&msg_3.id));\n    }\n\n    let foo_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?channel=foo\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert!(foo_attempts.data.is_empty());\n\n    let obits_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?channel=obits\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(obits_attempts.data.len(), 1);\n\n    let exploded_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types=user.exploded\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(exploded_attempts.data.len(), 1);\n\n    let regular_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types[]=event.type\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(regular_attempts.data.len(), 2);\n\n    let all_attempts_1: ListResponse<MessageAttemptOut> = client\n    .get(\n        &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types[0]=event.type&event_types[1]=user.exploded\"),\n        StatusCode::OK,\n    )\n    .await\n    .unwrap();\n    assert_eq!(all_attempts_1.data.len(), 3);\n\n    let all_attempts_2: ListResponse<MessageAttemptOut> = client\n    .get(\n        &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types=event.type,user.exploded\"),\n        StatusCode::OK,\n    )\n    .await\n    .unwrap();\n    assert_eq!(all_attempts_2.data.len(), 3);\n\n    receiver_1.jh.abort();\n    receiver_2.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_message_attempts() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = (0..2).map(|_| Duration::from_millis(1)).collect();\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    for (status_code, msg_status, attempt_count) in [\n        // Success\n        (StatusCode::OK, MessageStatus::Success, Some(1)),\n        // HTTP 400\n        (StatusCode::FORBIDDEN, MessageStatus::Fail, None),\n        // HTTP 500\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None),\n    ] {\n        let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n        let receiver = TestReceiver::start(status_code);\n\n        let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n            .await\n            .unwrap()\n            .id;\n\n        let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n            .await\n            .unwrap();\n\n        let list = get_msg_attempt_list_and_assert_count(\n            &client,\n            &app_id,\n            &msg.id,\n            attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n        )\n        .await\n        .unwrap();\n\n        for i in list.data.iter() {\n            assert_eq!(i.status, msg_status);\n            println!(\"{} {status_code}\", i.response_status_code);\n            assert_eq!(\n           <|fim_middle|>", "completion": "let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id}/\"),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [625, 631]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse reqwest::StatusCode;\nuse serde_json::json;\nuse svix_server::{\n    core::types::{EndpointUid, MessageStatus},\n    v1::{\n        endpoints::{\n            attempt::{EndpointMessageOut, MessageAttemptOut},\n            endpoint::{EndpointIn, EndpointOut},\n        },\n        utils::ListResponse,\n    },\n};\nu<|fim_suffix|>\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, create_test_msg_with,\n        endpoint_in, get_msg_attempt_list_and_assert_count,\n    },\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n    TestReceiver,\n};\n\n#[tokio::test]\nasync fn test_expunge_attempt_response_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let sensitive_response_json = serde_json::json!({\"sensitive\":\"data\"});\n    let mut receiver = TestReceiver::start_with_body(\n        axum::http::StatusCode::OK,\n        axum::Json(sensitive_response_json.clone()),\n    );\n\n    let endpoint_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap()\n        .id;\n\n    receiver.data_recv.recv().await;\n\n    let attempt = run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endpoint_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        if attempts.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", attempts.data.len());\n        }\n        Ok(attempts.data[0].clone())\n    })\n    .await\n    .unwrap();\n\n    let attempt_response: serde_json::Value = serde_json::from_str(&attempt.response).unwrap();\n    assert_eq!(sensitive_response_json, attempt_response);\n\n    let attempt_id = &attempt.id;\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let attempt: MessageAttemptOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\"EXPUNGED\", &attempt.response);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_te<|fim_middle|>", "completion": "use wiremock::{matchers, Mock, MockServer, Respond, ResponseTemplate};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "use_declaration", "line_range": [21, 21]}
{"prompt": "<|fim_prefix|> {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let mut input = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            input.insert(\"__TRANSFORMED__\".into(), json!(true));\n            let out = json!({ \"payload\": input });\n\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"transformed\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: Some(\n                    \"handler = (x) => ({ payload: {__TRANSFORMED__: true, ...x }})\".into(),\n                ),\n            },\n        ),\n        (\n            \"as-is\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The `__TRANSFORMED__` key should have been added\n    assert_eq!(\n        json!(forwarded),\n        json!({\"a\": true, \"__TRANSFORMED__\": true})\n    );\n\n    let request = Request::builder()\n        .uri(\"/webhook/as-is\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    // The same payload should come through, without any transformation.\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_transformation_string() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let out = match x.input {\n                TransformerInput::String(input) => json!({\"payload\": { \"got\": input }})\n                    .as_object()\n                    .cloned()\n                    .unwrap(),\n                _ => unreachable!(),\n            };\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"transformed\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: Some(TransformationConfig::Explicit {\n                format: TransformerInputFormat::String,\n                src: String::from(\"handler = (x) => ({ payload: { got: x }})\"),\n            }),\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"text/plain\")\n        .body(axum::body::Body::from(\"plain text\"))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The plain text message should have been added in the key \"got\"\n    assert_eq!(json!(forwarded), json!({\"got\": \"plain text\"}));\n\n    assert!(a_rx.try_recv().is_err());\n}\n\n// Two different bodies - one used during signing, then the other is what we send in the request.\n// This should result in a bad response status.\n#[tokio::test]\nasync fn test_forwarding_svix_verification_mismatch() {\n    let signed_payload_bytes = serde_json::to_vec(&json!({\"a\": true})).unwrap();\n    let sent_payload_bytes = serde_json::to_vec(&json!({\"a\": false})).unwrap();\n\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n\n    let webhook = Arc::new(Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap());\n\n    let timestamp = chrono::Utc::now().timestamp();\n    let signature = webhook\n        .sign(\"msg_valid\", timestamp, &signed_payload_bytes)\n        .unwrap();\n\n    let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: SvixVerifier::new(webhook).into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .header(\"svix-id\", \"msg_valid\")\n                .header(\"svix-signature\", signature.clone())\n                .header(\"svix-timestamp\", &format!(\"{timestamp}\"))\n                .body(axum::body::Body::from(sent_payload_bytes))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n\n    // Expect a rejection due to signature verification failure.\n    assert_eq!(response.status(), StatusCode::BAD_REQUEST);\n    // There should be noting in the channel since the request should _not have been forwarded_.\n    assert!(a_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_forwarding_svix_verification_match() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n\n    let webhook = Arc::new(Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap());\n\n    <|fim_suffix|>\n    let payload_bytes = serde_json::to_vec(&payload).unwrap();\n    let timestamp = chrono::Utc::now().timestamp();\n    let signature = webhook\n        .sign(\"msg_valid\", timestamp, &payload_bytes)\n        .unwrap();\n\n    let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: SvixVerifier::new(webhook).into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .header(\"svix-id\", \"msg_valid\")\n                .header(\"svix-signature\", signature.clone())\n                .header(\"svix-timestamp\", timestamp.to_string())\n                .body(axum::body::Body::from(payload_bytes))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({ \"a\": true }));\n}\n<|fim_middle|>", "completion": "let payload = json!({\"a\": true});", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/tests.rs", "node_type": "let_declaration", "line_range": [359, 359]}
{"prompt": "<|fim_prefix|>use std::time::Duration;\n\nuse svix::api::{Svix, SvixOptions};\n\npub struct TestClient {\n    pub client: Svix,\n}\n\npub struct TestClientBuilder {\n    token: Option<String>,\n    url: Option<String>,\n    retries: Option<u32>,\n    retry_schedule: Option<Vec<Duration>>,\n}\n\nimpl TestClientBuilder {\n    pub fn new() -> Self {\n        Self {\n            token: None,\n            url: None,\n            retries: None,\n            retry_schedule: None,\n        }\n    }\n\n    #[allow(unused)]\n    pub fn token(mut self, token: String) -> Self {\n        self.token = Some(token);\n        self\n    }\n\n    pub fn url(mut self, url: String) -> Self {\n        self.url = Some(url);\n        self\n    }\n\n    pub fn retries(mut self, retries: u32) -> Self {\n        self.retries = Some(retries);\n        self\n    }\n\n    pub fn retry_schedule(mut self, retry_schedule: Vec<Duration>) -> Self {\n        self.retry_schedule = Some(retry_schedule);\n        self\n    }\n\n    pub fn build(self) -> TestClient {\n        let token = self.token.unwrap_or_else(|| {\n            std::env::var(\"SVIX_TOKEN\").expect(\"SVIX_TOKEN is required to run this test\")\n        });\n        <|fim_suffix|>\n        let client = Svix::new(\n            token.clone(),\n            Some(SvixOptions {\n                server_url: Some(url.clone()),\n                num_retries: self.retries,\n                retry_schedule: self.retry_schedule,\n                ..Default::default()\n            }),\n        );\n\n        TestClient { client }\n    }\n}\n<|fim_middle|>", "completion": "let url = self.url.unwrap_or_else(|| {\n            std::env::var(\"SVIX_SERVER_URL\").expect(\"SVIX_SERVER_URL is required to run this test\")\n        });", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/tests/it/utils/test_client.rs", "node_type": "let_declaration", "line_range": [51, 53]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct StreamingSinkListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct StreamingSinkCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct StreamingSinkRotateSecretOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct StreamingSink<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> StreamingSink<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List of all the stream's sinks.\n    pub async fn list(\n        &self,\n        stream_id: String,\n        options: Option<StreamingSinkListOptions>,\n    ) -> Result<ListResponseStreamSinkOut> {\n        let StreamingSinkListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/stream/{stream_id}/sink\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Creates a new sink.\n    pub async fn create(\n        &self,\n        stream_id: String,\n        stream_sink_in: StreamSinkIn,\n        options: Option<StreamingSinkCreateOptions>,\n    ) -> Result<StreamSinkOut> {\n        let StreamingSinkCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/stream/{stream_id}/sink\")\n            .with_path_param(\"stream_id\", stream_id)\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(stream_sink_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get a sink by id or uid.\n    pub async fn get(&self, stream_id: String, sink_id: String) -> Result<StreamSinkOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Update a sink.\n    pub async fn update(\n        &self,\n        stream_id: String,\n        sink_id: String,\n        stream_sink_in: StreamSinkIn,\n    ) -> Result<StreamSinkOut> {\n        crate::request::Request::new(\n            http1::Method::PUT,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .with_body_param(stream_sink_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Delete a sink.\n    pub async fn delete(&self, stream_id: String, sink_id: String) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Partially update a sink.\n    pub async fn patch(\n        &self,\n        stream_id: String,\n        sink_id: String,\n        stream_sink_patch: StreamSinkPatch,\n    ) -> Result<StreamSinkOut> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .with_body_param(stream_sink_patch)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get the sink's signing secret (only supported for http sinks)\n    ///\n    /// This is used to verify the authenticity of the delivery.\n    ///\n    /// For more information please refer to [the consuming webhooks docs](https://docs.svix.com/consuming-webhooks/).\n    <|fim_suffix|>\n\n    /// Rotates the signing secret (only supported for http sinks).\n    pub async fn rotate_secret(\n        &self,\n        stream_id: String,\n        sink_id: String,\n        endpoint_secret_rotate_in: EndpointSecretRotateIn,\n        options: Option<StreamingSinkRotateSecretOptions>,\n    ) -> Result<EmptyResponse> {\n        let StreamingSinkRotateSecretOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/secret/rotate\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .with_body_param(endpoint_secret_rotate_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Set or unset the transformation code associated with this sink.\n    pub async fn transformation_partial_update(\n        &self,\n        stream_id: String,\n        sink_id: String,\n        sink_transform_in: SinkTransformIn,\n    ) -> Result<EmptyResponse> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/transformation\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .with_body_param(sink_transform_in)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub async fn get_secret(&self, stream_id: String, sink_id: String) -> Result<SinkSecretOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/secret\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .execute(self.cfg)\n        .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/streaming_sink.rs", "node_type": "function_item", "line_range": [139, 148]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct MessagePollerPollOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filters messages sent with this event type (optional).\n    #[arg(long)]\n    pub event_type: Option<String>,\n    /// Filters messages sent with this channel (optional).\n    #[arg(long)]\n    pub channel: Option<String>,\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n}\n\nimpl From<MessagePollerPollOptions> for svix::api::MessagePollerPollOptions {\n    fn from(value: MessagePollerPollOptions) -> Self {\n        <|fim_suffix|>\n        Self {\n            limit,\n            iterator,\n            event_type,\n            channel,\n            after: after.map(|dt| dt.to_rfc3339()),\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessagePollerConsumerPollOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n}\n\nimpl From<MessagePollerConsumerPollOptions> for svix::api::MessagePollerConsumerPollOptions {\n    fn from(value: MessagePollerConsumerPollOptions) -> Self {\n        let MessagePollerConsumerPollOptions { limit, iterator } = value;\n        Self { limit, iterator }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessagePollerConsumerSeekOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<MessagePollerConsumerSeekOptions> for svix::api::MessagePollerConsumerSeekOptions {\n    fn from(value: MessagePollerConsumerSeekOptions) -> Self {\n        let MessagePollerConsumerSeekOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct MessagePollerArgs {\n    #[command(subcommand)]\n    pub command: MessagePollerCommands,\n}\n\n#[derive(Subcommand)]\npub enum MessagePollerCommands {\n    /// Reads the stream of created messages for an application, filtered on the Sink's event types and Channels.\n    Poll {\n        app_id: String,\n        sink_id: String,\n        #[clap(flatten)]\n        options: MessagePollerPollOptions,\n    },\n    /// Reads the stream of created messages for an application, filtered on the Sink's event types and\n    /// Channels, using server-managed iterator tracking.\n    ConsumerPoll {\n        app_id: String,\n        sink_id: String,\n        consumer_id: String,\n        #[clap(flatten)]\n        options: MessagePollerConsumerPollOptions,\n    },\n    /// Sets the starting offset for the consumer of a polling endpoint.\n    ConsumerSeek {\n        app_id: String,\n        sink_id: String,\n        consumer_id: String,\n        polling_endpoint_consumer_seek_in: crate::json::JsonOf<PollingEndpointConsumerSeekIn>,\n        #[clap(flatten)]\n        options: MessagePollerConsumerSeekOptions,\n    },\n}\n\nimpl MessagePollerCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Poll {\n                app_id,\n                sink_id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .poll(app_id, sink_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ConsumerPoll {\n                app_id,\n                sink_id,\n                consumer_id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .consumer_poll(app_id, sink_id, consumer_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ConsumerSeek {\n                app_id,\n                sink_id,\n                consumer_id,\n                polling_endpoint_consumer_seek_in,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .consumer_seek(\n                        app_id,\n                        sink_id,\n                        consumer_id,\n                        polling_endpoint_consumer_seek_in.into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let MessagePollerPollOptions {\n            limit,\n            iterator,\n            event_type,\n            channel,\n            after,\n        } = value;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/message_poller.rs", "node_type": "let_declaration", "line_range": [25, 31]}
{"prompt": "<|fim_prefix|>//! Module that test the dashboard-access endpoint and associated JWT tokens. This module will test\n//! that the tokens returned by the endpoint have restricted functionality and that the response\n//! from the endpoint is valid in the process.\n\nuse rand::distributions::DistString;\nuse reqwest::StatusCode;\nuse serde::de::IgnoredAny;\nuse serde_json::{json, Value};\nuse svix_server::{\n    core::{\n        security::{INVALID_TOKEN_ERR, JWT_SECRET_ERR},\n        types::ApplicationId,\n    },\n    v1::endpoints::application::ApplicationOut,\n};\n\nuse crate::utils::{\n    common_calls::{app_portal_access, application_in},\n    get_default_test_config, start_svix_server,\n};\n\n#[tokio::test]\n/// Users with application-level tokens should only be allowed to read the information related to\n/// their one application. All other endpoints should error.\nasync fn test_restricted_application_access() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n    <|fim_suffix|>\n\n    let client = app_portal_access(&client, &app_id, Default::default()).await;\n\n    // CREATE, UPDATE, DELETE, and LIST ops\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::FORBIDDEN,\n        )\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/\"),\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::FORBIDDEN,\n        )\n        .await\n        .unwrap();\n    client\n        .delete(&format!(\"api/v1/app/{app_id}/\"), StatusCode::FORBIDDEN)\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .get(\"api/v1/app/\", StatusCode::FORBIDDEN)\n        .await\n        .unwrap();\n\n    // READ should succeed when accessing the app_id the token is authorized for but no others\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{app_id_2}/\"), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n    let _: ApplicationOut = client\n        .get(&format!(\"api/v1/app/{app_id}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_dashboard_access_without_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    // We just need to ensure we get an OK response without a body.\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/dashboard-access/{app_id}/\"),\n            (),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_invalid_auth_error_detail() {\n    let (mut client, _jh) = start_svix_server().await;\n    let cfg = get_default_test_config();\n    let jwt_secret = match cfg.jwt_signing_config.as_ref() {\n        svix_server::core::security::JwtSigningConfig::Default { jwt_secret } => {\n            std::str::from_utf8(&jwt_secret.to_bytes())\n                .unwrap()\n                .to_owned()\n        }\n\n        _ => return,\n    };\n\n    client.set_auth_header(\"some-nonsense-key\".to_string());\n    match client\n        .post::<_, Value>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::UNAUTHORIZED,\n        )\n        .await\n    {\n        Ok(Value::Object(i)) => {\n            assert_eq!(i.get(\"detail\").unwrap(), INVALID_TOKEN_ERR);\n        }\n        _ => {\n            panic!(\"Unexpected response\");\n        }\n    }\n    client.set_auth_header(jwt_secret);\n    match client\n        .post::<_, Value>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::UNAUTHORIZED,\n        )\n        .await\n    {\n        Ok(Value::Object(i)) => {\n            assert_eq!(i.get(\"detail\").unwrap(), JWT_SECRET_ERR);\n        }\n        _ => {\n            panic!(\"Unexpected response\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_app_portal_access_with_application() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_uid = format!(\n        \"app-created-in-portal-{}\",\n        rand::distributions::Alphanumeric.sample_string(&mut rand::thread_rng(), 15)\n    );\n\n    // app-portal-access without the application field fails\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": []\n            }),\n            StatusCode::NOT_FOUND,\n        )\n        .await\n        .unwrap();\n\n    // app-portal-access with application\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App Created With Portal Access\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // app was created\n    let app: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(app[\"uid\"], app_uid);\n    assert_eq!(app[\"name\"], \"Test App Created With Portal Access\");\n\n    // Access portal again with application field - should be ignored since app exists\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Updated name will be ignored\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Verify the app name didn't change\n    let app_after: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(app_after[\"name\"], \"Test App Created With Portal Access\");\n\n    // UID in path must match UID in body\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/auth/app-portal-access/different-uid/\",\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App\",\n                    \"uid\": app_uid,  // This doesn't match the path\n                }\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    // UID must be set in body when creating\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/auth/app-portal-access/new-app-uid/\",\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App Without UID\",\n                    // Missing uid field\n                }\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n}\n<|fim_middle|>", "completion": "let app_id_2: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME_2\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_auth.rs", "node_type": "let_declaration", "line_range": [37, 45]}
{"prompt": "<|fim_prefix|>_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    l<|fim_suffix|>    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"2\"] {\n            let list_failed: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_failed.data.len() == 2);\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_3));\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_4));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // No messages should still be listed as `sending`\n    let l: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status=3\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(l.data.len(), 0);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_sending() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"3\"] {\n            let list_sending: ListResponse<EndpointMessageOut> = client\n                .get<|fim_middle|>", "completion": "let mut cfg = get_default_test_config();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [226, 226]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\nmod crud;\nmod headers;\nmod recovery;\nmod secrets;\n\nuse std::collections::{HashMap, HashSet};\n\nuse aide::axum::{\n    routing::{get_with, post_with},\n    ApiRouter,\n};\nuse axum::{\n    extract::{Path, Query, State},\n    Json,\n};\nuse chrono::{DateTime, Duration, Utc};\nuse schemars::JsonSchema;\nuse sea_orm::{ActiveValue::Set, ColumnTrait, FromQueryResult, QuerySelect};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::{aide_annotate, ModelIn, ModelOut};\nuse url::Url;\nuse validator::{Validate, ValidationError};\n\nuse self::secrets::generate_secret;\nuse super::message::{create_message_inner, MessageIn, MessageOut, RawPayload};\nuse crate::{\n    cfg::DefaultSignatureType,\n    core::{\n        cryptography::Encryption,\n        permissions,\n        types::{\n            metadata::Metadata, ApplicationIdOrUid, EndpointHeaders, EndpointHeadersPatch,\n            EndpointId, EndpointSecret, EndpointSecretInternal, EndpointUid, EventChannelSet,\n            EventTypeName, EventTypeNameSet, MessageStatus,\n        },\n    },\n    db::models::{\n        endpoint, eventtype,\n        messageattempt::{self, Query as _},\n    },\n    error::{self, HttpError},\n    v1::utils::{\n        openapi_tag,\n        patch::{\n            patch_field_non_nullable, patch_field_nullable, UnrequiredField,\n            UnrequiredNullableField,\n        },\n        validate_no_control_characters, validate_no_control_characters_unrequired,\n        validation_error, ApplicationEndpointPath, ModelIn, ValidatedJson,\n    },\n    AppState,\n};\n\npub fn validate_event_types_ids(event_types_ids: &EventTypeNameSet) -> Result<(), ValidationError> {\n    if event_types_ids.0.is_empty() {\n        Err(validation_error(\n            Some(\"filterTypes\"),\n            Some(\"filterTypes can't be empty, it must have at least one item.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nf<|fim_suffix|>\npub fn validate_channels_endpoint(channels: &EventChannelSet) -> Result<(), ValidationError> {\n    let len = channels.0.len();\n    if !(1..=10).contains(&len) {\n        Err(validation_error(\n            Some(\"channels\"),\n            Some(\"Channels must have at least 1 and at most 10 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nfn validate_channels_endpoint_unrequired_nullable(\n    channels: &UnrequiredNullableField<EventChannelSet>,\n) -> Result<(), ValidationError> {\n    match channels {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(channels) => validate_channels_endpoint(channels),\n    }\n}\n\npub fn validate_url(url: &Url) -> Result<(), ValidationError> {\n    let scheme = url.scheme();\n    if scheme == \"https\" || scheme == \"http\" {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"url\"),\n            Some(\"Endpoint URL schemes must be http or https\"),\n        ))\n    }\n}\n\nfn validate_url_unrequired(val: &UnrequiredField<Url>) -> Result<(), ValidationError> {\n    match val {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(val) => validate_url(val),\n    }\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nfn example_endpoint_description() -> &'static str {\n    \"An example endpoint name\"\n}\n\nfn example_filter_types() -> Vec<&'static str> {\n    vec![\"user.signup\", \"user.deleted\"]\n}\n\nfn endpoint_disabled_default() -> bool {\n    false\n}\n\nfn example_endpoint_url() -> &'static str {\n    \"https://example.com/webhook/\"\n}\n\nfn example_endpoint_version() -> u16 {\n    1\n}\n\nfn default_endpoint_version() -> Option<u16> {\n    Some(1)\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointIn {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n    /// List of message channels this endpoint listens to (omit for all)\n    #[validate(custom = \"validate_channels_endpoint\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n\n    #[validate]\n    #[serde(default)]\n    #[serde(rename = \"secret\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub key: Option<EndpointSecret>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\nimpl EndpointIn {\n    pub fn key_take_or_generate(\n        &mut self,\n        encryption: &Encryption,\n        sig_type: &DefaultSignatureType,\n    ) -> error::Result<EndpointSecretInternal> {\n        if let Some(key) = self.key.take() {\n            EndpointSecretInternal::from_endpoint_secret(key, encryption)\n        } else {\n            generate_secret(encryption, sig_type)\n        }\n    }\n}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for EndpointIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    #[allow(deprecated)]\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointIn {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            key: _,\n            metadata: _,\n        } = self;\n\n        model.description = Set(description);\n        model.rate_limit = Set(rate_limit.map(|x| x.into()));\n        model.uid = Set(uid);\n        model.url = Set(url.into());\n        model.version = Set(version.unwrap_or(1).into());\n        model.disabled = Set(disabled);\n        model.event_types_ids = Set(event_types_ids);\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\nstruct EndpointUpdate {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[schemars(example = \"exa<|fim_middle|>", "completion": "fn validate_event_types_ids_unrequired_nullable(\n    event_types_ids: &UnrequiredNullableField<EventTypeNameSet>,\n) -> Result<(), ValidationError> {\n    match event_types_ids {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(event_type_ids) => validate_event_types_ids(event_type_ids),\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "function_item", "line_range": [67, 74]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse super::{StreamingEventType, StreamingEvents, StreamingSink, StreamingStream};\nuse crate::{error::Result, models::*, Configuration};\n\npub struct Streaming<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> Streaming<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    pub fn event_type(&self) -> StreamingEventType<'a> {\n        StreamingEventType::new(self.cfg)\n    }\n\n    pub fn events(&self) -> StreamingEvents<'a> {\n        StreamingEvents::new(self.cfg)\n    }\n\n    <|fim_suffix|>\n\n    pub fn stream(&self) -> StreamingStream<'a> {\n        StreamingStream::new(self.cfg)\n    }\n\n    /// Get the HTTP sink headers. Only valid for `http` or `otelTracing` sinks.\n    pub async fn sink_headers_get(\n        &self,\n        stream_id: String,\n        sink_id: String,\n    ) -> Result<EndpointHeadersOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/headers\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Updates the Sink's headers. Only valid for `http` or `otelTracing`\n    /// sinks.\n    pub async fn sink_headers_patch(\n        &self,\n        stream_id: String,\n        sink_id: String,\n        http_sink_headers_patch_in: HttpSinkHeadersPatchIn,\n    ) -> Result<EndpointHeadersOut> {\n        crate::request::Request::new(\n            http1::Method::PATCH,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/headers\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .with_body_param(http_sink_headers_patch_in)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get the transformation code associated with this sink.\n    pub async fn sink_transformation_get(\n        &self,\n        stream_id: String,\n        sink_id: String,\n    ) -> Result<SinkTransformationOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/stream/{stream_id}/sink/{sink_id}/transformation\",\n        )\n        .with_path_param(\"stream_id\", stream_id)\n        .with_path_param(\"sink_id\", sink_id)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub fn sink(&self) -> StreamingSink<'a> {\n        StreamingSink::new(self.cfg)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/streaming.rs", "node_type": "function_item", "line_range": [22, 24]}
{"prompt": "<|fim_prefix|>())\n                    .map_err(Error::InvalidHttpRequest)?\n            };\n\n            *req.headers_mut() = request.headers;\n\n            if let Some(header_names) = request.header_names {\n                req.extensions_mut().insert(header_names);\n            }\n\n            let start = Instant::now();\n            let res = if let Some(timeout) = request.timeout {\n                match tokio::time::timeout(timeout, self.client.request(req)).await {\n                    Ok(Ok(resp)) => Ok(resp),\n                    Ok(Err(e)) => Err(e.into()),\n                    Err(_to) => Err(Error::TimedOut),\n                }\n            } else {\n                self.client.request(req).await.map_err(Into::into)\n            };\n\n            if !retry {\n                return res;\n            }\n\n            match res {\n                Err(Error::FailedRequest(e)) if start.elapsed() < Duration::from_millis(1000) => {\n                    tracing::info!(\"Insta-retrying: {e}\");\n                    self.execute_inner(org_req, false).await\n                }\n                res => res,\n            }\n        }\n        .boxed()\n    }\n}\n\n#[derive(Clone)]\npub struct Request {\n    method: Method,\n    uri: Uri,\n    headers: HeaderMap,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    timeout: Option<Duration>,\n    version: Version,\n}\n\npub struct RequestBuilder {\n    method: Option<Method>,\n    uri: Option<Uri>,\n    accept: Option<HeaderValue>,\n    user_agent: Option<HeaderValue>,\n    headers: Option<HeaderMap>,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    version: Option<Version>,\n    timeout: Option<Duration>,\n    basic_auth: Option<Vec<u8>>,\n\n    // Derived from body\n    content_type: Option<HeaderValue>,\n}\n\n#[derive(Debug)]\npub struct RequestBuildError(pub Vec<BuildError>);\n\nimpl std::fmt::Display for RequestBuildError {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        let mut iter = self.0.iter();\n\n        f.write_str(\"Build failed\")?;\n\n        if let Some(first) = iter.next() {\n            write!(f, \": {first}\")?;\n\n            for err in iter {\n                write!(f, \"; {err}\")?;\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Error)]\npub enum BuildError {\n    #[error(\"uri missing\")]\n    UriMissing,\n    #[error(\"version missing\")]\n    VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub fn new() -> Self {\n        Self {\n            method: None,\n            uri: None,\n            accept: None,\n            user_agent: None,\n            headers: None,\n            header_names: None,\n            body: None,\n            version: None,\n            timeout: None,\n            content_type: None,\n            basic_auth: None,\n        }\n    }\n\n    pub fn method(mut self, method: Method) -> Self {\n        self.method = Some(method);\n        self\n    }\n\n    pub fn uri(mut self, uri: url::Url) -> Self {\n        let basic_auth = if uri.password().is_some() || !uri.username().is_empty() {\n            let username = decode_or_log(uri.username());\n            let password = uri.password().map(decode_or_log).unwrap_or_default();\n\n            Some(\n                Authorization::basic(&username, &password)\n                    .0\n                    .encode()\n                    .as_bytes()\n                    .to_vec(),\n            )\n        } else {\n            None\n        };\n        self.basic_auth = basic_auth;\n\n        let uri =\n            Uri::from_str(uri.as_str()).expect(\"If it's a valid url::Url, it's also a valid Uri\");\n        self.uri = Some(uri);\n        self\n    }\n\n    pub fn uri_str(self, uri: &str) -> Result<Self, url::ParseError> {\n        let uri = url::Url::from_str(uri)?;\n        Ok(self.uri(uri))\n    }\n\n    fn build_headers(\n        headers: CaseSensitiveHeaderMap,\n    ) -> (hyper::HeaderMap, hyper::ext::HeaderCaseMap) {\n        <|fim_suffix|>\n        let mut case_sensitive_hdrs: hyper::HeaderMap<Bytes> =\n            hyper::HeaderMap::with_capacity(headers.len());\n        for (k, v) in headers.into_iter() {\n            match HeaderName::from_str(&k) {\n                Ok(key) => {\n                    hdr_map.insert(key.clone(), v);\n                    case_sensitive_hdrs.insert(key, Bytes::copy_from_slice(k.as_bytes()));\n                }\n                Err(e) => {\n                    tracing::error!(\"Failed to parse header {} {}\", k, e);\n                }\n            }\n        }\n        (hdr_map, case_sensitive_hdrs.into())\n    }\n\n    pub fn headers(mut self, headers: CaseSensitiveHeaderMap) -> Self {\n        let (hdrs, case_map) = Self::build_headers(headers);\n        self.headers = Some(hdrs);\n        self.header_names = Some(case_map);\n        self\n    }\n\n    pub fn body(mut self, body: Vec<u8>, content_type: HeaderValue) -> Self {\n        self.body = Some(body);\n        self.content_type = Some(content_type);\n        self\n    }\n\n    pub fn json_body<T: Serialize>(self, body: T) -> Result<Self, serde_json::Error> {\n        let body = serde_json::to_vec(&body)?;\n        Ok(self.body(body, HeaderValue::from_static(\"application/json\")))\n    }\n\n    pub fn version(mut self, version: Version) -> Self {\n        self.version = Some(version);\n        self\n    }\n\n    pub fn timeout(mut self, timeout: Duration) -> Self {\n        self.timeout = Some(timeout);\n        self\n    }\n\n    pub fn user_agent(mut self, user_agent: HeaderValue) -> Self {\n        self.user_agent = Some(user_agent);\n        self\n    }\n}\n\nimpl Default for RequestBuilder {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl RequestBuilder {\n    fn validate(&self) -> Result<(), RequestBuildError> {\n        let mut errs: Vec<BuildError> = Vec::new();\n        if self.uri.is_none() {\n            errs.push(BuildError::UriMissing);\n        }\n        if self.version.is_none() {\n            errs.push(BuildError::VersionMissing);\n        }\n\n        if !errs.is_empty() {\n            Err(RequestBuildError(errs))\n        } else {\n            Ok(())\n        }\n    }\n\n    pub fn build(self) -> Result<Request, RequestBuildError> {\n        self.validate()?;\n\n        let custom_headers = self.headers.unwrap_or_default();\n\n        let uri = self.uri.unwrap();\n        let authority = uri.authority().expect(\"Missing authority\");\n        let host = match authority.port() {\n            Some(port) => HeaderValue::from_str(&format!(\"{}:{port}\", authority.host())),\n            None => HeaderValue::from_str(authority.host()),\n        }\n        .unwrap();\n\n        let mut headers = HeaderMap::with_capacity(3 + custom_headers.len());\n\n        // Ensure that host header is first -- even though this is technically\n        // not required by HTTP spec, some clients fail if it's not first:\n        headers.insert(http::header::HOST, host);\n        headers.insert(\n            http::header::ACCEPT,\n            self.accept.unwrap_or(HeaderValue::from_static(\"*/*\")),\n        );\n        headers.insert(\n            http::header::CONTENT_TYPE,\n            self.content_type\n                .unwrap_or(HeaderValue::from_static(\"application/json\")),\n        );\n\n        headers.extend(custom_headers);\n\n        if let Some(user_agent) = self.user_agent {\n            headers.insert(http::header::USER_AGENT, user_agent);\n        }\n\n        if let Some(auth_header) = self.basic_auth {\n            if !headers.contains_key(http::header::AUTHORIZATION) {\n                headers.insert(\n                    http::header::AUTHORIZATION,\n                    HeaderValue::from_bytes(&auth_header).unwrap(),\n                );\n            }\n        }\n\n        Ok(Request {\n            method: self.method.unwrap_or(Method::POST),\n            uri,\n            headers,\n            header_names: self.header_names,\n            body: self.body,\n            timeout: self.timeout,\n            version: self.version.unwrap(),\n        })\n    }\n}\n\n/// HTTP connector that blocks outgoing requests to private IPs with support\n/// for HTTPS and optionally proxying <|fim_middle|>", "completion": "let mut hdr_map = hyper::HeaderMap::with_capacity(headers.len());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [323, 323]}
{"prompt": "<|fim_prefix|>    assert_eq!(out.ep.channels, None);\n\n    // Test the UID may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"uid\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.uid, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the URL may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"url\": \"http://bad.url2\" }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the version may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"version\": 2 }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.version, 2);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that disabled may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"disabled\": true }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert!(out.ep.disabled);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that event type IDs may be set\n\n    // But first make an event type to set it to\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            json!({\n                \"description\": \"a test event type\",\n                \"name\": \"test\",\n            }),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"filterTypes\": [\"test\"],\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(\n        out.ep.event_types_ids,\n        Some(EventTypeNameSet(HashSet::from([EventTypeName(\n            \"test\".to_owned()\n        )])))\n    );\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that event type IDs may be unset\n    l<|fim_suffix|>\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.event_types_ids, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that channels may be set\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"channels\": [\"test\"],\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(\n        out.ep.channels,\n        Some(EventChannelSet(HashSet::from([EventChannel(\n            \"test\".to_owned()\n        )])))\n    );\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n\n    // Test that channels may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"channels\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.channels, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n}\n\n#[allow(deprecated)]\n#[tokio::test]\nasync fn test_crud() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1: &str = \"v1EndpointCrudTestApp1\";\n    const APP_NAME_2: &str = \"v1EndpointCrudTestApp2\";\n\n    const EP_URI_APP_1_EP_1_VER_1: &str = \"http://v1EndpointCrudTestApp1Ep1Ver1.test/foo\";\n    const EP_URI_APP_1_EP_1_VER_2: &str = \"http://v1EndpointCrudTestApp1Ep1Ver2.test/\";\n    const EP_URI_APP_1_EP_2: &str = \"http://v1EndpointCrudTestApp1Ep2.test/\";\n    const EP_URI_APP_2_EP_1: &str = \"http://v1EndpointCrudTestApp2Ep1.test/\";\n    const EP_URI_APP_2_EP_2: &str = \"http://v1EndpointCrudTestApp2Ep2.test/\";\n\n    let app_1 = create_test_app(&client, APP_NAME_1).await.unwrap().id;\n    let app_2 = create_test_app(&client, APP_NAME_2).await.unwrap().id;\n\n    // CREATE\n    let app_1_ep_1 = create_test_endpoint(&client, &app_1, EP_URI_APP_1_EP_1_VER_1)\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_1.ep.url, EP_URI_APP_1_EP_1_VER_1.to_lowercase());\n    assert_eq!(app_1_ep_1.ep.version, 1);\n\n    let app_1_ep_2 = create_test_endpoint(&client, &app_1, EP_URI_APP_1_EP_2)\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_2.ep.url, EP_URI_APP_1_EP_2.to_lowercase());\n    assert_eq!(app_1_ep_2.ep.version, 1);\n\n    let app_2_ep_1 = create_test_endpoint(&client, &app_2, EP_URI_APP_2_EP_1)\n        .await\n        .unwrap();\n    assert_eq!(app_2_ep_1.ep.url, EP_URI_APP_2_EP_1.to_lowercase());\n    assert_eq!(app_2_ep_1.ep.version, 1);\n\n    let app_2_ep_2 = create_test_endpoint(&client, &app_2, EP_URI_APP_2_EP_2)\n        .await\n        .unwrap();\n    assert_eq!(app_2_ep_2.ep.url, EP_URI_APP_2_EP_2.to_lowercase());\n    assert_eq!(app_2_ep_2.ep.version, 1);\n\n    // READ\n\n    // Can read from correct app\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_1.id).await.unwrap(),\n        app_1_ep_1\n    );\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_2.id).await.unwrap(),\n        app_1_ep_2\n    );\n    assert_e<|fim_middle|>", "completion": "let _: EndpointOut = client\n        .patch(&url, json!({ \"filterTypes\": null }), StatusCode::OK)\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [370, 373]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for PATCH requests focused mostly around non-required field types.\n\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse validator::Validate;\n\n/// This is an enum that will wrap every nullable field for a PATCH request.\n///\n/// Nonnullable fields can be represented via an [`UnrequiredField`]. This\n/// differs from an [`Option`] in that it distinguishes null values and absent\n/// values such that an optional value in a model may be made `None` via\n/// PATCHing while allowing omitted fields to be skipped when updating.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredNullableField<T> {\n    #[default]\n    Absent,\n    None,\n    Some(T),\n}\n\n/// This enum is a non-nullable equivalent to [`UnrequiredNullableField`].\n///\n/// This is effectively an [`Option`] with the additional context that any field\n/// which uses this type is a member of a PATCH request model and that the field\n/// may be absent, meaning it is not to be updated. In comparison, [`Option`]s\n/// are used in other [`ModelIn`]s to define a field, that when absent, is\n/// `null`.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredField<T> {\n    #[default]\n    Absent,\n    Some(T),\n}\n\nimpl<T> UnrequiredNullableField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredNullableField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredNullableField<U> {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> UnrequiredField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredField<U> {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> From<Option<T>> for UnrequiredNullableField<T> {\n    fn from(opt: Option<T>) -> Self {\n        match opt {\n            Some(v) => UnrequiredNullableField::Some(v),\n            None => UnrequiredNullableField::None,\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredNullableField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n            UnrequiredNullableField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredField::Absent => Ok(()),\n            UnrequiredField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredNullableField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone + Copy> Copy for UnrequiredNullableField<T> {}\ni<|fim_suffix|>\nimpl<'de, T> Deserialize<'de> for UnrequiredNullableField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        Option::deserialize(deserializer).map(Into::into)\n    }\n}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        T::deserialize(deserializer).map(UnrequiredField::Some)\n    }\n}\n\nimpl<T> Serialize for UnrequiredNullableField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredNullableField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredNullableField must skip serializing if field is absent\",\n            )),\n            UnrequiredNullableField::None => serializer.serialize_none(),\n            UnrequiredNullableField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\nimpl<T> Serialize for UnrequiredField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredField must skip serializing if field is absent\",\n            )),\n            UnrequiredField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"Unrequired_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<T>()\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredNullableField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"UnrequiredNullable_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<Option<T>>()\n    }\n}\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`Option`] is `Some`, but\n/// performs no operation in the case it is `None`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The nullable equivalent which is used for [`UnrequiredNullableField`] is [`patch_field_nullable`].\nmacro_rules! patch_field_non_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n}\npub(crate) use patch_field_non_nullable;\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`UnrequiredNullableField`] is\n/// `Some` and unsetting the field when it is `None`, but performs no operation in the case it is\n///  `Absent`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being<|fim_middle|>", "completion": "impl<T: Clone + Copy> Copy for UnrequiredField<T> {}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/patch.rs", "node_type": "impl_item", "line_range": [118, 118]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{collections::HashMap, sync::Arc};\n\nuse axum::async_trait;\nuse tokio::{\n    sync::RwLock,\n    task,\n    time::{sleep, Duration, Instant},\n};\n\nuse super::{Cache, CacheBehavior, CacheKey, Result};\n\n#[derive(Debug)]\nstruct ValueWrapper {\n    value: Vec<u8>,\n    ttl: Duration,\n    timer: Instant,\n}\n\nimpl ValueWrapper {\n    fn new(value: Vec<u8>, ttl: Duration) -> ValueWrapper {\n        ValueWrapper {\n            value,\n            ttl,\n            timer: Instant::now(),\n        }\n    }\n}\n\ntype State = HashMap<Vec<u8>, ValueWrapper>;\ntype SharedState = Arc<RwLock<State>>;\n\npub fn new() -> Cache {\n    l<|fim_suffix|>\n    let shared_state_clone = shared_state.clone();\n    task::spawn(async move {\n        loop {\n            sleep(Duration::from_secs(60 * 5)).await;\n            shared_state_clone\n                .write()\n                .await\n                .retain(|_, v| check_is_expired(v))\n        }\n    });\n\n    MemoryCache { map: shared_state }.into()\n}\n\n#[derive(Clone)]\npub struct MemoryCache {\n    map: SharedState,\n}\n\n#[async_trait]\nimpl CacheBehavior for MemoryCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(self\n            .map\n            .read()\n            .await\n            .get(key)\n            .filter(|wrapper| check_is_expired(wrapper))\n            .map(|wrapper| wrapper.value.clone()))\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()> {\n        self.map\n            .write()\n            .await\n            .insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n        Ok(())\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool> {\n        let mut lock = self.map.write().await;\n\n        // TODO: use HashMap::try_insert when stable\n        // https://github.com/rust-lang/rust/issues/82766\n        if !lock.contains_key(key) {\n            lock.insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n            return Ok(true);\n        }\n\n        Ok(false)\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()> {\n        self.map.write().await.remove(key.as_ref().as_bytes());\n\n        Ok(())\n    }\n}\n\nfn check_is_expired(vw: &ValueWrapper) -> bool {\n    vw.timer.elapsed().as_millis() <= vw.ttl.as_millis()\n}\n\n#[cfg(test)]\nmod tests {\n    use serde::{Deserialize, Serialize};\n\n    use super::{\n        super::{kv_def, CacheValue},\n        *,\n    };\n    use crate::core::cache::string_kv_def;\n\n    // Test structures\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValA(usize);\n    kv_def!(TestKeyA, TestValA);\n    impl TestKeyA {\n        fn new(id: String) -> TestKeyA {\n            TestKeyA(format!(\"SVIX_TEST_KEY_A_{id}\"))\n        }\n    }\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValB(String);\n    kv_def!(TestKeyB, TestValB);\n    impl TestKeyB {\n        fn new(id: String) -> TestKeyB {\n            TestKeyB(format!(\"SVIX_TEST_KEY_B_{id}\"))\n        }\n    }\n\n    string_kv_def!(StringTestKey);\n    impl StringTestKey {\n        fn new(id: String) -> StringTestKey {\n            StringTestKey(format!(\"SVIX_TEST_KEY_STRING_{id}\"))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_cache_crud_no_ttl() {\n        let cache = new();\n\n        let (first_key, first_val_a, first_val_b) =\n            (TestKeyA::new(\"1\".to_owned()), TestValA(1), TestValA(2));\n        let (second_key, second_val_a, second_val_b) = (\n            TestKeyB::new(\"1\".to_owned()),\n            TestValB(\"1\".to_owned()),\n            TestValB(\"2\".to_owned()),\n        );\n        let (third_key, third_val_a, third_val_b) = (\n            StringTestKey::new(\"1\".to_owned()),\n            \"1\".to_owned(),\n            \"2\".to_owned(),\n        );\n\n        // Create\n        assert!(cache\n            .set(&first_key, &first_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Read\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_a));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_a));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_a)\n        );\n\n        // Update (overwrite)\n        assert!(cache\n            .set(&first_key, &first_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Confirm update\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_b));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_b));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_b)\n        );\n\n        // Delete\n        assert!(cache.delete(&first_key).await.is_ok());\n        assert!(cache.delete(&second_key).await.is_ok());\n        assert!(cache.delete(&third_key).await.is_ok());\n\n        // Confirm deletion\n        assert_eq!(cache.get::<TestValA>(&first_key).await.unwrap(), None);\n        assert_eq!(cache.get::<TestValB>(&second_key).await.unwrap(), None);\n        assert_eq!(cache.get_string(&third_key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_ttl() {\n        let cache = new();\n        let key = TestKeyA::new(\"key\".to_owned());\n\n        assert!(cache\n            .set(&key, &TestValA(1), Duration::from_secs(1),)\n            .await\n            .is_ok());\n        tokio::time::sleep(std::time::Duration::from_millis(1200)).await;\n        assert_eq!(cache.get::<TestValA>(&key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_nx_status() {\n        let cache = new();\n        let key = TestKeyA::new(\"nx_status_test_key\".to_owned());\n\n        assert!(cache\n            .set_if_not_exists(&key, &TestValA(1), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(!cache\n            .set_if_not_exists(&key, &TestValA(2), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(cache.delete(&key).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "let shared_state = Arc::new(RwLock::new(State::new()));", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/memory.rs", "node_type": "let_declaration", "line_range": [36, 36]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for crating `tracing` spans compatible with OpenTelemetry's\n//! conventions.\nuse std::net::SocketAddr;\n\nuse axum::extract::{ConnectInfo, MatchedPath};\nuse http::header;\nuse opentelemetry::trace::TraceContextExt;\nuse svix_ksuid::{KsuidLike, KsuidMs};\nuse tower_http::{\n    classify::ServerErrorsFailureClass,\n    trace::{MakeSpan, OnFailure, OnResponse},\n};\nuse tracing::field::{debug, Empty};\nuse tracing_opentelemetry::OpenTelemetrySpanExt;\n\n/// An implementor of [`MakeSpan`] which creates `tracing` spans populated with information about\n/// the request received by an `axum` web server.\n#[derive(Clone, Copy)]\npub struct AxumOtelSpanCreator;\n\nimpl<B> MakeSpan<B> for AxumOtelSpanCreator {\n    fn make_span(&mut self, request: &http::Request<B>) -> tracing::Span {\n        let user_agent = request\n            .headers()\n            .get(header::USER_AGENT)\n            .and_then(|header| header.to_str().ok());\n\n        let host = request\n            .headers()\n            .get(header::HOST)\n            .and_then(|header| header.to_str().ok());\n\n        let http_route = request\n            .extensions()\n            .get::<MatchedPath>()\n            .map(|p| p.as_str());\n\n        let client_ip = request\n            .extensions()\n            .get::<ConnectInfo<SocketAddr>>()\n            .map(|ConnectInfo(ip)| debug(ip));\n\n        let request_id = request\n            .headers()\n            .get(\"x-request-id\")\n            .and_then(|id| id.to_str().map(ToOwned::to_owned).ok())\n            // If `x-request-id` isn't set, check `svix-req-id`. If the `svix-req-id` isn't a\n            // valid `str`, or it isn't set, then fallback to a random [`KsuidMs`]\n            .or_else(|| {\n                request\n                    .headers()\n                    .get(\"svix-req-id\")\n                    .and_then(|v| v.to_str().map(ToOwned::to_owned).ok())\n            })\n            .unwrap_or_else(|| KsuidMs::new(None, None).to_string());\n\n        let remote_context = opentelemetry::global::get_text_map_propagator(|p| {\n            p.extract(&opentelemetry_http::HeaderExtractor(request.headers()))\n        });\n        let remote_span = remote_context.span();\n        let span_context = remote_span.span_context();\n        let trace_id = span_context\n            .is_valid()\n            .then(|| span_context.trace_id().to_string());\n\n        l<|fim_suffix|>\n        let span = tracing::error_span!(\n            \"HTTP request\",\n            grpc.code = Empty,\n            http.client_ip = client_ip,\n            http.versions = ?request.version(),\n            http.host = host,\n            http.method = ?request.method(),\n            http.route = http_route,\n            http.scheme = request.uri().scheme().map(debug),\n            http.status_code = Empty,\n            http.target = request.uri().path_and_query().map(|p| p.as_str()),\n            http.user_agent = user_agent,\n            otel.kind = \"server\",\n            otel.status_code = Empty,\n            request_id,\n            trace_id,\n            idempotency_key,\n            org_id = tracing::field::Empty,\n            app_id = tracing::field::Empty,\n        );\n\n        span.set_parent(remote_context);\n\n        span\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnResponse;\n\nimpl<B> OnResponse<B> for AxumOtelOnResponse {\n    fn on_response(\n        self,\n        response: &http::Response<B>,\n        latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        let status = response.status().as_u16().to_string();\n        span.record(\"http.status_code\", tracing::field::display(status));\n        span.record(\"otel.status_code\", \"OK\");\n\n        tracing::debug!(\n            \"finished processing request latency={} ms status={}\",\n            latency.as_millis(),\n            response.status().as_u16(),\n        );\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnFailure;\n\nimpl OnFailure<ServerErrorsFailureClass> for AxumOtelOnFailure {\n    fn on_failure(\n        &mut self,\n        failure_classification: ServerErrorsFailureClass,\n        _latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        match failure_classification {\n            ServerErrorsFailureClass::StatusCode(status) if status.is_server_error() => {\n                span.record(\"otel.status_code\", \"ERROR\");\n            }\n            _ => {}\n        }\n    }\n}\n<|fim_middle|>", "completion": "let idempotency_key = request\n            .headers()\n            .get(\"idempotency-key\")\n            .and_then(|v| v.to_str().ok());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/otel_spans.rs", "node_type": "let_declaration", "line_range": [69, 72]}
{"prompt": "<|fim_prefix|>est_rate_limit() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = EndpointIn {\n        rate_limit: Some(100),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    assert_eq!(endp.ep.rate_limit.unwrap(), 100);\n\n    let endp = put_endpoint(\n        &client,\n        &app_id,\n        &endp.id,\n        EndpointIn {\n            rate_limit: None,\n            ..ep_in.clone()\n        },\n    )\n    .await\n    .unwrap();\n\n    assert!(endp.ep.rate_limit.is_none());\n\n    let endp = get_endpoint(&client, &app_id, &endp.id).await.unwrap();\n\n    assert!(endp.ep.rate_limit.is_none());\n}\n\n#[tokio::test]\nasync fn test_msg_event_types_filter() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    for et in [\n        event_type_in(\"et1\", None).unwrap(),\n        event_type_in(\"et2\", None).unwrap(),\n    ] {\n        let _: EventTypeOut = client\n            .post(\"api/v1/event-type/\", et, StatusCode::CREATED)\n            .await\n            .unwrap();\n    }\n\n    for event_types in [\n        Some(EventTypeNameSet(HashSet::from([EventTypeName(\n            \"et1\".to_owned(),\n        )]))),\n        Some(EventTypeNameSet(HashSet::from([\n            EventTypeName(\"et1\".to_owned()),\n            EventTypeName(\"et2\".to_owned()),\n        ]))),\n        None,\n    ] {\n        post_endpoint(\n            &client,\n            &app_id,\n            EndpointIn {\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                event_types_ids: event_types,\n                ..default_test_endpoint()\n            },\n        )\n        .await\n        .unwrap();\n    }\n\n    // Number of attempts should match based on event-types registered to endpoints\n    for (event_name, expected_count) in [\n        (EventTypeName(\"et1\".to_owned()), 3),\n        (EventTypeName(\"et2\".to_owned()), 2),\n    ] {\n        let msg: MessageOut = client\n            .post(\n                &format!(\"api/v1/app/{app_id}/msg/\"),\n                json!({\n                    \"eventType\": event_name,\n                    \"payload\": {},\n                    \"payloadRetentionPeriod\": 5,\n                }),\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        tokio::time::sleep(Duration::from_millis(10)).await;\n\n        let _list =\n            get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, expected_count)\n                .await\n                .unwrap();\n    }\n}\n\n#[tokio::test]\nasync fn test_msg_channels_filter() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ec = EventChannelSet(HashSet::from([EventChannel(\"tag1\".to_owned())]));\n\n    for channels in [Some(ec.clone()), None] {\n        let _endp = post_endpoint(\n            &client,\n            &app_id,\n            EndpointIn {\n                channels,\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n        )\n        .await\n        .unwrap();\n    }\n\n    for (channels, expected_count) in [(Some(&ec), 2), (None, 1)] {\n        let mut message_in = json!({\n            \"eventType\": \"et1\",\n            \"payload\": {},\n            \"payloadRetentionPeriod\": 5,\n        });\n        if let Some(channels) = channels {\n            message_in[\"channels\"] = json!(channels);\n        }\n\n        let msg: MessageOut = client\n            .post(\n                &format!(\"api/v1/app/{}/msg/\", &app_id),\n                message_in,\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        let _list =\n            get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, expected_count)\n                .await\n                .unwrap();\n\n        l<|fim_suffix|>\n        assert_eq!(msg.channels.as_ref(), channels);\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_headers_manipulation() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap();\n\n    let patched_headers_in = EndpointHeadersPatchIn {\n        headers: EndpointHeadersPatch(HashMap::from([\n            (\"x-test-3\".to_owned(), Some(\"4\".to_owned())),\n            (\"x-test-2\".to_owned(), None),\n        ])),\n    };\n\n    client\n        .patch_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            patched_headers_in,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let recvd_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([(\"x-test-3\".to_owned(), \"4\".to_owned()),]),\n        recvd_headers.headers\n    );\n\n    let endp = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap();\n\n    for bad_hdr in [\n        \"content-length\",\n        \"some:thing\",\n        \"some\\u{0000}thing\",\n        \"svix-foo\",\n        \"x-svix-foo\",\n        \"x-amzn-foo\",\n    ] {\n        let _: IgnoredAny = client\n            .put(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                json!({\n                    \"headers\": { bad_hdr: \"123\" },\n                }),\n                StatusCode::UNPROCESSABLE_ENTITY,\n            )\n            .await\n            .unwrap();\n    }\n\n    let org_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };\n\n    let updated_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"3\".to_owned()),\n            (\"x-test-2\".to_owned(), \"2\".to_owned()),\n        ])),\n    };\n\n    for hdrs in [&org_headers, &updated_headers] {\n        client\n            .put_without_response(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                hdrs,\n                StatusCode::NO_CONTENT,\n            )\n            .await\n            .unwrap();\n\n        let recvd_headers: EndpointHeadersOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(hdrs.headers.0, recvd_headers.headers);\n    }\n\n    let patched_headers_in = EndpointHeadersPatchIn {\n        headers: EndpointHeadersPatch(HashMap::from([\n            (\"x-test-3\".to_owned(), Some(\"4\".to_owned())),\n            (\"x-test-2\".to_owned(), None),\n        ])),\n    };\n\n    client\n        .patch_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            &patched_headers_in,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let recvd_headers: EndpointHeadersOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        HashMap::from([\n            (\"x-test-1\".to_owned(), \"3\".to_owned()),\n            (\"x-test-3\".to_owned(), \"4\".to_owned()),\n        ]),\n        recvd_headers.headers\n    );\n\n    let redacted_headers = EndpointHeadersIn {\n        headers: EndpointHeaders(HashMap::from([\n            (\"x-test-1\".to_owned(), \"1\".to_owned()),\n            (\"authorization\".to_owned(), \"secret\".to_owned()),\n        ])),\n    };\n\n    client\n        .put_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/headers/\", endp.id),\n            redacted_headers,\n            <|fim_middle|>", "completion": "let msg: MessageOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [2026, 2032]}
{"prompt": "<|fim_prefix|>ap().to_string());\n    assert_eq!(msg_payload.to_string(), rec_body_2.unwrap().to_string());\n    assert_eq!(msg_payload.to_string(), rec_body_3.unwrap().to_string());\n\n    receiver_2.set_response_status_code(axum::http::StatusCode::OK);\n\n    let rec_body_2 = receiver_2.data_recv.recv().await.unwrap();\n\n    assert_eq!(msg_payload.to_string(), rec_body_2.to_string());\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 200) {\n            anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_failed_message_gets_requeued() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver_1 = TestReceiver::start(axum::http::StatusCode::INTERNAL_SERVER_ERROR);\n    let _endp_id = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value\" });\n\n    let msg_res: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    receiver_1.data_recv.recv().await;\n\n    receiver_1.set_response_status_code(axum::http::StatusCode::OK);\n\n    let last_body = receiver_1.data_recv.recv().await.unwrap();\n\n    assert_eq!(msg_payload.to_string(), last_body.to_string());\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 200) {\n            anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, json!({ \"test\": \"value\" })).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n    let msg_id = msg.id.clone();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap();\n    assert_eq!(content.unwrap().id, msg_id.clone());\n\n    let res = messagecontent::Entity::update_many()\n        .col_expr(\n            messagecontent::Column::Expiration,\n            Expr::value(Utc::now() - Duration::days(1)),\n        )\n        .filter(messagecontent::Column::Id.eq(msg_id.clone()))\n        .exec(&pool)\n        .await\n        .unwrap();\n    assert_eq!(1, res.rows_affected);\n\n    expired_message_cleaner::clean_expired_messages(&pool, 5000, false)\n        .await\n        .unwrap();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id)\n        .one(&pool)\n        .await\n        .unwrap();\n    assert!(content.is_none());\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period_messagecontent() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    l<|fim_suffix|>\n    let custom_retention_period = 5;\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value\" },\n                \"payloadRetentionPeriod\": custom_retention_period\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n    let msg_id = msg.id.clone();\n\n    let content: messagecontent::Model = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap()\n        .unwrap();\n\n    let expected = Utc::now() + Duration::days(custom_retention_period) + Duration::hours(1);\n    let actual: chrono::DateTime<Utc> = content.expiration.into();\n\n    assert!(actual < expected);\n}\n\n#[tokio::test]\nasync fn test_expunge_message_payload() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testApp\").await.unwrap().id;\n\n    let payload = json!({ \"sensitive\": \"data\" });\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{}/content/\", msg.id),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(msg.payload.0.get(), r#\"{\"expired\":true}\"#);\n}\n\n#[tokio::test]\nasync fn test_message_conflict() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg_in = json!({\n        \"eventType\": \"user.signup\",\n        \"payload\": { \"test\": \"value\" },\n        \"payloadRetentionPeriod\": 5,\n        \"eventId\": \"test1\",\n    });\n\n    let _: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            msg_in.clone(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            msg_in,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_message_validation() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testApp\").await.unwrap().id;\n    let payload = json!({ \"large_payload\": \"payload-\".repeat(1_000_000) });\n\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, payload).unwrap(),\n            StatusCode::PAYLOAD_TOO_LARGE,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_raw_payload() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testRawPayload\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(axum::http::StatusCode::OK);\n\n    create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let msg_payload = json!({ \"test\": \"value1\" });\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            json!({\n                \"eventType\": \"payload.raw\",\n                \"payload\": {},\n             <|fim_middle|>", "completion": "let app_id = create_test_app(&client, \"test-content-expiration-period\")\n        .await\n        .unwrap()\n        .id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_message.rs", "node_type": "let_declaration", "line_range": [438, 441]}
{"prompt": "<|fim_prefix|>equest<Validated>`]s  is via the associated method on unvalidated\n/// request's, [`SerializableRequest<Unvalidated>::validate`].\npub trait RequestState {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Unvalidated;\nimpl RequestState for Unvalidated {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Validated;\nimpl RequestState for Validated {}\n\n/// This intermediary representation is necessary because it is preferable to serialize the headers\n/// and/or body as a [`String`] over bytes when dealing with some [`VerificationMethod`]s and some\n/// [`ForwardingMethod`]s. This struct represents both the headers and body as enums which allow for\n/// either textual representations or byte representations when [`Serialize`]d via [`serde`].\n///\n/// On trying to convert a [`Standard`] variant into a [`StringSerializable`] variant, HTTP headers\n/// will be represented textually if and only if they are completely ASCII, while any bodies will\n/// attempt to be read as UTF-8 before falling back to bytes.\n///\n/// NOTE: This conversion *should* be lazy. The [`String`] variant are only acceptable in a subset\n/// of all cases, so lazy-conversion will prevent needless conversion back and forth. You may check\n/// whether the conversion is required and/or helpful with [`VerificationMethod::want_string_rep`]\n/// or [`VerificationMethod::need_string_rep`] plus the [`ForwardingMethod`] equivalents.\n///\n/// The intended course of action is to attempt to convert to string-serializable variants of the\n/// header map and the body immediately if either of the aforementioned methods are true --  but\n/// only returning an [`Err`] response if it *needs* it. Then, if the validation is a success (see\n/// [`SerializableRequest<Unvalidated>::validate`] and a validated equivalent is returned, then the\n/// same checks are to be performed, but with the [`ForwardingMethod`] methods before being sent to\n/// the appropriate [`ForwardingMethod`] implementor.\n#[derive(Clone, Debug, Serialize)]\npub struct SerializableRequest<S: RequestState> {\n    headers: SerializableHeaderMap,\n    payload: SerializablePayload,\n\n    #[serde(skip)]\n    _pd: PhantomData<S>,\n}\n\nimpl<S: RequestState> SerializableRequest<S> {\n    pub fn headers(&self) -> &SerializableHeaderMap {\n        &self.headers\n    }\n\n    pub fn payload(&self) -> &SerializablePayload {\n        &self.payload\n    }\n}\n\nimpl From<RequestFromParts> for SerializableRequest<Unvalidated> {\n    fn from(value: RequestFromParts) -> Self {\n        Self {\n            headers: SerializableHeaderMap::Standard(value.headers),\n            payload: SerializablePayload::Standard(value.payload.to_vec()),\n\n            _pd: PhantomData,\n        }\n    }\n}\n\n#[async_trait]\nimpl<S> FromRequest<S> for SerializableRequest<Unvalidated>\nwhere\n    S: Send + Sync,\n{\n    type Rejection = <RequestFromParts as FromRequest<S>>::Rejection;\n\n    async fn from_request(req: Request, state: &S) -> Result<Self, Self::Rejection> {\n        RequestFromParts::from_request(req, state)\n            .await\n            .map(Into::into)\n    }\n}\n\nimpl SerializableRequest<Unvalidated> {\n    /// Given a specific validator\n    pub async fn validate<V: VerificationMethod>(\n        mut self,\n        verifier: &V,\n    ) -> Result<SerializableRequest<Validated>, http::StatusCode> {\n        // Do relevant conversions to [`String`] representations if wanted/needed\n        match (verifier.want_string_rep(), verifier.need_string_rep()) {\n            // Needed\n            (true, true) | (false, true) => {\n                self.headers = self\n                    .headers\n                    .try_to_string()\n                    .map_err(|_| http::StatusCode::BAD_REQUEST)?;\n                self.payload = self\n                    .payload\n                    .try_to_string()\n                    .map_err(|_| http::StatusCode::BAD_REQUEST)?;\n            }\n\n            // Wanted, but not needed\n            (true, false) => {\n                self.headers = match self.headers.try_to_string() {\n                    Ok(h) => h,\n                    Err(h) => h,\n                };\n\n                self.payload = match self.payload.try_to_string() {\n                    Ok(p) => p,\n                    Err(p) => p,\n                };\n            }\n\n            // Not wanted\n            (false, false) => {}\n        };\n\n        // FIXME: No cloning\n        // Then actually use the [`VerificationMethod`] implementor.\n        match verifier.validate(self.clone()).await {\n            Ok(true) => Ok(SerializableRequest::<Validated> {\n                headers: self.headers,\n                payload: self.payload,\n\n                _pd: PhantomData,\n            }),\n\n            Ok(false) => {\n                // FIXME: Read config to know whether to log\n                Err(http::StatusCode::BAD_REQUEST)\n            }\n\n            Err(e) => {\n                tracing::error!(\"Error validating request: {}\", e);\n                Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n            }\n        }\n    }\n}\n\n#[derive(Clone, Debug)]\npub enum SerializableHeaderMap {\n    Standard(HeaderMap),\n    StringSerializable(HashMap<String, String>),\n}\n\nimpl<'a> IntoIterator for &'a SerializableHeaderMap {\n    type Item = (&'a str, &'a [u8]);\n    type IntoIter = SerializableHeaderMapIter<'a>;\n\n    fn into_iter(self) -> Self::IntoIter {\n        match self {\n            SerializableHeaderMap::Standard(hm) => SerializableHeaderMapIter::HeaderMap(hm.iter()),\n            SerializableHeaderMap::StringSerializable(hm) => {\n                SerializableHeaderMapIter::HashMap(hm.iter())\n            }\n        }\n    }\n}\n\nimpl SerializableHeaderMap {\n    pub fn try_to_string(self) -> Result<Self, Self> {\n        match self {\n            Self::Standard(header_map) => Ok(Self::StringSerializable(\n                header_map\n                    .iter()\n                    .map(|(name, value)| Ok((name.as_str().to_owned(), value.to_str()?.to_owned())))\n                    .collect::<Result<HashMap<String, String>>>()\n                    .map_err(|_| Self::Standard(header_map))?,\n            )),\n            Self::StringSerializable(hash_map) => Ok(Self::StringSerializable(hash_map)),\n        }\n    }\n\n    pub fn len(&self) -> usize {\n        match self {\n            Self::Standard(m) => m.len(),\n            Self::StringSerializable(m) => m.len(),\n        }\n    }\n}\n\n/// Serialize is not implemented on [`HeaderMap`]s themselves, so custom serialization is required.\nimpl Serialize for SerializableHeaderMap {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            Self::Standard(header_map) => header_map\n                .iter()\n                .map(|(name, value)| (name.as_str().to_owned(), value.as_bytes().to_vec()))\n                .collect::<HashMap<String, Vec<u8>>>()\n                .serialize(serializer),\n            Self::StringSerializable(hash_map) => hash_map.serialize(serializer),\n        }\n    }\n}\n\npub enum SerializableHeaderMapIter<'a> {\n    HeaderMap(http::header::Iter<'a, HeaderValue>),\n    HashMap(std::collections::hash_map::Iter<'a, String, String>),\n}\n\nimpl<'a> Iterator for SerializableHeaderMapIter<'a> {\n    type Item = (&'a str, &'a [u8]);\n\n    <|fim_suffix|>\n}\n\n#[derive(Clone, Debug, Serialize)]\n#[serde(untagged)]\npub enum SerializablePayload {\n    Standard(Vec<u8>),\n    StringSerializable(String),\n}\n\nimpl SerializablePayload {\n    fn try_to_string(self) -> Result<Self, Self> {\n        match self {\n            Self::Standard(v) => Ok(Self::StringSerializable(\n                String::from_utf8(v).map_err(|e| Self::Standard(e.into_bytes()))?,\n            )),\n            Self::StringSerializable(s) => Ok(Self::StringSerializable(s)),\n        }\n    }\n\n    pub fn as_json(&self) -> Result<serde_json::Value> {\n        Ok(match self {\n            Self::Standard(v) => serde_json::from_slice(v)?,\n            Self::StringSerializable(s) => serde_json::from_str(s)?,\n        })\n    }\n\n    pub fn as_string(&self) -> Result<String> {\n        match self {\n            Self::Standard(v) => Ok(String::from_utf8(v.clone())?),\n            Self::StringSerializable(s) => Ok(s.clone()),\n        }\n    }\n}\n<|fim_middle|>", "completion": "fn next(&mut self) -> Option<Self::Item> {\n        match self {\n            Self::HeaderMap(hm) => hm.next().map(|(k, v)| (k.as_str(), v.as_bytes())),\n            Self::HashMap(hm) => hm.next().map(|(k, v)| (k.as_str(), v.as_bytes())),\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/types.rs", "node_type": "function_item", "line_range": [354, 359]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct MessagePollerPollOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filters messages sent with this event type (optional).\n    #[arg(long)]\n    pub event_type: Option<String>,\n    /// Filters messages sent with this channel (optional).\n    #[arg(long)]\n    pub channel: Option<String>,\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n}\n\nimpl From<MessagePollerPollOptions> for svix::api::MessagePollerPollOptions {\n    fn from(value: MessagePollerPollOptions) -> Self {\n        let MessagePollerPollOptions {\n            limit,\n            iterator,\n            event_type,\n            channel,\n            after,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            event_type,\n            channel,\n            after: after.map(|dt| dt.to_rfc3339()),\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessagePollerConsumerPollOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n}\n\nimpl From<MessagePollerConsumerPollOptions> for svix::api::MessagePollerConsumerPollOptions {\n    fn from(value: MessagePollerConsumerPollOptions) -> Self {\n        let MessagePollerConsumerPollOptions { limit, iterator } = value;\n        Self { limit, iterator }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessagePollerConsumerSeekOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<MessagePollerConsumerSeekOptions> for svix::api::MessagePollerConsumerSeekOptions {\n    fn from(value: MessagePollerConsumerSeekOptions) -> Self {\n        <|fim_suffix|>\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct MessagePollerArgs {\n    #[command(subcommand)]\n    pub command: MessagePollerCommands,\n}\n\n#[derive(Subcommand)]\npub enum MessagePollerCommands {\n    /// Reads the stream of created messages for an application, filtered on the Sink's event types and Channels.\n    Poll {\n        app_id: String,\n        sink_id: String,\n        #[clap(flatten)]\n        options: MessagePollerPollOptions,\n    },\n    /// Reads the stream of created messages for an application, filtered on the Sink's event types and\n    /// Channels, using server-managed iterator tracking.\n    ConsumerPoll {\n        app_id: String,\n        sink_id: String,\n        consumer_id: String,\n        #[clap(flatten)]\n        options: MessagePollerConsumerPollOptions,\n    },\n    /// Sets the starting offset for the consumer of a polling endpoint.\n    ConsumerSeek {\n        app_id: String,\n        sink_id: String,\n        consumer_id: String,\n        polling_endpoint_consumer_seek_in: crate::json::JsonOf<PollingEndpointConsumerSeekIn>,\n        #[clap(flatten)]\n        options: MessagePollerConsumerSeekOptions,\n    },\n}\n\nimpl MessagePollerCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Poll {\n                app_id,\n                sink_id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .poll(app_id, sink_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ConsumerPoll {\n                app_id,\n                sink_id,\n                consumer_id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .consumer_poll(app_id, sink_id, consumer_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ConsumerSeek {\n                app_id,\n                sink_id,\n                consumer_id,\n                polling_endpoint_consumer_seek_in,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .consumer_seek(\n                        app_id,\n                        sink_id,\n                        consumer_id,\n                        polling_endpoint_consumer_seek_in.into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let MessagePollerConsumerSeekOptions { idempotency_key } = value;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/message_poller.rs", "node_type": "let_declaration", "line_range": [67, 67]}
{"prompt": "<|fim_prefix|>    StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    l<|fim_suffix|>    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"2\"] {\n            let list_failed: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_failed.data.len() == 2);\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_3));\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_4));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // No messages should still be listed as `sending`\n    let l: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status=3\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(l.data.len(), 0);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_sending() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"3\"] {\n            let list_sending: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_sending.data.len() == 2);\n            anyhow::ensure!(list_sending.data.iter().any(|x| x.msg == msg_3));\n            anyhow::ensure!(list_sending.data.iter().any(|x| x.msg == msg_4));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\nstruct FailFirstSucceedSecond {\n    first_done: Arc<Mutex<bool>>,\n}\n\nimpl FailFirstSucceedSecond {\n    fn new() -> Self {\n        Self {\n            first_done: Arc::new(Mutex::new(false)),\n        }\n    }\n}\n\nimpl Respond for FailFirstSucceedSecond {\n    fn respond(&self, _request: &wiremock::Request) -> ResponseTemplate {\n        if *self.first_done.lock().unwrap() {\n            return ResponseTemplate::new(200);\n        }\n        let mut first_done = self.first_done.lock().unwrap();\n        *first_done = true;\n        ResponseTemplate::new(500)\n    }\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_success_are_not_sending() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mock_server = MockServer::start().await;\n    let responder <|fim_middle|>", "completion": "let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [265, 267]}
{"prompt": "<|fim_prefix|>              .unwrap(),\n        );\n        headers\n    }\n\n    fn get_unbranded_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(UNBRANDED_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    #[test]\n    fn test_sign() {\n        let wh = Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap();\n        assert_eq!(\n            \"v1,tZ1I4/hDygAJgO5TYxiSd6Sd0kDW6hPenDe+bTa3Kkw=\".to_owned(),\n            wh.sign(\n                \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\",\n                1649367553,\n                br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#\n            )\n            .unwrap()\n        );\n    }\n\n    #[test]\n    fn test_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            wh.verify(payload, &headers).unwrap();\n        }\n    }\n\n    #[test]\n    fn test_no_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = \"v1,R3PTzyfHASBKHH98a7yexTwaJ4yNIcGhFQc1yuN+BPU=\".to_owned();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_partial_signature() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        // Just `v1,`\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = format!(\n                \"{},\",\n                signature.split(',').collect::<Vec<&str>>().first().unwrap()\n            );\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n\n        // Non-empty but still partial signature (first few bytes)\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = &signature[0..8];\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_incorrect_timestamp() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        // Checks that timestamps that are in the future or too old are rejected by\n        // `verify` but okay for `verify_ignoring_timestamp`.\n        for ts in [\n            OffsetDateTime::now_utc().unix_timestamp() - (super::TOLERANCE_IN_SECONDS + 1),\n            OffsetDateTime::now_utc().unix_timestamp() + (super::TOLERANCE_IN_SECONDS + 1),\n        ] {\n            let signature = wh.sign(msg_id, ts, payload).unwrap();\n            l<|fim_suffix|>            headers.insert(\n                super::SVIX_MSG_TIMESTAMP_KEY,\n                ts.to_string().parse().unwrap(),\n            );\n\n            assert!(wh.verify(payload, &headers,).is_err());\n            // Timestamp tolerance is not considered in this case.\n            assert!(wh.verify_ignoring_timestamp(payload, &headers,).is_ok());\n        }\n\n        let ts = OffsetDateTime::now_utc().unix_timestamp();\n        let signature = wh.sign(msg_id, ts, payload).unwrap();\n        let mut headers = get_svix_headers(msg_id, &signature);\n        headers.insert(\n            super::SVIX_MSG_TIMESTAMP_KEY,\n            // Timestamp mismatch!\n            (ts + 1).to_string().parse().unwrap(),\n        );\n\n        // Both versions should reject the timestamp if it's not the same one used to\n        // produce the signature.\n        assert!(wh.verify(payload, &headers,).is_err());\n        assert!(wh.verify_ignoring_timestamp(payload, &headers,).is_err());\n    }\n\n    #[test]\n    fn test_verify_with_multiple_signatures() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        let multi_sig = format!(\n            \"{} {} {} {}\",\n            \"v1,tFtCZ5RDCPxzWQRWXWPgrCgE2frDBe9gjpbWQxnVfsQ=\",\n            \"v1,Mm7xgUVICxZfQ3bgf0h0Dof65L/IFx+PnZvnDWPCX6Q=\",\n            signature,\n            \"v1,9DfC1c3eeOrXB6w/5dIDydLNQaEyww5KalE5jLBZucE=\",\n        );\n\n        let headers = get_svix_headers(msg_id, &multi_sig);\n\n        wh.verify(payload, &headers).unwrap();\n    }\n\n    #[test]\n    fn test_no_verify_with_multiple_signatures() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let missing_sig = format!(\n            \"{} {} {}\",\n            \"v1,tFtCZ5RDCPxzWQRWXWPgrCgE2frDBe9gjpbWQxnVfsQ=\",\n            \"v1,Mm7xgUVICxZfQ3bgf0h0Dof65L/IFx+PnZvnDWPCX6Q=\",\n            \"v1,9DfC1c3eeOrXB6w/5dIDydLNQaEyww5KalE5jLBZucE=\",\n        );\n\n        let headers = get_svix_headers(msg_id, &missing_sig);\n\n        assert!(wh.verify(payload, &headers).is_err());\n    }\n\n    #[test]\n    fn test_missing_headers() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for (mut hdr_map, hdrs) in [\n            (\n                get_svix_headers(msg_id, &signature),\n                [\n                    SVIX_MSG_ID_KEY,\n                    SVIX_MSG_SIGNATURE_KEY,\n                    SVIX_MSG_TIMESTAMP_KEY,\n                ],\n            ),\n            (\n                get_unbranded_headers(msg_id, &signature),\n                [\n                    UNBRANDED_MSG_ID_KEY,\n                    UNBRANDED_MSG_SIGNATURE_KEY,\n                    UNBRANDED_MSG_TIMESTAMP_KEY,\n                ],\n            ),\n        ] {\n            for hdr in hdrs {\n                hdr_map.remove(hdr);\n                assert!(wh.verify(payload, &hdr_map).is_err());\n            }\n        }\n    }\n}\n<|fim_middle|>", "completion": "let mut headers = get_svix_headers(msg_id, &signature);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/webhooks.rs", "node_type": "let_declaration", "line_range": [346, 346]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct IngestSourceListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct IngestSourceCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct IngestSourceRotateTokenOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct IngestSource<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> IngestSource<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List of all the organization's Ingest Sources.\n    pub async fn list(\n        &self,\n        options: Option<IngestSourceListOptions>,\n    ) -> Result<ListResponseIngestSourceOut> {\n        let IngestSourceListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/ingest/api/v1/source\")\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create Ingest Source.\n    pub async fn create(\n        &self,\n        ingest_source_in: IngestSourceIn,\n        options: Option<IngestSourceCreateOptions>,\n    ) -> Result<IngestSourceOut> {\n        let IngestSourceCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/ingest/api/v1/source\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(ingest_source_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an Ingest Source by id or uid.\n    pub async fn get(&self, source_id: String) -> Result<IngestSourceOut> {\n        crate::request::Request::new(http1::Method::GET, \"/ingest/api/v1/source/{source_id}\")\n            .with_path_param(\"source_id\", source_id)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Update an Ingest Source.\n    <|fim_suffix|>\n\n    /// Delete an Ingest Source.\n    pub async fn delete(&self, source_id: String) -> Result<()> {\n        crate::request::Request::new(http1::Method::DELETE, \"/ingest/api/v1/source/{source_id}\")\n            .with_path_param(\"source_id\", source_id)\n            .returns_nothing()\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Rotate the Ingest Source's Url Token.\n    ///\n    /// This will rotate the ingest source's token, which is used to\n    /// construct the unique `ingestUrl` for the source. Previous tokens\n    /// will remain valid for 48 hours after rotation. The token can be\n    /// rotated a maximum of three times within the 48-hour period.\n    pub async fn rotate_token(\n        &self,\n        source_id: String,\n        options: Option<IngestSourceRotateTokenOptions>,\n    ) -> Result<RotateTokenOut> {\n        let IngestSourceRotateTokenOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/ingest/api/v1/source/{source_id}/token/rotate\",\n        )\n        .with_path_param(\"source_id\", source_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub async fn update(\n        &self,\n        source_id: String,\n        ingest_source_in: IngestSourceIn,\n    ) -> Result<IngestSourceOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/ingest/api/v1/source/{source_id}\")\n            .with_path_param(\"source_id\", source_id)\n            .with_body_param(ingest_source_in)\n            .execute(self.cfg)\n            .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/ingest_source.rs", "node_type": "function_item", "line_range": [78, 88]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\nuse super::operational_webhook_endpoint::OperationalWebhookEndpointArgs;\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct OperationalWebhookArgs {\n    #[command(subcommand)]\n    pub command: OperationalWebhookCommands,\n}\n\n#[derive(Subcommand)]\npub enum OperationalWebhookCommands {\n    Endpoint(OperationalWebhookEndpointArgs),\n}\n\nimpl OperationalWebhookCommands {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Endpoint(args) => {\n                args.command.exec(client, color_mode).await?;\n            }\n        }\n\n        Ok(())\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/operational_webhook.rs", "node_type": "function_item", "line_range": [20, 32]}
{"prompt": "<|fim_prefix|>// Modified version of the file openapi-generator would usually put in\n// apis/request.rs\n\nuse std::{collections::HashMap, time::Duration};\n\nuse http1::header::{HeaderValue, AUTHORIZATION, CONTENT_LENGTH, CONTENT_TYPE, USER_AGENT};\nuse http_body_util::{BodyExt as _, Full};\nuse hyper::body::Bytes;\nuse itertools::Itertools as _;\nuse percent_encoding::{utf8_percent_encode, AsciiSet, CONTROLS};\nuse rand::Rng;\nuse serde::de::DeserializeOwned;\n\nuse crate::{error::Error, models, Configuration};\n\n#[allow(dead_code)]\npub(crate) enum Auth {\n    None,\n    Bearer,\n}\n\n/// If the authorization type is unspecified then it will be automatically\n/// detected based on the configuration. This functionality is useful when the\n/// OpenAPI definition does not include an authorization scheme.\n#[derive(Clone)]\npub(crate) struct Request {\n    method: http1::Method,\n    path: &'static str,\n    query_params: HashMap<&'static str, String>,\n    no_return_type: bool,\n    path_params: HashMap<&'static str, String>,\n    header_params: HashMap<&'static str, String>,\n    // TODO: multiple body params are possible technically, but not supported here.\n    serialized_body: Option<String>,\n}\n\nimpl Request {\n    pub fn new(method: http1::Method, path: &'static str) -> Self {\n        Request {\n            method,\n            path,\n            query_params: HashMap::new(),\n            path_params: HashMap::new(),\n            header_params: HashMap::new(),\n            serialized_body: None,\n            no_return_type: false,\n        }\n    }\n\n    pub fn with_body_param<T: serde::Serialize>(mut self, param: T) -> Self {\n        self.serialized_body = Some(serde_json::to_string(&param).unwrap());\n        self\n    }\n\n    <|fim_suffix|>\n\n    pub fn with_query_param(mut self, basename: &'static str, param: impl QueryParamValue) -> Self {\n        self.query_params.insert(basename, param.encode());\n        self\n    }\n\n    pub fn with_optional_query_param<T: QueryParamValue>(\n        mut self,\n        basename: &'static str,\n        param: Option<T>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.query_params.insert(basename, value.encode());\n        }\n        self\n    }\n\n    pub fn with_path_param(mut self, basename: &'static str, param: String) -> Self {\n        self.path_params.insert(basename, param);\n        self\n    }\n\n    pub fn returns_nothing(mut self) -> Self {\n        self.no_return_type = true;\n        self\n    }\n\n    pub async fn execute<T: DeserializeOwned>(self, conf: &Configuration) -> Result<T, Error> {\n        match self.execute_with_backoff(conf).await? {\n            // This is a hack; if there's no_ret_type, T is (), but serde_json gives an\n            // error when deserializing \"\" into (), so deserialize 'null' into it\n            // instead.\n            // An alternate option would be to require T: Default, and then return\n            // T::default() here instead since () implements that, but then we'd\n            // need to impl default for all models.\n            None => Ok(serde_json::from_str(\"null\").expect(\"serde null value\")),\n            Some(bytes) => Ok(serde_json::from_slice(&bytes).map_err(Error::generic)?),\n        }\n    }\n\n    async fn execute_with_backoff(mut self, conf: &Configuration) -> Result<Option<Bytes>, Error> {\n        let no_return_type = self.no_return_type;\n        if self.method == http1::Method::POST && !self.header_params.contains_key(\"idempotency-key\")\n        {\n            self.header_params\n                .insert(\"idempotency-key\", format!(\"auto_{}\", uuid::Uuid::new_v4()));\n        }\n\n        const MAX_BACKOFF: Duration = Duration::from_secs(5);\n\n        let retry_schedule = match &conf.retry_schedule {\n            Some(schedule) => schedule,\n            None => &std::iter::successors(Some(Duration::from_millis(20)), |last_backoff| {\n                Some(MAX_BACKOFF.min(*last_backoff * 2))\n            })\n            .take(conf.num_retries as usize)\n            .collect(),\n        };\n        let mut retries = retry_schedule.iter();\n\n        let mut request = self.build_request(conf)?;\n        request\n            .headers_mut()\n            .insert(\"svix-req-id\", rand::rng().random::<u32>().into());\n\n        let mut retry_count = 0;\n\n        let execute_request = async |request| {\n            let response = conf.client.request(request).await.map_err(Error::generic)?;\n\n            let status = response.status();\n            if !status.is_success() {\n                Err(Error::from_response(status, response.into_body()).await)\n            } else if no_return_type {\n                Ok(None)\n            } else {\n                let bytes = response\n                    .into_body()\n                    .collect()\n                    .await\n                    .map_err(Error::generic)?\n                    .to_bytes();\n                Ok(Some(bytes))\n            }\n        };\n\n        loop {\n            let request_fut = execute_request(request.clone());\n            let res = if let Some(duration) = conf.timeout {\n                tokio::time::timeout(duration, request_fut)\n                    .await\n                    .map_err(Error::generic)?\n            } else {\n                request_fut.await\n            };\n\n            let next_backoff = retries.next().copied();\n\n            match res {\n                Ok(result) => return Ok(result),\n                e @ Err(Error::Validation(_)) => return e,\n                Err(Error::Http(err)) if err.status.as_u16() < 500 => return Err(Error::Http(err)),\n                e @ Err(_) => {\n                    if next_backoff.is_none() {\n                        return e;\n                    }\n                }\n            }\n\n            tokio::time::sleep(next_backoff.expect(\"next_backoff is always Some\")).await;\n            retry_count += 1;\n\n            request\n                .headers_mut()\n                .insert(\"svix-retry-count\", retry_count.into());\n        }\n    }\n\n    fn build_request(self, conf: &Configuration) -> Result<http1::Request<Full<Bytes>>, Error> {\n        const FRAGMENT: &AsciiSet = &CONTROLS.add(b' ').add(b'\"').add(b'<').add(b'>').add(b'`');\n        const PATH: &AsciiSet = &FRAGMENT.add(b'#').add(b'?').add(b'{').add(b'}');\n        const PATH_SEGMENT: &AsciiSet = &PATH.add(b'/').add(b'%');\n\n        let mut path = self.path.to_owned();\n        for (k, v) in self.path_params {\n            // replace {id} with the value of the id path param\n            let percent_encoded_path_param_value =\n                utf8_percent_encode(&v, PATH_SEGMENT).to_string();\n            path = path.replace(&format!(\"{{{k}}}\"), &percent_encoded_path_param_value);\n        }\n\n        let mut uri = format!(\"{}{}\", conf.base_path, path);\n\n        let mut query_string = url::form_urlencoded::Serializer::new(\"\".to_owned());\n        for (key, val) in self.query_params {\n            query_string.append_pair(key, &val);\n        }\n\n        let query_string_str = query_string.finish();\n        if !query_string_str.is_empty() {\n            uri += \"?\";\n            uri += &query_string_str;\n        }\n\n        let uri = http1::Uri::try_from(uri).map_err(Error::generic)?;\n        let mut req_builder = http1::Request::builder().uri(uri).method(self.method);\n\n        let mut request = if let Some(body) = self.serialized_body {\n            let req_headers = req_builder.headers_mut().unwrap();\n            req_headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n            req_headers.insert(CONTENT_LENGTH, body.len().into());\n            req_builder.body(Full::from(body)).map_err(Error::generic)?\n        } else {\n            req_builder.body(Full::default()).map_err(Error::generic)?\n        };\n\n        let request_headers = request.headers_mut();\n\n        // Detect the authorization type if it hasn't been set.\n        let auth = if conf.bearer_access_token.is_some() {\n            Auth::Bearer\n        } else {\n            Auth::None\n        };\n        match auth {\n            Auth::Bearer => {\n                if let Some(token) = &conf.bearer_access_token {\n                    let value = format!(\"Bearer {token}\")\n                        .try_into()\n                        .map_err(Error::gener<|fim_middle|>", "completion": "pub fn with_optional_header_param(\n        mut self,\n        basename: &'static str,\n        param: Option<String>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.header_params.insert(basename, value);\n        }\n        self\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/request.rs", "node_type": "function_item", "line_range": [55, 64]}
{"prompt": "<|fim_prefix|><|fim_suffix|>\nuse std::{collections::HashSet, time::Duration};\nuse svix::{\n    api::{ApplicationIn, EndpointIn, EndpointPatch, EventTypeIn},\n    error::Error,\n};\nuse wiremock::{Mock, MockServer, ResponseTemplate};\n\nuse crate::utils::test_client::TestClientBuilder;\n\nfn check_for_conflict(e: Error) {\n    match e {\n        Error::Http(e) => {\n            assert_eq!(\n                e.status,\n                http02::StatusCode::CONFLICT,\n                \"conflicts are expected but other statuses are not\"\n            );\n        }\n        _ => panic!(\"unexpected error: {e}\"),\n    }\n}\n\n// Opt-in with `cargo test --ignored`\n#[ignore]\n#[tokio::test]\nasync fn test_endpoint_crud() {\n    let client = TestClientBuilder::new().build();\n    let client = client.client;\n\n    let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();\n\n    if let Err(e) = client\n        .event_type()\n        .create(\n            EventTypeIn {\n                name: String::from(\"event.started\"),\n                description: String::from(\"Something started\"),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n    {\n        check_for_conflict(e);\n    }\n\n    if let Err(e) = client\n        .event_type()\n        .create(\n            EventTypeIn {\n                name: String::from(\"event.ended\"),\n                description: String::from(\"Something ended\"),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n    {\n        check_for_conflict(e);\n    }\n\n    let ep = client\n        .endpoint()\n        .create(\n            app.id.clone(),\n            EndpointIn {\n                channels: Some(vec![String::from(\"ch0\"), String::from(\"ch1\")]),\n                url: String::from(\"https://example.svix.com/\"),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();\n\n    let want_channels: HashSet<_> = [String::from(\"ch0\"), String::from(\"ch1\")]\n        .into_iter()\n        .collect();\n    let got_channels = ep.channels.clone().unwrap().into_iter().collect();\n    assert_eq!(want_channels, got_channels);\n    assert_eq!(0, ep.filter_types.unwrap_or_default().len());\n\n    let ep_patched = client\n        .endpoint()\n        .patch(\n            app.id.clone(),\n            ep.id.clone(),\n            EndpointPatch {\n                filter_types: JsOption::Some(vec![\n                    String::from(\"event.started\"),\n                    String::from(\"event.ended\"),\n                ]),\n                ..Default::default()\n            },\n        )\n        .await\n        .unwrap();\n\n    let want_filter_types: HashSet<_> =\n        [String::from(\"event.started\"), String::from(\"event.ended\")]\n            .into_iter()\n            .collect();\n    let got_channels = ep_patched.channels.clone().unwrap().into_iter().collect();\n    let got_filter_types = ep_patched\n        .filter_types\n        .clone()\n        .unwrap()\n        .into_iter()\n        .collect();\n    assert_eq!(want_channels, got_channels);\n    assert_eq!(want_filter_types, got_filter_types);\n\n    // Should complete without error if the deserialization handles empty bodies\n    // correctly.\n    client\n        .endpoint()\n        .delete(app.id.clone(), ep.id)\n        .await\n        .unwrap();\n\n    client.application().delete(app.id).await.unwrap()\n}\n\n#[tokio::test]\nasync fn test_default_retries() {\n    let mock_server: MockServer = MockServer::start().await;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .and(wiremock::matchers::header(\"svix-retry-count\", 1))\n        .and(wiremock::matchers::header_exists(\"svix-req-id\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .and(wiremock::matchers::header(\"svix-retry-count\", 2))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    let t0 = std::time::Instant::now();\n    let client = TestClientBuilder::new()\n        .url(mock_server.uri())\n        .token(\"test\".to_string())\n        .build()\n        .client;\n\n    let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await;\n    assert!(app.is_err());\n\n    let diff = std::time::Instant::now() - t0;\n    assert!(diff.as_millis() >= 60);\n\n    mock_server.verify().await;\n}\n\n#[tokio::test]\nasync fn test_custom_retries() {\n    let mock_server: MockServer = MockServer::start().await;\n    let num_retries = 6;\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    for i in 1..=num_retries {\n        Mock::given(wiremock::matchers::method(\"POST\"))\n            .and(wiremock::matchers::path(\"/api/v1/app\"))\n            .and(wiremock::matchers::header(\"svix-retry-count\", i))\n            .respond_with(ResponseTemplate::new(500))\n            .up_to_n_times(1)\n            .expect(1)\n            .mount(&mock_server)\n            .await;\n    }\n\n    let t0 = std::time::Instant::now();\n    let client = TestClientBuilder::new()\n        .url(mock_server.uri())\n        .token(\"test\".to_string())\n        .retries(num_retries)\n        .build()\n        .client;\n\n    let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await;\n    assert!(app.is_err());\n\n    let diff = std::time::Instant::now() - t0;\n    let expected: u32 = (1..=num_retries).map(|x| 20 * x).sum();\n    assert!(diff.as_millis() >= u128::from(expected));\n\n    mock_server.verify().await;\n}\n\n#[tokio::test]\nasync fn test_custom_retry_schedule() {\n    let mock_server: MockServer = MockServer::start().await;\n    let retry_schedule_in_ms = [50, 100, 200, 400];\n    let retry_schedule = retry_schedule_in_ms.map(Duration::from_millis).into();\n\n    Mock::given(wiremock::matchers::method(\"POST\"))\n        .and(wiremock::matchers::path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(500))\n        .up_to_n_times(1)\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    for i in 1..=retry_schedule_in_ms.len() {\n        Mock::given(wiremock::matchers::method(\"POST\"))\n            .and(wiremock::matchers::path(\"/api/v1/app\"))\n            .and(wiremock::matchers::header(\"svix-retry-count\", i))\n            .respond_with(ResponseTemplate::new(500))\n            .up_to_n_times(1)\n            .expect(1)\n            .mount(&mock_server)\n            .await;\n    }\n\n    let t0 = std::time::Instant::now();\n    let client = TestClientBuilder::new()\n        .url(mock_server.uri())\n        .token(\"test\".to_string())\n        .retry_schedule(retry_schedule)\n        .build()\n        .client;\n\n    let app = client\n        .application()\n        .create(\n            ApplicationIn {\n                name: \"app\".to_string(),\n                ..Default::default()\n            },\n            None,\n        )\n        .await;\n    assert!(app.is_err());\n\n    let diff = std::time::Instant::now() - t0;\n    let expected: u64 = retry_schedule_in_ms.iter().sum();\n    assert!(diff.as_millis() >= u128::from(expected));\n\n    mock_server.verify().await;\n}\n<|fim_middle|>", "completion": "use js_option::JsOption;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/tests/it/kitchen_sink.rs", "node_type": "use_declaration", "line_range": [1, 1]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse js_option::JsOption;\nuse serde::{Deserialize, Serialize};\n\n<|fim_suffix|>\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ConnectorPatch {\n    #[serde(rename = \"allowedEventTypes\")]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub allowed_event_types: JsOption<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub description: Option<String>,\n\n    #[serde(rename = \"featureFlags\")]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub feature_flags: JsOption<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub instructions: Option<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub kind: Option<ConnectorKind>,\n\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub logo: JsOption<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub name: Option<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub transformation: Option<String>,\n}\n\nimpl ConnectorPatch {\n    pub fn new() -> Self {\n        Self {\n            allowed_event_types: JsOption::Undefined,\n            description: None,\n            feature_flags: JsOption::Undefined,\n            instructions: None,\n            kind: None,\n            logo: JsOption::Undefined,\n            name: None,\n            transformation: None,\n        }\n    }\n}\n<|fim_middle|>", "completion": "use super::connector_kind::ConnectorKind;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/connector_patch.rs", "node_type": "use_declaration", "line_range": [5, 5]}
{"prompt": "<|fim_prefix|>ize, Serialize};\nuse svix_bridge_types::{\n    svix, ReceiverInputOpts, ReceiverOutput, TransformationConfig, TransformerTx, WebhookVerifier,\n};\n\nuse super::verification::{NoVerifier, SvixVerifier, VerificationMethod, Verifier};\nuse crate::config::WebhookReceiverConfig;\n\n#[derive(Clone)]\n/// The [`InternalState`] is passed to the Axum route and is used to map the \"IntegrationId\" in the\n/// URL to the configured [`Verifier`] and [`Forwarder`] variants.\npub struct InternalState {\n    pub routes: Arc<HashMap<IntegrationId, IntegrationState>>,\n    pub transformer_tx: TransformerTx,\n}\n\nimpl std::fmt::Debug for InternalState {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        f.debug_struct(\"InternalState\").finish()\n    }\n}\n\nimpl InternalState {\n    /// For most production use cases, favor [`InternalState::from_receiver_configs`].\n    /// Mostly this is an escape hatch to help with testing.\n    ///\n    /// Constructs an [`InternalState`] from a raw mapping of [`IntegrationId`] to\n    /// [`IntegrationState`], allowing us to bypass all the config parsing machinery.\n    ///\n    /// By skipping the config parsing, we can provide custom (i.e. not exposed through the public\n    /// config) [`ReceiverOutput`] implementations.\n    pub fn new(\n        state_map: HashMap<IntegrationId, IntegrationState>,\n        transformer_tx: TransformerTx,\n    ) -> Self {\n        InternalState {\n            routes: Arc::new(state_map),\n            transformer_tx,\n        }\n    }\n\n    pub async fn from_receiver_configs(\n        routes: Vec<WebhookReceiverConfig>,\n        transformer_tx: TransformerTx,\n    ) -> Result<Self> {\n        let mut state_map = HashMap::new();\n\n        for cfg in routes {\n            let verifier = match &cfg.input {\n                ReceiverInputOpts::Webhook {\n                    verification: WebhookVerifier::Svix { endpoint_secret },\n                    ..\n                }\n                | ReceiverInputOpts::SvixWebhook {\n                    endpoint_secret, ..\n                } => SvixVerifier::new(Arc::new(\n                    svix::webhooks::Webhook::new(endpoint_secret).expect(\"Invalid Svix secret\"),\n                ))\n                .into(),\n                ReceiverInputOpts::Webhook {\n                    verification: WebhookVerifier::None,\n                    ..\n                } => NoVerifier.into(),\n            };\n\n            state_map.insert(\n                IntegrationId(cfg.input.path_id().to_string()),\n                IntegrationState {\n                    verifier,\n                    transformation: cfg.transformation.clone(),\n                    output: Arc::new(cfg.into_receiver_output().await?),\n                },\n            );\n        }\n\n        Ok(InternalState::new(state_map, transformer_tx))\n    }\n}\n\n/// Each [`IntegrationId`] is a valid route for webhooks to be dispatched to managed by this server,\n/// and each [`IntegrationId`] has an associated configuration which defines how the webhook is\n/// verified (the [`VerificationScheme`]) and where the webhook is routed to once it is verified\n/// (the [`ForwardDestination`]).\n///\n/// Internally it is also associated with an [`IntegrationState`] which will contain the necessary\n/// members to actually perform these actions eg. a handle to a [`FutureProducer`] instead of simply\n/// the address(es) of the Kafka bootstrap server(s).\n///\n/// This type is simply a wrapper for a [`String`] which *should* be safe to use in a URL. If it is\n/// not a valid path component for a URL, then the [`IntegrationId`] will never receive any\n/// webhooks. However, for simplicity, the inner [`String`] is not validated for URL-safety at this\n/// time.\n#[repr(transparent)]\n#[derive(Clone, Debug, Deserialize, Eq, Hash, PartialEq, Serialize)]\npub struct IntegrationId(String);\n\nimpl From<String> for IntegrationId {\n    fn from(value: String) -> Self {\n        IntegrationId(value)\n    }\n}\n\nimpl From<&str> for IntegrationId {\n    fn from(value: &str) -> Self {\n        IntegrationId(value.to_string())\n    }\n}\n\nimpl AsRef<str> for IntegrationId {\n    <|fim_suffix|>\n}\n\n/// The [`IntegrationState`] is a struct which is only able to be created via conversion from a\n/// [`IntegrationConfig`]. This struct is what is associated with an `[IntegrationId`] internally\n/// after the configuration has been read.\n///\n/// What distinguishes it from the [`IntegrationConfig`] is that it contains the necessary members\n/// for validating and forwarding a webhook instead of just containing the definition of how to\n/// derive these necessary members.\n#[derive(Clone)]\npub struct IntegrationState {\n    pub verifier: Verifier,\n    pub output: Arc<Box<dyn ReceiverOutput>>,\n    pub transformation: Option<TransformationConfig>,\n}\n\n/// The [`RequestFromParts`] is a structure consisting of all relevant parts of the HTTP request to\n/// be validated by a [`Verifier`] implementor. This is to be immediately converted into the struct\n/// [`SerializableRequest<Unvalidated>`] via its [`FromRequest`] implementation.\n///\n/// NOTE: This struct is never to be used directly unless by proxy of the aforementioned impl of\n/// [`FromRequest`]. It's simply used as any easy way to implement [`FromRequest`] via a macro .\n#[derive(Clone, Debug, FromRequest)]\npub struct RequestFromParts {\n    headers: HeaderMap,\n    payload: Bytes,\n}\n\n/// A simple marker trait to denote the state of a [`SerializableRequest`]. The only way to publicly\n/// construct any [`SerializableRequest<Validated>`]s  is via the associated method on unvalidated\n/// request's, [`SerializableRequest<Unvalidated>::validate`].\npub trait RequestState {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Unvalidated;\nimpl RequestState for Unvalidated {}\n\n#[derive(Clone, Copy, Debug)]\npub struct Validated;\nimpl RequestState for Validated {}\n\n/// This intermediary representation is necessary because it is preferable to serialize the headers\n/// and/or body as a [`String`] over bytes when dealing with some [`VerificationMethod`]s and some\n/// [`ForwardingMethod`]s. This struct represents both the headers and body as enums which allow for\n/// either textual representations or byte representations when [`Serialize`]d via [`serde`].\n///\n/// On trying to convert a [`Standard`] variant into a [`StringSerializable`] variant, HTTP headers\n/// will be represented textually if and only if they are completely ASCII, while any bodies will\n/// attempt to be read as UTF-8 before falling back to bytes.\n///\n/// NOTE: This conversion *should* be lazy. The [`String`] variant are only acceptable in a subset\n/// of all cases, so lazy-conversion will prevent needless conversion back and forth. You may check\n/// whether the conversion is required and/or helpful with [`VerificationMethod::want_string_rep`]\n/// or [`VerificationMethod::need_string_rep`] plus the [`ForwardingMethod`] equivalents.\n///\n/// The intended course of action is to attempt to convert to string-serializable variants of the\n/// header map and the body immediately if either of the aforementioned methods are true --  but\n/// only returning an [`Err`] response if it *needs* it. Then, if the validation is a success (see\n/// [`SerializableRequest<Unvalidated>::validate`] and a validated equivalent is returned, then the\n/// same checks are to be performed, but with the [`ForwardingMethod`] methods before being sent to\n/// the appropriate [`ForwardingMethod`] implementor.\n#[derive(Clone, Debug, Serialize)]\npub struct SerializableRequest<S: RequestState> {\n    headers: SerializableHeaderMap,\n    payload: SerializablePayload,\n\n    #[serde(skip)]\n    _pd: PhantomData<S>,\n}\n\nimpl<S: RequestState> SerializableRequest<S> {\n    pub fn headers(&self) -> &SerializableHeaderMap {\n        &self.headers\n    }\n\n    pub fn payload(&self) -> &SerializablePayload {\n        &self.payload\n    }\n}\n\nimpl From<RequestFromParts> for SerializableRequest<Unvalidated> {\n    fn from(value: RequestFromParts) -> Self {\n        Self {\n            headers: SerializableHeaderMap::Standard(value.headers),\n            payload: SerializablePayload::Standard(value.payload.to_vec()),\n\n            _pd: PhantomData,\n        }\n    }\n}\n\n#[async_trait]\nimpl<S> FromRequest<S><|fim_middle|>", "completion": "fn as_ref(&self) -> &str {\n        &self.0\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/types.rs", "node_type": "function_item", "line_range": [119, 121]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse reqwest::StatusCode;\nuse serde_json::json;\nuse svix_server::{\n    core::types::{EndpointUid, MessageStatus},\n    v1::{\n        endpoints::{\n            attempt::{EndpointMessageOut, MessageAttemptOut},\n            endpoint::{EndpointIn, EndpointOut},\n        },\n        utils::ListResponse,\n    },\n};\nuse wiremock::{matchers, Mock, MockServer, Respond, ResponseTemplate};\n\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, create_test_msg_with,\n        endpoint_in, get_msg_attempt_list_and_assert_count,\n    },\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n    TestReceiver,\n};\n\n#[tokio::test]\nasync fn test_expunge_attempt_response_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let sensitive_response_json = serde_json::json!({\"sensitive\":\"data\"});\n    let mut receiver = TestReceiver::start_with_body(\n        axum::http::StatusCode::OK,\n        axum::Json(sensitive_response_json.clone()),\n    );\n\n    let endpoint_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap()\n        .id;\n\n    receiver.data_recv.recv().await;\n\n    let attempt = run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endpoint_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        if attempts.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", attempts.data.len());\n        }\n        Ok(attempts.data[0].clone())\n    })\n    .await\n    .unwrap();\n\n    l<|fim_suffix|>    assert_eq!(sensitive_response_json, attempt_response);\n\n    let attempt_id = &attempt.id;\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let attempt: MessageAttemptOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\"EXPUNGED\", &attempt.response);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &<|fim_middle|>", "completion": "let attempt_response: serde_json::Value = serde_json::from_str(&attempt.response).unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [72, 72]}
{"prompt": "<|fim_prefix|>use std::{future::Future, pin::Pin};\n\nuse http1::Uri;\nuse hyper_util::{\n    client::legacy::connect::{\n        proxy::{SocksV5, Tunnel},\n        HttpConnector,\n    },\n    rt::TokioIo,\n};\nuse tokio::net::TcpStream;\nuse tower_service::Service;\n\n// If no TLS backend is enabled, use plain http connector.\n#[cfg(not(any(feature = \"native-tls\", feature = \"rustls-tls\")))]\ntype HttpsIfAvailable<T> = T;\n#[cfg(not(any(feature = \"native-tls\", feature = \"rustls-tls\")))]\ntype MaybeHttpsStream<T> = T;\n\n// If only native TLS is enabled, use that.\n#[cfg(all(feature = \"native-tls\", not(feature = \"rustls-tls\")))]\nuse hyper_tls::{HttpsConnector as HttpsIfAvailable, MaybeHttpsStream};\n\n// If rustls is enabled, use that.\n#[cfg(feature = \"rustls-tls\")]\nuse hyper_rustls::{HttpsConnector as HttpsIfAvailable, MaybeHttpsStream};\n\nfn wrap_connector<H>(conn: H) -> HttpsIfAvailable<H> {\n    #[cfg(not(any(feature = \"native-tls\", feature = \"rustls-tls\")))]\n    return conn;\n\n    #[cfg(feature = \"rustls-tls\")]\n    {\n        let crypto_provider = rustls::crypto::CryptoProvider::get_default()\n            .map(std::sync::Arc::clone)\n            .unwrap_or_else(|| std::sync::Arc::new(rustls::crypto::aws_lc_rs::default_provider()));\n\n        let builder = hyper_rustls::HttpsConnectorBuilder::new()\n            .with_provider_and_native_roots(crypto_provider)\n            .unwrap()\n            .https_or_http();\n\n        #[cfg(feature = \"http1\")]\n        let builder = builder.enable_http1();\n\n        #[cfg(feature = \"http2\")]\n        let builder = builder.enable_http2();\n\n        builder.wrap_connector(conn)\n    }\n\n    #[cfg(all(feature = \"native-tls\", not(feature = \"rustls-tls\")))]\n    return hyper_tls::HttpsConnector::new_with_connector(conn);\n}\n\n#[derive(Clone, Debug)]\npub(crate) enum Connector {\n    Regular(HttpsIfAvailable<HttpConnector>),\n    Socks5Proxy(HttpsIfAvailable<SocksV5<HttpConnector>>),\n    HttpProxy(HttpsIfAvailable<Tunnel<HttpConnector>>),\n}\n\n// FIXME: If we ever do a breaking release, change this\n// to be fallible and bubble the error up from `Svix::new`.\npub(crate) fn make_connector(proxy_addr: Option<String>) -> Connector {\n    let mut http = hyper_util::client::legacy::connect::HttpConnector::new();\n    if cfg!(any(feature = \"native-tls\", feature = \"rustls-tls\")) {\n        http.enforce_http(false);\n    }\n\n    let Some(proxy_addr) = proxy_addr else {\n        return Connector::Regular(wrap_connector(http));\n    };\n    let proxy_addr = match proxy_addr.parse::<Uri>() {\n        Ok(proxy_addr) => proxy_addr,\n        Err(e) => {\n            tracing::error!(\n                error = &e as &dyn std::error::Error,\n                \"invalid proxy address\"\n            );\n            return Connector::Regular(wrap_connector(http));\n        }\n    };\n\n    match proxy_addr.scheme_str() {\n        Some(\"http\" | \"https\") => {\n            let tunnel = Tunnel::new(proxy_addr, http);\n            Connector::HttpProxy(wrap_connector(tunnel))\n        }\n        Some(\"socks5\") => {\n            <|fim_suffix|>\n            Connector::Socks5Proxy(wrap_connector(socks))\n        }\n        Some(\"socks5h\") => {\n            let socks = SocksV5::new(proxy_addr, http);\n            Connector::Socks5Proxy(wrap_connector(socks))\n        }\n        scheme => {\n            tracing::error!(\n                scheme,\n                \"invalid proxy address: scheme must be one of http, https, socks5, socks5h\"\n            );\n            Connector::Regular(wrap_connector(http))\n        }\n    }\n}\n\nimpl Service<Uri> for Connector {\n    type Response = MaybeHttpsStream<TokioIo<TcpStream>>;\n    type Error = Box<dyn std::error::Error + Send + Sync>;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(\n        &mut self,\n        cx: &mut std::task::Context<'_>,\n    ) -> std::task::Poll<Result<(), Self::Error>> {\n        match self {\n            Self::Regular(inner) => inner.poll_ready(cx),\n            Self::Socks5Proxy(inner) => inner.poll_ready(cx),\n            Self::HttpProxy(inner) => inner.poll_ready(cx),\n        }\n    }\n\n    fn call(&mut self, req: Uri) -> Self::Future {\n        match self {\n            Self::Regular(inner) => Box::pin(inner.call(req)),\n            Self::Socks5Proxy(inner) => Box::pin(inner.call(req)),\n            Self::HttpProxy(inner) => Box::pin(inner.call(req)),\n        }\n    }\n}\n<|fim_middle|>", "completion": "let socks = SocksV5::new(proxy_addr, http).local_dns(true);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/connector.rs", "node_type": "let_declaration", "line_range": [91, 91]}
{"prompt": "<|fim_prefix|>messageattempt(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgAttemptPath {\n        msg_id, attempt_id, ..\n    }): Path<ApplicationMsgAttemptPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<MessageAttemptOut>> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let attempt = messageattempt::Entity::secure_find_by_msg(msg.id)\n        .filter(messageattempt::Column::Id.eq(attempt_id))\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    Ok(Json(attempt.into()))\n}\n\n/// Resend a message to the specified endpoint.\n#[aide_annotate(op_id = \"v1.message-attempt.resend\")]\nasync fn resend_webhook(\n    State(AppState {\n        ref db, queue_tx, ..\n    }): State<AppState>,\n    Path(ApplicationMsgEndpointPath {\n        msg_id,\n        endpoint_id,\n        ..\n    }): Path<ApplicationMsgEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<NoContentWithCode<202>> {\n    let (msg, msg_content) = message::Entity::secure_find_by_id_or_uid(app.id.clone(), msg_id)\n        .find_also_related(messagecontent::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let msg_content = match msg_content {\n        Some(m) => serde_json::from_slice(&m.payload).ok(),\n        None => msg.legacy_payload,\n    };\n    if msg_content.is_none() {\n        return Err(HttpError::bad_request(\n            Some(\"missing_payload\".to_string()),\n            Some(\"Unable to resend message. Payload is missing (probably expired).\".to_string()),\n        )\n        .into());\n    }\n\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    queue_tx\n        .send(\n            &MessageTask::new_task(\n                msg.id.clone(),\n                app.id,\n                endp.id,\n                MessageAttemptTriggerType::Manual,\n            ),\n            None,\n        )\n        .await?;\n    Ok(NoContentWithCode)\n}\n\n/// Deletes the given attempt's response body. Useful when an endpoint accidentally returned sensitive content.\n#[aide_annotate(op_id = \"v1.message-attempt.expunge-content\")]\nasync fn expunge_attempt_content(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgAttemptPath {\n        msg_id, attempt_id, ..\n    }): Path<ApplicationMsgAttemptPath>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n) -> Result<StatusCode> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, Some(\"Message not found\".to_string())))?;\n\n    let mut attempt = messageattempt::Entity::secure_find_by_msg(msg.id)\n        .filter(messageattempt::Column::Id.eq(attempt_id))\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, Some(\"Message attempt not found\".to_string())))?\n        .into_active_model();\n\n    attempt.response = sea_orm::Set(\"EXPUNGED\".to_string());\n    attempt.update(db).await?;\n\n    Ok(StatusCode::NO_CONTENT)\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Message Attempt\");\n    ApiRouter::new()\n        // NOTE: [`list_messageattempts`] is deprecated\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/attempt\",\n            get_with(list_messageattempts, list_messageattempts_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/attempt/:attempt_id\",\n            get_with(get_messageattempt, get_messageattempt_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/attempt/:attempt_id/content\",\n            delete_with(expunge_attempt_content, expunge_attempt_content_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/endpoint\",\n            get_with(\n                list_attempted_destinations,\n                list_attempted_destinations_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/endpoint/:endpoint_id/resend\",\n            post_with(resend_webhook, resend_webhook_operation),\n            &tag,\n        )\n        // NOTE: [`list_attempts_for_endpoint`] is deprecated\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/endpoint/:endpoint_id/attempt\",\n            get_with(\n                list_attempts_for_endpoint,\n                list_attempts_for_endpoint_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/msg\",\n            get_with(list_attempted_messages, list_attempted_messages_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/attempt/endpoint/:endpoint_id\",\n            get_with(\n                list_attempts_by_endpoint,\n                list_attempts_by_endpoint_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/attempt/msg/:msg_id\",\n            get_with(list_attempts_by_msg, list_attempts_by_msg_operation),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{\n        AttemptListFetchQueryParams, ListAttemptedMessagesQueryParams,\n        ListAttemptsByEndpointQueryParams, ListAttemptsByMsgQueryParams,\n        ListAttemptsForEndpointQueryParams,\n    };\n\n    const INVALID_CHANNEL: &str = \"$$invalid-channel\";\n    const VALID_CHANNEL: &str = \"valid-channel\";\n    const INVALID_ENDPOINT_ID: &str = \"$$invalid-endpoint\";\n    const VALID_ENDPOINT_ID: &str = \"ep_valid-endpoint\";\n\n    #[test]\n    fn test_list_attempted_messages_query_params_validation() {\n        let q: ListAttemptedMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptedMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": VALID_CHANNEL })).unwrap();\n        q.validate().unwrap();\n    }\n\n    #[test]\n    f<|fim_suffix|>\n    #[test]\n    fn test_list_attempts_by_msg_query_parameters_validation() {\n        let q: ListAttemptsByMsgQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptsByMsgQueryParams =\n            serde_json::from_value(json!({ \"endpoint_id\": INVALID_ENDPOINT_ID })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptsByMsgQueryParams = serde_json::from_value(json!(\n            {\n                \"channel\": VALID_CHANNEL,\n                \"endpoint_id\": VALID_ENDPOINT_ID\n            }\n        ))\n        .unwrap();\n        q.validate().unwrap();\n    }\n\n    #[test]\n    fn test_list_attempts_for_endpoint_query_parameters_validation() {\n        let q: ListAttemptsForEndpointQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptsForEndpointQueryParams =\n            serde_json::from_value(json!({ \"channel\": VALID_CHANNEL })).unwrap();\n        q.validate().unwrap();\n    }\n\n    #[test]\n    fn test_attempt_list_fetch_options_validation() {\n        let q: AttemptListFetchQueryParams =\n            serde_json::from_value(json!({ \"endpoint_id\": INVALID_ENDPOINT_ID })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: AttemptListFetchQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: AttemptListFetchQueryParams = serde_json::from_value(json!(\n            {\n                \"endpoint_id\": VALID_ENDPOINT_ID,\n                \"channel\": VALID_CHANNEL\n            }\n        ))\n        .unwrap();\n        q.validate().unwrap();\n    }\n}\n<|fim_middle|>", "completion": "fn test_list_attempts_by_endpoint_query_parameters_validation() {\n        let q: ListAttemptsByEndpointQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/attempt.rs", "node_type": "function_item", "line_range": [1144, 1148]}
{"prompt": "<|fim_prefix|>//! Allocator stats are only available when we're using jemalloc, and jemalloc doesn't work on windows.\n//!\n//! 2 impls for the helper functions are therefore provided. One set that does nothing (for windows)\n//! and another that works in the non-windows world.\n//!\n//! Care should be taken to keep the signatures aligned between these two so the callsites can be\n//! used consistently regardless of whether jemalloc is in use or not.\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\npub use supported::*;\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\npub use unsupported::*;\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\nmod supported {\n    use std::sync::Arc;\n\n    use tikv_jemalloc_ctl::{epoch, stats};\n\n    pub struct AllocatorStatMibs {\n        epoch: tikv_jemalloc_ctl::epoch_mib,\n        allocated: stats::allocated_mib,\n        resident: stats::resident_mib,\n    }\n\n    pub fn get_allocator_stats(\n        bust_cache: bool,\n        mibs: &AllocatorStatMibs,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        if bust_cache {\n            // Stats are cached internally and advancing the epoch is a way to invalidate those caches.\n            mibs.epoch.advance()?;\n        }\n        let allocated = mibs.allocated.read()?;\n        <|fim_suffix|>\n        Ok(Some((allocated, resident)))\n    }\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        let e = epoch::mib()?;\n        let allocated = stats::allocated::mib()?;\n        let resident = stats::resident::mib()?;\n\n        Ok(Arc::new(AllocatorStatMibs {\n            epoch: e,\n            allocated,\n            resident,\n        }))\n    }\n}\n\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\nmod unsupported {\n    use std::sync::Arc;\n\n    use anyhow::anyhow;\n\n    pub struct AllocatorStatMibs;\n\n    pub fn get_allocator_stats(\n        _bust_cache: bool,\n        _mibs: Arc<AllocatorStatMibs>,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        Ok(None)\n    }\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        Err(anyhow!(\"metric collection is not supported\"))\n    }\n}\n<|fim_middle|>", "completion": "let resident = mibs.resident.read()?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/allocator.rs", "node_type": "let_declaration", "line_range": [35, 35]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::ingest_message_attempt_failing_event_data::IngestMessageAttemptFailingEventData;\n\n/// Sent after a message has been failing for a few times.\n/// It's sent on the fourth failure. It complements\n/// `ingest.message.attempt.exhausted` which is sent after the last failure.\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct IngestMessageAttemptFailingEvent {\n    pub data: IngestMessageAttemptFailingEventData,\n\n    pub r#type: String,\n}\n\nimpl IngestMessageAttemptFailingEvent {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(data: IngestMessageAttemptFailingEventData, r#type: String) -> Self {\n        Self { data, r#type }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/ingest_message_attempt_failing_event.rs", "node_type": "function_item", "line_range": [17, 19]}
{"prompt": "<|fim_prefix|>     org_id: Set(app.org_id),\n        ..data.into()\n    };\n\n    let (msg, msg_content) = db\n        .transaction(|txn| {\n            async move {\n                let msg = msg.insert(txn).await.map_err(http_error_on_conflict)?;\n                let msg_content =\n                    messagecontent::ActiveModel::new(msg.id.clone(), payload, msg.expiration);\n                let msg_content = msg_content.insert(txn).await?;\n                Ok((msg, msg_content))\n            }\n            .boxed()\n        })\n        .await?;\n\n    let trigger_type = MessageAttemptTriggerType::Scheduled;\n    if !create_message_app\n        .filtered_endpoints(trigger_type, &msg.event_type, msg.channels.as_ref())\n        .is_empty()\n    {\n        queue_tx\n            .send(\n                &MessageTaskBatch::new_task(\n                    msg.id.clone(),\n                    app.id.clone(),\n                    force_endpoint,\n                    MessageAttemptTriggerType::Scheduled,\n                ),\n                None,\n            )\n            .await?;\n    }\n\n    let msg_out = MessageOut::from_msg_and_payload(msg, Some(msg_content.payload), with_content);\n\n    Ok(msg_out)\n}\n\npub fn validate_create_app_uid(\n    app_id_or_uid: &ApplicationIdOrUid,\n    data: &ApplicationIn,\n) -> Result<()> {\n    // If implicit app creation is requested then the UID must be set\n    // in the request body, and it must match the UID given in the path\n    if let Some(uid) = &data.uid {\n        if uid.0 != app_id_or_uid.0 {\n            return Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n                loc: vec![\n                    \"body\".to_string(),\n                    \"application\".to_string(),\n                    \"uid\".to_string(),\n                ],\n                msg: \"Application UID in the path and body must match\".to_string(),\n                ty: \"application_uid_mismatch\".to_string(),\n            }])\n            .into());\n        }\n    } else {\n        return Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n            loc: vec![\n                \"body\".to_string(),\n                \"application\".to_string(),\n                \"uid\".to_string(),\n            ],\n            msg: \"Application UID not set in body\".to_string(),\n            ty: \"application_uid_missing\".to_string(),\n        }])\n        .into());\n    }\n    Ok(())\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct GetMessageQueryParams {\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// Get a message by its ID or eventID.\n#[aide_annotate(op_id = \"v1.message.get\")]\nasync fn get_message(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgPath { msg_id, .. }): Path<ApplicationMsgPath>,\n    ValidatedQuery(GetMessageQueryParams { with_content }): ValidatedQuery<GetMessageQueryParams>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<MessageOut>> {\n    let (msg, msg_content) = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .find_also_related(messagecontent::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    let msg_out =\n        MessageOut::from_msg_and_payload(msg, msg_content.map(|c| c.payload), with_content);\n    Ok(Json(msg_out))\n}\n\n/// Delete the given message's payload. Useful in cases when a message was accidentally sent with sensitive content.\n///\n/// The message can't be replayed or resent once its payload has been deleted or expired.\n#[aide_annotate(op_id = \"v1.message.expunge-content\")]\nasync fn expunge_message_content(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgPath { msg_id, .. }): Path<ApplicationMsgPath>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n) -> Result<StatusCode> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    let msg_id = msg.id.clone();\n    let mut msg = msg.into_active_model();\n\n    msg.legacy_payload = Set(None);\n    msg.update(db).await?;\n\n    messagecontent::Entity::delete_by_id(msg_id)\n        .exec(db)\n        .await?;\n    Ok(StatusCode::NO_CONTENT)\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Message\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app/:app_id/msg\",\n            post_with(create_message, create_message_operation)\n                .get_with(list_messages, list_messages_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id\",\n            get_with(get_message, get_message_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/content\",\n            delete_with(expunge_message_content, expunge_message_content_operation),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{\n        default_true, CreateMessageQueryParams, GetMessageQueryParams, ListMessagesQueryParams,\n        MessageIn,\n    };\n\n    const CHANNEL_INVALID: &str = \"$$invalid-channel\";\n    const CHANNEL_VALID: &str = \"valid-channel\";\n    const EVENT_TYPE_INVALID: &str = \"$$invalid-eventType\";\n    const EVENT_TYPE_VALID: &str = \"valid-eventType\";\n    const EVENT_ID_INVALID: &str = \"$$invalid-eventId\";\n    const EVENT_ID_VALID: &str = \"valid-eventId\";\n    const EVENT_CHANNELS_INVALID: &[&str] = &[\"valid-event-channel\", \"&&invalid-event-channel\"];\n    const EVENT_CHANNELS_VALID: &[&str] = &[\"valid-event-channel1\", \"valid-event-channel2\"];\n\n    #[test]\n    fn test_message_in_validation() {\n        let invalid_1: MessageIn = serde_json::from_value(json!({\n            \"eventId\": EVENT_ID_INVALID,\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": {}\n        }))\n        .unwrap();\n\n        let invalid_2: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_INVALID,\n            \"payload\": {}\n        }))\n        .unwrap();\n\n        let invalid_3: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": {},\n            \"channels\": EVENT_CHANNELS_INVALID\n        }))\n        .unwrap();\n\n        let invalid_4: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": \"this should be invalid\",\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n\n        let invalid_5: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": json!([ \"this should be invalid\" ]),\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n\n        let invalid_6: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": json!([ { \"msg\": \"this should be invalid\" } ]),\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n\n        for m in [\n            invalid_1, invalid_2, invalid_3, invalid_4, invalid_5, invalid_6,\n        ] {\n            assert!(m.validate().is_err());\n        }\n\n        l<|fim_suffix|>        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_list_messages_query_params_validation() {\n        let invalid: ListMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": CHANNEL_INVALID })).unwrap();\n        assert!(invalid.validate().is_err());\n\n        let valid: ListMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": CHANNEL_VALID })).unwrap();\n        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_default_true() {\n        assert!(default_true());\n    }\n\n    #[test]\n    fn test_create_message_query_params_default() {\n        let q: CreateMessageQueryParams = serde_json::from_value(json!({})).unwrap();\n        assert!(q.with_content);\n    }\n\n    #[test]\n    fn test_get_message_query_params_default() {\n        let q: GetMessageQueryParams = serde_json::from_value(json!({})).unwrap();\n        assert!(q.with_content);\n    }\n}\n<|fim_middle|>", "completion": "let valid: MessageIn = serde_json::from_value(json!({\n            \"eventId\": EVENT_ID_VALID,\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": {},\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/message.rs", "node_type": "let_declaration", "line_range": [636, 642]}
{"prompt": "<|fim_prefix|> = std::result::Result<T, E>;\n\n/// The error type returned from the Svix API\n#[derive(Debug)]\npub struct Error {\n    // the file name and line number of the error. Used for debugging non Http errors\n    pub trace: Vec<&'static Location<'static>>,\n    pub typ: ErrorType,\n}\n\nimpl Error {\n    #[track_caller]\n    fn new(typ: ErrorType) -> Self {\n        let trace = vec![Location::caller()];\n        Self { trace, typ }\n    }\n\n    #[track_caller]\n    pub fn generic(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Generic(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn database(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Database(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn conflict(e: DbErr) -> Self {\n        Self::new(ErrorType::Conflict(e))\n    }\n\n    #[track_caller]\n    pub fn queue(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Queue(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn validation(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Validation(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn http(h: HttpError) -> Self {\n        Self {\n            trace: Vec::with_capacity(0), // no debugging necessary\n            typ: ErrorType::Http(h),\n        }\n    }\n\n    #[track_caller]\n    pub fn cache(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Cache(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Timeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn db_timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::DbTimeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn connection_timeout(e: DbErr) -> Self {\n        Self::new(ErrorType::ConnectionTimeout(e))\n    }\n\n    #[track_caller]\n    pub fn trace(mut self) -> Self {\n        self.trace.push(Location::caller());\n        self\n    }\n}\n\nimpl fmt::Display for Error {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        self.typ.fmt(f)\n    }\n}\n\nimpl error::Error for Error {\n    fn source(&self) -> Option<&(dyn error::Error + 'static)> {\n        None\n    }\n}\n\nimpl IntoResponse for Error {\n    fn into_response(self) -> Response {\n        let stringified: Vec<String> = self.trace.into_iter().map(ToString::to_string).collect();\n        match self.typ {\n            ErrorType::Http(s) => {\n                tracing::debug!(\"{:?}, location: {:?}\", &s, stringified);\n                s.into_response()\n            }\n            s => {\n                tracing::error!(\"type: {:?}, location: {:?}\", s, stringified);\n                (StatusCode::INTERNAL_SERVER_ERROR, Json(json!({}))).into_response()\n            }\n        }\n    }\n}\n\nimpl OperationOutput for Error {\n    type Inner = Self;\n}\n\npub trait Traceable<T> {\n    /// Pushes the current [`Location`] onto the error's trace stack\n    #[track_caller]\n    fn trace(self) -> Result<T>;\n}\n\nimpl<T> Traceable<T> for Result<T> {\n    fn trace(self) -> Result<T> {\n        // Using `map_err` would lose `#[track_caller]` information\n        match self {\n            Err(e) => Err(e.trace()),\n            ok => ok,\n        }\n    }\n}\n\nimpl From<DbErr> for Error {\n    #[track_caller]\n    fn from(err: DbErr) -> Self {\n        if is_timeout_error(&err) {\n            Error::db_timeout(err)\n        } else if is_conflict_err(&err) {\n            Error::conflict(err)\n        } else if is_connection_timeout_error(&err) {\n            Error::connection_timeout(err)\n        } else {\n            Error::database(err)\n        }\n    }\n}\n\nimpl From<redis::RedisError> for Error {\n    #[track_caller]\n    fn from(value: redis::RedisError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<omniqueue::QueueError> for Error {\n    #[track_caller]\n    fn from(value: omniqueue::QueueError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl<E: error::Error + 'static> From<bb8::RunError<E>> for Error {\n    #[track_caller]\n    fn from(value: bb8::RunError<E>) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<ExtensionRejection> for Error {\n    #[track_caller]\n    f<|fim_suffix|>}\n\nimpl From<PathRejection> for Error {\n    #[track_caller]\n    fn from(value: PathRejection) -> Self {\n        Error::generic(value)\n    }\n}\n\nimpl From<crate::core::cache::Error> for Error {\n    #[track_caller]\n    fn from(value: crate::core::cache::Error) -> Self {\n        Error::cache(value)\n    }\n}\n\nimpl From<TransactionError<Error>> for Error {\n    #[track_caller]\n    fn from(value: TransactionError<Error>) -> Self {\n        match value {\n            TransactionError::Connection(db_err) => Error::database(db_err),\n            TransactionError::Transaction(crate_err) => crate_err, // preserve the trace that comes from within the transaction\n        }\n    }\n}\n\nimpl From<lapin::Error> for Error {\n    #[track_caller]\n    fn from(value: lapin::Error) -> Self {\n        Error::queue(format_args!(\"{value:?}\"))\n    }\n}\n\n#[derive(Debug)]\npub enum ErrorType {\n    /// A generic error\n    Generic(String),\n    /// Database error\n    Database(String),\n    /// Queue error\n    Queue(String),\n    /// Database error\n    Validation(String),\n    /// Any kind of HttpError\n    Http(HttpError),\n    /// Cache error\n    Cache(String),\n    /// Timeout error\n    Timeout(String),\n    /// Database timeout error\n    DbTimeout(String),\n    /// Connection timeout error\n    ConnectionTimeout(DbErr),\n    /// Conflict error\n    Conflict(DbErr),\n}\n\nimpl fmt::Display for ErrorType {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Generic(s) => s.fmt(f),\n            Self::Database(s) => s.fmt(f),\n            Self::Queue(s) => s.fmt(f),\n            Self::Validation(s) => s.fmt(f),\n            Self::Http(s) => s.fmt(f),\n            Self::Cache(s) => s.fmt(f),\n            Self::Timeout(s) => s.fmt(f),\n            Self::DbTimeout(s) => s.fmt(f),\n            Self::ConnectionTimeout(s) => s.fmt(f),\n            Self::Conflict(s) => s.fmt(f),\n        }\n    }\n}\n\nimpl From<HttpError> for ErrorType {\n    fn from(e: HttpError) -> Self {\n        Self::Http(e)\n    }\n}\n\n// Python generation relies on the title of this being `HttpError`\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HttpErrorOut\", title = \"HttpError\")]\npub struct StandardHttpError {\n    code: String,\n    detail: String,\n}\n\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HTTPValidationError\")]\npub struct ValidationHttpError {\n    detail: Vec<ValidationErrorItem>,\n}\n\n#[derive(Debug, Clone, Serialize)]\n#[serde(untagged)]\npub enum HttpErrorBody {\n    Standard(StandardHttpError),\n    Validation(ValidationHttpError),\n}\n\n#[derive(Debug, Clone, Serialize, PartialEq, Eq, JsonSchema)]\n/// Validation errors have their own schema to provide context for invalid requests eg. mismatched\n/// types and out of bounds values. There may be any number of these per 422 UNPROCESSABLE ENTITY\n/// error.\npub struct ValidationErrorItem {\n    /// The location as a [`Vec`] of [`String`]s -- often in the form `[\"body\", \"field_name\"]`,\n    /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, detail: String) -> Self {\n        Self {\n            status,\n            body: HttpErrorBody::Standard(StandardHttpError { code, detail }),\n        }\n    }\n\n    pub fn bad_request(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::BAD_REQUEST,\n            code.unwrap_or_else(|| \"generic_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Generic error\".to_owned()),\n        )\n    }\n\n    pub fn not_found(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            Statu<|fim_middle|>", "completion": "fn from(value: ExtensionRejection) -> Self {\n        Error::generic(value)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "function_item", "line_range": [184, 186]}
{"prompt": "<|fim_prefix|>//! Requires a rabbitmq node to be running on localhost:5672 (the default port) and using the\n//! default guest/guest credentials.\n//! Try using the `testing-docker-compose.yml` in the repo root to get this going.\n\nuse std::time::Duration;\n\nuse lapin::{\n    options::QueueDeclareOptions, types::FieldTable, Channel, Connection, ConnectionProperties,\n    Queue,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_queue::config::{QueueForwarder, QueueOutputOpts, RabbitMqOutputOpts};\nuse svix_bridge_types::{ForwardRequest, ReceiverOutput};\nuse tokio::{\n    io::copy_bidirectional,\n    net::{TcpListener, TcpStream, ToSocketAddrs},\n    task::JoinHandle,\n};\n\nasync fn declare_queue(name: &str, channel: &Channel) -> Queue {\n    channel\n        .queue_declare(\n            name,\n            QueueDeclareOptions {\n                auto_delete: true,\n                ..Default::default()\n            },\n            FieldTable::default(),\n        )\n        .await\n        .unwrap()\n}\n\nasync fn mq_connection(uri: &str) -> Connection {\n    let options = ConnectionProperties::default()\n        .with_connection_name(\"test\".into())\n        .with_executor(tokio_executor_trait::Tokio::current())\n        .with_reactor(tokio_reactor_trait::Tokio);\n    Connection::connect(uri, options).await.unwrap()\n}\n\nconst WAIT_MS: u64 = 200;\n\n/// These tests assume a \"vanilla\" rabbitmq instance, using the default port, creds, exchange...\nconst MQ_URI: &str = \"amqp://guest:guest@localhost:5672/%2f\";\n\n/// TCP proxy. Useful for giving us control over the connection to rabbit inside our tests.\nasync fn proxy(\n    listener: TcpListener,\n    server_addr: impl ToSocketAddrs + Clone + Sync + Send + 'static,\n) -> Result<JoinHandle<()>, ()> {\n    let handle = tokio::task::spawn(async move {\n        <|fim_suffix|>\n    });\n    Ok(handle)\n}\n\n#[tokio::test]\nasync fn test_connection_recovery() {\n    let mq_conn = mq_connection(MQ_URI).await;\n    let channel = mq_conn.create_channel().await.unwrap();\n    // setup the queue before running the consumer or the consumer will error out\n    let queue = declare_queue(\"\", &channel).await;\n    let queue_name = queue.name().as_str();\n\n    let proxy_listener = TcpListener::bind((\"127.0.0.1\", 0)).await.unwrap();\n    let port = proxy_listener.local_addr().unwrap().port();\n    // Start the proxy\n    let proxy_handle = proxy(proxy_listener, \"127.0.0.0:5672\").await.unwrap();\n\n    // Configure the receiver output to connect to the proxy so we can interrupt the connection as needed.\n    let proxied_mq_uri = format!(\"amqp://guest:guest@localhost:{port}/%2f\");\n\n    let opts = QueueOutputOpts::RabbitMQ(RabbitMqOutputOpts {\n        uri: proxied_mq_uri.clone(),\n        exchange: \"\".to_string(),\n        routing_key: queue_name.to_string(),\n        publish_options: Default::default(),\n        publish_properties: Default::default(),\n    });\n\n    let output = QueueForwarder::from_receiver_output_opts(String::from(\"test\"), opts)\n        .await\n        .unwrap();\n\n    let req = ForwardRequest {\n        payload: json!({\"test\": true}),\n    };\n\n    assert!(\n        output.handle(req.clone()).await.is_ok(),\n        \"expected ok when rabbit available\"\n    );\n\n    // Disconnect the proxy\n    proxy_handle.abort();\n    // Sleep a beat to give time for the proxy tear down.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    assert!(\n        output.handle(req.clone()).await.is_err(),\n        \"expected err when rabbit unavailable\"\n    );\n\n    // Reconnect the proxy on the same port\n    let proxy_listener = TcpListener::bind((\"127.0.0.1\", port)).await.unwrap();\n    let proxy_handle = proxy(proxy_listener, \"127.0.0.0:5672\").await.unwrap();\n\n    assert!(\n        output.handle(req.clone()).await.is_ok(),\n        \"expected ok when rabbit available\"\n    );\n\n    proxy_handle.abort();\n}\n<|fim_middle|>", "completion": "while let Ok((mut inbound, _)) = listener.accept().await {\n            let mut outbound = TcpStream::connect(server_addr.clone()).await.unwrap();\n            if let Err(e) = copy_bidirectional(&mut inbound, &mut outbound).await {\n                eprintln!(\"Failed to transfer; error={e}\");\n            }\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/rabbitmq_receiver.rs", "node_type": "while_expression", "line_range": [53, 58]}
{"prompt": "<|fim_prefix|>//! Module that test the dashboard-access endpoint and associated JWT tokens. This module will test\n//! that the tokens returned by the endpoint have restricted functionality and that the response\n//! from the endpoint is valid in the process.\n\nuse rand::distributions::DistString;\nuse reqwest::StatusCode;\nuse serde::de::IgnoredAny;\nuse serde_json::{json, Value};\n<|fim_suffix|>\n\nuse crate::utils::{\n    common_calls::{app_portal_access, application_in},\n    get_default_test_config, start_svix_server,\n};\n\n#[tokio::test]\n/// Users with application-level tokens should only be allowed to read the information related to\n/// their one application. All other endpoints should error.\nasync fn test_restricted_application_access() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n    let app_id_2: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME_2\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let client = app_portal_access(&client, &app_id, Default::default()).await;\n\n    // CREATE, UPDATE, DELETE, and LIST ops\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::FORBIDDEN,\n        )\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/\"),\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::FORBIDDEN,\n        )\n        .await\n        .unwrap();\n    client\n        .delete(&format!(\"api/v1/app/{app_id}/\"), StatusCode::FORBIDDEN)\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .get(\"api/v1/app/\", StatusCode::FORBIDDEN)\n        .await\n        .unwrap();\n\n    // READ should succeed when accessing the app_id the token is authorized for but no others\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{app_id_2}/\"), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n    let _: ApplicationOut = client\n        .get(&format!(\"api/v1/app/{app_id}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_dashboard_access_without_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    // We just need to ensure we get an OK response without a body.\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/dashboard-access/{app_id}/\"),\n            (),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_invalid_auth_error_detail() {\n    let (mut client, _jh) = start_svix_server().await;\n    let cfg = get_default_test_config();\n    let jwt_secret = match cfg.jwt_signing_config.as_ref() {\n        svix_server::core::security::JwtSigningConfig::Default { jwt_secret } => {\n            std::str::from_utf8(&jwt_secret.to_bytes())\n                .unwrap()\n                .to_owned()\n        }\n\n        _ => return,\n    };\n\n    client.set_auth_header(\"some-nonsense-key\".to_string());\n    match client\n        .post::<_, Value>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::UNAUTHORIZED,\n        )\n        .await\n    {\n        Ok(Value::Object(i)) => {\n            assert_eq!(i.get(\"detail\").unwrap(), INVALID_TOKEN_ERR);\n        }\n        _ => {\n            panic!(\"Unexpected response\");\n        }\n    }\n    client.set_auth_header(jwt_secret);\n    match client\n        .post::<_, Value>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::UNAUTHORIZED,\n        )\n        .await\n    {\n        Ok(Value::Object(i)) => {\n            assert_eq!(i.get(\"detail\").unwrap(), JWT_SECRET_ERR);\n        }\n        _ => {\n            panic!(\"Unexpected response\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_app_portal_access_with_application() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_uid = format!(\n        \"app-created-in-portal-{}\",\n        rand::distributions::Alphanumeric.sample_string(&mut rand::thread_rng(), 15)\n    );\n\n    // app-portal-access without the application field fails\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": []\n            }),\n            StatusCode::NOT_FOUND,\n        )\n        .await\n        .unwrap();\n\n    // app-portal-access with application\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App Created With Portal Access\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // app was created\n    let app: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(app[\"uid\"], app_uid);\n    assert_eq!(app[\"name\"], \"Test App Created With Portal Access\");\n\n    // Access portal again with application field - should be ignored since app exists\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Updated name will be ignored\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Verify the app name didn't change\n    let app_after: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(app_after[\"name\"], \"Test App Created With Portal Access\");\n\n    // UID in path must match UID in body\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/auth/app-portal-access/different-uid/\",\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App\",\n                    \"uid\": app_uid,  // This doesn't match the path\n                }\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    // UID must be set in body when creating\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/auth/app-portal-access/new-app-uid/\",\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App Without UID\",\n                    // Missing uid field\n                }\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n}\n<|fim_middle|>", "completion": "use svix_server::{\n    core::{\n        security::{INVALID_TOKEN_ERR, JWT_SECRET_ERR},\n        types::ApplicationId,\n    },\n    v1::endpoints::application::ApplicationOut,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_auth.rs", "node_type": "use_declaration", "line_range": [9, 15]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for PATCH requests focused mostly around non-required field types.\n\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\nuse validator::Validate;\n\n/// This is an enum that will wrap every nullable field for a PATCH request.\n///\n/// Nonnullable fields can be represented via an [`UnrequiredField`]. This\n/// differs from an [`Option`] in that it distinguishes null values and absent\n/// values such that an optional value in a model may be made `None` via\n/// PATCHing while allowing omitted fields to be skipped when updating.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredNullableField<T> {\n    #[default]\n    Absent,\n    None,\n    Some(T),\n}\n\n/// This enum is a non-nullable equivalent to [`UnrequiredNullableField`].\n///\n/// This is effectively an [`Option`] with the additional context that any field\n/// which uses this type is a member of a PATCH request model and that the field\n/// may be absent, meaning it is not to be updated. In comparison, [`Option`]s\n/// are used in other [`ModelIn`]s to define a field, that when absent, is\n/// `null`.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredField<T> {\n    #[default]\n    Absent,\n    Some(T),\n}\n\nimpl<T> UnrequiredNullableField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredNullableField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredNullableField<U> {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> UnrequiredField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredField<U> {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> From<Option<T>> for UnrequiredNullableField<T> {\n    fn from(opt: Option<T>) -> Self {\n        match opt {\n            Some(v) => UnrequiredNullableField::Some(v),\n            None => UnrequiredNullableField::None,\n        }\n    }\n}\n\ni<|fim_suffix|>\nimpl<T: Validate> Validate for UnrequiredField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredField::Absent => Ok(()),\n            UnrequiredField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredNullableField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone + Copy> Copy for UnrequiredNullableField<T> {}\nimpl<T: Clone + Copy> Copy for UnrequiredField<T> {}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredNullableField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        Option::deserialize(deserializer).map(Into::into)\n    }\n}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        T::deserialize(deserializer).map(UnrequiredField::Some)\n    }\n}\n\nimpl<T> Serialize for UnrequiredNullableField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredNullableField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredNullableField must skip serializing if field is absent\",\n            )),\n            UnrequiredNullableField::None => serializer.serialize_none(),\n            UnrequiredNullableField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\nimpl<T> Serialize for UnrequiredField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredField must skip serializing if field is absent\",\n            )),\n            UnrequiredField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"Unrequired_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<T>()\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredNullableField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"UnrequiredNullable_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<Option<T>>()\n    }\n}\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`Option`] is `Some`, but\n/// performs no operation in the case it is `None`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The nullable equivalent which is used for [`UnrequiredNullableField`] is [`patch_field_nullable`].\nmacro_rules! patch_field_non_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n}\npub(crate) use patch_field_non_nullable;\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`UnrequiredNullableField`] is\n/// `Some` and unsetting the field when it is `None`, but performs no operation in the case it is\n///  `Absent`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The non-nullable equivalent which is used for [`Option`] is [`patch_field_non_nullable`].\nmacro_rules! patch_field_nullable {\n    ($mode<|fim_middle|>", "completion": "impl<T: Validate> Validate for UnrequiredNullableField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n            UnrequiredNullableField::Some(v) => v.validate(),\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/patch.rs", "node_type": "impl_item", "line_range": [80, 87]}
{"prompt": "<|fim_prefix|>t_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret_1, secret_2, secret_3] {\n        match sec {\n            EndpointSecret::Symmetric(key) => {\n                let sec = STANDARD.encode(key);\n                let wh = Webhook::new(&sec).unwrap();\n                wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n            }\n            EndpointSecret::Asymmetric(key) => {\n                let msg_id = last_headers.get(\"svix-id\").unwrap().to_str().unwrap();\n                let timestamp = last_headers\n                    .get(\"svix-timestamp\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let signatures = last_headers\n                    .get(\"svix-signature\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let to_sign = format!(\"{msg_id}.{timestamp}.{}\", &last_body);\n                let found =\n                    signatures\n                        .split(' ')\n                        .filter(|x| x.starts_with(\"v1a,\"))\n                        .any(|signature| {\n                            let sig: Signature = Signature::from_slice(\n                                STANDARD\n                                    .decode(&signature[\"v1a,\".len()..])\n                                    .unwrap()\n                                    .as_slice(),\n                            )\n                            .unwrap();\n                            key.0.pk.verify(to_sign.as_bytes(), &sig).is_ok()\n                        });\n                assert!(found);\n            }\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_config() {\n    let mut cfg = get_default_test_config();\n    cfg.default_signature_type = DefaultSignatureType::Ed25519;\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let key1 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key1.starts_with(\"whpk_\"));\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let key2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key2.starts_with(\"whpk_\"));\n}\n\n#[tokio::test]\nasync fn test_custom_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Long key\n    let secret_2 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n    // Asymmetric key\n    let secret_3 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    assert_eq!(\n        secret_3.serialize_public_key(),\n        \"whpk_1SiA4o9hyqTCpIqC5V9HUakiiaeACeqfZTInDBbOir4=\"\n    );\n\n    let mut ep_in = EndpointIn {\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    ep_in.key = Some(secret_2.clone());\n    l<|fim_suffix|>\n    // We rotate the key after because it's easier than setting json! for everything\n    ep_in.key = Some(secret_2.clone());\n    let endp_3 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp_3.id),\n            json!({\n                \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\",\n            }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    for (secret, ep) in [(secret_1, endp_1), (secret_2, endp_2), (secret_3, endp_3)] {\n        assert_eq!(\n            secret.serialize_public_key(),\n            client\n                .get::<EndpointSecretOutTest>(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                    StatusCode::OK\n                )\n                .await\n                .unwrap()\n                .key\n        );\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_encryption() {\n    let org_id = OrganizationId::new(None, None);\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let cfg = get_default_test_config();\n    let (client, jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n    jh.abort();\n\n    // Now add encryption and check the secret is still fine\n    let mut cfg = get_default_test_config();\n    cfg.encryption = Encryption::new([1; 32]);\n    let (client, jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading the existing secret works\n    assert_eq!(secret, secret2);\n\n    // Generate a new encrypted secret\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": secret }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading and saving works for encrypted\n    assert_eq!(secret, secret2);\n    jh.abort();\n\n    // Make sure we can't read it with the secret unset\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n    client\n        .get::<IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::INTERNAL_SERVER_ERROR,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_invalid_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_too_short = \"whsec_C2FVsBQIhrscChlQIM+b5sSYspob\".to_owned();\n    let secret_too_long =\n        \"whsec_V09IYXZUaFJoSnFobnpJQkpPMXdpdGFNWnJsRzAxdXZCeTVndVpwRmxSSXFsc0oyYzBTRWRUekJhYnlaZ0JSRGNPQ3BGZG1xYjFVVmRGQ3UK\"\n            .to_owned();\n    let invalid_prefix = \"hwsec_C2FVsBQIhrscChlQIM+b5sSYspob7oDazfgh\".to_owned();\n\n    for sec in [secret_too_short, secret_too_long, invalid_prefix] {\n        let _: IgnoredAny = client\n            .post(\n     <|fim_middle|>", "completion": "let endp_2 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1284, 1286]}
{"prompt": "<|fim_prefix|>   patch_field_non_nullable!(model, version, map);\n        patch_field_non_nullable!(model, disabled);\n        patch_field_nullable!(model, event_types_ids);\n        patch_field_nullable!(model, channels);\n    }\n}\n\nfn validate_rate_limit_patch(\n    rate_limit: &UnrequiredNullableField<u16>,\n) -> Result<(), ValidationError> {\n    match rate_limit {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(rate_limit) => {\n            if *rate_limit > 0 {\n                Ok(())\n            } else {\n                Err(validation_error(\n                    Some(\"range\"),\n                    Some(\"Endpoint rate limits must be at least 1 if set\"),\n                ))\n            }\n        }\n    }\n}\n\nfn validate_minimum_version_patch(version: &UnrequiredField<u16>) -> Result<(), ValidationError> {\n    match version {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(version) => {\n            if *version == 0 {\n                Err(validation_error(\n                    Some(\"range\"),\n                    Some(\"Endpoint versions must be at least one\"),\n                ))\n            } else {\n                Ok(())\n            }\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointOutCommon {\n    /// An example endpoint name\n    pub description: String,\n    pub rate_limit: Option<u16>,\n    /// Optional unique identifier for the endpoint\n    pub uid: Option<EndpointUid>,\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: String,\n    #[deprecated]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: u16,\n    #[schemars(\n        example = \"endpoint_disabled_default\",\n        default = \"endpoint_disabled_default\"\n    )]\n    pub disabled: bool,\n    #[serde(rename = \"filterTypes\")]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n    /// List of message channels this endpoint listens to (omit for all)\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n    pub created_at: DateTime<Utc>,\n    pub updated_at: DateTime<Utc>,\n}\n\nimpl From<endpoint::Model> for EndpointOutCommon {\n    #[allow(deprecated)]\n    fn from(model: endpoint::Model) -> Self {\n        Self {\n            description: model.description,\n            rate_limit: model.rate_limit.map(|x| x as u16),\n            uid: model.uid,\n            url: model.url,\n            version: model.version as u16,\n            disabled: model.disabled,\n            event_types_ids: model.event_types_ids,\n            channels: model.channels,\n            created_at: model.created_at.into(),\n            updated_at: model.updated_at.into(),\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointOut {\n    #[serde(flatten)]\n    pub ep: EndpointOutCommon,\n    pub id: EndpointId,\n    pub metadata: Metadata,\n}\n\n// FIXME: This can and should be a derive macro\nimpl From<(endpoint::Model, Metadata)> for EndpointOut {\n    fn from((endp, metadata): (endpoint::Model, Metadata)) -> Self {\n        Self {\n            id: endp.id.clone(),\n            ep: endp.into(),\n            metadata,\n        }\n    }\n}\n\n#[derive(Default, Clone, Debug, PartialEq, Eq, Validate, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointSecretRotateIn {\n    #[validate]\n    #[serde(default)]\n    key: Option<EndpointSecret>,\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointSecretOut {\n    pub key: EndpointSecret,\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Validate, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct RecoverIn {\n    pub since: DateTime<Utc>,\n    pub until: Option<DateTime<Utc>>,\n}\n\nf<|fim_suffix|>\n#[derive(Clone, Debug, PartialEq, Eq, Validate, Deserialize, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointHeadersIn {\n    #[schemars(example = \"endpoint_headers_example\")]\n    pub headers: EndpointHeaders,\n}\n\nimpl ModelIn for EndpointHeadersIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointHeadersIn { headers } = self;\n        model.headers = Set(Some(headers));\n    }\n}\n\nfn sensitive_headers_example() -> HashSet<String> {\n    HashSet::from([\"Authorization\".to_string()])\n}\n\n/// The value of the headers is returned in the `headers` field.\n///\n/// Sensitive headers that have been redacted are returned in the sensitive field.\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Serialize, Default, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointHeadersOut {\n    #[schemars(example = \"endpoint_headers_example\")]\n    pub headers: HashMap<String, String>,\n    #[schemars(example = \"sensitive_headers_example\")]\n    pub sensitive: HashSet<String>,\n}\n\nimpl EndpointHeadersOut {\n    const SENSITIVE_HEADERS: &'static [&'static str] = &[\n        \"x-auth-token\",\n        \"x-api-key\",\n        \"www-authenticate\",\n        \"authorization\",\n        \"proxy-authenticate\",\n        \"proxy-authorization\",\n    ];\n}\n\nimpl From<EndpointHeaders> for EndpointHeadersOut {\n    fn from(hdr: EndpointHeaders) -> Self {\n        let (sens, remaining) = hdr.0.into_iter().partition(|(k, _)| {\n            let k = k.to_lowercase();\n            Self::SENSITIVE_HEADERS.iter().any(|&x| x == k)\n        });\n\n        Self {\n            headers: remaining,\n            sensitive: sens.into_keys().collect(),\n        }\n    }\n}\n\nfn endpoint_headers_patch_example() -> EndpointHeadersPatch {\n    EndpointHeadersPatch(HashMap::from([\n        (\"X-Example\".to_string(), Some(\"123\".to_string())),\n        (\"X-Foobar\".to_string(), Some(\"Bar\".to_string())),\n    ]))\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Validate, Deserialize, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointHeadersPatchIn {\n    #[validate]\n    #[schemars(example = \"endpoint_headers_patch_example\")]\n    pub headers: EndpointHeadersPatch,\n}\n\nimpl ModelIn for EndpointHeadersPatchIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointHeadersPatchIn { headers } = self;\n\n        model.headers = if let Some(Some(mut hdrs)) = model.headers.take() {\n            for (k, v) in headers.0 {\n                if let Some(v) = v {\n                    hdrs.0.insert(k, v);\n                } else {\n                    hdrs.0.remove(&k);\n                }\n            }\n            Set(Some(hdrs))\n        } else {\n            let headers: HashMap<String, String> = headers\n                .0\n                .into_iter()\n                .filter_map(|(k, v)| v.map(|v| (k, v)))\n                .collect();\n            Set(Some(EndpointHeaders(headers)))\n        };\n    }\n}\n\n#[derive(Deserialize, JsonSchema)]\nstruct EndpointStatsRange {\n    since: Option<DateTime<Utc>>,\n    until: Option<DateTime<Utc>>,\n}\n\nimpl EndpointStatsRange {\n    fn validate_unwrap_or_default(self) -> error::Result<(DateTime<Utc>, DateTime<Utc>)> {\n        let until = self.until.unwrap_or_else(Utc::now);\n\n        if until > Utc::now() {\n            return Err(HttpError::bad_request(\n                Some(\"invalid_range\".into()),\n                Some(\"'until' cannot be in the future\".into()),\n            )\n            .into());\n        }\n\n        let since = self.since.unwrap_or(until - Duration::days(28));\n\n        // Add five minutes so that people can easily just do `now() - 28 days`\n        // without having to worry about clock sync\n        if until - since > (Duration::days(28) + Duration::minutes(5)) {\n            return Err(HttpError::bad_request(\n                Some(\"invalid_range\".into()),\n                Some(format!(\n                    \"'since' cannot be more than 28 days prior to {until}\"\n                )),\n            )\n     <|fim_middle|>", "completion": "fn endpoint_headers_example() -> HashMap<&'static str, &'static str> {\n    HashMap::from([(\"X-Example\", \"123\"), (\"X-Foobar\", \"Bar\")])\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "function_item", "line_range": [544, 546]}
{"prompt": "<|fim_prefix|>ion: None,\n    };\n\n    let create_message = create_message_inner(\n        db,\n        queue_tx,\n        cache,\n        false,\n        Some(endpoint.id),\n        msg_in,\n        app.org_id,\n        ApplicationIdOrUid(app.id.0),\n    )\n    .await?;\n\n    Ok(Json(create_message))\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Endpoint\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app/:app_id/endpoint\",\n            post_with(crud::create_endpoint, crud::create_endpoint_operation)\n                .get_with(crud::list_endpoints, crud::list_endpoints_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id\",\n            get_with(crud::get_endpoint, crud::get_endpoint_operation)\n                .put_with(crud::update_endpoint, crud::update_endpoint_operation)\n                .patch_with(crud::patch_endpoint, crud::patch_endpoint_operation)\n                .delete_with(crud::delete_endpoint, crud::delete_endpoint_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret\",\n            get_with(\n                secrets::get_endpoint_secret,\n                secrets::get_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret/rotate\",\n            post_with(\n                secrets::rotate_endpoint_secret,\n                secrets::rotate_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/stats\",\n            get_with(endpoint_stats, endpoint_stats_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/send-example\",\n            post_with(send_example, send_example_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/recover\",\n            post_with(\n                recovery::recover_failed_webhooks,\n                recovery::recover_failed_webhooks_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/headers\",\n            get_with(\n                headers::get_endpoint_headers,\n                headers::get_endpoint_headers_operation,\n            )\n            .patch_with(\n                headers::patch_endpoint_headers,\n                headers::patch_endpoint_headers_operation,\n            )\n            .put_with(\n                headers::update_endpoint_headers,\n                headers::update_endpoint_headers_operation,\n            ),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use std::collections::{HashMap, HashSet};\n\n    use reqwest::Url;\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{validate_url, EndpointHeadersOut, EndpointHeadersPatchIn, EndpointIn};\n    use crate::core::types::EndpointHeaders;\n\n    const URL_VALID: &str = \"https://www.example.com\";\n    const URL_INVALID: &str = \"invalid url\";\n    const VERSION_VALID: u16 = 1;\n    const VERSION_INVALID: u16 = 0;\n    const RATE_LIMIT_VALID: u16 = 1;\n    const RATE_LIMIT_INVALID: u16 = 0;\n    const EVENT_TYPES_INVALID: &[&str] = &[\"valid-event-type\", \"&&invalid-event-type\"];\n    const EVENT_TYPES_VALID: &[&str] = &[\"valid-event-type1\", \"valid-event-type2\"];\n    const EVENT_CHANNELS_INVALID: &[&str] = &[\"valid-event-channel\", \"&&invalid-event-channel\"];\n    const EVENT_CHANNELS_VALID: &[&str] = &[\"valid-event-channel1\", \"valid-event-channel2\"];\n    const ENDPOINT_ID_INVALID: &str = \"$$invalid-endpoint\";\n    const ENDPOINT_ID_VALID: &str = \"valid-endpoint\";\n\n    #[allow(deprecated)]\n    #[test]\n    fn test_endpoint_in_validation() {\n        let invalid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_INVALID,\n             \"url\": URL_VALID\n        }))\n        .unwrap();\n\n        let invalid_2: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"channels\": EVENT_CHANNELS_INVALID\n        }))\n        .unwrap();\n\n        let invalid_3: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_INVALID\n        }))\n        .unwrap();\n\n        let invalid_4: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"uid\": ENDPOINT_ID_INVALID\n        }))\n        .unwrap();\n\n        let invalid_5: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"filterTypes\": EVENT_TYPES_INVALID\n        }))\n        .unwrap();\n\n        let invalid_6: Result<EndpointIn, _> = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_INVALID\n        }));\n        assert!(invalid_6.is_err());\n\n        f<|fim_suffix|>\n        let valid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_1.validate().unwrap();\n\n        let valid_2: EndpointIn = serde_json::from_value(json!({\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_2.validate().unwrap();\n        assert_eq!(1, valid_2.version.unwrap());\n    }\n\n    #[test]\n    fn test_endpoint_headers_sensitive() {\n        let headers = EndpointHeaders(HashMap::from([\n            (\"foo\".to_string(), \"1\".to_string()),\n            (\"authorization\".to_string(), \"test\".to_string()),\n            (\"X-Auth-Token\".to_string(), \"test2\".to_string()),\n        ]));\n\n        let headers_out: EndpointHeadersOut = headers.into();\n\n        assert_eq!(\n            headers_out.headers,\n            HashMap::from([(\"foo\".to_string(), \"1\".to_string())])\n        );\n        assert_eq!(\n            headers_out.sensitive,\n            HashSet::from([\"authorization\".to_string(), \"X-Auth-Token\".to_string()])\n        );\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_in_validation() {\n        let headers_valid = HashMap::from([(\"x-valid\", \"1\")]);\n        let headers_invalid = HashMap::from([(\"x-invalid???\", \"1\")]);\n\n        let invalid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_invalid })).unwrap();\n        assert!(invalid.validate().is_err());\n\n        let valid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_valid })).unwrap();\n        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_url_validation() {\n        let valid_https = Url::parse(\"https://test.url\").unwrap();\n        let valid_http = Url::parse(\"http://test.url\").unwrap();\n        let invalid_scheme = Url::parse(\"anythingelse://test.url\").unwrap();\n        let invalid_format = \"http://[:::1]\";\n\n        assert!(validate_url(&valid_https).is_ok());\n        assert!(validate_url(&valid_http).is_ok());\n        assert!(validate_url(&invalid_scheme).is_err());\n\n        let valid_https: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_https, \"version\": 1})).unwrap();\n        let valid_http: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_http, \"version\": 1})).unwrap();\n        let invalid_scheme: EndpointIn =\n            serde_json::from_value(json!({\"url\": invalid_scheme, \"version\": 1})).unwrap();\n        let invalid_format: Result<EndpointIn, _> =\n            serde_json::from_value(json!({\"url\": invalid_format, \"version\": 1}));\n\n        assert!(valid_https.validate().is_ok());\n        assert!(valid_http.validate().is_ok());\n        assert!(invalid_scheme.validate().is_err());\n        assert!(invalid_format.is_err());\n    }\n}\n<|fim_middle|>", "completion": "for e in [invalid_1, invalid_2, invalid_3, invalid_4, invalid_5] {\n            assert!(e.validate().is_err());\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "for_expression", "line_range": [952, 954]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct EventTypeUpdate {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub archived: Option<bool>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub deprecated: Option<bool>,\n\n    pub description: String,\n\n    #[deprecated]\n    #[serde(rename = \"featureFlag\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub feature_flag: Option<String>,\n\n    #[serde(rename = \"featureFlags\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub feature_flags: Option<Vec<String>>,\n\n    /// The event type group's name\n    #[serde(rename = \"groupName\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub group_name: Option<String>,\n\n    /// The schema for the event type for a specific version as a JSON schema.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub schemas: Option<serde_json::Value>,\n}\n\nimpl EventTypeUpdate {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(description: String) -> Self {\n        #[allow(deprecated)]\n        Self {\n            archived: None,\n            deprecated: None,\n            description,\n            feature_flag: None,\n            feature_flags: None,\n            group_name: None,\n            schemas: None,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/event_type_update.rs", "node_type": "function_item", "line_range": [34, 45]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::endpoint_disabled_event_data::EndpointDisabledEventData;\n\n/// Sent when an endpoint has been automatically disabled after continuous\n/// failures, or manually via an API call.\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct EndpointDisabledEvent {\n    pub data: EndpointDisabledEventData,\n\n    pub r#type: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl EndpointDisabledEvent {\n    pub fn new(data: EndpointDisabledEventData, r#type: String) -> Self {\n        Self { data, r#type }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/endpoint_disabled_event.rs", "node_type": "impl_item", "line_range": [15, 19]}
{"prompt": "<|fim_prefix|>//! Use the `testing-docker-compose.yml` in the repo root to run the dependencies for testing,\n//! including Redis.\n\nuse std::time::Duration;\n\nuse redis::{AsyncCommands, Client};\nuse serde_json::json;\nuse svix_bridge_plugin_queue::{\n    config::{QueueInputOpts, RedisInputOpts},\n    sender_input::QueueSender,\n};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nfn get_test_plugin(\n    svix_url: String,\n    queue_key: String,\n    use_transformation: Option<TransformerInputFormat>,\n) -> QueueSender {\n    QueueSender::new(\n        \"test\".into(),\n        QueueInputOpts::Redis(RedisInputOpts {\n            dsn: \"redis://localhost/\".to_owned(),\n            max_connections: 8,\n            reinsert_on_nack: false,\n            queue_key,\n            delayed_queue_key: None,\n            consumer_group: \"test_cg\".to_owned(),\n            consumer_name: \"test_cn\".to_owned(),\n            ack_deadline_ms: 2_000,\n        }),\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n}\n\nasync fn redis_connection() -> Client {\n    Client::open(\"redis://localhost/\").unwrap()\n}\n\nasync fn create_test_stream(client: &Client) -> String {\n    let name: String = std::iter::repeat_with(fastrand::alphanumeric)\n        .take(8)\n        .collect();\n\n    <|fim_suffix|>\n\n    let _: () = conn\n        .xgroup_create_mkstream(&name, \"test_cg\", 0i8)\n        .await\n        .unwrap();\n\n    name\n}\n\nasync fn delete_test_stream(client: &Client, key: &str) {\n    let mut conn = client.get_multiplexed_async_connection().await.unwrap();\n    let _: () = conn.del(key).await.unwrap();\n}\n\nasync fn publish(client: &Client, key: &str, payload: &str) {\n    let mut conn = client.get_multiplexed_async_connection().await.unwrap();\n    // N.b. the redis code relies on the messages being json with a `payload` key in there.\n    // The `payload` key can be any valid JSON value.\n    let _: () = conn.xadd(key, \"*\", &[(\"payload\", payload)]).await.unwrap();\n}\n\n/// General \"pause while we wait for messages to travel\" beat. If you're seeing flakes, bump this up.\nconst WAIT_MS: u64 = 250;\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let client = redis_connection().await;\n    let key = create_test_stream(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), key.clone(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&client, &key, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    delete_test_stream(&client, &key).await;\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = redis_connection().await;\n    let key = create_test_stream(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        key.clone(),\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&client, &key, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    delete_test_stream(&client, &key).await;\n}\n\n#[tokio::test]\nasync fn test_consume_transformed_string_ok() {\n    let client = redis_connection().await;\n    let key = create_test_stream(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(\n            json!({ \"payload\": { \"hello\": \"world\" } }),\n        ))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"hello\": \"world\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        key.clone(),\n        Some(TransformerInputFormat::String),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJo<|fim_middle|>", "completion": "let mut conn = client.get_multiplexed_async_connection().await.unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/redis_stream_consumer.rs", "node_type": "let_declaration", "line_range": [62, 62]}
{"prompt": "<|fim_prefix|>use std::{\n    str,\n    time::{Duration, Instant},\n};\n\nuse rdkafka::{\n    consumer::{CommitMode, Consumer as _},\n    error::KafkaError,\n    Message as _,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::api::{MessageCreateOptions, Svix},\n    CreateMessageRequest, JsObject, SenderInput, SenderOutputOpts, TransformationConfig,\n    TransformerInput, TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\nuse tokio::task::spawn_blocking;\n\nuse crate::{config::KafkaInputOpts, Error, Result};\n\npub struct KafkaConsumer {\n    name: String,\n    opts: KafkaInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n}\n\nimpl KafkaConsumer {\n    pub fn new(\n        name: String,\n        opts: KafkaInputOpts,\n        transformation: Option<TransformationConfig>,\n        output: SenderOutputOpts,\n    ) -> Result<Self> {\n        Ok(Self {\n            name,\n            transformation,\n            transformer_tx: None,\n            opts,\n            svix_client: match output {\n                SenderOutputOpts::Svix(output) => {\n                    Svix::new(output.token, output.options.map(Into::into))\n                }\n            },\n        })\n    }\n\n    #[tracing::instrument(skip_all)]\n    async fn process(&self, msg: &rdkafka::message::BorrowedMessage<'_>) -> Result<()> {\n        let payload = msg.payload().ok_or_else(|| Error::MissingPayload)?;\n        let payload = if let Some(transformation) = &self.transformation {\n            let input = match transformation.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload =\n                        serde_json::from_slice(payload).map_err(Error::Deserialization)?;\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    let raw_payload = str::from_utf8(payload).map_err(Error::NonUtf8Payload)?;\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n\n            let script = transformation.source().clone();\n            let object = self.transform(script, input).await?;\n            serde_json::from_value(serde_json::Value::Object(object))\n                .map_err(Error::Deserialization)?\n        } else {\n            serde_json::from_slice(payload).map_err(Error::Deserialization)?\n        };\n\n        <|fim_suffix|>\n\n        let KafkaInputOpts::Inner {\n            group_id, topic, ..\n        } = &self.opts;\n\n        let options = MessageCreateOptions {\n            with_content: None,\n            // If committing the message fails or the process crashes after posting the webhook but\n            // before committing, this makes sure that the next run of this fn with the same kafka\n            // message doesn't end up creating a duplicate webhook in svix.\n            idempotency_key: Some(format!(\n                \"svix_bridge_kafka_{group_id}_{topic}_{}\",\n                msg.offset()\n            )),\n        };\n\n        self.svix_client\n            .message()\n            .create(app_id, message, Some(options))\n            .await?;\n\n        Ok(())\n    }\n\n    async fn transform(&self, script: String, input: TransformerInput) -> Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx\n            .as_ref()\n            .ok_or_else(|| Error::transformation(\"transformations not configured\"))?\n            .send(job)\n            .map_err(|e| Error::transformation(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::transformation(\"transformation rx failed\"))\n            .and_then(|x| {\n                x.map_err(|_e| Error::transformation(\"transformation execution failed\"))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => Err(Error::transformation(\n                \"transformation produced unexpected value\",\n            )),\n        }\n    }\n\n    async fn run_inner(&self) -> Result<()> {\n        let opts = self.opts.clone();\n        // `ClientConfig::create` does blocking I/O.\n        // Same for subscribe, most likely.\n        let consumer = spawn_blocking(move || {\n            let KafkaInputOpts::Inner { topic, .. } = &opts;\n            let topic = topic.clone();\n\n            let consumer = opts.create_consumer()?;\n            tracing::debug!(\"Created StreamConsumer\");\n\n            consumer.subscribe(&[&topic])?;\n            tracing::debug!(topic, \"Subscribed\");\n\n            Ok::<_, KafkaError>(consumer)\n        })\n        .await\n        .expect(\"create_consumer task panicked\")?;\n\n        loop {\n            // It's fine to pull messages one-by-one without any buffering in our own code because\n            // rdkafka buffers messages internally through a background task / thread.\n            let msg = consumer.recv().await?;\n            tracing::debug!(\"Received a message\");\n\n            let mut process_error_count = 0;\n            while let Err(e) = self.process(&msg).await {\n                match e {\n                    // If the payload is invalid, log an error and continue.\n                    // It would fail the same way if retried.\n                    Error::MissingPayload\n                    | Error::Deserialization(_)\n                    | Error::NonUtf8Payload(_) => {\n                        tracing::error!(error = &e as &dyn std::error::Error, \"invalid payload\");\n                        break;\n                    }\n\n                    // If the error is (possibly) transient, retry a few times.\n                    // After that, bubble up the error so it's logged at error level.\n                    Error::Kafka(_) | Error::SvixClient(_) | Error::Transformation { .. } => {\n                        process_error_count += 1;\n                        if process_error_count >= 3 {\n                            return Err(e);\n                        }\n\n                        tracing::warn!(\n                            error = &e as &dyn std::error::Error,\n                            \"failed to process payload from kafka\"\n                        );\n\n                        // retry\n                    }\n                }\n            }\n\n            // FIXME(jplatte): Should we introduce logic to only commit every N messages to reduce\n            // back and forth on the Kafka connection / disk writes inside Kafka?\n            //\n            // Background: messages in Kafka are not committed individually, rather what this call\n            // does is update the stored stream position for the consumer group.\n            consumer.commit_message(&msg, CommitMode::Async)?;\n        }\n    }\n}\n\n#[async_trait]\nimpl SenderInput for KafkaConsumer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        let mut fails: u64 = 0;\n        let mut last_fail = Instant::now();\n\n        let KafkaInputOpts::Inner { topic, .. } = &self.opts;\n        tracing::info!(topic, \"Starting to listen for messages\");\n\n        loop {\n            if let Err(e) = self.run_inner().await {\n                tracing::error!(\"{e}\");\n            }\n\n            if last_fail.elapsed() > Duration::from_secs(10) {\n                // reset the fail count if we didn't have a hiccup in the past short while.\n                tracing::trace!(\"been a while since last fail, resetting count\");\n                fails = 0;\n            } else {\n                fails += 1;\n            }\n\n            last_fail = Instant::now();\n            tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n        }\n    }\n}\n<|fim_middle|>", "completion": "let CreateMessageRequest { app_id, message } = payload;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/input.rs", "node_type": "let_declaration", "line_range": [73, 73]}
{"prompt": "<|fim_prefix|>If here then we're processing old tasks from before\n                            // messagedest was removed, so we'll need to query messagedestination\n                            None if attempt.msg_dest_id.is_some() => {\n                                messagedestination::Entity::secure_find_by_msg(task.msg_id.clone())\n                                    .filter(\n                                        messagedestination::Column::EndpId\n                                            .eq(task.endpoint_id.clone()),\n                                    )\n                                    .one(db)\n                                    .await?\n                                    .ok_or_else(|| {\n                                        Error::generic(format!(\n                                            \"MessageDestination not found for message {}\",\n                                            &task.msg_id\n                                        ))\n                                    })?\n                                    .status\n                            }\n                            None => MessageStatus::Fail,\n                        }\n                    }\n                    None => MessageStatus::Sending,\n                };\n\n                (\n                    msg,\n                    msg_content,\n                    Some(task.endpoint_id),\n                    task.trigger_type,\n                    task.attempt_count,\n                    status,\n                )\n            }\n            QueueTask::MessageBatch(task) => {\n                let (msg, msg_content) = message::Entity::find_by_id(task.msg_id.clone())\n                    .find_also_related(messagecontent::Entity)\n                    .one(db)\n                    .await?\n                    .ok_or_else(|| {\n                        Error::generic(format_args!(\n                            \"Unexpected: message doesn't exist {}\",\n                            task.msg_id\n                        ))\n                    })?;\n                (\n                    msg,\n                    msg_content,\n                    task.force_endpoint,\n                    task.trigger_type,\n                    0,\n                    MessageStatus::Sending,\n                )\n            }\n        };\n\n    span.record(\"msg_id\", &msg.id.0);\n    span.record(\"app_id\", &msg.app_id.0);\n    span.record(\"org_id\", &msg.org_id.0);\n\n    let payload = msg_content\n        .and_then(|m| String::from_utf8(m.payload).ok())\n        .or_else(|| {\n            msg.legacy_payload\n                .take()\n                .and_then(|m| serde_json::to_string(&m).ok())\n        });\n\n    let Some(payload) = payload else {\n        tracing::warn!(\"Message payload is NULL; payload has most likely expired\");\n        return Ok(());\n    };\n\n    let Some(create_message_app) = CreateMessageApp::layered_fetch(\n        cache,\n        db,\n        None,\n        msg.org_id.clone(),\n        msg.app_id.clone(),\n        Duration::from_secs(30),\n    )\n    .await?\n    else {\n        tracing::info!(\"Application doesn't exist: {}\", &msg.app_id);\n        return Ok(());\n    };\n\n    let endpoints: Vec<CreateMessageEndpoint> = create_message_app\n        .filtered_endpoints(trigger_type, &msg.event_type, msg.channels.as_ref())\n        .iter()\n        .filter(|endpoint| match force_endpoint.as_ref() {\n            Some(endp_id) => endp_id == &endpoint.id,\n            None => true,\n        })\n        .cloned()\n        .collect();\n\n    let futures = endpoints.into_iter().map(|endpoint| {\n        let task = MessageTask {\n            msg_id: msg.id.clone(),\n            app_id: create_message_app.id.clone(),\n            endpoint_id: endpoint.id.clone(),\n            attempt_count,\n            trigger_type,\n        };\n\n        dispatch_message_task(\n            &worker_context,\n            &msg,\n            &create_message_app,\n            task,\n            &payload,\n            endpoint,\n            status,\n        )\n    });\n\n    let join = future::join_all(futures).await;\n\n    let errs: Vec<_> = join.iter().filter(|x| x.is_err()).collect();\n    i<|fim_suffix|>\n    Ok(())\n}\n\npub static LAST_QUEUE_POLL: LazyLock<AtomicU64> = LazyLock::new(|| get_unix_timestamp().into());\n\nasync fn update_last_poll_time() {\n    LAST_QUEUE_POLL.swap(get_unix_timestamp(), Ordering::Relaxed);\n}\n\n/// Listens on the message queue for new tasks\n#[allow(clippy::too_many_arguments)]\npub async fn queue_handler(\n    cfg: &Configuration,\n    cache: Cache,\n    db: DatabaseConnection,\n    queue_tx: TaskQueueProducer,\n    mut queue_rx: TaskQueueConsumer,\n    op_webhook_sender: OperationalWebhookSender,\n) -> Result<()> {\n    let recv_deadline = Duration::from_secs(cfg.queue_max_poll_secs.into());\n\n    static NUM_WORKERS: AtomicUsize = AtomicUsize::new(0);\n\n    let task_limit = cfg.worker_max_tasks;\n    if task_limit == 0 {\n        tracing::info!(\"Worker concurrent task limit: unlimited\");\n    } else {\n        tracing::info!(\"Worker concurrent task limit: {}\", task_limit);\n    }\n\n    let webhook_client = WebhookClient::new(\n        cfg.whitelist_subnets.clone(),\n        Some(Arc::new(vec![\"backend\".to_owned()])),\n        cfg.dangerous_disable_tls_verification,\n        cfg.proxy_config.as_ref(),\n    );\n\n    tokio::spawn(\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_millis(500));\n            loop {\n                interval.tick().await;\n                let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                if num_workers > 0 {\n                    tracing::info!(\"{} active workers\", num_workers);\n                }\n            }\n        }\n        .instrument(tracing::error_span!(\n            \"worker_monitor\",\n            instance_id = tracing::field::Empty\n        )),\n    );\n\n    loop {\n        if task_limit > 0 {\n            let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n            if num_workers > task_limit.into() {\n                tokio::time::sleep(Duration::from_millis(100)).await;\n                continue;\n            }\n        }\n\n        if crate::is_shutting_down() {\n            tokio::join!(async move {\n                let mut interval = tokio::time::interval(Duration::from_millis(500));\n                loop {\n                    interval.tick().await;\n                    let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                    if num_workers > 0 {\n                        tracing::info!(\n                            \"{} active workers, waiting to shut down worker.\",\n                            num_workers\n                        );\n                    } else {\n                        tracing::info!(\"No active workers, shutting down worker.\");\n                        break;\n                    }\n                }\n            });\n            break;\n        }\n\n        match queue_rx.receive_all(recv_deadline).await {\n            Ok(batch) => {\n                for delivery in batch {\n                    let cfg = cfg.clone();\n                    let cache = cache.clone();\n                    let db = db.clone();\n                    let queue_tx = queue_tx.clone();\n                    let queue_task = delivery.task.clone();\n                    let op_webhook_sender = op_webhook_sender.clone();\n                    let webhook_client = webhook_client.clone();\n\n                    tokio::spawn(async move {\n                        NUM_WORKERS.fetch_add(1, Ordering::Relaxed);\n                        let worker_context = WorkerContext {\n                            cfg: &cfg,\n                            db: &db,\n                            cache: &cache,\n                            op_webhook_sender: &op_webhook_sender,\n                            queue_tx: &queue_tx,\n                            webhook_client: &webhook_client,\n                        };\n\n                        let queue_task =\n                            Arc::try_unwrap(queue_task).unwrap_or_else(|arc| (*arc).clone());\n                        if process_queue_task(worker_context, queue_task)\n                            .await\n                            .is_err()\n                        {\n                            if let Err(err) = delivery.nack().await {\n      <|fim_middle|>", "completion": "if !errs.is_empty() {\n        return Err(Error::generic(format_args!(\n            \"Some dispatches failed unexpectedly: {errs:?}\",\n        )));\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "if_expression", "line_range": [914, 918]}
{"prompt": "<|fim_prefix|>mber\", \"minimum\": -90, \"maximum\": 90},\n                        \"longitude\": {\"type\": \"number\", \"minimum\": -180, \"maximum\": 180},\n                        },\n                    }\n                }),\n            )\n            .unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that PUT with invalid ID creates an event type\n    let _: EventTypeOut = client\n        .put(\n            \"api/v1/event-type/fake-id/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that description may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"description\": \"updated_description\",\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.schemas, et.schemas);\n\n    // Test that schemas may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": {},\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, Some(Schema::default()));\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that schemas may be unset while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": null,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, None);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that deleted may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"archived\": true,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert!(out.deleted);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.schemas, None);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n}\n\n#[tokio::test]\nasync fn test_event_type_create_read_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        client\n            .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n            .await\n            .unwrap(),\n        et\n    );\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/?with_content=true\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&et));\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&EventTypeOut {\n        schemas: None,\n        ..et\n    }));\n}\n\n#[tokio::test]\nasync fn test_event_type_feature_flags() {\n    let (client, _jh) = start_svix_server().await;\n\n    let feature = FeatureFlag(\"foo-feature\".into());\n    let another_feature = FeatureFlag(\"bar-feature\".into());\n    let (features, other_features, union) = {\n        let mut s1 = HashSet::new();\n        s1.insert(feature.clone());\n        let mut s2 = HashSet::new();\n        s2.insert(another_feature);\n        l<|fim_suffix|>\n        (s1, s2, union)\n    };\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"event-type-with-flag\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: Some(feature),\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"no-flag-event\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: None,\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let app: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let path = format!(\"api/v1/event-type/{}/\", et.name);\n\n    for (flag_set, should_see) in [\n        (FeatureFlagSet::default(), false),\n        (other_features, false),\n        (union.clone(), true),\n        (features.clone(), true),\n    ] {\n        let client = app_portal_access(&client, &app, flag_set).await;\n\n        let list: ListResponse<EventTypeOut> = client\n            .get(\"api/v1/event-type/\", StatusCode::OK)\n            .await\n            .unwrap();\n\n        if should_see {\n            // If the client is expected to see both event types it should be able to retrieve it\n            let got_et: EventTypeOut = client.get(&path, StatusCode::OK).await.unwrap();\n            assert_eq!(et, got_et);\n\n            // ... and see it in the list.\n            assert_eq!(list.data.len(), 2);\n            assert!(list.data.contains(&et));\n        } else {\n            // If the client is not supposed to see it it shouldn't be able to retrieve it\n            let _: IgnoredAny = client.get(&path, StatusCode::NOT_FOUND).await.unwrap();\n\n            // ... and it shouldn't be in the list.\n            assert_eq!(list.data.len(), 1);\n            assert!(!list.data.contains(&et));\n        };\n    }\n}\n\n#[tokio::test]\nasync fn test_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    common_test_list::<EventTypeOut, EventTypeIn>(\n        &client,\n        \"api/v1/event-type/\",\n        |i| event_type_in(&format!(\"test-event-type-{i}\"), None).unwrap(),\n        true,\n        false,\n    )\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_schema() {\n    let (client, _jh) = start_svix_server().await;\n    let _: serde_json::Value = client\n        .post(\n            \"api/v1/event-type/\",\n            serde_json::json!({\n                            \"name\": \"bad-schema\",\n                            \"description\": \"I have a bad schema\",\n                            \"schemas\": {\n                                \"1\": {\"readOnly\": 15},\n                            },\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n}\n<|fim_middle|>", "completion": "let union: FeatureFlagSet = s1.union(&s2).cloned().collect();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_event_type.rs", "node_type": "let_declaration", "line_range": [209, 209]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse sea_orm::{\n    ColumnTrait, DatabaseConnection, DbBackend, DeleteResult, EntityTrait, QueryFilter,\n    SqlxPostgresConnector,\n};\nuse sqlx::postgres::PgPoolOptions;\n\nuse crate::{cfg::Configuration, core::types::OrganizationId};\n\npub mod models;\nuse models::{application, endpoint, eventtype, message, messageattempt};\n\nstatic MIGRATIONS: sqlx::migrate::Migrator = sqlx::migrate!();\n\nasync fn connect(dsn: &str, max_pool_size: u16) -> sqlx::Pool<sqlx::Postgres> {\n    if DbBackend::Postgres.is_prefix_of(dsn) {\n        PgPoolOptions::new()\n            .max_connections(max_pool_size.into())\n            .connect(dsn)\n            .await\n            .expect(\"Error connecting to Postgres\")\n    } else {\n        panic!(\"db_dsn format not recognized. {dsn}\")\n    }\n}\n\npub async fn init_db(cfg: &Configuration) -> DatabaseConnection {\n    SqlxPostgresConnector::from_sqlx_postgres_pool(connect(&cfg.db_dsn, cfg.db_pool_max_size).await)\n}\n\npub async fn run_migrations(cfg: &Configuration) {\n    let db = connect(&cfg.db_dsn, cfg.db_pool_max_size).await;\n    MIGRATIONS.run(&db).await.unwrap();\n}\n\n/// Wipe an organization from existence in a way that ensures the operation can be tried again on\n/// failure.\npub async fn wipe_org(cfg: &Configuration, org_id: OrganizationId) {\n    let db = init_db(cfg).await;\n\n    let applications: Vec<application::Model> = application::Entity::secure_find(org_id.clone())\n        .all(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error fetching applications associated with org ID {org_id}\"));\n\n    for application in applications {\n        l<|fim_suffix|>\n        for endpoint in endpoints {\n            // First [`messageattempt`]s, then [`messagedestination`]s\n            let _: DeleteResult = messageattempt::Entity::delete_many()\n                .filter(messageattempt::Column::EndpId.eq(endpoint.id.clone()))\n                .exec(&db)\n                .await\n                .unwrap_or_else(|_| {\n                    panic!(\n                        \"Error deleting messageattempts associated with endpoint ID {}\",\n                        endpoint.id\n                    )\n                });\n        }\n\n        // Then [`message`]s, then [`endpoint`]s\n        let _: DeleteResult = message::Entity::delete_many()\n            .filter(message::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting messages associated with application ID {}\",\n                    application.id\n                )\n            });\n\n        let _: DeleteResult = endpoint::Entity::delete_many()\n            .filter(endpoint::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting endpoints associated with application ID {}\",\n                    application.id\n                )\n            });\n    }\n\n    // Then [`application`]s, then [`eventtype`]s\n    let _: DeleteResult = application::Entity::delete_many()\n        .filter(application::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting applications associated with org ID {org_id}\"));\n\n    let _: DeleteResult = eventtype::Entity::delete_many()\n        .filter(eventtype::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting event types associated with org ID {org_id}\"));\n}\n<|fim_middle|>", "completion": "let endpoints: Vec<endpoint::Model> = endpoint::Entity::secure_find(application.id.clone())\n            .all(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error fetching endpoints associated with application ID {}\",\n                    application.id\n                )\n            });", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/mod.rs", "node_type": "let_declaration", "line_range": [49, 57]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{error, fmt, panic::Location};\n\nuse aide::OperationOutput;\nuse axum::{\n    extract::rejection::{ExtensionRejection, PathRejection},\n    response::{IntoResponse, Response},\n    Json,\n};\nuse hyper::StatusCode;\nuse schemars::JsonSchema;\nuse sea_orm::{DbErr, RuntimeErr, TransactionError};\nuse serde::Serialize;\nuse serde_json::json;\n\nuse crate::core::webhook_http_client;\n\n/// A short-hand version of a [`std::result::Result`] that defaults to Svix'es [Error].\npub type Result<T, E = Error> = std::result::Result<T, E>;\n\n/// The error type returned from the Svix API\n#[derive(Debug)]\npub struct Error {\n    // the file name and line number of the error. Used for debugging non Http errors\n    pub trace: Vec<&'static Location<'static>>,\n    pub typ: ErrorType,\n}\n\nimpl Error {\n    #[track_caller]\n    fn new(typ: ErrorType) -> Self {\n        let trace = vec![Location::caller()];\n        Self { trace, typ }\n    }\n\n    #[track_caller]\n    pub fn generic(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Generic(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn database(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Database(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn conflict(e: DbErr) -> Self {\n        Self::new(ErrorType::Conflict(e))\n    }\n\n    #[track_caller]\n    pub fn queue(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Queue(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn validation(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Validation(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn http(h: HttpError) -> Self {\n        Self {\n            trace: Vec::with_capacity(0), // no debugging necessary\n            typ: ErrorType::Http(h),\n        }\n    }\n\n    #[track_caller]\n    pub fn cache(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Cache(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Timeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn db_timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::DbTimeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn connection_timeout(e: DbErr) -> Self {\n        Self::new(ErrorType::ConnectionTimeout(e))\n    }\n\n    #[track_caller]\n    pub fn trace(mut self) -> Self {\n        self.trace.push(Location::caller());\n        self\n    }\n}\n\nimpl fmt::Display for Error {\n    f<|fim_suffix|>}\n\nimpl error::Error for Error {\n    fn source(&self) -> Option<&(dyn error::Error + 'static)> {\n        None\n    }\n}\n\nimpl IntoResponse for Error {\n    fn into_response(self) -> Response {\n        let stringified: Vec<String> = self.trace.into_iter().map(ToString::to_string).collect();\n        match self.typ {\n            ErrorType::Http(s) => {\n                tracing::debug!(\"{:?}, location: {:?}\", &s, stringified);\n                s.into_response()\n            }\n            s => {\n                tracing::error!(\"type: {:?}, location: {:?}\", s, stringified);\n                (StatusCode::INTERNAL_SERVER_ERROR, Json(json!({}))).into_response()\n            }\n        }\n    }\n}\n\nimpl OperationOutput for Error {\n    type Inner = Self;\n}\n\npub trait Traceable<T> {\n    /// Pushes the current [`Location`] onto the error's trace stack\n    #[track_caller]\n    fn trace(self) -> Result<T>;\n}\n\nimpl<T> Traceable<T> for Result<T> {\n    fn trace(self) -> Result<T> {\n        // Using `map_err` would lose `#[track_caller]` information\n        match self {\n            Err(e) => Err(e.trace()),\n            ok => ok,\n        }\n    }\n}\n\nimpl From<DbErr> for Error {\n    #[track_caller]\n    fn from(err: DbErr) -> Self {\n        if is_timeout_error(&err) {\n            Error::db_timeout(err)\n        } else if is_conflict_err(&err) {\n            Error::conflict(err)\n        } else if is_connection_timeout_error(&err) {\n            Error::connection_timeout(err)\n        } else {\n            Error::database(err)\n        }\n    }\n}\n\nimpl From<redis::RedisError> for Error {\n    #[track_caller]\n    fn from(value: redis::RedisError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<omniqueue::QueueError> for Error {\n    #[track_caller]\n    fn from(value: omniqueue::QueueError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl<E: error::Error + 'static> From<bb8::RunError<E>> for Error {\n    #[track_caller]\n    fn from(value: bb8::RunError<E>) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<ExtensionRejection> for Error {\n    #[track_caller]\n    fn from(value: ExtensionRejection) -> Self {\n        Error::generic(value)\n    }\n}\n\nimpl From<PathRejection> for Error {\n    #[track_caller]\n    fn from(value: PathRejection) -> Self {\n        Error::generic(value)\n    }\n}\n\nimpl From<crate::core::cache::Error> for Error {\n    #[track_caller]\n    fn from(value: crate::core::cache::Error) -> Self {\n        Error::cache(value)\n    }\n}\n\nimpl From<TransactionError<Error>> for Error {\n    #[track_caller]\n    fn from(value: TransactionError<Error>) -> Self {\n        match value {\n            TransactionError::Connection(db_err) => Error::database(db_err),\n            TransactionError::Transaction(crate_err) => crate_err, // preserve the trace that comes from within the transaction\n        }\n    }\n}\n\nimpl From<lapin::Error> for Error {\n    #[track_caller]\n    fn from(value: lapin::Error) -> Self {\n        Error::queue(format_args!(\"{value:?}\"))\n    }\n}\n\n#[derive(Debug)]\npub enum ErrorType {\n    /// A generic error\n    Generic(String),\n    /// Database error\n    Database(String),\n    /// Queue error\n    Queue(String),\n    /// Database error\n    Validation(String),\n    /// Any kind of HttpError\n    Http(HttpError),\n    /// Cache error\n    Cache(String),\n    /// Timeout error\n    Timeout(String),\n    /// Database timeout error\n    DbTimeout(String),\n    /// Connection timeout error\n    ConnectionTimeout(DbErr),\n    /// Conflict error\n    Conflict(DbErr),\n}\n\nimpl fmt::Display for ErrorType {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Generic(s) => s.fmt(f),\n            Self::Database(s) => s.fmt(f),\n            Self::Queue(s) => s.fmt(f),\n            Self::Validation(s) => s.fmt(f),\n            Self::Http(s) => s.fmt(f),\n            Self::Cache(s) => s.fmt(f),\n            Self::Timeout(s) => s.fmt(f),\n            Self::DbTimeout(s) => s.fmt(f),\n            Self::ConnectionTimeout(s) => s.fmt(f),\n            Self::Conflict(s) => s.fmt(f),\n        }\n    }\n}\n\nimpl From<HttpError> for ErrorType {\n    fn from(e: HttpError) -> Self {\n        Self::Http(e)\n    }\n}\n\n// Python generation relies on the title of this being `HttpError`\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HttpErrorOut\", title = \"HttpError\")]\npub struct StandardHttpError {\n    code: String,\n    detail: String,\n}\n\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HTTPValidationError\")]\npub struct ValidationHttpError {\n    detail: Vec<ValidationErrorItem>,\n}\n\n#[derive(Debug, Clone, Serialize)]\n#[serde(untagged)]\npub enum HttpErrorBody {\n    Standard(StandardHttpError),\n    Validation(ValidationHttpError),\n}\n\n#[derive(Debug, Clone, Serialize, PartialEq, Eq, JsonSchema)]\n/// Validation errors have their own schema to provide context for invalid requests eg. mismatched\n/// types and out of bounds values. There may be any number of these per 422 UNPROCESSABLE ENTITY\n/// error.\npub struct ValidationErrorItem {\n    /// The location as a [`Vec`] of [`String`]s -- often in the form `[\"body\", \"field_name\"]`,\n    /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, de<|fim_middle|>", "completion": "fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        self.typ.fmt(f)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "function_item", "line_range": [99, 101]}
{"prompt": "<|fim_prefix|>use std::sync::Arc;\n\nuse axum::{\n    body::Body,\n    http::{Request, StatusCode},\n};\nuse serde_json::json;\n<|fim_suffix|>\nuse tower::{Service, ServiceExt};\n\nuse super::router;\nuse crate::webhook_receiver::{\n    types::{IntegrationState, InternalState},\n    verification::{NoVerifier, SvixVerifier},\n};\n\nstruct FakeReceiverOutput {\n    tx: tokio::sync::mpsc::UnboundedSender<serde_json::Value>,\n}\n\nimpl FakeReceiverOutput {\n    pub fn new() -> (\n        Self,\n        tokio::sync::mpsc::UnboundedReceiver<serde_json::Value>,\n    ) {\n        let (tx, rx) = tokio::sync::mpsc::unbounded_channel();\n        (Self { tx }, rx)\n    }\n}\n\n#[async_trait]\nimpl ReceiverOutput for FakeReceiverOutput {\n    fn name(&self) -> &str {\n        \"fake output\"\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.tx.send(request.payload)?;\n        Ok(())\n    }\n}\n\n#[tokio::test]\nasync fn test_forwarding_no_verification() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n}\n\n/// Registers 2 receivers and sends 1 request to each.\n#[tokio::test]\nasync fn test_forwarding_multiple_receivers() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"a\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: None,\n            },\n        ),\n        (\n            \"b\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/a\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n\n    let request = Request::builder()\n        .uri(\"/webhook/b\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n/// Registers 2 receivers, one with a transformation and one without. Sends 1 request to each.\n#[tokio::test]\nasync fn test_transformation_json() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let mut input = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            input.insert(\"__TRANSFORMED__\".into(), json!(true));\n            let out = json!({ \"payload\": input });\n\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"transformed\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: Some(\n                    \"handler = (x) => ({ payload: {__TRANSFORMED__: true, ...x }})\".into(),\n                ),\n            },\n        ),\n        (\n            \"as-is\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The `__TRANSFORMED__` key should have been added\n    assert_eq!(\n        json!(forwarded),\n        json!({\"a\": true, \"__TRANSFORMED__\": true})\n    );\n\n    let request = Request::builder()\n        .uri(\"/webhook/as-is\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    // The same payload should come through, without any transformation.\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_transformation_string() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let out = match x.input {\n                TransformerInput::String(input) => json!({\"payload\": { \"got\": input }})\n                    .as_object()\n                    .cloned()\n                    .unwrap(),\n                _ => unreachable!(),\n            };\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"transformed\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: Some(TransformationConfig::Explicit {\n                format: TransformerInputFormat::String,\n                src: String::from(\"handler = (x) => ({ payload: { got: x }})\"),\n            }),\n        },\n    )]\n    .into_<|fim_middle|>", "completion": "use svix_bridge_types::{\n    async_trait, svix::webhooks::Webhook, BoxError, ForwardRequest, ReceiverOutput,\n    TransformationConfig, TransformerInput, TransformerInputFormat, TransformerJob,\n    TransformerOutput,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/tests.rs", "node_type": "use_declaration", "line_range": [8, 12]}
{"prompt": "<|fim_prefix|>use std::{convert::Infallible, net::SocketAddr, sync::Arc, time::Duration};\n\nuse axum::{\n    extract::{FromRequestParts, Path, State},\n    http::{self, request},\n    routing::{get, post},\n    Router,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::{\n        api::{MessagePollerConsumerPollOptions, PollingEndpointMessageOut, Svix},\n        error::Error,\n    },\n    ForwardRequest, PollerInput, ReceiverOutput, TransformationConfig, TransformerInput,\n    TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\nuse tracing::instrument;\nuse types::{IntegrationId, IntegrationState, InternalState, SerializableRequest, Unvalidated};\n\nuse crate::{\n    config::{PollerInputOpts, PollerReceiverConfig, WebhookReceiverConfig},\n    webhook_receiver::types::SerializablePayload,\n};\n\nmod config;\nmod types;\nmod verification;\n\nfn router() -> Router<InternalState> {\n    Router::new()\n        .route(\n            \"/webhook/:integration_id\",\n            post(route).put(route).get(route).patch(route),\n        )\n        .route(\n            \"/webhook/:integration_id/\",\n            post(route).put(route).get(route).patch(route),\n        )\n        .route(\"/health\", get(health_handler))\n}\nstatic START_TIME: once_cell::sync::Lazy<std::time::Instant> =\n    once_cell::sync::Lazy::new(std::time::Instant::now);\n\nfn get_uptime_seconds() -> u64 {\n    START_TIME.elapsed().as_secs()\n}\n#[derive(serde::Serialize)]\nstruct HealthResponse {\n    pub status: &'static str,\n    pub version: &'static str,\n    pub uptime: u64,\n}\nasync fn health_handler() -> impl axum::response::IntoResponse {\n    let health_response = HealthResponse {\n        status: \"OK\",\n        version: env!(\"CARGO_PKG_VERSION\"),\n        uptime: get_uptime_seconds(),\n    };\n    axum::Json(health_response)\n}\npub async fn run(\n    listen_addr: SocketAddr,\n    routes: Vec<WebhookReceiverConfig>,\n    transformer_tx: TransformerTx,\n) -> std::io::Result<()> {\n    once_cell::sync::Lazy::force(&START_TIME);\n    let state = InternalState::from_receiver_configs(routes, transformer_tx)\n        .await\n        .map_err(std::io::Error::other)?;\n\n    let router = router().with_state(state);\n\n    tracing::info!(\"Listening on: {listen_addr}\");\n    let listener = tokio::net::TcpListener::bind(listen_addr).await.unwrap();\n    axum::serve(listener, router)\n        .await\n        .map_err(std::io::Error::other)\n}\n\nstruct WebhookIdHeader(Option<String>);\n\n#[async_trait]\nimpl<S> FromRequestParts<S> for WebhookIdHeader {\n    type Rejection = Infallible;\n\n    async fn from_request_parts(\n        parts: &mut request::Parts,\n        _: &S,\n    ) -> Result<Self, Self::Rejection> {\n        Ok(Self(\n            parts\n                .headers\n                .get(\"svix-id\")\n                .or_else(|| parts.headers.get(\"webhook-id\"))\n                .and_then(|val| Some(val.to_str().ok()?.to_owned())),\n        ))\n    }\n}\n\n#[instrument(\n    skip_all,\n    level = \"error\",\n    fields(\n        msg_id = _msg_id,\n        integration_id = integration_id.as_ref(),\n    )\n)]\nasync fn route(\n    Path(integration_id): Path<IntegrationId>,\n    WebhookIdHeader(_msg_id): WebhookIdHeader,\n    State(InternalState {\n        routes,\n        transformer_tx,\n    }): State<InternalState>,\n    req: SerializableRequest<Unvalidated>,\n) -> Result<http::StatusCode, http::StatusCode> {\n    let IntegrationState {\n        verifier,\n        output,\n        transformation,\n    } = routes\n        .get(&integration_id)\n        .ok_or(http::StatusCode::NOT_FOUND)?;\n\n    <|fim_suffix|>\n\n    let payload = parse_payload(\n        req.payload(),\n        transformation.as_ref(),\n        transformer_tx.clone(),\n    )\n    .await?;\n\n    handle(payload, Arc::clone(output)).await\n}\n\n// FIXME: Really odd return type - artifact of being extracted from the HTTP server\nasync fn handle(\n    payload: ForwardRequest,\n    output: Arc<Box<dyn ReceiverOutput>>,\n) -> Result<http::StatusCode, http::StatusCode> {\n    tracing::debug!(\"forwarding request\");\n    Ok(match output.handle(payload).await {\n        Ok(_) => http::StatusCode::NO_CONTENT,\n        Err(e) => {\n            tracing::error!(\"Error forwarding request: {}\", e);\n            http::StatusCode::INTERNAL_SERVER_ERROR\n        }\n    })\n}\n\n/// Figures out how to build a JSON object from the payload, optionally running it through a\n/// transformation.\n///\n/// WRT \"raw\" payloads, the return value here is going to be a JSON object regardless of whether\n/// or not the queue producer wants \"raw\" data.\n///\n/// When there's no transformation defined we therefore attempt to parse the body as json.\n/// When a transformation is defined, we branch to see if it expects string or json input.\n///\n/// For either case, we expect the value produced to match the schema of a [`ForwardRequest`].\nasync fn parse_payload(\n    payload: &SerializablePayload,\n    transformation: Option<&TransformationConfig>,\n    transformer_tx: TransformerTx,\n) -> Result<ForwardRequest, http::StatusCode> {\n    match transformation {\n        Some(xform) => {\n            let input = match xform.format() {\n                TransformerInputFormat::String => {\n                    TransformerInput::String(payload.as_string().map_err(|_| {\n                        tracing::error!(\"Unable to parse request body as string\");\n                        http::StatusCode::BAD_REQUEST\n                    })?)\n                }\n                TransformerInputFormat::Json => {\n                    TransformerInput::Json(payload.as_json().map_err(|_| {\n                        tracing::error!(\"Unable to parse request body as json\");\n                        http::StatusCode::BAD_REQUEST\n                    })?)\n                }\n            };\n            transform(input, xform.source().clone(), transformer_tx).await\n        }\n        // Keep the original payload as-is if there's no transformation specified, but stuff the\n        // whole thing into the payload field.\n        // The as_json() only gets us to `Value`, so we also need a `from_value` call to marshal\n        // into a [`ForwardRequest`] type.\n        None => Ok(ForwardRequest {\n            payload: payload.as_json().map_err(|_| {\n                tracing::error!(\"Unable to parse request body as json\");\n                http::StatusCode::BAD_REQUEST\n            })?,\n        }),\n    }\n}\n\n/// Attempts to run the payload through a js transformation.\nasync fn transform(\n    input: TransformerInput,\n    script: String,\n    tx: TransformerTx,\n) -> Result<ForwardRequest, http::StatusCode> {\n    let (job, callback) = TransformerJob::new(script, input);\n    if let Err(e) = tx.send(job) {\n        tracing::error!(\"transformations are not available: {}\", e);\n        return Err(http::StatusCode::INTERNAL_SERVER_ERROR);\n    }\n\n    match callback.await {\n        // This is the only \"good\" outcome giving a RHS value for the assignment.\n        // All other match arms should bail with a non-2xx status.\n        Ok(Ok(TransformerOutput::Object(obj))) => Ok(serde_json::from_value(\n            serde_json::Value::Object(obj),\n        )\n        .map_err(|e| {\n            tracing::error!(\"transformation produced invalid payload: {}\", e);\n            http::StatusCode::INTERNAL_SERVER_ERROR\n        })?),\n        Ok(Ok(TransformerOutput::Invalid)) => {\n            tracing::error!(\"transformation produced invalid payload\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n        _ => {\n            tracing::error!(\"transformation failed\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n    }\n}\n\nstruct SvixEventsPoller {\n    name: String,\n    input_opts: PollerInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n    output: Arc<Box<dyn ReceiverOutput>>,\n}\n\n#[async_trait]\nimpl PollerInput for SvixEventsPoller {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        run_inner(self).await\n    }\n}\n\nimpl PollerReceiverConfig {\n    pub async fn into_poller_input(\n        self,\n        transformer_tx: TransformerTx,\n    ) -> std::<|fim_middle|>", "completion": "let req = req.validate(verifier).await.inspect_err(|code| {\n        tracing::warn!(\"validation failed: {code}\");\n    })?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/mod.rs", "node_type": "let_declaration", "line_range": [126, 128]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Configuration-dependent tests for the main database\n\nuse http::StatusCode;\nuse sea_orm::DatabaseConnection;\nuse svix_server::{\n    core::types::{ApplicationId, BaseId, EndpointId, OrganizationId},\n    db::{\n        models::{application, endpoint, eventtype, message, messageattempt},\n        wipe_org,\n    },\n    v1::endpoints::event_type::EventTypeOut,\n};\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, create_test_message, event_type_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id,\n};\n\nasync fn test_data() -> (OrganizationId, Vec<ApplicationId>, Vec<EndpointId>) {\n    let org_id = OrganizationId::new(None, None);\n\n    let (client, jh) =\n        start_svix_server_with_cfg_and_org_id(&get_default_test_config(), org_id.clone()).await;\n\n    // Make apps\n    let mut app_ids = Vec::new();\n    for _ in 0..10 {\n        app_ids.push(create_test_app(&client, \"Test1\").await.unwrap().id);\n    }\n\n    let mut endp_ids = Vec::new();\n    for app_id in &app_ids {\n        for _ in 0..2 {\n            endp_ids.push(\n                create_test_endpoint(&client, app_id, \"https://bad.url\")\n                    .await\n                    .unwrap()\n                    .id,\n            );\n        }\n    }\n\n    for app_id in &app_ids {\n        for _ in 0..2 {\n            create_test_message(&client, app_id, serde_json::json!({\"test\": \"value\"}))\n                .await\n                .unwrap();\n        }\n    }\n\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type\",\n            event_type_in(\"test\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    jh.abort();\n\n    (org_id, app_ids, endp_ids)\n}\n\nasync fn count_message_attempts(db: &DatabaseConnection, endp_id: EndpointId) -> usize {\n    messageattempt::Entity::secure_find_by_endpoint(endp_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_messages(db: &DatabaseConnection, app_id: ApplicationId) -> usize {\n    message::Entity::secure_find(app_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_endpoints(db: &DatabaseConnection, app_id: ApplicationId) -> usize {\n    endpoint::Entity::secure_find(app_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_applications(db: &DatabaseConnection, org_id: OrganizationId) -> usize {\n    application::Entity::secure_find(org_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_event_types(db: &DatabaseConnection, org_id: OrganizationId) -> usize {\n    eventtype::Entity::secure_find(org_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\n#[ignore]\n#[tokio::test]\nasync fn test_wiping_organization() {\n    // Make two orgs, one of which will be wiped\n    let (org_id_1, app_ids_1, endp_ids_1) = test_data().await;\n\n    let (org_id_2, app_ids_2, _) = test_data().await;\n\n    // Wipe\n    let cfg = svix_server::cfg::load().unwrap();\n    wipe_org(&cfg, org_id_1.clone()).await;\n\n    // Start asserting everything is gone for org_id_1, but not org_id_2\n    let db = svix_server::db::init_db(&cfg).await;\n\n    for endp_id in endp_ids_1 {\n        assert_eq!(count_message_attempts(&db, endp_id.clone()).await, 0);\n    }\n\n    for app_id in app_ids_1 {\n        assert_eq!(count_messages(&db, app_id.clone()).await, 0);\n        assert_eq!(count_endpoints(&db, app_id.clone()).await, 0);\n    }\n\n    f<|fim_suffix|>\n    assert_eq!(count_applications(&db, org_id_1.clone()).await, 0);\n    assert_eq!(count_event_types(&db, org_id_1.clone()).await, 0);\n\n    assert_eq!(count_applications(&db, org_id_2.clone()).await, 10);\n    assert_eq!(count_event_types(&db, org_id_2.clone()).await, 1);\n}\n<|fim_middle|>", "completion": "for app_id in app_ids_2 {\n        assert_eq!(count_messages(&db, app_id.clone()).await, 2);\n        assert_eq!(count_endpoints(&db, app_id.clone()).await, 2);\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/db.rs", "node_type": "for_expression", "line_range": [132, 135]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for crating `tracing` spans compatible with OpenTelemetry's\n//! conventions.\nuse std::net::SocketAddr;\n\nuse axum::extract::{ConnectInfo, MatchedPath};\nuse http::header;\nuse opentelemetry::trace::TraceContextExt;\nuse svix_ksuid::{KsuidLike, KsuidMs};\nuse tower_http::{\n    classify::ServerErrorsFailureClass,\n    trace::{MakeSpan, OnFailure, OnResponse},\n};\nu<|fim_suffix|>use tracing_opentelemetry::OpenTelemetrySpanExt;\n\n/// An implementor of [`MakeSpan`] which creates `tracing` spans populated with information about\n/// the request received by an `axum` web server.\n#[derive(Clone, Copy)]\npub struct AxumOtelSpanCreator;\n\nimpl<B> MakeSpan<B> for AxumOtelSpanCreator {\n    fn make_span(&mut self, request: &http::Request<B>) -> tracing::Span {\n        let user_agent = request\n            .headers()\n            .get(header::USER_AGENT)\n            .and_then(|header| header.to_str().ok());\n\n        let host = request\n            .headers()\n            .get(header::HOST)\n            .and_then(|header| header.to_str().ok());\n\n        let http_route = request\n            .extensions()\n            .get::<MatchedPath>()\n            .map(|p| p.as_str());\n\n        let client_ip = request\n            .extensions()\n            .get::<ConnectInfo<SocketAddr>>()\n            .map(|ConnectInfo(ip)| debug(ip));\n\n        let request_id = request\n            .headers()\n            .get(\"x-request-id\")\n            .and_then(|id| id.to_str().map(ToOwned::to_owned).ok())\n            // If `x-request-id` isn't set, check `svix-req-id`. If the `svix-req-id` isn't a\n            // valid `str`, or it isn't set, then fallback to a random [`KsuidMs`]\n            .or_else(|| {\n                request\n                    .headers()\n                    .get(\"svix-req-id\")\n                    .and_then(|v| v.to_str().map(ToOwned::to_owned).ok())\n            })\n            .unwrap_or_else(|| KsuidMs::new(None, None).to_string());\n\n        let remote_context = opentelemetry::global::get_text_map_propagator(|p| {\n            p.extract(&opentelemetry_http::HeaderExtractor(request.headers()))\n        });\n        let remote_span = remote_context.span();\n        let span_context = remote_span.span_context();\n        let trace_id = span_context\n            .is_valid()\n            .then(|| span_context.trace_id().to_string());\n\n        let idempotency_key = request\n            .headers()\n            .get(\"idempotency-key\")\n            .and_then(|v| v.to_str().ok());\n\n        let span = tracing::error_span!(\n            \"HTTP request\",\n            grpc.code = Empty,\n            http.client_ip = client_ip,\n            http.versions = ?request.version(),\n            http.host = host,\n            http.method = ?request.method(),\n            http.route = http_route,\n            http.scheme = request.uri().scheme().map(debug),\n            http.status_code = Empty,\n            http.target = request.uri().path_and_query().map(|p| p.as_str()),\n            http.user_agent = user_agent,\n            otel.kind = \"server\",\n            otel.status_code = Empty,\n            request_id,\n            trace_id,\n            idempotency_key,\n            org_id = tracing::field::Empty,\n            app_id = tracing::field::Empty,\n        );\n\n        span.set_parent(remote_context);\n\n        span\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnResponse;\n\nimpl<B> OnResponse<B> for AxumOtelOnResponse {\n    fn on_response(\n        self,\n        response: &http::Response<B>,\n        latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        let status = response.status().as_u16().to_string();\n        span.record(\"http.status_code\", tracing::field::display(status));\n        span.record(\"otel.status_code\", \"OK\");\n\n        tracing::debug!(\n            \"finished processing request latency={} ms status={}\",\n            latency.as_millis(),\n            response.status().as_u16(),\n        );\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnFailure;\n\nimpl OnFailure<ServerErrorsFailureClass> for AxumOtelOnFailure {\n    fn on_failure(\n        &mut self,\n        failure_classification: ServerErrorsFailureClass,\n        _latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        match failure_classification {\n            ServerErrorsFailureClass::StatusCode(status) if status.is_server_error() => {\n                span.record(\"otel.status_code\", \"ERROR\");\n            }\n            _ => {}\n        }\n    }\n}\n<|fim_middle|>", "completion": "use tracing::field::{debug, Empty};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/otel_spans.rs", "node_type": "use_declaration", "line_range": [16, 16]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ApplicationOut {\n    #[serde(rename = \"createdAt\")]\n    pub created_at: String,\n\n    /// The Application's ID.\n    pub id: String,\n\n    pub metadata: std::collections::HashMap<String, String>,\n\n    pub name: String,\n\n    #[serde(rename = \"rateLimit\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    /// The Application's UID.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<String>,\n\n    #[serde(rename = \"updatedAt\")]\n    pub updated_at: String,\n}\n\nimpl ApplicationOut {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(\n        created_at: String,\n        id: String,\n        metadata: std::collections::HashMap<String, String>,\n        name: String,\n        updated_at: String,\n    ) -> Self {\n        Self {\n            created_at,\n            id,\n            metadata,\n            name,\n            rate_limit: None,\n            uid: None,\n            updated_at,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/application_out.rs", "node_type": "function_item", "line_range": [29, 45]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ZoomConfigOut {}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl ZoomConfigOut {\n    pub fn new() -> Self {\n        Self {}\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/zoom_config_out.rs", "node_type": "impl_item", "line_range": [7, 11]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining an interface for sending webhook events about the service.\n\nuse std::sync::Arc;\n\nuse chrono::{DateTime, Utc};\nuse schemars::JsonSchema;\nuse serde::Serialize;\nu<|fim_suffix|>\nuse super::{\n    security::generate_management_token,\n    types::{\n        ApplicationId, ApplicationUid, EndpointId, EndpointUid, MessageAttemptId, MessageId,\n        MessageUid, OrganizationId,\n    },\n};\nuse crate::{\n    core::security::JwtSigningConfig,\n    db::models::{endpoint, messageattempt},\n    error::{Error, HttpError, Result},\n};\n\n/// Sent when an endpoint has been automatically disabled after continuous failures.\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointDisabledEventData {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub endpoint_id: EndpointId,\n    pub endpoint_uid: Option<EndpointUid>,\n    pub fail_since: DateTime<Utc>,\n}\n\n/// Sent when an endpoint is created, updated, or deleted\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointEvent {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub endpoint_id: EndpointId,\n    pub endpoint_uid: Option<EndpointUid>,\n}\n\nimpl EndpointEvent {\n    pub fn new(app_uid: Option<&ApplicationUid>, endp: &endpoint::Model) -> Self {\n        Self {\n            app_id: endp.app_id.clone(),\n            app_uid: app_uid.cloned(),\n            endpoint_id: endp.id.clone(),\n            endpoint_uid: endp.uid.clone(),\n        }\n    }\n}\n\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageAttempetLast {\n    pub id: MessageAttemptId,\n    pub response_status_code: i16,\n    pub timestamp: DateTime<Utc>,\n}\n\nimpl From<messageattempt::Model> for MessageAttempetLast {\n    fn from(attempt: messageattempt::Model) -> Self {\n        Self {\n            id: attempt.id,\n            response_status_code: attempt.response_status_code,\n            timestamp: attempt.created_at.into(),\n        }\n    }\n}\n\n/// Sent when a message delivery has failed (all of the retry attempts have been exhausted) as a\n/// \"message.attempt.exhausted\" type or after it's failed four times as a \"message.attempt.failing\"\n/// event.\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageAttemptEvent {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub msg_id: MessageId,\n    pub msg_event_id: Option<MessageUid>,\n    pub endpoint_id: EndpointId,\n    pub last_attempt: MessageAttempetLast,\n}\n\n#[derive(Debug, Serialize)]\n#[serde(tag = \"type\", content = \"data\")]\npub enum OperationalWebhook {\n    #[serde(rename = \"endpoint.disabled\")]\n    EndpointDisabled(EndpointDisabledEventData),\n    #[serde(rename = \"endpoint.created\")]\n    EndpointCreated(EndpointEvent),\n    #[serde(rename = \"endpoint.updated\")]\n    EndpointUpdated(EndpointEvent),\n    #[serde(rename = \"endpoint.deleted\")]\n    EndpointDeleted(EndpointEvent),\n    #[serde(rename = \"message.attempt.exhausted\")]\n    MessageAttemptExhausted(MessageAttemptEvent),\n    #[serde(rename = \"message.attempt.failing\")]\n    MessageAttemptFailing(MessageAttemptEvent),\n    #[serde(rename = \"message.attempt.recovered\")]\n    MessageAttemptRecovered(MessageAttemptEvent),\n}\n\npub type OperationalWebhookSender = Arc<OperationalWebhookSenderInner>;\n\npub struct OperationalWebhookSenderInner {\n    signing_config: Arc<JwtSigningConfig>,\n    url: Option<String>,\n}\n\nimpl OperationalWebhookSenderInner {\n    pub fn new(keys: Arc<JwtSigningConfig>, mut url: Option<String>) -> Arc<Self> {\n        // Sanitize the URL if present\n        if let Some(url) = &mut url {\n            // Remove trailing slashes\n            while url.ends_with('/') {\n                url.pop();\n            }\n        }\n\n        Arc::new(Self {\n            signing_config: keys,\n            url,\n        })\n    }\n\n    pub async fn send_operational_webhook(\n        &self,\n        recipient_org_id: &OrganizationId,\n        payload: OperationalWebhook,\n    ) -> Result<()> {\n        let Some(url) = &self.url else { return Ok(()) };\n\n        let op_webhook_token =\n            generate_management_token(&self.signing_config).map_err(Error::generic)?;\n        let svix_api = Svix::new(\n            op_webhook_token,\n            Some(SvixOptions {\n                server_url: Some(url.to_string()),\n                ..Default::default()\n            }),\n        );\n\n        let payload = serde_json::to_value(payload)\n            .map_err(|_| HttpError::internal_server_error(None, None))?;\n\n        // Get the event type from the type field\n        let event_type: String = payload\n            .get(\"type\")\n            .ok_or_else(|| HttpError::internal_server_error(None, None))?\n            .as_str()\n            .ok_or_else(|| HttpError::internal_server_error(None, None))?\n            .to_string();\n\n        let recipient_org_id = recipient_org_id.to_string();\n\n        tokio::spawn(async move {\n            // This sends a webhook under the Svix management organization. This organization contains\n            // applications which are each a regular organization. The recipient's OrganizationId is the\n            // app UID to use.\n            let resp = svix_api\n                .message()\n                .create(\n                    recipient_org_id.clone(),\n                    MessageIn {\n                        event_type,\n                        payload,\n                        ..MessageIn::default()\n                    },\n                    None,\n                )\n                .await;\n\n            match resp {\n                Ok(_) => {}\n                // Ignore 404s because not every org will have an associated application\n                Err(svix::error::Error::Http(svix::error::HttpErrorContent {\n                    status: http02::StatusCode::NOT_FOUND,\n                    ..\n                })) => {\n                    tracing::warn!(\n                        \"Operational webhooks are enabled, but no listener found for organization {}\",\n                        recipient_org_id,\n                    );\n                }\n                Err(e) => {\n                    tracing::error!(\n                        \"Failed sending operational webhook for {} {}\",\n                        recipient_org_id,\n                        e.to_string()\n                    );\n                }\n            }\n        });\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "use svix::api::{MessageIn, Svix, SvixOptions};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/operational_webhooks.rs", "node_type": "use_declaration", "line_range": [11, 11]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::collections::HashMap;\n\nuse chrono::Utc;\nuse jsonschema::Draft;\nuse schemars::JsonSchema;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set};\nuse serde::{Deserialize, Serialize};\n\nuse crate::{\n    core::types::{\n        BaseId, EventTypeId, EventTypeName, FeatureFlag, FeatureFlagSet, OrganizationId,\n    },\n    json_wrapper,\n};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"eventtype\")]\npub struct Model {\n    pub created_at: DateTimeWithTimeZone,\n    pub updated_at: DateTimeWithTimeZone,\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: EventTypeId,\n    pub org_id: OrganizationId,\n    pub description: String,\n    pub deleted: bool,\n    pub deprecated: bool,\n    pub schemas: Option<Schema>,\n    pub name: EventTypeName,\n    pub feature_flag: Option<FeatureFlag>,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter)]\npub enum Relation {}\n\nimpl RelationTrait for Relation {\n    fn def(&self) -> RelationDef {\n        panic!(\"No RelationDef\")\n    }\n}\n\n#[axum::async_trait]\nimpl ActiveModelBehavior for ActiveModel {\n    fn new() -> Self {\n        let timestamp = Utc::now();\n        Self {\n            id: Set(EventTypeId::new(timestamp.into(), None)),\n            created_at: Set(timestamp.into()),\n            updated_at: Set(timestamp.into()),\n            deleted: Set(false),\n            ..ActiveModelTrait::default()\n        }\n    }\n\n    async fn before_save<C>(mut self, _db: &C, _insert: bool) -> Result<Self, DbErr>\n    where\n        C: ConnectionTrait,\n    {\n        self.updated_at = Set(Utc::now().into());\n        Ok(self)\n    }\n}\n\nimpl Entity {\n    pub fn secure_find(org_id: OrganizationId) -> Select<Entity> {\n        Self::find().filter(Column::OrgId.eq(org_id))\n    }\n\n    pub fn secure_find_by_name(org_id: OrganizationId, name: EventTypeName) -> Select<Entity> {\n        Self::secure_find(org_id).filter(Column::Name.eq(name))\n    }\n\n    pub fn filter_feature_flags(query: Select<Self>, flags: FeatureFlagSet) -> Select<Self> {\n        query.filter(\n            sea_orm::Condition::any()\n                .add(Column::FeatureFlag.is_in(flags))\n                .add(Column::FeatureFlag.is_null()),\n        )\n    }\n}\n\npub fn schema_example() -> serde_json::Value {\n    serde_json::json!({\n        \"description\": \"An invoice was paid by a user\",\n        \"properties\": {\n            \"invoiceId\": {\n                \"description\": \"The invoice id\",\n                \"type\": \"string\"\n            },\n            \"userId\": {\n                \"description\": \"The user id\",\n                \"type\": \"string\"\n            }\n        },\n        \"required\": [\n            \"invoiceId\",\n            \"userId\"\n        ],\n        \"title\": \"Invoice Paid Event\",\n        \"type\": \"object\"\n    })\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Default)]\npub struct Schema(HashMap<String, Json>);\njson_wrapper!(Schema);\n\nimpl Schema {\n    pub fn example(&self) -> Option<&serde_json::Value> {\n        self.0\n            .get(\"1\")\n            .and_then(|version| match version {\n                serde_json::Value::Object(obj) => obj.get(\"examples\"),\n                _ => None,\n            })\n            .and_then(|examples| m<|fim_suffix|>\n    }\n}\n\nimpl JsonSchema for Schema {\n    fn schema_name() -> String {\n        stringify!(Schema).to_string()\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        let mut schema = gen.subschema_for::<HashMap<String, Json>>();\n\n        if let schemars::schema::Schema::Object(obj) = &mut schema {\n            obj.extensions\n                .insert(\"example\".to_string(), schema_example());\n        }\n\n        schema\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\nimpl<'de> Deserialize<'de> for Schema {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let inner: HashMap<String, Json> = Deserialize::deserialize(deserializer)?;\n\n        // JSONSchema doesn't implement (De)Serialize, so we have to\n        // manually enforce the values are valid JSON schemas\n\n        let opts = jsonschema::Validator::options().with_draft(Draft::Draft7);\n        if let Some(error) = inner.values().find_map(|schema| opts.build(schema).err()) {\n            return Err(serde::de::Error::custom(error));\n        }\n\n        Ok(Self(inner))\n    }\n}\n<|fim_middle|>", "completion": "match examples {\n                serde_json::Value::Array(arr) => arr.iter().next(),\n                _ => None,\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/eventtype.rs", "node_type": "match_expression", "line_range": [118, 121]}
{"prompt": "<|fim_prefix|>   set.shutdown().await;\n        }\n\n        Ok(())\n    }\n}\n\npub async fn listen(\n    local_url: url::Url,\n    relay_token: String,\n    relay_debug_url: Option<&str>,\n    relay_disable_security: bool,\n    disable_tls_verification: bool,\n) -> Result<()> {\n    let scheme = if relay_disable_security { \"ws\" } else { \"wss\" };\n    let api_host = relay_debug_url.unwrap_or(DEFAULT_API_HOST);\n    let token = format!(\"c_{relay_token}\");\n\n    let websocket_url = format!(\"{scheme}://{api_host}/{API_PREFIX}/listen/\").parse()?;\n\n    let http_client = HttpClient::builder()\n        .danger_accept_invalid_certs(disable_tls_verification)\n        .build()?;\n\n    let mut client = Client {\n        token,\n        websocket_url,\n        local_url,\n        http_client,\n    };\n\n    const MAX_BACKOFF: Duration = Duration::from_millis(5000);\n    let backoff_schedule = [\n        Duration::ZERO,\n        Duration::from_millis(100),\n        Duration::from_millis(1000),\n        MAX_BACKOFF,\n    ];\n\n    let mut attempt_count = 0;\n    let mut last_attempt = Instant::now();\n\n    // We may ditch this token, generating a new one on the fly, depending on how the server\n    // responds when we connect.\n    let orig_token = client.token.clone();\n    loop {\n        // Any termination Ok or Err... try to reconnect.\n        let show_welcome_message = attempt_count == 0 || orig_token != client.token;\n\n        if let Err(e) = client.connect(show_welcome_message).await {\n            eprintln!(\"Failed to connect to Webhook Relay: {e:#}\");\n            if e.downcast_ref::<TokenInUse>().is_some() {\n                eprintln!(\"Generating a new token for this session.\");\n                client.token = {\n                    let relay_token = generate_token()?;\n                    format!(\"c_{relay_token}\")\n                };\n            }\n        } else {\n            eprintln!(\"Failed to connect to Webhook Relay\");\n        }\n\n        // Reset the backoff schedule if it's been a while since we've seen a disconnect.\n        if last_attempt.elapsed() > MAX_BACKOFF * 2 {\n            // N.b. attempt_count `0` is special because that's what prompts the printing of a\n            // welcome message in `Client::connect`.\n            // When we reset here, starting at `0` here will still avoid the\n            // re-print because we increment after selecting the sleep duration.\n            attempt_count = 0;\n        }\n\n        let backoff = *backoff_schedule.get(attempt_count).unwrap_or(&MAX_BACKOFF);\n        eprintln!(\"Reattempting connection in: {}ms\", backoff.as_millis());\n\n        attempt_count += 1;\n        last_attempt = Instant::now();\n\n        tokio::time::sleep(backoff).await;\n    }\n}\n\nfn receive_url(token: &str) -> String {\n    format!(\"https://play.svix.com/in/{token}/\")\n}\n\nfn view_url(token: &str) -> String {\n    format!(\"https://play.svix.com/view/{token}/\")\n}\n\ntype S = WebSocketStream<MaybeTlsStream<TcpStream>>;\n\nstruct WsConnection {\n    stream: S,\n}\n\nimpl WsConnection {\n    async fn new(websocket_url: &url::Url) -> Result<Self> {\n        let request = websocket_url.to_string().into_client_request()?;\n        let (stream, _resp) = connect_async(request)\n            .await\n            .inspect_err(|e| eprintln!(\"{e}\"))\n            .context(\"failed to connect to websocket server\")?;\n\n        Ok(Self { stream })\n    }\n}\n\nasync fn read_from_ws_loop(\n    mut rx: SplitStream<S>,\n    tx: UnboundedSender<MessageOut>,\n    local_url: url::Url,\n    client: HttpClient,\n) -> Result<()> {\n    // We expect to see roughly _at least one Ping_ in each `SERVER_PING_PERIOD`.\n    // Other messages may arrive ahead of this schedule.\n    // Tracking the time each message is received, we can know if the server has been quiet for too\n    // long, possibly requiring us to reconnect.\n    let mut last_msg = Instant::now();\n\n    loop {\n        const REMOTE_SERVER_CLOSED: &str = \"remote server closed connection\";\n\n        match tokio::time::timeout(SERVER_PING_PERIOD, rx.next()).await {\n            Err(_timeout_hit) => {\n                // Generous. 1.5x the ping frequency. If we go that long without\n                // seeing anything from the server, force a reconnect.\n                if last_msg.elapsed() > SERVER_PING_PERIOD + (SERVER_PING_PERIOD / 2) {\n                    anyhow::bail!(REMOTE_SERVER_CLOSED);\n                }\n            }\n            // Stream empty/closed\n            Ok(None) => break,\n            Ok(Some(msg)) => {\n                last_msg = Instant::now();\n\n                let data = match msg? {\n                    // Control messages.\n                    Message::Close(_) => anyhow::bail!(REMOTE_SERVER_CLOSED),\n                    Message::Ping(_) | Message::Pong(_) | Message::Frame(_) => continue,\n\n                    // Messages that carry data we care to process.\n                    Message::Text(s) => s.into(),\n                    Message::Binary(bytes) => bytes,\n                };\n\n                handle_incoming_message(client.clone(), data, &local_url, tx.clone()).await;\n            }\n        }\n    }\n\n    Ok(())\n}\n\nasync fn send_to_ws_loop(\n    mut rx: UnboundedReceiver<MessageOut>,\n    mut tx: SplitSink<S, Message>,\n) -> Result<()> {\n    while let Some(msg) = rx.recv().await {\n        tokio::time::timeout(\n            WRITE_WAIT,\n            tx.send(Message::Binary(\n                serde_json::to_vec(&msg)\n                    .expect(\"trivial serialization\")\n                    .into(),\n            )),\n        )\n        .await?\n        .context(\"Websocket write timeout\")?;\n    }\n\n    Ok(())\n}\n\nasync fn make_local_request(\n    client: HttpClient,\n    url: &url::Url,\n    data: MessageInEvent,\n) -> Result<LocalServerResponse> {\n    let method = data.method.parse()?;\n    // FIXME: deprecation warning\n    #[allow(deprecated)]\n    let body = base64::decode(&data.body)?;\n    <|fim_suffix|>\n    for (k, v) in &data.headers {\n        // FIXME: there's a remark about the Go client freaking out if there's more than one host header set.\n        //   Do we care now that we're not using Go? TBD.\n        headers.insert(\n            HeaderName::try_from(k.as_str())?,\n            HeaderValue::try_from(v.as_str())?,\n        );\n    }\n    Ok(client\n        .request(method, url.clone())\n        .timeout(DEFAULT_TIMEOUT)\n        .body(body)\n        .headers(headers)\n        .send()\n        .await?)\n}\n\nfn format_resp_headers(headers: &HeaderMap) -> Result<HashMap<String, String>> {\n    let mut out = HashMap::new();\n    for (k, v) in headers {\n        out.insert(k.to_string(), v.to_str()?.to_string());\n    }\n    Ok(out)\n}\n\nasync fn handle_incoming_message(\n    client: HttpClient,\n    bytes: Bytes,\n    local_url: &url::Url,\n    tx: UnboundedSender<MessageOut>,\n) {\n    match serde_json::from_slice::<MessageIn>(&bytes) {\n        Ok(MessageIn::Event { data, .. }) => {\n            let msg_id = data.id.clone();\n            println!(\"<- Forwarding message id={msg_id} to: {local_url}\");\n            match make_local_request(client, local_url, data).await {\n                Err(err) => {\n                    eprintln!(\"Failed to make request to local server: \\n{err:#}\");\n                }\n                Ok(resp) => {\n                    if let Err(err) = process_response(msg_id, resp, tx).await {\n                        eprintln!(\"Failed to read response from local server: \\n{err:#}\");\n                    }\n                }\n            }\n        }\n        Ok(MessageIn::Start { .. }) => { /* nothing to do */ }\n        Err(_err) => {\n            eprintln!(\"Received invalid webhook message... skipping\");\n        }\n    }\n}\n\nasync fn process_response(\n    id: String,\n    resp: LocalServerResponse,\n    tx: UnboundedSender<MessageOut>,\n) -> Result<()> {\n    let status = resp.status().as_u16();\n    let headers = format_resp_headers(resp.headers())?;\n    #[allow(deprecated)]\n    let body = base64::encode(resp.bytes().await?);\n    let msg = MessageOut::Event {\n        version: message::VERSION,\n        data: MessageOutEvent {\n            id,\n            body,\n            headers,\n            status,\n        },\n    };\n\n    println!(\"-> Received \\\"{status}\\\" response from local server, forwarding to webhook sender\");\n    Ok(tx.send(msg)?)\n}\n<|fim_middle|>", "completion": "let mut headers = HeaderMap::with_capacity(data.headers.len());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/mod.rs", "node_type": "let_declaration", "line_range": [388, 388]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::message_attempt_log::MessageAttemptLog;\n\n/// Sent after message attempts are made. Contains metadata about message\n/// attempts and their results. In order to reduce the frequency of webhooks,\n/// these are sent in batches periodically.\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct MessageAttemptLogEvent {\n    pub data: Vec<MessageAttemptLog>,\n\n    pub r#type: String,\n}\n\nimpl MessageAttemptLogEvent {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(data: Vec<MessageAttemptLog>, r#type: String) -> Self {\n        Self { data, r#type }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/message_attempt_log_event.rs", "node_type": "function_item", "line_range": [17, 19]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::fmt;\n\nuse http_body_util::BodyExt;\nuse hyper::body::Incoming;\n\nuse crate::http1_to_02_status_code;\n\npub type Result<T> = std::result::Result<T, Error>;\n\n/// The error type returned from the Svix API\n#[derive(Debug, Clone)]\npub enum Error {\n    /// A generic error\n    Generic(String),\n    /// Http Error\n    Http(HttpErrorContent<crate::models::HttpErrorOut>),\n    /// Http Validation Error\n    Validation(HttpErrorContent<crate::models::HttpValidationError>),\n}\n\nimpl Error {\n    pub(crate) fn generic(err: impl std::error::Error) -> Self {\n        Self::Generic(format!(\"{err:?}\"))\n    }\n\n    pub(crate) async fn from_response(status_code: http1::StatusCode, body: Incoming) -> Self {\n        match body.collect().await {\n            Ok(collected) => {\n                let bytes = collected.to_bytes();\n                if status_code == http1::StatusCode::UNPROCESSABLE_ENTITY {\n                    Self::Validation(HttpErrorContent {\n                        status: http02::StatusCode::UNPROCESSABLE_ENTITY,\n                        payload: serde_json::from_slice(&bytes).ok(),\n                    })\n                } else {\n                    Error::Http(HttpErrorContent {\n                        status: http1_to_02_status_code(status_code),\n                        payload: serde_json::from_slice(&bytes).ok(),\n                    })\n                }\n            }\n            Err(e) => Self::Generic(e.to_string()),\n        }\n    }\n}\n\n// TODO: Remove for v2.0 of the library (very uncommon impl for an error type)\nimpl From<Error> for String {\n    fn from(err: Error) -> Self {\n        err.to_string()\n    }\n}\n\nimpl fmt::Display for Error {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        m<|fim_suffix|>    }\n}\n\nimpl std::error::Error for Error {}\n\n#[derive(Debug, Clone)]\npub struct HttpErrorContent<T> {\n    pub status: http02::StatusCode,\n    pub payload: Option<T>,\n}\n<|fim_middle|>", "completion": "match self {\n            Error::Generic(s) => s.fmt(f),\n            Error::Http(e) => format!(\"Http error (status={}) {:?}\", e.status, e.payload).fmt(f),\n            Error::Validation(e) => format!(\"Validation error {:?}\", e.payload).fmt(f),\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/error.rs", "node_type": "match_expression", "line_range": [59, 63]}
{"prompt": "<|fim_prefix|>Result,\n    metrics::RedisQueueType,\n    redis::{RedisConnection, RedisManager},\n};\n\n/// This is the key of the main queue. As a KV store, redis places the entire stream under this key.\n/// Confusingly, each message in the queue may have any number of KV pairs.\nconst MAIN: &str = \"{queue}_svix_v3_main\";\n\n/// The key for the DELAYED queue in which scheduled messages are placed. This is the same DELAYED\n/// queue as v2 of the queue implementation.\nconst DELAYED: &str = \"{queue}_svix_delayed\";\n\n/// The key for the lock guarding the delayed queue background task.\nconst DELAYED_LOCK: &str = \"{queue}_svix_delayed_lock\";\n\n/// The key for the DLQ\nconst DLQ: &str = \"{queue}_svix_dlq\";\n\n// v2 KEY CONSTANTS\nconst LEGACY_V2_MAIN: &str = \"{queue}_svix_main\";\nconst LEGACY_V2_PROCESSING: &str = \"{queue}_svix_processing\";\n\n// v1 KEY CONSTANTS\nconst LEGACY_V1_MAIN: &str = \"svix_queue_main\";\nconst LEGACY_V1_PROCESSING: &str = \"svix_queue_processing\";\nconst LEGACY_V1_DELAYED: &str = \"svix_queue_delayed\";\n\n/// Consumer group name constant -- each consumer group is able to read and acknowledge messages\n/// from the queue, and messages are read by all consumer groups.\nconst WORKERS_GROUP: &str = \"svix_workers_group\";\n/// Consumer group consumer name constant -- consumer groups contain consumers which receive\n/// messages in a round-robin manner. Every worker uses the same consumer name such that they race\n/// for messages instead of having them evenly distributed.\nconst WORKER_CONSUMER: &str = \"svix_workers_consumer\";\n\n/// Special ID for XADD command's which generates a stream ID automatically\nconst GENERATE_STREAM_ID: &str = \"*\";\n\n/// Each queue item has a set of KV pairs associated with it, for simplicity a sing key, \"data\" is\n/// used with the entire [`QueueTask`] as the value in serialized JSON\nconst QUEUE_KV_KEY: &str = \"data\";\n\n/// Generates a [`TaskQueueProducer`] and a [`TaskQueueConsumer`] backed by Redis.\npub async fn new_pair(\n    cfg: &Configuration,\n    prefix: Option<&str>,\n) -> (TaskQueueProducer, TaskQueueConsumer) {\n    new_pair_inner(\n        cfg,\n        Duration::from_secs(cfg.redis_pending_duration_secs),\n        prefix.unwrap_or_default(),\n        MAIN,\n        DELAYED,\n        DELAYED_LOCK,\n        DLQ,\n    )\n    .await\n}\n\n/// Runs Redis queue migrations with the given delay schedule. Migrations are run on this schedule\n/// such that if an old instance of the server is online after the migrations are made, that no data\n/// will be lost assuming the old server is taken offline before the last scheduled delay.\nasync fn run_migration_schedule(delays: &[Duration], pool: RedisManager) {\n    let mut conn = pool\n        .get()\n        .await\n        .expect(\"Error retrieving connection from Redis pool\");\n\n    for delay in delays {\n        // drain legacy queues:\n        if let Err(e) = migrate_v1_to_v2_queues(&mut conn).await {\n            tracing::error!(\"Error migrating queue: {}\", e);\n            tokio::time::sleep(*delay).await;\n            continue;\n        }\n        if let Err(e) = migrate_v2_to_v3_queues(&mut conn).await {\n            tracing::error!(\"Error migrating queue: {}\", e);\n            tokio::time::sleep(*delay).await;\n            continue;\n        }\n\n        tokio::time::sleep(*delay).await;\n    }\n}\n\n/// An inner function allowing key constants to be variable for testing purposes\nasync fn new_pair_inner(\n    cfg: &Configuration,\n    pending_duration: Duration,\n    queue_prefix: &str,\n    main_queue_name: &'static str,\n    delayed_queue_name: &'static str,\n    delayed_lock_name: &'static str,\n    dlq_name: &'static str,\n) -> (TaskQueueProducer, TaskQueueConsumer) {\n    let main_queue_name = format!(\"{queue_prefix}{main_queue_name}\");\n    let delayed_queue_name = format!(\"{queue_prefix}{delayed_queue_name}\");\n    let delayed_lock_name = format!(\"{queue_prefix}{delayed_lock_name}\");\n    let dlq_name = format!(\"{queue_prefix}{dlq_name}\");\n\n    // This fn is only called from\n    // - `queue::new_pair` if the queue type is redis and a DSN is set\n    // - redis tests that only makes sense to run with the DSN set\n    <|fim_suffix|>\n    let pool =\n        RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await;\n\n    // Create the stream and consumer group for the MAIN queue should it not already exist. The\n    // consumer is created automatically upon use so it does not have to be created here.\n    {\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n\n        let consumer_group_resp: RedisResult<()> = conn\n            .xgroup_create_mkstream(&main_queue_name, WORKERS_GROUP, 0i8)\n            .await;\n\n        // If the error is a BUSYGROUP error, then the stream or consumer group already exists. This does\n        // not impact functionality, so continue as usual.\n        if let Err(e) = consumer_group_resp {\n            if !e.to_string().contains(\"BUSYGROUP\") {\n                panic!(\n                    \"error creating consumer group or stream: {:?}, {:?}, {:?}, {:?}, {e:?}\",\n                    e.kind(),\n                    e.detail(),\n                    e.code(),\n                    e.category()\n                )\n            };\n        }\n    }\n\n    // Redis durations are given in integer numbers of milliseconds, so the pending_duration (the\n    // time in which a task is allowed to be processing before being restarted) must be converted to\n    // one.\n    let pending_duration: i64 = pending_duration\n        .as_millis()\n        .try_into()\n        .expect(\"Pending duration out of bounds\");\n\n    // Migrate v1 queues to v2 and v2 queues to v3 on a loop with exponential backoff.\n    tokio::spawn({\n        let pool = pool.clone();\n\n        async move {\n            let delays = [\n                // 11.25 min\n                Duration::from_secs(60 * 11 + 15),\n                // 22.5 min\n                Duration::from_secs(60 * 22 + 30),\n                // 45 min\n                Duration::from_secs(60 * 45),\n                // 1.5 hours\n                Duration::from_secs(60 * 30 * 3),\n                // 3 hours\n                Duration::from_secs(60 * 60 * 3),\n                // 6 hours\n                Duration::from_secs(60 * 60 * 6),\n                // 12 hours\n                Duration::from_secs(60 * 60 * 12),\n                // 24 hours\n                Duration::from_secs(60 * 60 * 24),\n            ];\n\n            run_migration_schedule(&delays, pool).await;\n        }\n    });\n\n    // Metrics task\n    tokio::spawn({\n        let pool = pool.clone();\n        let main_queue_name = main_queue_name.clone();\n        let delayed_queue_name = delayed_queue_name.clone();\n        let deadletter_queue_name = dlq_name.clone();\n\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_secs(1));\n            let main_queue = RedisQueueType::Stream(&main_queue_name);\n            let pending = RedisQueueType::StreamPending {\n                stream: &main_queue_name,\n                group: WORKERS_GROUP,\n            };\n            let delayed_queue = RedisQueueType::SortedSet(&delayed_queue_name);\n            let deadletter_queue = RedisQueueType::List(&deadletter_queue_name);\n            let metrics =\n                crate::metrics::RedisQueueMetrics::new(&opentelemetry::global::meter(\"svix.com\"));\n            loop {\n                interval.tick().await;\n                metrics\n                    .record(\n                        &pool,\n                        &main_queue,\n                        &pending,\n                        &delayed_queue,\n                        &deadletter_queue,\n                    )\n                    .await;\n            }\n        }\n    });\n\n    let config = RedisConfig {\n        dsn: dsn.to_owned(),\n        max_connections: cfg.redis_pool_max_size,\n        reinsert_on_nack: false, // TODO\n        queue_key: main_queue_name,\n        delayed_queue_key: delayed_queue_name,\n        delayed_lock_key: delayed_lock_name,\n        consumer_group: WORKERS_GROUP.to_owned(),\n        consumer_name: WORKER_CONSUMER.to_owned(),\n        payload_key: QUEUE_KV_KEY.to_owned(),\n        ack_deadline_ms: pending_dur<|fim_middle|>", "completion": "let dsn = cfg.redis_dsn.as_deref().unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [142, 142]}
{"prompt": "<|fim_prefix|>//! Requires a rabbitmq node to be running on localhost:5672 (the default port) and using the\n//! default guest/guest credentials.\n//! Try using the `testing-docker-compose.yml` in the repo root to get this going.\n\nuse std::time::Duration;\n\nuse lapin::{\n    options::QueueDeclareOptions, types::FieldTable, Channel, Connection, ConnectionProperties,\n    Queue,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_queue::{\n    config::{QueueInputOpts, RabbitMqInputOpts},\n    sender_input::QueueSender,\n};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nfn get_test_plugin(\n    svix_url: String,\n    mq_uri: &str,\n    queue_name: &str,\n    use_transformation: Option<TransformerInputFormat>,\n) -> QueueSender {\n    QueueSender::new(\n        \"test\".into(),\n        QueueInputOpts::RabbitMQ(RabbitMqInputOpts {\n            uri: mq_uri.to_string(),\n            queue_name: queue_name.to_string(),\n            consumer_tag: None,\n            consume_opts: None,\n            consume_args: None,\n            requeue_on_nack: false,\n        }),\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n}\n\nasync fn declare_queue(name: &str, channel: &Channel) -> Queue {\n    channel\n        .queue_declare(\n            name,\n            QueueDeclareOptions {\n                auto_delete: true,\n                ..Default::default()\n            },\n            FieldTable::default(),\n        )\n        .await\n        .unwrap()\n}\n\nasync fn mq_connection(uri: &str) -> Connection {\n    let options = ConnectionProperties::default()\n        .with_connection_name(\"test\".into())\n        .with_executor(tokio_executor_trait::Tokio::current())\n        .with_reactor(tokio_reactor_trait::Tokio);\n    Connection::connect(uri, options).await.unwrap()\n}\n\nasync fn publish(channel: &Channel, queue_name: &str, payload: &[u8]) {\n    <|fim_suffix|>\n    confirm.await.unwrap();\n}\n\n/// General \"pause while we wait for messages to travel\" beat. If you're seeing flakes, bump this up.\nconst WAIT_MS: u64 = 200;\n/// These tests assume a \"vanilla\" rabbitmq instance, using the default port, creds, exchange...\nconst MQ_URI: &str = \"amqp://guest:guest@localhost:5672/%2f\";\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let mq_conn = mq_connection(MQ_URI).await;\n    let channel = mq_conn.create_channel().await.unwrap();\n    // setup the queue before running the consumer or the consumer will error out\n    let queue = declare_queue(\"\", &channel).await;\n    let queue_name = queue.name().as_str();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), MQ_URI, queue_name, None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&channel, queue_name, &serde_json::to_vec(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n    channel\n        .queue_delete(queue_name, Default::default())\n        .await\n        .ok();\n}\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let mq_conn = mq_connection(MQ_URI).await;\n    let channel = mq_conn.create_channel().await.unwrap();\n    // setup the queue before running the consumer or the consumer will error out\n    let queue = declare_queue(\"\", &channel).await;\n    let queue_name = queue.name().as_str();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        MQ_URI,\n        queue_name,\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&channel, queue_name, &serde_json::to_vec(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n    channel\n        .queue_delete(queue_name, Default::default())\n        .await\n        .ok();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_string_ok() {\n    let mq_conn = mq_connection(MQ_URI).await;\n    let channel = mq_conn.create_channel().await.unwrap();\n    // setup the queue before running the consumer or the consumer will error out\n    let queue = declare_queue(\"\", &channel).await;\n    let queue_name = queue.name().as_str();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when <|fim_middle|>", "completion": "let confirm = channel\n        .basic_publish(\n            \"\",\n            queue_name,\n            Default::default(),\n            payload,\n            Default::default(),\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/rabbitmq_consumer.rs", "node_type": "let_declaration", "line_range": [79, 88]}
{"prompt": "<|fim_prefix|> => { /* nothing to do  */ }\n            // The outer Result is for the timeout, the inner is for if there was some other failure\n            // during `send`.\n            Ok(Err(_)) | Err(_) => {\n                anyhow::bail!(\"failed to complete handshake with Webhook Relay server: remote didn't accept start message\");\n            }\n        }\n\n        // The assumption is the very first message we get from the websocket reader will be the\n        // response to our `MessageOut::Start` but it could also be any number of control messages.\n        // Keep reading until we see a `MessageIn::Start` or give up after some attempts.\n        const MAX_ATTEMPTS: u8 = 10;\n        let mut attempts = 0;\n        let start_response = loop {\n            if attempts > MAX_ATTEMPTS {\n                anyhow::bail!(\"failed to complete handshake with Webhook Relay server: no response from remote\");\n            }\n            attempts += 1;\n\n            match tokio::time::timeout(SERVER_PING_PERIOD, ws_rx.next()).await {\n                Err(_timeout) => continue,\n                Ok(None) => {\n                    anyhow::bail!(\"no response from server for start message\");\n                }\n                Ok(Some(msg)) => {\n                    let data = match msg? {\n                        // Control messages.\n                        Message::Close(Some(CloseFrame { code, reason }))\n                            if code == Policy && reason == SOCKET_IN_USE_REASON =>\n                        {\n                            return Err(TokenInUse.into())\n                        }\n                        Message::Close(_) => {\n                            anyhow::bail!(\"Relay server refused connection\");\n                        }\n                        Message::Ping(_) | Message::Pong(_) | Message::Frame(_) => continue,\n\n                        // Messages that carry data we care to process.\n                        Message::Text(s) => s.into(),\n                        Message::Binary(bytes) => bytes,\n                    };\n\n                    match serde_json::from_slice::<MessageIn>(&data)? {\n                        // This is what we're waiting to see. A `MessageOut::Start` sent to the writer\n                        // should result in a `MessageInStart` coming back on the reader.\n                        MessageIn::Start { data, .. } => break data,\n                        MessageIn::Event { .. } => continue,\n                    };\n                }\n            }\n        };\n\n        if show_welcome_message {\n            printdoc!(\n                r#\"\n\n                Webhook Relay is now listening at:\n                {}\n\n                All requests on this endpoint will be forwarded to your local URL:\n                {}\n\n                View logs and debug information at:\n                {}\n\n                \"#,\n                receive_url(&start_response.token),\n                self.local_url,\n                view_url(&self.token),\n            );\n        } else {\n            // Shows that a reconnection attempt succeeded after some failing initial attempts.\n            println!(\"Connected!\");\n        }\n\n        set.spawn({\n            let local_url = self.local_url.clone();\n            let http_client = self.http_client.clone();\n            async move {\n                read_from_ws_loop(ws_rx, remote_tx, local_url.clone(), http_client.clone())\n                    .await\n                    .inspect_err(|e| eprintln!(\"read loop terminated: {e:#}\"))\n            }\n        });\n\n        set.spawn(async move {\n            send_to_ws_loop(remote_rx, ws_tx)\n                .await\n                .inspect_err(|e| eprintln!(\"write loop terminated: {e:#}\"))\n        });\n\n        // If any task terminates, trash the rest so we can reconnect.\n        if set.join_next().await.is_some() {\n            set.shutdown().await;\n        }\n\n        Ok(())\n    }\n}\n\npub async fn listen(\n    local_url: url::Url,\n    relay_token: String,\n    relay_debug_url: Option<&str>,\n    relay_disable_security: bool,\n    disable_tls_verification: bool,\n) -> Result<()> {\n    let scheme = <|fim_suffix|>;\n    let api_host = relay_debug_url.unwrap_or(DEFAULT_API_HOST);\n    let token = format!(\"c_{relay_token}\");\n\n    let websocket_url = format!(\"{scheme}://{api_host}/{API_PREFIX}/listen/\").parse()?;\n\n    let http_client = HttpClient::builder()\n        .danger_accept_invalid_certs(disable_tls_verification)\n        .build()?;\n\n    let mut client = Client {\n        token,\n        websocket_url,\n        local_url,\n        http_client,\n    };\n\n    const MAX_BACKOFF: Duration = Duration::from_millis(5000);\n    let backoff_schedule = [\n        Duration::ZERO,\n        Duration::from_millis(100),\n        Duration::from_millis(1000),\n        MAX_BACKOFF,\n    ];\n\n    let mut attempt_count = 0;\n    let mut last_attempt = Instant::now();\n\n    // We may ditch this token, generating a new one on the fly, depending on how the server\n    // responds when we connect.\n    let orig_token = client.token.clone();\n    loop {\n        // Any termination Ok or Err... try to reconnect.\n        let show_welcome_message = attempt_count == 0 || orig_token != client.token;\n\n        if let Err(e) = client.connect(show_welcome_message).await {\n            eprintln!(\"Failed to connect to Webhook Relay: {e:#}\");\n            if e.downcast_ref::<TokenInUse>().is_some() {\n                eprintln!(\"Generating a new token for this session.\");\n                client.token = {\n                    let relay_token = generate_token()?;\n                    format!(\"c_{relay_token}\")\n                };\n            }\n        } else {\n            eprintln!(\"Failed to connect to Webhook Relay\");\n        }\n\n        // Reset the backoff schedule if it's been a while since we've seen a disconnect.\n        if last_attempt.elapsed() > MAX_BACKOFF * 2 {\n            // N.b. attempt_count `0` is special because that's what prompts the printing of a\n            // welcome message in `Client::connect`.\n            // When we reset here, starting at `0` here will still avoid the\n            // re-print because we increment after selecting the sleep duration.\n            attempt_count = 0;\n        }\n\n        let backoff = *backoff_schedule.get(attempt_count).unwrap_or(&MAX_BACKOFF);\n        eprintln!(\"Reattempting connection in: {}ms\", backoff.as_millis());\n\n        attempt_count += 1;\n        last_attempt = Instant::now();\n\n        tokio::time::sleep(backoff).await;\n    }\n}\n\nfn receive_url(token: &str) -> String {\n    format!(\"https://play.svix.com/in/{token}/\")\n}\n\nfn view_url(token: &str) -> String {\n    format!(\"https://play.svix.com/view/{token}/\")\n}\n\ntype S = WebSocketStream<MaybeTlsStream<TcpStream>>;\n\nstruct WsConnection {\n    stream: S,\n}\n\nimpl WsConnection {\n    async fn new(websocket_url: &url::Url) -> Result<Self> {\n        let request = websocket_url.to_string().into_client_request()?;\n        let (stream, _resp) = connect_async(request)\n            .await\n            .inspect_err(|e| eprintln!(\"{e}\"))\n            .context(\"failed to connect to websocket server\")?;\n\n        Ok(Self { stream })\n    }\n}\n\nasync fn read_from_ws_loop(\n    mut rx: SplitStream<S>,\n    tx: UnboundedSender<MessageOut>,\n    local_url: url::Url,\n    client: HttpClient,\n) -> Result<()> {\n    // We expect to see roughly _at least one Ping_ in each `SERVER_PING_PERIOD`.\n    // Other messages may arrive ahead of this schedule.\n    // Tracking the time each message is received, we can know if the server has been quiet for too\n    // long, possibly requiring us to reconnect.\n    let mut last_msg = Instant::now();\n\n    loop {\n        const REMOTE_SERVER_CLOSED: &str = \"remote server closed connection\";\n\n        match tokio::time::timeout(SERVER_PING_PERIOD, rx.next()).await {\n            Err(_timeout_hit) => {\n                // Generous. 1.5x the ping frequency. If we go that long without\n                // seeing anything from the server, force a reconnect.\n                if last_msg.elapsed() > SERVER_PING_PERIOD + (SERVER_PING_PERIOD / 2) {\n                    anyhow::bail!(REMOTE_SERVER_CLOSED);\n                }\n            }\n            // Stream empty/closed\n            Ok(None) => break,\n     <|fim_middle|>", "completion": "if relay_disable_security { \"ws\" } else { \"wss\" }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/mod.rs", "node_type": "if_expression", "line_range": [220, 220]}
{"prompt": "<|fim_prefix|>ost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value\" });\n\n    let msg_1_w_payload: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        msg_1_w_payload.payload.0.get(),\n        serde_json::to_string(&msg_payload).unwrap()\n    );\n\n    let msg_2_wo_payload: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/?with_content=false\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(msg_2_wo_payload.payload.0.get(), \"{}\");\n\n    let msg_1_wo_payload = MessageOut {\n        payload: RawPayload::from_string(\"{}\".to_string()).unwrap(),\n        ..msg_1_w_payload.clone()\n    };\n    let msg_2_w_payload = MessageOut {\n        payload: RawPayload::from_string(serde_json::to_string(&msg_payload).unwrap()).unwrap(),\n        ..msg_2_wo_payload.clone()\n    };\n\n    for m in [&msg_1_w_payload, &msg_2_w_payload] {\n        assert_eq!(\n            client\n                .get::<MessageOut>(\n                    &format!(\"api/v1/app/{app_id}/msg/{}/\", m.id),\n                    StatusCode::OK\n                )\n                .await\n                .unwrap(),\n            *m,\n        );\n    }\n\n    for m in [&msg_1_wo_payload, &msg_2_wo_payload] {\n        assert_eq!(\n            client\n                .get::<MessageOut>(\n                    &format!(\"api/v1/app/{app_id}/msg/{}/?with_content=false\", m.id),\n                    StatusCode::OK\n                )\n                .await\n                .unwrap(),\n            *m\n        );\n    }\n\n    let list: ListResponse<MessageOut> = client\n        .get(&format!(\"api/v1/app/{app_id}/msg/\"), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 2);\n    assert!(list.data.contains(&msg_1_w_payload));\n    assert!(list.data.contains(&msg_2_w_payload));\n\n    let list: ListResponse<MessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/?with_content=false\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 2);\n    assert!(list.data.contains(&msg_1_wo_payload));\n    assert!(list.data.contains(&msg_2_wo_payload));\n}\n\n#[tokio::test]\nasync fn test_failed_message_gets_recorded() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver = TestReceiver::start(axum::http::StatusCode::INTERNAL_SERVER_ERROR);\n\n    let _endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value\" });\n\n    let msg_res: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    receiver.data_recv.recv().await;\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 500) {\n            anyhow::bail!(\"could not find failed attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_multiple_endpoints() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let mut receiver_2 = TestReceiver::start(axum::http::StatusCode::INTERNAL_SERVER_ERROR);\n    l<|fim_suffix|>\n    let _endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n    let _endp_id_2 = create_test_endpoint(&client, &app_id, &receiver_2.endpoint)\n        .await\n        .unwrap()\n        .id;\n    let _endp_id_3 = create_test_endpoint(&client, &app_id, &receiver_3.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value1\" });\n\n    let msg_res: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let (rec_body_1, rec_body_2, rec_body_3) = tokio::join!(\n        receiver_1.data_recv.recv(),\n        receiver_2.data_recv.recv(),\n        receiver_3.data_recv.recv()\n    );\n\n    assert_eq!(msg_payload.to_string(), rec_body_1.unwrap().to_string());\n    assert_eq!(msg_payload.to_string(), rec_body_2.unwrap().to_string());\n    assert_eq!(msg_payload.to_string(), rec_body_3.unwrap().to_string());\n\n    receiver_2.set_response_status_code(axum::http::StatusCode::OK);\n\n    let rec_body_2 = receiver_2.data_recv.recv().await.unwrap();\n\n    assert_eq!(msg_payload.to_string(), rec_body_2.to_string());\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 200) {\n            anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_failed_message_gets_requeued() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver_1 = TestReceiver::start(axum::http::StatusCode::INTERNAL_SERVER_ERROR);\n    let _endp_id = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value\" });\n\n    let msg_res: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    receiver_1.data_recv.recv().await;\n\n    receiver_1.set_response_status_code(axum::http::StatusCode::OK);\n\n    let last_body = receiver_1.data_recv.recv().await.unwrap();\n\n    assert_eq!(msg_payload.to_string(), last_body.to_string());\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 200) {\n            anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, json!({ \"test\": \"value\" })).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n    let msg_id = msg.id.clone();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap();\n    assert_eq!(content.unwrap().id, msg_id.clone());\n\n    <|fim_middle|>", "completion": "let mut receiver_3 = TestReceiver::start(axum::http::StatusCode::OK);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_message.rs", "node_type": "let_declaration", "line_range": [267, 267]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse reqwest::StatusCode;\nuse serde_json::json;\nuse svix_server::{\n    core::types::{EndpointUid, MessageStatus},\n    v1::{\n        endpoints::{\n            attempt::{EndpointMessageOut, MessageAttemptOut},\n            endpoint::{EndpointIn, EndpointOut},\n        },\n        utils::ListResponse,\n    },\n};\nuse wiremock::{matchers, Mock, MockServer, Respond, ResponseTemplate};\n\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, create_test_msg_with,\n        endpoint_in, get_msg_attempt_list_and_assert_count,\n    },\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n    TestReceiver,\n};\n\n#[tokio::test]\nasync fn test_expunge_attempt_response_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let sensitive_response_json = serde_json::json!({\"sensitive\":\"data\"});\n    let mut receiver = TestReceiver::start_with_body(\n        axum::http::StatusCode::OK,\n        axum::Json(sensitive_response_json.clone()),\n    );\n\n    let endpoint_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap()\n        .id;\n\n    receiver.data_recv.recv().await;\n\n    let attempt = run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endpoint_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        if attempts.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", attempts.data.len());\n        }\n        Ok(attempts.data[0].clone())\n    })\n    .await\n    .unwrap();\n\n    let attempt_response: serde_json::Value = serde_json::from_str(&attempt.response).unwrap();\n    assert_eq!(sensitive_response_json, attempt_response);\n\n    let attempt_id = &attempt.id;\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let attempt: MessageAttemptOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\"EXPUNGED\", &attempt.response);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    l<|fim_suffix|>\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 <|fim_middle|>", "completion": "let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [128, 135]}
{"prompt": "<|fim_prefix|>ues such that an optional value in a model may be made `None` via\n/// PATCHing while allowing omitted fields to be skipped when updating.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredNullableField<T> {\n    #[default]\n    Absent,\n    None,\n    Some(T),\n}\n\n/// This enum is a non-nullable equivalent to [`UnrequiredNullableField`].\n///\n/// This is effectively an [`Option`] with the additional context that any field\n/// which uses this type is a member of a PATCH request model and that the field\n/// may be absent, meaning it is not to be updated. In comparison, [`Option`]s\n/// are used in other [`ModelIn`]s to define a field, that when absent, is\n/// `null`.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredField<T> {\n    #[default]\n    Absent,\n    Some(T),\n}\n\nimpl<T> UnrequiredNullableField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredNullableField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredNullableField<U> {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> UnrequiredField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredField<U> {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> From<Option<T>> for UnrequiredNullableField<T> {\n    fn from(opt: Option<T>) -> Self {\n        match opt {\n            Some(v) => UnrequiredNullableField::Some(v),\n            None => UnrequiredNullableField::None,\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredNullableField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n            UnrequiredNullableField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredField::Absent => Ok(()),\n            UnrequiredField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredNullableField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone + Copy> Copy for UnrequiredNullableField<T> {}\nimpl<T: Clone + Copy> Copy for UnrequiredField<T> {}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredNullableField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        Option::deserialize(deserializer).map(Into::into)\n    }\n}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        T::deserialize(deserializer).map(UnrequiredField::Some)\n    }\n}\n\nimpl<T> Serialize for UnrequiredNullableField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredNullableField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredNullableField must skip serializing if field is absent\",\n            )),\n            UnrequiredNullableField::None => serializer.serialize_none(),\n            UnrequiredNullableField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\ni<|fim_suffix|>\nimpl<T: JsonSchema> JsonSchema for UnrequiredField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"Unrequired_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<T>()\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredNullableField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"UnrequiredNullable_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<Option<T>>()\n    }\n}\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`Option`] is `Some`, but\n/// performs no operation in the case it is `None`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The nullable equivalent which is used for [`UnrequiredNullableField`] is [`patch_field_nullable`].\nmacro_rules! patch_field_non_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n}\npub(crate) use patch_field_non_nullable;\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`UnrequiredNullableField`] is\n/// `Some` and unsetting the field when it is `None`, but performs no operation in the case it is\n///  `Absent`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The non-nullable equivalent which is used for [`Option`] is [`patch_field_non_nullable`].\nmacro_rules! patch_field_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredNullableField::Some(v) => $model.$member = Set(Some(v)),\n            UnrequiredNullableField::None => $model.$member = Set(None),\n            UnrequiredNullableField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredNullableField::Some(v) => $model.$member = Set(Some(v)),\n            UnrequiredNullableField::None => $model.$member = Set(None),\n            UnrequiredNullableField::Absent => {}\n        }\n    };\n}\npub(crate) use patch_field_nullable;\n<|fim_middle|>", "completion": "impl<T> Serialize for UnrequiredField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredField must skip serializing if field is absent\",\n            )),\n            UnrequiredField::Some(v) => v.serialize(serializer),\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/patch.rs", "node_type": "impl_item", "line_range": [161, 176]}
{"prompt": "<|fim_prefix|>            ep,\n            StatusCode::CREATED,\n        )\n        .await\n}\n\npub async fn put_endpoint(\n    client: &TestClient,\n    app_id: &str,\n    ep_id: &str,\n    ep: EndpointIn,\n) -> Result<EndpointOut> {\n    client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n            ep,\n            StatusCode::OK,\n        )\n        .await\n}\n\n// Message\n\npub fn message_in<T: Serialize>(event_type: &str, payload: T) -> Result<MessageIn> {\n    Ok(MessageIn {\n        event_type: EventTypeName(event_type.to_owned()),\n        payload: RawPayload::from_string(serde_json::to_string(&payload)?)?,\n        payload_retention_period: 5,\n        channels: None,\n        uid: None,\n        extra_params: None,\n        application: None,\n    })\n}\n\npub async fn create_test_message(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n) -> Result<MessageOut> {\n    client\n        .post(\n            &format!(\"api/v1/app/{}/msg/\", &app_id),\n            message_in(\"event.type\", payload)?,\n            StatusCode::ACCEPTED,\n        )\n        .await\n}\n\npub async fn create_test_msg_with(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n    event_type: &str,\n    channel: impl IntoIterator<Item = &str>,\n) -> MessageOut {\n    let channels: HashSet<EventChannel> = channel\n        .into_iter()\n        .map(|x| EventChannel(x.to_string()))\n        .collect();\n\n    let mut message_in = json!({\n        \"eventType\": event_type,\n        \"payload\": payload,\n        \"payloadRetentionPeriod\": 5,\n    });\n    if !channels.is_empty() {\n        message_in[\"channels\"] = json!(channels);\n    }\n\n    client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in,\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap()\n}\n\npub fn event_type_in(\n    name: &str,\n    schema: impl Into<Option<serde_json::Value>>,\n) -> Result<EventTypeIn> {\n    Ok(EventTypeIn {\n        name: EventTypeName(name.to_owned()),\n        description: \"test-event-description\".to_owned(),\n        deleted: false,\n        schemas: schema.into().map(|s| serde_json::from_value(s).unwrap()),\n        feature_flag: None,\n        deprecated: false,\n    })\n}\n\n// Common tests\npub async fn common_test_list<\n    ModelOut: DeserializeOwned + PartialEq + std::fmt::Debug,\n    ModelIn: Serialize,\n>(\n    client: &TestClient,\n    path: &str,\n    create_model: fn(usize) -> ModelIn,\n    sort_asc: bool,\n    supports_reverse: bool,\n) -> Result<()> {\n    let mut items = Vec::new();\n    for i in 0..10 {\n        let item: ModelOut = client\n            .post(path, create_model(i), StatusCode::CREATED)\n            .await\n            .unwrap();\n        // Sleep for 5ms because KsuidMs has 4ms accuracy so things got out of order\n        tokio::time::sleep(Duration::from_millis(5)).await;\n        items.push(item);\n    }\n\n    let original_list = run_with_retries(|| async {\n        let list = client\n            .get::<ListResponse<ModelOut>>(&format!(\"{path}?with_content=true\"), StatusCode::OK)\n            .await\n            .unwrap();\n\n        assert_eq!(list.data.len(), 10);\n\n        Ok(list)\n    })\n    .await\n    .unwrap();\n\n    if sort_asc {\n        for i in 0..10 {\n            assert_eq!(items.get(i), original_list.data.get(i));\n        }\n    } else {\n        for i in 0..10 {\n            assert_eq!(items.get(9 - i), original_list.data.get(i));\n        }\n    }\n\n    // Limit results\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=1\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 1);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=50\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=10\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    l<|fim_suffix|>\n    assert_eq!(list.data.len(), 6);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=6&iterator={}\", list.iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 4);\n    assert!(list.done);\n\n    let prev = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=3&iterator={}\", list.prev_iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(prev.data.len(), 3);\n    assert_eq!(\n        prev.data.first().unwrap(),\n        original_list.data.get(3).unwrap()\n    );\n\n    let _list = client\n        .get::<IgnoredAny>(\n            &format!(\"{path}?limit=6&iterator=BAD-$$$ITERATOR\"),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    if supports_reverse {\n        let opposite_order = if sort_asc { \"descending\" } else { \"ascending\" };\n\n        let opposite_1 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\"{path}?limit=3&order={opposite_order}\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let opposite_2 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\n                    \"{path}?limit=3&order={opposite_order}&iterator={}\",\n                    opposite_1.iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let opposite_prev = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\n                    \"{path}?limit=3&order={opposite_order}&iterator={}\",\n                    opposite_2.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for i in 0..3 {\n            assert_eq!(original_list.data.get(9 - i), opposite_1.data.get(i));\n            assert_eq!(original_list.data.get(6 - i), opposite_2.data.get(i));\n            assert_eq!(original_list.data.get(9 - i), opposite_prev.data.get(i));\n        }\n    }\n\n    // Test limits -- ten models were created previously and the default hard/soft cap is 250 so\n    // 10..251 is the sane range here.\n    for i in 10..251 {\n        let _: ModelOut = client\n            .post(path, create_model(i), StatusCode::CREATED)\n            .await\n            .unwrap();\n    }\n\n    // If limits are hard, it will be a 422 UNPROCESSABLE_ENTITY response, otherwise it'll be capped\n    // to 250\n    if client\n        .get::<IgnoredAny>(\n            &format!(\"{path}?limit=300\"),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .is_ok()\n        || client\n            .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=300\"), StatusCode::OK)\n            .await\n            .unwrap()\n            .data\n            .len()\n            == 250\n    {\n        Ok(())\n    } else {\n        panic!(\"Error with soft/hard caps to pagination limits\")\n    }\n}\n\npub async fn recover_webhooks(client: &TestClient, since: DateTime<Utc>, url: &str) {\n    client\n        .post_without_response(url, RecoverIn { since, until: None }, StatusCode::ACCEPTED)\n        .await\n        .unwrap();\n}\n\npub async fn get_msg_attempt_list_and_assert_count(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    msg_id: &MessageId,\n    expected_count: usize,\n) -> Result<ListResponse<MessageAttemptOut>> {\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{msg_id}/\"),\n                StatusCode::OK,\n            )\n            .await?;\n\n        if list.data.len() != expected_count {\n            anyhow::bail!(\n                \"Attempt count {} does not match expected length {}\",\n                list.data.len(),\n                expected_count\n            );\n        }\n        Ok(list)\n    })\n    .await\n}\n\npub fn metadata(s: &str) -> Metadata {\n    serde_json::fro<|fim_middle|>", "completion": "let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=6\"), StatusCode::OK)\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/common_calls.rs", "node_type": "let_declaration", "line_range": [260, 263]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct IntegrationListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct IntegrationCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct IntegrationRotateKeyOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct Integration<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> Integration<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List the application's integrations.\n    pub async fn list(\n        &self,\n        app_id: String,\n        options: Option<IntegrationListOptions>,\n    ) -> Result<ListResponseIntegrationOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/integration\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create an integration.\n    pub async fn create(\n        &self,\n        app_id: String,\n        integration_in: IntegrationIn,\n        options: Option<IntegrationCreateOptions>,\n    ) -> Result<IntegrationOut> {\n        let IntegrationCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/app/{app_id}/integration\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(integration_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an integration.\n    pub async fn get(&self, app_id: String, integ_id: String) -> Result<IntegrationOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/integration/{integ_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Update an integration.\n    pub async fn update(\n        &self,\n        app_id: String,\n        integ_id: String,\n        integration_update: IntegrationUpdate,\n    ) -> Result<IntegrationOut> {\n        crate::request::Request::new(\n            http1::Method::PUT,\n            \"/api/v1/app/{app_id}/integration/{integ_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .with_body_param(integration_update)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Delete an integration.\n    pub async fn delete(&self, app_id: String, integ_id: String) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/app/{app_id}/integration/{integ_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get an integration's key.\n    #[deprecated]\n    pub async fn get_key(&self, app_id: String, integ_id: String) -> Result<IntegrationKeyOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/integration/{integ_id}/key\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Rotate the integration's key. The previous key will be immediately\n    /// revoked.\n    pub async fn rotate_key(\n        &self,\n        app_id: String,\n        integ_id: String,\n        options: Option<IntegrationRotateKeyOptions>,\n    ) -> Result<IntegrationKeyOut> {\n        let IntegrationRotateKeyOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/integration/{integ_id}/key/rotate\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "let IntegrationListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/integration.rs", "node_type": "let_declaration", "line_range": [41, 45]}
{"prompt": "<|fim_prefix|>use lapin::{\n    options::{BasicConsumeOptions, ExchangeDeclareOptions, QueueBindOptions, QueueDeclareOptions},\n    types::{AMQPValue, FieldTable},\n    ConnectionProperties,\n};\nuse omniqueue::backends::{RabbitMqBackend, RabbitMqConfig};\nuse svix_ksuid::{KsuidLike, KsuidMs};\n\nuse super::{TaskQueueConsumer, TaskQueueProducer};\nuse crate::error::{Result, Traceable};\n\n/// Returns a new_pair producers/consumers that use RabbitMQ under the hood.\n///\n/// USE WITH CAUTION - For the time being, this implementation has only been exercised in local development\n/// and testing environments. There may be production kinks that need working out\npub async fn new_pair(\n    dsn: &str,\n    queue_name: String,\n    prefetch_size: u16,\n) -> Result<(TaskQueueProducer, TaskQueueConsumer)> {\n    let conn = lapin::Connection::connect(dsn, ConnectionProperties::default()).await?;\n    let channel = conn.create_channel().await.unwrap();\n\n    let exchange_name = declare_delayed_message_exchange(&channel).await.trace()?;\n    declare_bound_queue(&queue_name, &exchange_name, &channel)\n        .await\n        .trace()?;\n\n    drop(channel);\n\n    // Ref https://www.rabbitmq.com/amqp-0-9-1-reference.html#basic.consume.consumer-tag\n    let consumer_tag = format!(\n        \"{queue_name}-consumer-{}\",\n        // prevent possible errors around duplicate consumer tags\n        KsuidMs::new(None, None).to_string()\n    );\n\n    let (producer, consumer) = RabbitMqBackend::builder(RabbitMqConfig {\n        uri: dsn.to_owned(),\n        connection_properties: Default::default(),\n        publish_exchange: \"first-message\".to_owned(),\n        publish_routing_key: queue_name.clone(),\n        publish_options: Default::default(),\n        publish_properties: Default::default(),\n        consume_queue: queue_name,\n        consumer_tag,\n        consume_options: BasicConsumeOptions {\n            // https://www.rabbitmq.com/amqp-0-9-1-reference.html#domain.no-local\n            // false because I don't care if the same connection reads and publishes to the same\n            // queue\n            no_local: false,\n\n            // https://www.rabbitmq.com/amqp-0-9-1-reference.html#domain.no-ack\n            // Obviously want message ACKs to ensure message are handled\n            no_ack: false,\n\n            // https://www.rabbitmq.com/amqp-0-9-1-reference.html#basic.consume.exclusive\n            // More than one worker should be able to read from the same queue\n            exclusive: false,\n\n            // https://www.rabbitmq.com/amqp-0-9-1-reference.html#domain.no-wait\n            // want the server to respond if there's a failure\n            nowait: false,\n        },\n        consume_arguments: Default::default(),\n        // Ref https://www.rabbitmq.com/amqp-0-9-1-reference.html#basic.qos.prefetch-size\n        //\n        // prefetch_size tells the consumer how many messages to load in batches from the queue.\n        // Higher values generally means better queue performance (fewer messages stuck in the\n        // queue), at the cost of consumer memory.\n        // Additionally, if the prefetch_size is *too* large, a single worker can \"starve\" other\n        // workers, potentially hurting total message throughput.\n        //\n        // \"global\" enforces the same limit for other consumers on the channel, which isn't\n        // necessarily what we want\n        consume_prefetch_count: Some(prefetch_size),\n        requeue_on_nack: false, // TODO\n    })\n    .build_pair()\n    .await\n    .expect(\"Error initializing rabbitmq queue\");\n\n    let producer = TaskQueueProducer::new(producer);\n    let consumer = TaskQueueConsumer::new(consumer);\n\n    Ok((producer, consumer))\n}\n\nasync fn declare_delayed_message_exchange(channel: &lapin::Channel) -> Result<String> {\n    let exchange_name = \"first-message\";\n\n    // See https://www.rabbitmq.com/amqp-0-9-1-reference.html#exchange.declare\n    let opts = ExchangeDeclareOptions {\n        // Want the server to create the exchange if it doesn't already exist\n        passive: false,\n        nowait: false,\n        // Want the exchange to survive restarts, and not be deleted even when the underlying queues are\n        // deleted\n        durable: true,\n        auto_delete: false,\n        internal: false,\n    };\n\n    // See https://github.com/rabbitmq/rabbitmq-delayed-message-exchange#usage\n    let mut args = FieldTable::default();\n    args.insert(\n        \"x-delayed-type\".into(),\n        AMQPValue::LongString(\"direct\".into()),\n    );\n\n    channel\n        .exchange_declare(\n            exchange_name,\n            lapin::ExchangeKind::Custom(\"x-delayed-message\".into()),\n            opts,\n            args,\n        )\n        .await?;\n\n    Ok(exchange_name.to_owned())\n}\n\nasync fn declare_bound_queue(\n    queue_name: &str,\n    exchange_name: &str,\n    channel: &lapin::Channel,\n) -> Result<()> {\n    // Ref https://www.rabbitmq.com/amqp-0-9-1-quickref.html#queue.declare\n    let opts = QueueDeclareOptions {\n        // If the queue already exists with the same configuration, we want the server to respond w/OK\n        // Alternatively, if the queue exists with a _different_ configuration, we want this to fail\n        passive: false,\n        nowait: false,\n\n        // We want to support multiple consumers per queue\n        exclusive: false,\n\n        // We want the queue to survive rust-primary restarts\n        durable: true,\n        auto_delete: false,\n    };\n\n    // Refs https://www.rabbitmq.com/maxlength.html#definition-using-x-args and https://www.rabbitmq.com/dlx.html#using-optional-queue-arguments\n    // We may want to figure out what the queue length enforcement looks like and dead letter queueing at a later point in time\n    let args = FieldTable::default();\n    channel.queue_declare(queue_name, opts, args).await?;\n\n    let routing_key = queue_name;\n\n    channel\n        .queue_bind(\n            queue_name,\n            exchange_name,\n            routing_key,\n            QueueBindOptions { nowait: false },\n            FieldTable::default(),\n        )\n        .await?;\n\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests {\n    use futures::StreamExt as _;\n    use lapin::options::{BasicConsumeOptions, BasicQosOptions};\n    use svix_ksuid::{KsuidLike as _, KsuidMs};\n\n    use crate::{cfg, queue::QueueTask};\n\n    #[tokio::test]\n    // run with `cargo test -- --ignored rabbitmq` only when rabbitmq is up and configured\n    #[ignore]\n    async fn test_messages_have_ids() {\n        const QUEUE_NAME: &str = \"test_messages_have_ids_q\";\n\n        let cfg = cfg::load().expect(\"Error loading configuration\");\n        let cfg::QueueBackend::RabbitMq(dsn) = cfg.queue_backend() else {\n            panic!(\"This test must only run when the rabbitmq backend is enabled\");\n        };\n        let prefetch_size = cfg.rabbit_consumer_prefetch_size.unwrap_or(1);\n\n        // Send message with omniqueue\n        {\n            let (producer, _) = super::new_pair(dsn, QUEUE_NAME.to_owned(), prefetch_size)\n                .await\n                .unwrap();\n\n            producer.send(&QueueTask::HealthCheck, None).await.unwrap();\n        }\n\n        // Receive with lapin consumer\n        {\n            let conn = lapin::Connection::connect(dsn, lapin::ConnectionProperties::default())\n                .await\n                .unwrap();\n            let channel = conn.create_channel().await.unwrap();\n\n            let consumer_tag = format!(\n                \"{QUEUE_NAME}-consumer-{}\",\n                KsuidMs::new(None, None).to_string()\n            );\n\n            <|fim_suffix|>\n\n            channel\n                .basic_qos(prefetch_size, BasicQosOptions { global: false })\n                .await\n                .unwrap();\n\n            let mut consumer = channel\n                .basic_consume(QUEUE_NAME, &consumer_tag, opts, Default::default())\n                .await\n                .unwrap();\n\n            let delivery = consumer.next().await.unwrap().unwrap();\n\n            // ... to assert that there is a message ID\n            delivery.properties.message_id().as_ref().unwrap();\n        }\n    }\n}\n<|fim_middle|>", "completion": "let opts = BasicConsumeOptions {\n                no_local: false,\n                no_ack: false,\n                exclusive: false,\n                nowait: false,\n            };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/rabbitmq.rs", "node_type": "let_declaration", "line_range": [204, 209]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::collections::HashSet;\n\nuse reqwest::StatusCode;\nuse serde::de::IgnoredAny;\nu<|fim_suffix|>\nuse crate::utils::{\n    common_calls::{app_portal_access, application_in, common_test_list, event_type_in},\n    start_svix_server,\n};\n\n#[tokio::test]\nasync fn test_patch() {\n    let (client, _jh) = start_svix_server().await;\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\n                \"test-event-type\",\n                serde_json::json!({\n                    \"1\": {\n                        \"type\": \"object\",\n                        \"title\": \"Longitude and Latitude Values\",\n                        \"description\": \"A geographical coordinate.\",\n                        \"required\": [\"latitude\", \"longitude\"],\n                        \"properties\": {\n                        \"latitude\": {\"type\": \"number\", \"minimum\": -90, \"maximum\": 90},\n                        \"longitude\": {\"type\": \"number\", \"minimum\": -180, \"maximum\": 180},\n                        },\n                    }\n                }),\n            )\n            .unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that PUT with invalid ID creates an event type\n    let _: EventTypeOut = client\n        .put(\n            \"api/v1/event-type/fake-id/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that description may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"description\": \"updated_description\",\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.schemas, et.schemas);\n\n    // Test that schemas may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": {},\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, Some(Schema::default()));\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that schemas may be unset while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": null,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, None);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that deleted may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"archived\": true,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert!(out.deleted);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.schemas, None);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n}\n\n#[tokio::test]\nasync fn test_event_type_create_read_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        client\n            .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n            .await\n            .unwrap(),\n        et\n    );\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/?with_content=true\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&et));\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&EventTypeOut {\n        schemas: None,\n        ..et\n    }));\n}\n\n#[tokio::test]\nasync fn test_event_type_feature_flags() {\n    let (client, _jh) = start_svix_server().await;\n\n    let feature = FeatureFlag(\"foo-feature\".into());\n    let another_feature = FeatureFlag(\"bar-feature\".into());\n    let (features, other_features, union) = {\n        let mut s1 = HashSet::new();\n        s1.insert(feature.clone());\n        let mut s2 = HashSet::new();\n        s2.insert(another_feature);\n        let union: FeatureFlagSet = s1.union(&s2).cloned().collect();\n\n        (s1, s2, union)\n    };\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"event-type-with-flag\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: Some(feature),\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"no-flag-event\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: None,\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let app: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let path = format!(\"api/v1/event-type/{}/\", et.name);\n\n    for (flag_set, should_see) in [\n        (FeatureFlagSet::default(), false),\n        (other_features, false),\n        (union.clone(), true),\n        (features.clone(), true),\n    ] {\n        let client = app_portal_access(&client, &app, flag_set).await;\n\n        let list: ListResponse<EventTypeOut> = client\n            .get(\"api/v1/event-type/\", StatusCode::OK)\n            .await\n            .unwrap();\n\n        if should_see {\n            // If the client is expected to see both event types it should be able to retrieve it\n            let got_et: EventTypeOut = client.get(&path, StatusCode::OK).await.unwrap();\n            assert_eq!(et, got_et);\n\n            // ... and see it in the list.\n            assert_eq!(list.data.len(), 2);\n            assert!(list.data.contains(&et));\n        } else {\n            // If the client is not supposed to see it it shouldn't be able to retrieve it\n            let _: IgnoredAny = client.get(&path, StatusCode::NOT_FOUND).await.unwrap();\n\n            // ... and it shouldn't be in the list.\n          <|fim_middle|>", "completion": "use svix_server::{\n    core::types::{ApplicationId, EventTypeName, FeatureFlag, FeatureFlagSet},\n    db::models::eventtype::Schema,\n    v1::{\n        endpoints::{\n            application::ApplicationOut,\n            event_type::{EventTypeIn, EventTypeOut},\n        },\n        utils::ListResponse,\n    },\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_event_type.rs", "node_type": "use_declaration", "line_range": [8, 18]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse axum::async_trait;\n\nuse super::{Cache, CacheBehavior, CacheKey, CacheValue, Result, StringCacheKey};\n\npub fn new() -> Cache {\n    tracing::warn!(\"Running with caching disabled will negatively affect performance. Idempotency is not supported without a cache.\");\n    NoCache {}.into()\n}\n\n#[derive(Clone)]\npub struct NoCache;\n\n#[async_trait]\nimpl CacheBehavior for NoCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get<T: CacheValue>(&self, _key: &T::Key) -> Result<Option<T>> {\n        Ok(None)\n    }\n\n    async fn get_raw(&self, _key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(None)\n    }\n\n    a<|fim_suffix|>\n    async fn set<T: CacheValue>(&self, _key: &T::Key, _value: &T, _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_raw(&self, _key: &[u8], _value: &[u8], _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_string<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<()> {\n        Ok(())\n    }\n\n    async fn delete<T: CacheKey>(&self, _key: &T) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_if_not_exists<T: CacheValue>(\n        &self,\n        _key: &T::Key,\n        _value: &T,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n\n    async fn set_raw_if_not_exists(\n        &self,\n        _key: &[u8],\n        _value: &[u8],\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n\n    async fn set_string_if_not_exists<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n}\n<|fim_middle|>", "completion": "async fn get_string<T: StringCacheKey>(&self, _key: &T) -> Result<Option<String>> {\n        Ok(None)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/none.rs", "node_type": "function_item", "line_range": [32, 34]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct EventTypeListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n\n    /// When `true` archived (deleted but not expunged) items are included in\n    /// the response.\n    pub include_archived: Option<bool>,\n\n    /// When `true` the full item (including the schema) is included in the\n    /// response.\n    pub with_content: Option<bool>,\n}\n\n#[derive(Default)]\npub struct EventTypeCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EventTypeImportOpenapiOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EventTypeDeleteOptions {\n    /// By default event types are archived when \"deleted\". Passing this to\n    /// `true` deletes them entirely.\n    pub expunge: Option<bool>,\n}\n\npub struct EventType<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> EventType<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// Return the list of event types.\n    pub async fn list(\n        &self,\n        options: Option<EventTypeListOptions>,\n    ) -> Result<ListResponseEventTypeOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/event-type\")\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .with_optional_query_param(\"include_archived\", include_archived)\n            .with_optional_query_param(\"with_content\", with_content)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create new or unarchive existing event type.\n    ///\n    /// Unarchiving an event type will allow endpoints to filter on it and\n    /// messages to be sent with it. Endpoints filtering on the event type\n    /// before archival will continue to filter on it. This operation does\n    /// not preserve the description and schemas.\n    pub async fn create(\n        &self,\n        event_type_in: EventTypeIn,\n        options: Option<EventTypeCreateOptions>,\n    ) -> Result<EventTypeOut> {\n        let EventTypeCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/event-type\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(event_type_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Given an OpenAPI spec, create new or update existing event types.\n    /// If an existing `archived` event type is updated, it will be unarchived.\n    ///\n    /// The importer will convert all webhooks found in the either the\n    /// `webhooks` or `x-webhooks` top-level.\n    pub async fn import_openapi(\n        &self,\n        event_type_import_open_api_in: EventTypeImportOpenApiIn,\n        options: Option<EventTypeImportOpenapiOptions>,\n    ) -> Result<EventTypeImportOpenApiOut> {\n        let EventTypeImportOpenapiOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/event-type/import/openapi\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(event_type_import_open_api_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an event type.\n    pub async fn get(&self, event_type_name: String) -> Result<EventTypeOut> {\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/event-type/{event_type_name}\")\n            .with_path_param(\"event_type_name\", event_type_name)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Update an event type.\n    pub async fn update(\n        &self,\n        event_type_name: String,\n        event_type_update: EventTypeUpdate,\n    ) -> Result<EventTypeOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/api/v1/event-type/{event_type_name}\")\n            .with_path_param(\"event_type_name\", event_type_name)\n            .with_body_param(event_type_update)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Archive an event type.\n    ///\n    /// Endpoints already configured to filter on an event type will continue to\n    /// do so after archival. However, new messages can not be sent with it\n    /// and endpoints can not filter on it. An event type can be unarchived\n    /// with the [create operation](#operation/\n    /// create_event_type_api_v1_event_type__post).\n    pub async fn delete(\n        &self,\n        event_type_name: String,\n        options: Option<EventTypeDeleteOptions>,\n    ) -> Result<()> {\n        let EventTypeDeleteOptions { expunge } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/event-type/{event_type_name}\",\n        )\n        .with_path_param(\"event_type_name\", event_type_name)\n        .with_optional_query_param(\"expunge\", expunge)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Partially update an event type.\n    pub async fn patch(\n        &self,\n        event_type_name: String,\n        event_type_patch: EventTypePatch,\n    ) -> Result<EventTypeOut> {\n        crate::request::Request::new(http1::Method::PATCH, \"/api/v1/event-type/{event_type_name}\")\n            .with_path_param(\"event_type_name\", event_type_name)\n            .with_body_param(event_type_patch)\n            .execute(self.cfg)\n            .await\n    }\n}\n<|fim_middle|>", "completion": "let EventTypeListOptions {\n            limit,\n            iterator,\n            order,\n            include_archived,\n            with_content,\n        } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/event_type.rs", "node_type": "let_declaration", "line_range": [55, 61]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{borrow::Cow, collections::HashMap, fmt, net::SocketAddr, sync::Arc, time::Duration};\n\nuse anyhow::{bail, Context};\nuse figment::{\n    providers::{Env, Format, Toml},\n    Figment,\n};\nu<|fim_suffix|>use serde::{Deserialize, Deserializer};\nuse tracing::Level;\nuse url::Url;\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::{cryptography::Encryption, security::JwtSigningConfig},\n    error::Result,\n    v1::utils::validation_error,\n};\n\nfn deserialize_main_secret<'de, D>(deserializer: D) -> Result<Encryption, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    // Derive a key so we get a key of the right size\n    let key = hmac_sha256::HMAC::mac(b\"main\", key.as_bytes());\n    Ok(Encryption::new(key))\n}\n\n#[derive(Deserialize)]\n#[serde(untagged)]\nenum RetryScheduleDeserializer {\n    Array(Vec<u64>),\n    Legacy(String),\n}\n\nfn deserialize_retry_schedule<'de, D>(deserializer: D) -> Result<Vec<Duration>, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let buf = RetryScheduleDeserializer::deserialize(deserializer)?;\n    match buf {\n        RetryScheduleDeserializer::Array(buf) => {\n            Ok(buf.into_iter().map(|x| Duration::new(x, 0)).collect())\n        }\n        RetryScheduleDeserializer::Legacy(buf) => Ok(buf\n            .split(',')\n            .filter_map(|x| {\n                let x = x.trim();\n                if x.is_empty() {\n                    None\n                } else {\n                    Some(Duration::new(x.parse().expect(\"Error parsing duration\"), 0))\n                }\n            })\n            .collect()),\n    }\n}\n\nfn deserialize_hours<'de, D>(deserializer: D) -> Result<Duration, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let hours = u64::deserialize(deserializer)?;\n    Ok(Duration::from_secs(60 * 60 * hours))\n}\n\nconst DEFAULTS: &str = include_str!(\"../config.default.toml\");\n\npub type Configuration = Arc<ConfigurationInner>;\n\nfn default_redis_pending_duration_secs() -> u64 {\n    45\n}\n\nfn validate_operational_webhook_url(url: &str) -> Result<(), ValidationError> {\n    match Url::parse(url) {\n        Ok(url) => {\n            // Verify scheme is http or https\n            if url.scheme() != \"http\" && url.scheme() != \"https\" {\n                return Err(validation_error(\n                    Some(\"operational_webhook_address\"),\n                    Some(\"URL scheme must be http or https\"),\n                ));\n            }\n\n            // Verify there's a host\n            if url.host().is_none() {\n                return Err(validation_error(\n                    Some(\"operational_webhook_address\"),\n                    Some(\"URL must include a valid host\"),\n                ));\n            }\n        }\n        Err(_) => {\n            return Err(validation_error(\n                Some(\"operational_webhook_address\"),\n                Some(\"Invalid URL format\"),\n            ));\n        }\n    }\n\n    Ok(())\n}\n\n#[derive(Clone, Debug, Deserialize, Validate)]\n#[validate(\n    schema(function = \"validate_config_complete\"),\n    skip_on_field_errors = false\n)]\npub struct ConfigurationInner {\n    /// The address to listen on\n    pub listen_address: SocketAddr,\n\n    /// The address to send operational webhooks to. When None, operational webhooks will not be\n    /// sent. When Some, the API server with the given URL will be used to send operational webhooks.\n    #[validate(custom = \"validate_operational_webhook_url\")]\n    pub operational_webhook_address: Option<String>,\n\n    /// The main secret used by Svix. Used for client-side encryption of sensitive data, etc.\n    /// IMPORTANT: Once set, it can't be changed.\n    #[serde(\n        rename = \"main_secret\",\n        deserialize_with = \"deserialize_main_secret\",\n        default\n    )]\n    pub encryption: Encryption,\n\n    /// Contains the secret and algorithm for signing JWTs\n    #[serde(flatten)]\n    pub jwt_signing_config: Arc<JwtSigningConfig>,\n\n    /// This determines the type of key that is generated for endpoint secrets by default (when none is set).\n    /// Supported: hmac256 (default), ed25519\n    /// Note: this does not affect existing keys, which will continue signing based on the type they were created with.\n    pub default_signature_type: DefaultSignatureType,\n\n    /// The log level to run the service with. Supported: info, debug, trace\n    pub log_level: LogLevel,\n    /// The log format that all output will follow. Supported: default, json\n    pub log_format: LogFormat,\n    /// The OpenTelemetry address to send events to if given.\n    pub opentelemetry_address: Option<String>,\n    /// The ratio at which to sample spans when sending to OpenTelemetry. When not given it defaults\n    /// to always sending. If the OpenTelemetry address is not set, this will do nothing.\n    pub opentelemetry_sample_ratio: Option<f64>,\n    /// The service name to use for OpenTelemetry. If not provided, it defaults to \"svix_server\".\n    pub opentelemetry_service_name: String,\n    /// Whether to enable the logging of the databases at the configured log level. This may be\n    /// useful for analyzing their response times.\n    pub db_tracing: bool,\n    /// The Sentry DSN to use for error reporting. If this is `None`,\n    /// then sentry reporting is disabled\n    pub sentry_dsn: Option<sentry::types::Dsn>,\n    /// The environment (dev, staging, or prod) that the server is running in.\n    pub environment: Environment,\n\n    /// The wanted retry schedule in seconds. Each value is the time to wait between retries.\n    #[serde(deserialize_with = \"deserialize_retry_schedule\")]\n    pub retry_schedule: Vec<Duration>,\n\n    /// The DSN for the database. Only postgres is currently supported.\n    pub db_dsn: String,\n    // The maximum number of connections for the PostgreSQL pool\n    #[validate(range(min = 10))]\n    pub db_pool_max_size: u16,\n\n    /// The DSN for redis (can be left empty if not using redis)\n    /// Note that if using Redis Sentinel, this will be the the DSN\n    /// for a Sentinel instance.\n    pub redis_dsn: Option<String>,\n    /// The maximum number of connections for the Redis pool\n    #[validate(range(min = 10))]\n    pub redis_pool_max_size: u16,\n\n    #[serde(flatten, default)]\n    pub redis_sentinel_cfg: Option<SentinelConfig>,\n\n    /// What kind of message queue to use. Supported: memory, redis (must have redis_dsn or\n    /// queue_dsn configured).\n    pub queue_type: QueueType,\n    /// The DSN for the Redis-backed queue. Overrides `redis_dsn`. (can be left empty if not using\n    /// redis)\n    pub queue_dsn: Option<String>,\n    /// Optional prefix to use for the queue.\n    ///\n    /// Allows multiple instances of svix-server to share a single queue backend\n    /// without interfering with each other.\n    pub queue_prefix: Option<String>,\n\n    /// What kind of cache to use. Supported: memory, redis (must have redis_dsn or cache_dsn\n    /// configured), none.\n    pub cache_type: CacheType,\n    /// The DSN for the Redis-backed cache. Overrides `redis_dsn`. (can be left empty if not using\n    /// redis)\n    pub cache_dsn: Option<String>,\n\n    /// If true, headers are prefixed with `Webhook-`, otherwise with `Svix-` (default).\n    pub whitelabel_headers: bool,\n\n    /// If true, only allow https endpoints, otherwise also allow http.\n    pub endpoint_https_only: bool,\n\n    /// How long to wait when making a request (in seconds)\n    #[validate(range(min = 1, max = 30))]\n    pub worker_request_timeout: u16,\n\n    /// How long of a period an endpoint must be consistently failing to be disabled. If a message\n    /// is successfully sent during this time, then the endpoint will not disable.\n    #[serde(deserialize_with = \"deserialize_hours\")]\n    pub endpoint_failure_disable_after: Duration,\n\n    // Execution mode\n    /// Should this instance run the API\n    pub api_enabled: bool,\n    /// Should this instance run the message worker\n    pub worker_enabled: bool,\n\n    /// Subnets to whitelist for outbound webhooks. Note that allowing endpoints in private IP space\n    /// is a security risk and should only be allowed if you are using the service internally or for\n    /// testing purposes. S<|fim_middle|>", "completion": "use ipnet::IpNet;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/cfg.rs", "node_type": "use_declaration", "line_range": [11, 11]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ShopifyConfigOut {}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl ShopifyConfigOut {\n    pub fn new() -> Self {\n        Self {}\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/shopify_config_out.rs", "node_type": "impl_item", "line_range": [7, 11]}
{"prompt": "<|fim_prefix|>e_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }\n\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket limit on future\n        .filter(sort_column.lt(I::start_id(future_limit)));\n\n    (query, iter_direction)\n}\n\n/// Marker trait for any type that is used for iterating through results\n/// in the public API.\npub trait IdIterator: Validate + Into<sea_orm::Value> {}\n\nimpl<T: BaseId + Validate + Into<sea_orm::Value>> IdIterator for T {}\nimpl IdIterator for EventTypeName {}\n\npub fn apply_pagination<\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    C: ColumnTrait,\n    I: IdIterator,\n>(\n    query: Q,\n    sort_column: C,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    ordering: Ordering,\n) -> Q {\n    use Ordering::*;\n    use ReversibleIterator::*;\n\n    // Query for an extra element to be able to tell whether there's more\n    // data than the user requested.\n    let query = query.limit(limit + 1);\n\n    let iterator = if let Some(it) = iterator {\n        it\n    } else {\n        return match ordering {\n            Ascending => query.order_by_asc(sort_column),\n            Descending => query.order_by_desc(sort_column),\n        };\n    };\n\n    match (iterator, ordering) {\n        (Prev(id), Ascending) | (Normal(id), Descending) => {\n            query.order_by_desc(sort_column).filter(sort_column.lt(id))\n        }\n        (Prev(id), Descending) | (Normal(id), Ascending) => {\n            query.order_by_asc(sort_column).filter(sort_column.gt(id))\n        }\n    }\n}\n\n/// A response with no body content and a specific response code, specified by\n/// the generic parameter `N`.\npub struct NoContentWithCode<const N: u16>;\n\nimpl<const N: u16> IntoResponse for NoContentWithCode<N> {\n    fn into_response(self) -> axum::response::Response {\n        (StatusCode::from_u16(N).unwrap(), ()).into_response()\n    }\n}\n\nimpl<const N: u16> OperationOutput for NoContentWithCode<N> {\n    type Inner = Self;\n\n    f<|fim_suffix|>\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(response) = Self::operation_response(ctx, operation) {\n            vec![(Some(N), response)]\n        } else {\n            vec![]\n        }\n    }\n}\n\n/// A response with no body content and HTTP status code 204, the standard code\n/// for such responses.\n#[derive(OperationIo)]\n#[aide(output_with = \"()\")]\npub struct NoContent;\n\nimpl IntoResponse for NoContent {\n    fn into_response(self) -> axum::response::Response {\n        NoContentWithCode::<204>::into_response(NoContentWithCode)\n    }\n}\n\n#[derive(Serialize, JsonSchema)]\npub struct EmptyResponse {}\n\n// If you change the internal representation of this then you must also update\n// it in the `JsonSchema` impl below to match.\n#[derive(Serialize, Deserialize, Clone)]\n#[serde(rename_all = \"camelCase\")]\npub struct ListResponse<T> {\n    pub data: Vec<T>,\n    pub iterator: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub prev_iterator: Option<String>,\n    pub done: bool,\n}\n\nimpl<T> ListResponse<T> {\n    pub fn empty() -> Self {\n        Self {\n            data: Vec::new(),\n            iterator: None,\n            prev_iterator: None,\n            done: true,\n        }\n    }\n}\n\n// This custom impl is needed because we want to customize the name of the\n// schema that goes into the spec, but that can only be done by having a custom\n// `JsonSchema` implementation.\n// Tracking issue: https://github.com/GREsau/schemars/issues/193\nimpl<T: JsonSchema> JsonSchema for ListResponse<T> {\n    fn schema_name() -> String {\n        let data_type_name = T::schema_name();\n        format!(\"ListResponse_{data_type_name}_\")\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        fn example_iterator() -> &'static str {\n            \"iterator\"\n        }\n\n        fn example_prev_iterator() -> &'static str {\n            \"-iterator\"\n        }\n\n        // The actual schema generation is still delegated to the derive macro.\n        #[derive(JsonSchema)]\n        #[allow(unused)]\n        #[serde(rename_all = \"camelCase\")]\n        struct ListResponse<T> {\n            pub data: Vec<T>,\n            #[schemars(example = \"example_iterator\")]\n            pub iterator: Option<String>,\n            #[serde(skip_serializing_if = \"Option::is_none\")]\n            #[schemars(example = \"example_prev_iterator\")]\n            pub prev_iterator: Option<String>,\n            pub done: bool,\n        }\n\n        ListResponse::<T>::json_schema(gen)\n    }\n}\n\npub trait ModelIn {\n    type ActiveModel;\n\n    fn update_model(self, model: &mut Self::ActiveModel);\n}\n\n/// Defines the ordering in a listing of results.\n#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub enum Ordering {\n    Ascending,\n    Descending,\n}\n\n#[derive(PartialEq, Eq)]\npub(crate) enum IteratorDirection {\n    /// Regular iteration - backwards in time.\n    Normal,\n    /// Reversed iteration - forwards in time.\n    Prev,\n}\n\nfn list_response_inner<T: ModelOut>(\n    mut data: Vec<T>,\n    limit: usize,\n    iter_direction: IteratorDirection,\n    supports_prev_iterator: bool,\n) -> ListResponse<T> {\n    // Our queries use a LIMIT of (limit + 1), so if there is more data than\n    // the user requested, `data.len()` is going to be larger than limit.\n    let done = data.len() <= limit;\n\n    // Drop the excess element(s). Should be only one.\n    data.truncate(limit);\n\n    // If iterating forwards in time, the query requests the DB rows in reverse\n    // to what the user actually wants.\n    if iter_direction == IteratorDirection::Prev {\n        data.reverse();\n    }\n\n    let prev_iterator = if supports_prev_iterator {\n        data.first().map(|x| format!(\"-{}\", x.id_copy()))\n    } else {\n        None\n    };\n    let iterator = data.last().map(|x| x.id_copy());\n\n    ListResponse {\n        data,\n        iterator,\n        prev_iterator,\n        done,\n    }\n}\n\npub<|fim_middle|>", "completion": "fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        <() as OperationOutput>::operation_response(ctx, operation)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "function_item", "line_range": [330, 335]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse js_option::JsOption;\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct EndpointPatch {\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub channels: JsOption<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub description: Option<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub disabled: Option<bool>,\n\n    #[serde(rename = \"filterTypes\")]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub filter_types: JsOption<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub metadata: Option<std::collections::HashMap<String, String>>,\n\n    #[serde(rename = \"rateLimit\")]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub rate_limit: JsOption<u16>,\n\n    /// The endpoint's verification secret.\n    ///\n    /// Format: `base64` encoded random bytes optionally prefixed with `whsec_`.\n    /// It is recommended to not set this and let the server generate the\n    /// secret.\n    #[deprecated]\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub secret: JsOption<String>,\n\n    /// The Endpoint's UID.\n    #[serde(default, skip_serializing_if = \"JsOption::is_undefined\")]\n    pub uid: JsOption<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub url: Option<String>,\n\n    #[deprecated]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub version: Option<u16>,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl EndpointPatch {\n    pub fn new() -> Self {\n        #[allow(deprecated)]\n        Self {\n            channels: JsOption::Undefined,\n            description: None,\n            disabled: None,\n            filter_types: JsOption::Undefined,\n            metadata: None,\n            rate_limit: JsOption::Undefined,\n            secret: JsOption::Undefined,\n            uid: JsOption::Undefined,\n            url: None,\n            version: None,\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/endpoint_patch.rs", "node_type": "impl_item", "line_range": [48, 64]}
{"prompt": "<|fim_prefix|>       \"signature\",\n        )?;\n        let msg_ts = Self::get_header(\n            headers,\n            SVIX_MSG_TIMESTAMP_KEY,\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            \"timestamp\",\n        )\n        .and_then(Self::parse_timestamp)?;\n\n        if enforce_tolerance {\n            Self::verify_timestamp(msg_ts)?;\n        }\n\n        let versioned_signature = self.sign(msg_id, msg_ts, payload)?;\n        let expected_signature = versioned_signature\n            .split_once(',')\n            .map(|x| x.1)\n            .ok_or(WebhookError::InvalidSignature)?;\n\n        msg_signature\n            .split(' ')\n            .filter_map(|x| x.split_once(','))\n            .filter(|x| x.0 == SIGNATURE_VERSION)\n            .any(|x| {\n                (x.1.len() == expected_signature.len())\n                    && (x\n                        .1\n                        .bytes()\n                        .zip(expected_signature.bytes())\n                        .fold(0, |acc, (a, b)| acc | (a ^ b))\n                        == 0)\n            })\n            .then_some(())\n            .ok_or(WebhookError::InvalidSignature)\n    }\n\n    pub fn sign(\n        &self,\n        msg_id: &str,\n        timestamp: i64,\n        payload: &[u8],\n    ) -> Result<String, WebhookError> {\n        let payload = std::str::from_utf8(payload).map_err(|_| WebhookError::InvalidPayload)?;\n        let to_sign = format!(\"{msg_id}.{timestamp}.{payload}\",);\n        let signed = hmac_sha256::HMAC::mac(to_sign.as_bytes(), &self.key);\n        let encoded = base64::encode(signed);\n\n        Ok(format!(\"{SIGNATURE_VERSION},{encoded}\"))\n    }\n\n    fn get_header<'a, HM: HeaderMap>(\n        headers: &'a HM,\n        svix_hdr: &'static str,\n        unbranded_hdr: &'static str,\n        err_name: &'static str,\n    ) -> Result<&'a str, WebhookError> {\n        use private::HeaderValueSealed as _;\n\n        headers\n            ._get(svix_hdr)\n            .or_else(|| headers._get(unbranded_hdr))\n            .ok_or(WebhookError::MissingHeader(err_name))?\n            ._to_str()\n            .ok_or(WebhookError::InvalidHeader(err_name))\n    }\n\n    fn parse_timestamp(hdr: &str) -> Result<i64, WebhookError> {\n        str::parse::<i64>(hdr).map_err(|_| WebhookError::InvalidTimestamp)\n    }\n\n    fn verify_timestamp(ts: i64) -> Result<(), WebhookError> {\n        let now = OffsetDateTime::now_utc().unix_timestamp();\n        if now - ts > TOLERANCE_IN_SECONDS {\n            Err(WebhookError::TimestampTooOldError)\n        } else if ts > now + TOLERANCE_IN_SECONDS {\n            Err(WebhookError::FutureTimestampError)\n        } else {\n            Ok(())\n        }\n    }\n}\n\n/// Trait to abstract over the `HeaderMap` types from both v0.2 and v1.0 of the\n/// `http` crate.\npub trait HeaderMap: private::HeaderMapSealed {}\n\nimpl HeaderMap for http02::HeaderMap {}\nimpl HeaderMap for http1::HeaderMap {}\n\nmod private {\n    pub trait HeaderMapSealed {\n        type HeaderValue: HeaderValueSealed;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue>;\n    }\n\n    impl HeaderMapSealed for http02::HeaderMap {\n        type HeaderValue = http02::HeaderValue;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue> {\n            self.get(name)\n        }\n    }\n    impl HeaderMapSealed for http1::HeaderMap {\n        type HeaderValue = http1::HeaderValue;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue> {\n            self.get(name)\n        }\n    }\n\n    pub trait HeaderValueSealed {\n        fn _to_str(&self) -> Option<&str>;\n    }\n\n    impl HeaderValueSealed for http02::HeaderValue {\n        fn _to_str(&self) -> Option<&str> {\n            self.to_str().ok()\n        }\n    }\n    impl HeaderValueSealed for http1::HeaderValue {\n        fn _to_str(&self) -> Option<&str> {\n            self.to_str().ok()\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use http02::HeaderMap;\n    use time::OffsetDateTime;\n\n    use super::{\n        Webhook, SVIX_MSG_ID_KEY, SVIX_MSG_SIGNATURE_KEY, SVIX_MSG_TIMESTAMP_KEY,\n        UNBRANDED_MSG_ID_KEY, UNBRANDED_MSG_SIGNATURE_KEY, UNBRANDED_MSG_TIMESTAMP_KEY,\n    };\n\n    f<|fim_suffix|>\n    fn get_unbranded_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(UNBRANDED_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    #[test]\n    fn test_sign() {\n        let wh = Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap();\n        assert_eq!(\n            \"v1,tZ1I4/hDygAJgO5TYxiSd6Sd0kDW6hPenDe+bTa3Kkw=\".to_owned(),\n            wh.sign(\n                \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\",\n                1649367553,\n                br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#\n            )\n            .unwrap()\n        );\n    }\n\n    #[test]\n    fn test_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            wh.verify(payload, &headers).unwrap();\n        }\n    }\n\n    #[test]\n    fn test_no_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = \"v1,R3PTzyfHASBKHH98a7yexTwaJ4yNIcGhFQc1yuN+BPU=\".to_owned();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_partial_signature() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        // Just `v1,`\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = format!(\n                \"{},\",\n                signature.split(',').collect::<Vec<&str>>().first().unwrap()\n            );\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n\n        // Non-empty but still partial signature (first few bytes)\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = &signature[0..8];\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_incorrect_timestamp() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        // Checks that timestamps that are in the future or too old are rejected by\n       <|fim_middle|>", "completion": "fn get_svix_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(SVIX_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(SVIX_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            SVIX_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/webhooks.rs", "node_type": "function_item", "line_range": [217, 230]}
{"prompt": "<|fim_prefix|>use std::{\n    collections::HashMap,\n    future::Future,\n    net::{IpAddr, Ipv4Addr, Ipv6Addr, SocketAddr},\n    pin::Pin,\n    str::FromStr,\n    sync::Arc,\n    task::{ready, Poll},\n    time::{Duration, Instant},\n};\n\nuse axum::{body::Body, response::Response};\nuse axum_extra::headers::{authorization::Credentials, Authorization};\nuse bytes::Bytes;\nuse futures::{future::BoxFuture, FutureExt};\nuse hickory_resolver::{lookup_ip::LookupIpIntoIter, ResolveError, Resolver, TokioResolver};\nuse http::{header::HeaderName, HeaderMap, HeaderValue, Method, StatusCode, Version};\n<|fim_suffix|>\nuse hyper::{body::Incoming, ext::HeaderCaseMap, Uri};\nuse hyper_openssl::client::legacy::{HttpsConnector, MaybeHttpsStream};\nuse hyper_util::{\n    client::{\n        legacy::{\n            connect::{\n                dns::Name,\n                proxy::{SocksV5, Tunnel},\n                HttpConnector,\n            },\n            Client,\n        },\n        proxy::matcher::Matcher,\n    },\n    rt::{TokioExecutor, TokioIo},\n};\nuse ipnet::IpNet;\nuse openssl::ssl::{SslConnector, SslConnectorBuilder, SslMethod, SslVerifyMode};\nuse serde::Serialize;\nuse thiserror::Error;\nuse tokio::{net::TcpStream, sync::Mutex};\nuse tower::Service;\n\nuse crate::cfg::{ProxyAddr, ProxyBypassCfg, ProxyConfig};\n\npub type CaseSensitiveHeaderMap = HashMap<String, HeaderValue>;\n\n#[derive(Debug, Error)]\npub enum Error {\n    #[error(\"failure response: {0}\")]\n    FailureStatus(StatusCode),\n\n    #[error(\"requests to this IP range are blocked (see the server configuration)\")]\n    BlockedIp,\n    #[error(\"error resolving name: {0}\")]\n    Resolve(#[from] ResolveError),\n\n    #[error(\"request timed out\")]\n    TimedOut,\n\n    #[error(\"error forming request: {0}\")]\n    InvalidHttpRequest(http::Error),\n    #[error(\"error making request: {0}\")]\n    FailedRequest(hyper_util::client::legacy::Error),\n}\n\nimpl From<hyper_util::client::legacy::Error> for Error {\n    fn from(e: hyper_util::client::legacy::Error) -> Self {\n        let mut dyn_e = &e as &dyn std::error::Error;\n        loop {\n            if dyn_e\n                .to_string()\n                .contains(\"requests to this IP range are blocked\")\n            {\n                return Error::BlockedIp;\n            }\n\n            match dyn_e.source() {\n                Some(source) => dyn_e = source,\n                None => return Error::FailedRequest(e),\n            }\n        }\n    }\n}\n\n#[derive(Clone)]\npub struct WebhookClient {\n    client: Client<SvixHttpsConnector, Full<Bytes>>,\n    whitelist_nets: Arc<Vec<IpNet>>,\n}\n\nfn ssl_builder(disable_tls_verification: bool) -> SslConnectorBuilder {\n    // Openssl is required here -- in practice, rustls does not support many\n    // ciphers that we encounter on a regular basis:\n    let mut ssl = SslConnector::builder(SslMethod::tls()).expect(\"SslConnector build failed\");\n    if disable_tls_verification {\n        ssl.set_verify(SslVerifyMode::NONE);\n    }\n    ssl\n}\n\nimpl WebhookClient {\n    pub fn new(\n        whitelist_nets: Option<Arc<Vec<IpNet>>>,\n        whitelist_names: Option<Arc<Vec<String>>>,\n        dangerous_disable_tls_verification: bool,\n        proxy_config: Option<&ProxyConfig>,\n    ) -> Self {\n        let whitelist_nets = whitelist_nets.unwrap_or_else(|| Arc::new(Vec::new()));\n        let whitelist_names = whitelist_names.unwrap_or_else(|| Arc::new(Vec::new()));\n\n        let dns_resolver = NonLocalDnsResolver::new(whitelist_nets.clone(), whitelist_names);\n        let mut http = HttpConnector::new_with_resolver(dns_resolver);\n        http.enforce_http(false);\n\n        if dangerous_disable_tls_verification {\n            tracing::warn!(\"TLS certificate verification has been disabled by the configuration.\");\n        }\n        let https = SvixHttpsConnector::new(http, proxy_config, dangerous_disable_tls_verification)\n            .expect(\"SvixHttpsConnector build failed\");\n\n        let client = hyper_util::client::legacy::Client::builder(TokioExecutor::new())\n            .http1_ignore_invalid_headers_in_responses(true)\n            .http1_title_case_headers(true)\n            .build(https);\n\n        Self {\n            client,\n            whitelist_nets,\n        }\n    }\n\n    pub async fn execute(&self, request: Request) -> Result<Response, Error> {\n        let resp = self.execute_inner(request, true).await?;\n        Ok(resp.map(Body::new))\n    }\n\n    pub fn execute_inner(\n        &self,\n        request: Request,\n        retry: bool,\n    ) -> BoxFuture<'_, Result<Response<Incoming>, Error>> {\n        async move {\n            let org_req = request.clone();\n            if let Some(auth) = request.uri.authority() {\n                if let Ok(ip) = auth.host().parse::<IpAddr>() {\n                    if !is_allowed(ip)\n                        && !self\n                            .whitelist_nets\n                            .iter()\n                            .any(|subnet| subnet.contains(&ip))\n                    {\n                        return Err(Error::BlockedIp);\n                    }\n                }\n            }\n\n            let mut req = if let Some(body) = request.body {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::from(body))\n                    .map_err(Error::InvalidHttpRequest)?\n            } else {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::default())\n                    .map_err(Error::InvalidHttpRequest)?\n            };\n\n            *req.headers_mut() = request.headers;\n\n            if let Some(header_names) = request.header_names {\n                req.extensions_mut().insert(header_names);\n            }\n\n            let start = Instant::now();\n            let res = if let Some(timeout) = request.timeout {\n                match tokio::time::timeout(timeout, self.client.request(req)).await {\n                    Ok(Ok(resp)) => Ok(resp),\n                    Ok(Err(e)) => Err(e.into()),\n                    Err(_to) => Err(Error::TimedOut),\n                }\n            } else {\n                self.client.request(req).await.map_err(Into::into)\n            };\n\n            if !retry {\n                return res;\n            }\n\n            match res {\n                Err(Error::FailedRequest(e)) if start.elapsed() < Duration::from_millis(1000) => {\n                    tracing::info!(\"Insta-retrying: {e}\");\n                    self.execute_inner(org_req, false).await\n                }\n                res => res,\n            }\n        }\n        .boxed()\n    }\n}\n\n#[derive(Clone)]\npub struct Request {\n    method: Method,\n    uri: Uri,\n    headers: HeaderMap,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    timeout: Option<Duration>,\n    version: Version,\n}\n\npub struct RequestBuilder {\n    method: Option<Method>,\n    uri: Option<Uri>,\n    accept: Option<HeaderValue>,\n    user_agent: Option<HeaderValue>,\n    headers: Option<HeaderMap>,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    version: Option<Version>,\n    timeout: Option<Duration>,\n    basic_auth: Option<Vec<u8>>,\n\n    // Derived from body\n    content_type: Option<HeaderValue>,\n}\n\n#[derive(Debug)]\npub struct RequestBuildError(pub Vec<BuildError>);\n\nimpl std::fmt::Display for RequestBuildError {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        let mut iter = self.0.iter();\n\n        f.write_str(\"Build failed\")?;\n\n        if let Some(first) = iter.next() {\n            write!(f, \": {first}\")?;\n\n            for err in iter {\n                write!(f, \"; {err}\")?;\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Error)]\npub enum BuildError {\n    #[error(\"uri missing\")]\n    UriMissing,\n    #[error(\"version missing\")]\n    VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub f<|fim_middle|>", "completion": "use http_body_util::Full;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "use_declaration", "line_range": [18, 18]}
{"prompt": "<|fim_prefix|>lf(v.0)\n            }\n        }\n\n        // Id or uid\n        #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\n        pub struct $name_id_or_uid(pub String);\n\n        common_jsonschema_impl!(\n            $name_id_or_uid,\n            $crate::core::types::StringSchema::schema_for_uids($key_prefix)\n        );\n\n        impl From<$name_id_or_uid> for $name_uid {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        impl From<$name_id_or_uid> for $name_id {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        impl From<$name_id_or_uid> for sea_orm::Value {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self::String(Some(Box::new(v.0)))\n            }\n        }\n\n        impl Validate for $name_id_or_uid {\n            fn validate(&self) -> Result<(), validator::ValidationErrors> {\n                validate_limited_str(&self.0)\n            }\n        }\n    };\n}\n\ncreate_id_type!(OrganizationId, \"org_\");\ncreate_id_type!(\n    MessageAttemptId,\n    \"atmpt_\",\n    crate::core::types::StringSchema {\n        string_validation: None,\n        example: Some(\"atmpt_1srOrx2ZWZBpBUvZwXKQmoEYga2\".to_string()),\n    }\n);\ncreate_id_type!(MessageEndpointId, \"msgep_\");\ncreate_id_type!(EventTypeId, \"evtype_\");\ncreate_id_type!(QueueBackgroundTaskId, \"qtask_\");\n\ncreate_all_id_types!(ApplicationId, ApplicationUid, ApplicationIdOrUid, \"app_\");\ncreate_all_id_types!(EndpointId, EndpointUid, EndpointIdOrUid, \"ep_\");\ncreate_all_id_types!(MessageId, MessageUid, MessageIdOrUid, \"msg_\");\n\nstring_wrapper!(\n    EventTypeName,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            max_length: Some(256),\n            min_length: None,\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"user.signup\".to_string()),\n    }\n);\n\nimpl Validate for EventTypeName {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        validate_limited_str(&self.0)\n    }\n}\n\nstring_wrapper!(\n    EventChannel,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            max_length: Some(128),\n            min_length: None,\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"project_1337\".to_string()),\n    }\n);\n\nimpl Validate for EventChannel {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        validate_limited_str(&self.0)\n    }\n}\n\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventChannelSet(pub HashSet<EventChannel>);\njson_wrapper!(EventChannelSet);\n\nimpl Validate for EventChannelSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventTypeNameSet(pub HashSet<EventTypeName>);\njson_wrapper!(EventTypeNameSet);\n\nimpl Validate for EventTypeNameSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\npub struct ExpiringSigningKeys(pub Vec<ExpiringSigningKey>);\njson_wrapper!(ExpiringSigningKeys);\n\nimpl ExpiringSigningKeys {\n    pub const MAX_OLD_KEYS: usize = 10;\n    pub const OLD_KEY_EXPIRY_HOURS: i64 = 24;\n}\n\n/// The type of encryption key\n#[repr(u8)]\n#[derive(Clone, Debug, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum EndpointSecretType {\n    Hmac256 = 1,\n    Ed25519 = 2,\n    // Reserved = 3,\n}\n\nimpl EndpointSecretType {\n    pub const fn secret_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whsk_\",\n        }\n    }\n\n    p<|fim_suffix|>}\n\n/// Properties of the encryption key\n#[derive(Clone, Debug, PartialEq, Eq)]\nstruct EndpointSecretMarker {\n    type_: EndpointSecretType,\n    encrypted: bool,\n}\n\nimpl EndpointSecretMarker {\n    const ENCRYPTED_FLAG: u8 = 0b1000_0000;\n\n    fn from_u8(v: u8) -> crate::error::Result<Self> {\n        let encrypted = (v & Self::ENCRYPTED_FLAG) != 0;\n        let v = v & !Self::ENCRYPTED_FLAG;\n        let type_ = EndpointSecretType::try_from(v)\n            .map_err(|_| crate::error::Error::generic(\"Invalid marker value\"))?;\n\n        Ok(Self { type_, encrypted })\n    }\n\n    fn to_u8(&self) -> u8 {\n        let mut ret = self.type_.clone().into();\n        if self.encrypted {\n            ret |= Self::ENCRYPTED_FLAG;\n        }\n        ret\n    }\n\n    fn type_(&self) -> &EndpointSecretType {\n        &self.type_\n    }\n}\n\n/// The internal representation of the endpoint secret.\n/// This is used to store it securely in the database and cache, and to ensure it doesn't get\n/// sent externally.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub struct EndpointSecretInternal {\n    marker: EndpointSecretMarker,\n\n    key: Vec<u8>,\n}\n\nimpl EndpointSecretInternal {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MINUS_ONE: usize = Self::KEY_SIZE - 1;\n\n    fn new(\n        encryption: &Encryption,\n        type_: EndpointSecretType,\n        key: &[u8],\n    ) -> crate::error::Result<Self> {\n        Ok(Self {\n            marker: EndpointSecretMarker {\n                type_,\n                encrypted: encryption.enabled(),\n            },\n            key: encryption.encrypt(key)?,\n        })\n    }\n\n    pub fn generate_symmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let buf: [u8; Self::KEY_SIZE] = rand::thread_rng().gen();\n        Self::new(encryption, EndpointSecretType::Hmac256, &buf)\n    }\n\n    pub fn generate_asymmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let key = AsymmetricKey::generate();\n        Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())\n    }\n\n    fn into_vec(mut self) -> Vec<u8> {\n        let marker: u8 = self.marker.to_u8();\n\n        let mut vec = vec![marker];\n        vec.append(&mut self.key);\n        vec\n    }\n\n    fn from_vec(v: Vec<u8>) -> crate::error::Result<Self> {\n        // Legacy had exact size\n        match v.len() {\n            0..=Self::KEY_SIZE_MINUS_ONE => Err(crate::error::Error::generic(\"Value too small\")),\n            Self::KEY_SIZE => Ok(Self {\n                marker: EndpointSecretMarker {\n                    type_: EndpointSecretType::Hmac256,\n                    encrypted: false,\n                },\n                key: v,\n            }),\n            _ => {\n                let marker = EndpointSecretMarker::from_u8(v[0])?;\n                Ok(Self {\n                    marker,\n                    key: v[1..].to_vec(),\n                })\n            }\n        }\n    }\n\n    pub fn into_endpoint_secret(\n        self,\n        encryption: &Encryption,\n    ) -> crate::error::Result<EndpointSecret> {\n        let key = self.key(encryption)?;\n        Ok(match self.type_() {\n            EndpointSecretType::Hmac256 => EndpointSecret::Symmetric(key),\n            EndpointSecretType::Ed25519 => {\n                EndpointSecret::Asymmetric(AsymmetricKey::from_slice(&key[..])?)\n            }\n        })\n    }\n\n    pub fn from_endpoint_secret(\n        endpoint_secret: EndpointSecret,\n        encryption: &Encryption,\n    ) -> crate::error::Result<Self> {\n        Ok(match endpoint_secret {\n            EndpointSecret::Symmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Hmac256, &key)?\n            }\n            EndpointSecret::Asymmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())?\n            }\n        })\n    }\n\n    pub fn sign(&self, encryption: &Encryption, by<|fim_middle|>", "completion": "pub const fn public_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whpk_\",\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "function_item", "line_range": [662, 667]}
{"prompt": "<|fim_prefix|>use std::{collections::HashMap, fmt::Debug};\n\nuse serde::de::DeserializeOwned;\nuse serde_json::json;\nuse svix::api::{\n    CronConfig, IngestSourceIn, IngestSourceInConfig, IngestSourceOut, IngestSourceOutConfig,\n    ListResponseApplicationOut, SegmentConfig, SegmentConfigOut, SvixConfig, SvixConfigOut,\n};\n\n#[test]\nfn test_list_response_xxx_out() {\n    // first test with iterator and prevIterator\n    let json_str =\n        r#\"{\"data\":[],\"done\":true,\"iterator\":\"iterator-str\",\"prevIterator\":\"prevIterator-str\"}\"#;\n    let loaded_json: ListResponseApplicationOut = serde_json::from_str(json_str).unwrap();\n\n    let expected_model = ListResponseApplicationOut {\n        data: vec![],\n        done: true,\n        iterator: Some(\"iterator-str\".to_string()),\n        prev_iterator: Some(\"prevIterator-str\".to_string()),\n    };\n\n    assert_eq!(expected_model, loaded_json);\n\n    // without iterator and prevIterator\n    let json_str = r#\"{\"data\":[],\"done\":true,\"iterator\":null,\"prevIterator\":null}\"#;\n    let loaded_json: ListResponseApplicationOut = serde_json::from_str(json_str).unwrap();\n\n    <|fim_suffix|>\n\n    assert_eq!(expected_model, loaded_json);\n}\n\n#[test]\nfn test_ingest_source_in() {\n    assert_eq!(\n        json!(IngestSourceIn {\n            name: \"foo\".to_owned(),\n            uid: None,\n            config: IngestSourceInConfig::GenericWebhook,\n            metadata: Some(HashMap::new())\n        }),\n        json!({\n            \"name\": \"foo\",\n            \"type\": \"generic-webhook\",\n            \"metadata\": {}\n        }),\n    );\n\n    assert_eq!(\n        json!(IngestSourceIn {\n            name: \"foo\".to_owned(),\n            uid: None,\n            config: IngestSourceInConfig::Svix(SvixConfig {\n                secret: \"xxx\".to_owned()\n            }),\n            metadata: Some(HashMap::new())\n        }),\n        json!({\n            \"name\": \"foo\",\n            \"type\": \"svix\",\n            \"config\": { \"secret\": \"xxx\" },\n            \"metadata\": {}\n        }),\n    );\n\n    assert_eq!(\n        json!(IngestSourceIn {\n            name: \"foo\".to_owned(),\n            uid: None,\n            config: IngestSourceInConfig::Segment(SegmentConfig { secret: None }),\n            metadata: Some(HashMap::new())\n        }),\n        json!({\n            \"name\": \"foo\",\n            \"type\": \"segment\",\n            \"config\": {},\n            \"metadata\": {}\n        }),\n    );\n\n    assert_eq!(\n        json!(IngestSourceIn {\n            name: \"foo\".to_owned(),\n            uid: None,\n            config: IngestSourceInConfig::Cron(CronConfig {\n                content_type: None,\n                payload: \"ð£\".to_owned(),\n                schedule: \"* * * * *\".to_owned(),\n            }),\n            metadata: Some(HashMap::new())\n        }),\n        json!({\n            \"name\": \"foo\",\n            \"type\": \"cron\",\n            \"config\": {\n                \"payload\": \"ð£\",\n                \"schedule\": \"* * * * *\",\n            },\n            \"metadata\": {}\n        }),\n    );\n}\n\n#[test]\nfn test_ingest_source_out() {\n    assert_deserializes_to(\n        json!({\n            \"id\": \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\",\n            \"createdAt\": \"2006-01-02T15:04:05Z\",\n            \"updatedAt\": \"2006-01-02T15:04:05Z\",\n            \"name\": \"foo\",\n            \"ingestUrl\": \"https://in.example.invalid/xyz\",\n            \"type\": \"generic-webhook\",\n            \"metadata\": {}\n        }),\n        IngestSourceOut {\n            created_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            id: \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\".to_owned(),\n            ingest_url: Some(\"https://in.example.invalid/xyz\".to_owned()),\n            name: \"foo\".to_owned(),\n            uid: None,\n            updated_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            config: IngestSourceOutConfig::GenericWebhook,\n            metadata: HashMap::new(),\n        },\n    );\n\n    assert_deserializes_to(\n        json!({\n            \"id\": \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\",\n            \"createdAt\": \"2006-01-02T15:04:05Z\",\n            \"updatedAt\": \"2006-01-02T15:04:05Z\",\n            \"name\": \"foo\",\n            \"ingestUrl\": \"https://in.example.invalid/xyz\",\n            \"type\": \"svix\",\n            \"config\": { \"secret\": \"xxx\" },\n            \"metadata\": {}\n        }),\n        IngestSourceOut {\n            created_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            id: \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\".to_owned(),\n            ingest_url: Some(\"https://in.example.invalid/xyz\".to_owned()),\n            name: \"foo\".to_owned(),\n            uid: None,\n            updated_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            config: IngestSourceOutConfig::Svix(SvixConfigOut {}),\n            metadata: HashMap::new(),\n        },\n    );\n\n    assert_deserializes_to(\n        json!({\n            \"id\": \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\",\n            \"createdAt\": \"2006-01-02T15:04:05Z\",\n            \"updatedAt\": \"2006-01-02T15:04:05Z\",\n            \"name\": \"foo\",\n            \"ingestUrl\": \"https://in.example.invalid/xyz\",\n            \"type\": \"segment\",\n            \"config\": {},\n            \"metadata\": {}\n        }),\n        IngestSourceOut {\n            created_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            id: \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\".to_owned(),\n            ingest_url: Some(\"https://in.example.invalid/xyz\".to_owned()),\n            name: \"foo\".to_owned(),\n            uid: None,\n            updated_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            config: IngestSourceOutConfig::Segment(SegmentConfigOut::default()),\n            metadata: HashMap::new(),\n        },\n    );\n\n    assert_deserializes_to(\n        json!({\n            \"id\": \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\",\n            \"createdAt\": \"2006-01-02T15:04:05Z\",\n            \"updatedAt\": \"2006-01-02T15:04:05Z\",\n            \"name\": \"foo\",\n            \"type\": \"cron\",\n            \"config\": {\n                \"payload\": \"ð£\",\n                \"schedule\": \"* * * * *\",\n            },\n            \"metadata\": {}\n        }),\n        IngestSourceOut {\n            created_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            id: \"Rjb52OFZK6aYPfF4EpqYqD8Ptcyr\".to_owned(),\n            ingest_url: None,\n            name: \"foo\".to_owned(),\n            uid: None,\n            updated_at: \"2006-01-02T15:04:05Z\".to_owned(),\n            config: IngestSourceOutConfig::Cron(CronConfig {\n                content_type: None,\n                payload: \"ð£\".to_owned(),\n                schedule: \"* * * * *\".to_owned(),\n            }),\n            metadata: HashMap::new(),\n        },\n    );\n}\n\nfn assert_deserializes_to<T: Debug + PartialEq + DeserializeOwned>(\n    value: serde_json::Value,\n    expected: T,\n) {\n    let actual = T::deserialize(value).unwrap();\n    assert_eq!(actual, expected);\n}\n<|fim_middle|>", "completion": "let expected_model = ListResponseApplicationOut {\n        data: vec![],\n        done: true,\n        ..Default::default()\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/tests/it/model_serialization.rs", "node_type": "let_declaration", "line_range": [30, 34]}
{"prompt": "<|fim_prefix|>{endp_id}/recover/\"),\n    )\n    .await;\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt)\n        .await\n        .unwrap();\n\n    // recovery time before msg -- should be 1 additional attempt\n    recover_webhooks(\n        &client,\n        before_msg,\n        &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/recover/\"),\n    )\n    .await;\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt + 1)\n        .await\n        .unwrap();\n\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_max() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp_id = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap()\n        .id;\n\n    for _ in 0..ExpiringSigningKeys::MAX_OLD_KEYS {\n        client\n            .post_without_response(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/secret/rotate/\"),\n                json!({ \"key\": null }),\n                StatusCode::NO_CONTENT,\n            )\n            .await\n            .unwrap();\n    }\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/secret/rotate/\"),\n            json!({ \"key\": null }),\n            StatusCode::BAD_REQUEST,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_signing_e2e() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let endp = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let secret1: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret2: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_ne!(secret1.key, secret2.key);\n\n    let secret3_key = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret3_key }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret3: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(secret3_key, secret3.key);\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret1, secret2, secret3] {\n        if let EndpointSecret::Symmetric(key) = &sec.key {\n            let sec = STANDARD.encode(key);\n            let wh = Webhook::new(&sec).unwrap();\n            wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_signing_symmetric_and_asymmetric() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    l<|fim_suffix|>    // Asymmetric key\n    let secret_2 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    // Long key\n    let secret_3 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n\n    let ep_in = EndpointIn {\n        url: Url::parse(&receiver.endpoint).unwrap(),\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\" }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Rotate back to symmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret_3.serialize_public_key() }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret_1, secret_2, secret_3] {\n        match sec {\n            EndpointSecret::Symmetric(key) => {\n                let sec = STANDARD.encode(key);\n                let wh = Webhook::new(&sec).unwrap();\n                wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n            }\n            EndpointSecret::Asymmetric(key) => {\n                let msg_id = last_headers.get(\"svix-id\").unwrap().to_str().unwrap();\n                let timestamp = last_headers\n                    .get(\"svix-timestamp\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let signatures = last_headers\n                    .get(\"svix-signature\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let to_sign = format!(\"{msg_id}.{timestamp}.{}\", &last_body);\n                let found =\n                    signatures\n                        .split(' ')\n                        .filter(|x| x.starts_with(\"v1a,\"))\n                        .any(|signature| {\n                            let sig: Signature = Signature::from_slice(\n                                STANDARD\n                                    .decode(&signature[\"v1a,\".len()..])\n                                    .unwrap()\n                                    .as_slice(),\n                            )\n                            .unwrap();\n                            key.0.pk.verify(to_sign.as_bytes(), &sig).is_ok()\n                        });\n                assert!(found);\n            }\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_config() {\n    let mut cfg = get_default_test_config();\n    cfg.default_signature_type = DefaultSignatureType::Ed25519;\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let key1 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key1.starts_with(\"whpk_\"));\n\n    // Rotate to asmmetric\n    client\n        .post_without_respo<|fim_middle|>", "completion": "let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1115, 1118]}
{"prompt": "<|fim_prefix|>use std::{sync::Arc, time::Duration};\n\nuse rdkafka::{\n    consumer::{Consumer, StreamConsumer},\n    ClientConfig, Message,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_kafka::{KafkaOutputOpts, KafkaProducer};\nuse svix_bridge_types::{ForwardRequest, ReceiverOutput as _};\n\n<|fim_suffix|>\n\n/// Time to wait for the consumer to be properly listening.\nconst LISTEN_WAIT_TIME: Duration = Duration::from_secs(8);\n\n#[tokio::test]\nasync fn test_produce_ok() {\n    let topic = unique_topic_name!();\n    let admin_client = kafka_admin_client();\n    create_topic(&admin_client, topic).await;\n\n    // Start listening for messages\n    let consumer: StreamConsumer = ClientConfig::new()\n        .set(\"bootstrap.servers\", BROKER_HOST)\n        .set(\"group.id\", \"svix_bridge_test_group_id\")\n        .create()\n        .unwrap();\n\n    consumer.subscribe(&[topic]).unwrap();\n\n    let consumer = Arc::new(consumer);\n    let recv_join_hdl = tokio::spawn({\n        let consumer = Arc::clone(&consumer);\n        async move { consumer.recv().await.unwrap().detach() }\n    });\n    tokio::time::sleep(LISTEN_WAIT_TIME).await;\n\n    let payload = json!({ \"test\": \"payload\" });\n    let payload_s = payload.to_string();\n\n    // Only then actually send a message\n    let producer = KafkaProducer::new(\n        \"test\".into(),\n        KafkaOutputOpts::Inner {\n            bootstrap_brokers: BROKER_HOST.to_owned(),\n            topic: topic.to_owned(),\n            security_protocol: svix_bridge_plugin_kafka::KafkaSecurityProtocol::Plaintext,\n            debug_contexts: None,\n        },\n    )\n    .unwrap();\n    producer.handle(ForwardRequest { payload }).await.unwrap();\n\n    // Assert that the message is received\n    let msg = recv_join_hdl.await.unwrap();\n    assert_eq!(msg.payload(), Some(payload_s.as_bytes()));\n\n    // Assert that no further messages are received in the next second\n    tokio::time::timeout(Duration::from_secs(1), consumer.recv())\n        .await\n        .expect_err(\"there must be no further messages\");\n\n    delete_topic(&admin_client, topic).await;\n}\n<|fim_middle|>", "completion": "use crate::{create_topic, delete_topic, kafka_admin_client, BROKER_HOST};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/tests/it/kafka_producer.rs", "node_type": "use_declaration", "line_range": [11, 11]}
{"prompt": "<|fim_prefix|>   let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt1 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test1\".to_owned()),\n            app_id: ApplicationId(\"test1\".to_owned()),\n            endpoint_id: EndpointId(\"test1\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Scheduled,\n            attempt_count: 0,\n        });\n        let mt2 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n\n        p.send(&mt1, Some(Duration::from_millis(2000)))\n            .await\n            .unwrap();\n        p.send(&mt2, None).await.unwrap();\n\n        let recv2 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv2.task, mt2);\n        recv2.ack().await.unwrap();\n\n        let recv1 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv1.task, mt1);\n        recv1.ack().await.unwrap();\n    }\n\n    fn to_redis_key(id: &str, task: &QueueTask) -> String {\n        format!(\"{id}|{}\", serde_json::to_string(task).unwrap())\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_migrations() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        // Test queue name constants\n        let v1_main = \"{test}_migrations_main_v1\";\n        let v2_main = \"{test}_migrations_main_v2\";\n        let v3_main = \"{test}_migrations_main_v3\";\n\n        let v1_processing = \"{test}_migrations_processing_v1\";\n        let v2_processing = \"{test}_migrations_processing_v2\";\n        // v3_processing is the stream pending queue for v3_main\n\n        let v1_delayed = \"{test}_migrations_delayed_v1\";\n        let v2_delayed = \"{test}_migrations_delayed_v2\";\n        let v2_delayed_lock = \"{test}_migrations_delayed_lock_v2\";\n        // v3_delayed doesn not yet exist\n\n        {\n            let mut conn = pool.get().await.unwrap();\n\n            // Clear test keys\n            let _: () = conn\n                .del(&[\n                    v1_main,\n                    v2_main,\n                    v3_main,\n                    v1_processing,\n                    v2_processing,\n                    v1_delayed,\n                    v2_delayed,\n                ])\n                .await\n                .unwrap();\n\n            // Add v3 consumer group\n            let _: () = conn\n                .xgroup_create_mkstream(v3_main, super::WORKERS_GROUP, 0i8)\n                .await\n                .unwrap();\n\n            // Add v1 data\n            for num in 1..=10 {\n                let _: () = conn\n                    .rpush(\n                        v1_main,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            for num in 11..=15 {\n                let _: () = conn\n                    .zadd(\n                        v1_delayed,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                        Utc::now().timestamp() + 2,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // Move the first five of v1_main to v1_processing\n            for _ in 0..5 {\n                let _: () = conn\n                    .blmove(\n                        v1_main,\n                        v1_processing,\n                        Direction::Left,\n                        Direction::Right,\n                        0.0,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // v1 to v2\n            migrate_list(&mut conn, v1_main, v2_main).await.unwrap();\n            migrate_list(&mut conn, v1_processing, v2_processing)\n                .await\n                .unwrap();\n            migrate_sset(&mut conn, v1_delayed, v2_delayed)\n                .await\n                .unwrap();\n\n            // v2 to v3\n            migrate_list_to_stream(&mut conn, v2_main, v3_main)\n                .await\n                .unwrap();\n            migrate_list_to_stream(&mut conn, v2_processing, v3_main)\n                .await\n                .unwrap();\n        }\n\n        // Read\n        let (_p, mut c) = new_pair_inner(\n            &cfg,\n            Duration::from_secs(5),\n            \"\",\n            v3_main,\n            v2_delayed,\n            v2_delayed_lock,\n            \"dlq-bruh\",\n        )\n        .await;\n\n        // 2 second delay on the delayed and pending queue is inserted after main queue, so first\n        // the 6-10 should appear, then 1-5, then 11-15\n\n        let mut items = c.receive_all(TEST_RECV_DEADLINE).await.unwrap();\n        while items.len() < 15 {\n            <|fim_suffix|>\n            assert!(!more_tasks.is_empty(), \"failed to receive all the tasks\");\n            items.extend(more_tasks);\n        }\n\n        let mut items = items.into_iter();\n        for num in 6..=10 {\n            let recv = items.next().unwrap();\n            assert_eq!(\n                &*recv.task,\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                    app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                })\n            );\n            recv.ack().await.unwrap();\n        }\n        for num in 1..=5 {\n            let recv = items.next().unwrap();\n            assert_eq!(\n                &*recv.task,\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                    app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                })\n            );\n            recv.ack().await.unwrap();\n        }\n        for num in 11..=15 {\n            let recv = items.next().unwrap();\n            assert_eq!(\n                &*recv.task,\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                    app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                })\n            );\n            recv.ack().await.unwrap();\n        }\n\n        if items.len() != 0 {\n            panic!(\"received more than the expected number of tasks, rest: {items:?}\");\n        }\n    }\n}\n<|fim_middle|>", "completion": "let more_tasks = c.receive_all(TEST_RECV_DEADLINE).await.unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [843, 843]}
{"prompt": "<|fim_prefix|>//! Use the `testing-docker-compose.yml` in the repo root to run the dependencies for testing,\n//! including the gcloud pubsub emulator.\n//!\n//! Use `run-tests.sh` to use the requisite environment for testing.\n\nuse std::time::Duration;\n\nuse gcloud_googleapis::pubsub::v1::{DeadLetterPolicy, PubsubMessage};\n<|fim_suffix|>\nuse serde_json::json;\nuse svix_bridge_plugin_queue::{\n    config::{GcpPubSubInputOpts, QueueInputOpts},\n    sender_input::QueueSender,\n};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nconst DEFAULT_PUBSUB_EMULATOR_HOST: &str = \"localhost:8085\";\n\nfn get_test_plugin(\n    svix_url: String,\n    subscription_id: String,\n    use_transformation: Option<TransformerInputFormat>,\n) -> QueueSender {\n    QueueSender::new(\n        \"test\".into(),\n        QueueInputOpts::GcpPubSub(GcpPubSubInputOpts {\n            subscription_id,\n            credentials_file: None,\n        }),\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n}\n\nasync fn mq_connection() -> Client {\n    // The `Default` impl for `ClientConfig` looks for this env var. When set it branches for\n    // local-mode use using the addr in the env var and a hardcoded project id of `local-project`.\n    if std::env::var(\"PUBSUB_EMULATOR_HOST\").is_err() {\n        std::env::set_var(\"PUBSUB_EMULATOR_HOST\", DEFAULT_PUBSUB_EMULATOR_HOST);\n    }\n    Client::new(ClientConfig::default()).await.unwrap()\n}\n\nfn random_chars() -> impl Iterator<Item = char> {\n    std::iter::repeat_with(fastrand::alphanumeric)\n}\n\nasync fn create_test_queue(client: &Client) -> (Topic, Subscription) {\n    let topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();\n    // Need to define a dead letter topic to avoid the \"bad\" test cases from pulling the nacked\n    // messages again and again.\n    let dead_letter_topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();\n    let subscription_name: String = \"subscription-\"\n        .chars()\n        .chain(random_chars().take(8))\n        .collect();\n\n    let topic = client.create_topic(&topic_name, None, None).await.unwrap();\n    let dead_letter_topic = client\n        .create_topic(&dead_letter_topic_name, None, None)\n        .await\n        .unwrap();\n    let subscription = client\n        .create_subscription(\n            &subscription_name,\n            &topic_name,\n            SubscriptionConfig {\n                // Messages published to the topic need to supply a unique ID to make use of this\n                enable_exactly_once_delivery: true,\n                dead_letter_policy: Some(DeadLetterPolicy {\n                    dead_letter_topic: dead_letter_topic.fully_qualified_name().into(),\n                    max_delivery_attempts: MAX_DELIVERY_ATTEMPTS,\n                }),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();\n\n    (topic, subscription)\n}\n\nasync fn publish(topic: &Topic, payload: &str) {\n    let publisher = topic.new_publisher(None);\n    let awaiter = publisher\n        .publish(PubsubMessage {\n            data: payload.to_owned().into_bytes(),\n            message_id: random_chars().take(6).collect(),\n            ..Default::default()\n        })\n        .await;\n    awaiter.get().await.unwrap();\n}\n\n/// General \"pause while we wait for messages to travel\" beat. If you're seeing flakes, bump this up.\nconst WAIT_MS: u64 = 100;\n/// Controls how many times a message can be nack'd before it lands on the dead letter topic.\nconst MAX_DELIVERY_ATTEMPTS: i32 = 5;\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), subscription.id(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&topic, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        // The transformed bit of the payload\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        subscription.id(),\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    <|fim_middle|>", "completion": "use gcloud_pubsub::{\n    client::{Client, ClientConfig},\n    subscription::{Subscription, SubscriptionConfig},\n    topic::Topic,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/gcp_pubsub_consumer.rs", "node_type": "use_declaration", "line_range": [9, 13]}
{"prompt": "<|fim_prefix|>use std::collections::HashMap;\n\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\n\nuse crate::json_wrapper;\n\npub const MAX_METADATA_SIZE: usize = 4096;\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Default, JsonSchema)]\n#[schemars(transparent)]\npub struct Metadata(HashMap<String, String>);\n\njson_wrapper!(Metadata);\n\nimpl Metadata {\n    pub fn is_empty(&self) -> bool {\n        self.0.is_empty()\n    }\n}\n\nimpl<'de> Deserialize<'de> for Metadata {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let inner: Option<HashMap<String, String>> = Deserialize::deserialize(deserializer)?;\n        let metadata = inner.unwrap_or_default(); // coerce `null` to `{}`\n\n        <|fim_suffix|>\n\n        if size > MAX_METADATA_SIZE {\n            return Err(serde::de::Error::custom(format!(\n                \"metadata must be less than or equal to {MAX_METADATA_SIZE} bytes\"\n            )));\n        }\n\n        Ok(Self(metadata))\n    }\n}\n<|fim_middle|>", "completion": "let size = serde_json::to_string(&metadata)\n            .map(|blob| blob.len())\n            .map_err(|_| serde::de::Error::custom(\"metadata is not valid json\"))?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/metadata.rs", "node_type": "let_declaration", "line_range": [30, 32]}
{"prompt": "<|fim_prefix|>\n        serde_json::json!({\"test\": \"data3\"}),\n        \"user.exploded\",\n        [\"obits\"],\n    )\n    .await;\n\n    // And wait at most one second for all attempts to be processed\n    run_with_retries(|| async {\n        for endp_id in [endp_id_1.clone(), endp_id_2.clone()] {\n            let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id}/\"),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_1: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_1}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    let list_2: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    for list in [list_1, list_2] {\n        let message_ids: Vec<_> = list.data.into_iter().map(|amo| amo.msg_id).collect();\n        assert!(message_ids.contains(&msg_1.id));\n        assert!(message_ids.contains(&msg_2.id));\n        assert!(message_ids.contains(&msg_3.id));\n    }\n\n    let foo_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?channel=foo\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert!(foo_attempts.data.is_empty());\n\n    let obits_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?channel=obits\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(obits_attempts.data.len(), 1);\n\n    let exploded_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types=user.exploded\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(exploded_attempts.data.len(), 1);\n\n    let regular_attempts: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types[]=event.type\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(regular_attempts.data.len(), 2);\n\n    let all_attempts_1: ListResponse<MessageAttemptOut> = client\n    .get(\n        &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types[0]=event.type&event_types[1]=user.exploded\"),\n        StatusCode::OK,\n    )\n    .await\n    .unwrap();\n    assert_eq!(all_attempts_1.data.len(), 3);\n\n    let all_attempts_2: ListResponse<MessageAttemptOut> = client\n    .get(\n        &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endp_id_2}/?event_types=event.type,user.exploded\"),\n        StatusCode::OK,\n    )\n    .await\n    .unwrap();\n    assert_eq!(all_attempts_2.data.len(), 3);\n\n    receiver_1.jh.abort();\n    receiver_2.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_message_attempts() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = (0..2).map(|_| Duration::from_millis(1)).collect();\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    for (status_code, msg_status, attempt_count) in [\n        // Success\n        (StatusCode::OK, MessageStatus::Success, Some(1)),\n        // HTTP 400\n        (StatusCode::FORBIDDEN, MessageStatus::Fail, None),\n        // HTTP 500\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None),\n    ] {\n        let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n        let receiver = TestReceiver::start(status_code);\n\n        let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n            .await\n            .unwrap()\n            .id;\n\n        l<|fim_suffix|>\n        let list = get_msg_attempt_list_and_assert_count(\n            &client,\n            &app_id,\n            &msg.id,\n            attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n        )\n        .await\n        .unwrap();\n\n        for i in list.data.iter() {\n            assert_eq!(i.status, msg_status);\n            println!(\"{} {status_code}\", i.response_status_code);\n            assert_eq!(\n                i.response_status_code,\n                TryInto::<i16>::try_into(status_code.as_u16()).unwrap()\n            );\n            assert_eq!(i.endpoint_id, endp_id);\n        }\n        receiver.jh.abort();\n    }\n\n    // non-HTTP-related failures:\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    // stop receiver before beginning tests:\n    receiver.jh.abort();\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        &cfg.retry_schedule.len() + 1,\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, MessageStatus::Fail);\n        assert_eq!(i.response_status_code, 0);\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n}\n\n#[tokio::test]\nasync fn test_message_attempts_empty_retry_schedule() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![];\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let (status_code, msg_status, attempt_count) =\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None);\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(status_code);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, msg_status);\n        println!(\"{} {status_code}\", i.response_status_code);\n        assert_eq!(\n            i.response_status_code,\n            TryInto::<i16>::try_into(status_code.as_u16()).unwrap()\n        );\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_combined_before_after_filtering() {\n    let (client, _) = start_svix_server().await;\n\n    let app = create_test_app(&client, \"test_app\").await.unwrap();\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ep = create_test_endpoint(&client, &app.id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    // Send a first message\n    create_test_message(\n        &client,\n        &app.id,\n        serde_json::json!({\n            \"test\": 1,\n        }),\n    )\n    .await\n    .unwrap();\n\n    // Wait until attempt was made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let ts1 = chrono::Utc::now();\n\n    // Send another two messages\n    for i in 1..=2 {\n        create_test_message(\n            &client,\n            &app.id,\n            serde_json::json!({\n                \"test\": i + 1,\n            }),\n        )\n        .await\n        .unwrap();\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n   <|fim_middle|>", "completion": "let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n            .await\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [747, 749]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nu<|fim_suffix|>\nuse axum::async_trait;\nuse tokio::{\n    sync::RwLock,\n    task,\n    time::{sleep, Duration, Instant},\n};\n\nuse super::{Cache, CacheBehavior, CacheKey, Result};\n\n#[derive(Debug)]\nstruct ValueWrapper {\n    value: Vec<u8>,\n    ttl: Duration,\n    timer: Instant,\n}\n\nimpl ValueWrapper {\n    fn new(value: Vec<u8>, ttl: Duration) -> ValueWrapper {\n        ValueWrapper {\n            value,\n            ttl,\n            timer: Instant::now(),\n        }\n    }\n}\n\ntype State = HashMap<Vec<u8>, ValueWrapper>;\ntype SharedState = Arc<RwLock<State>>;\n\npub fn new() -> Cache {\n    let shared_state = Arc::new(RwLock::new(State::new()));\n\n    let shared_state_clone = shared_state.clone();\n    task::spawn(async move {\n        loop {\n            sleep(Duration::from_secs(60 * 5)).await;\n            shared_state_clone\n                .write()\n                .await\n                .retain(|_, v| check_is_expired(v))\n        }\n    });\n\n    MemoryCache { map: shared_state }.into()\n}\n\n#[derive(Clone)]\npub struct MemoryCache {\n    map: SharedState,\n}\n\n#[async_trait]\nimpl CacheBehavior for MemoryCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(self\n            .map\n            .read()\n            .await\n            .get(key)\n            .filter(|wrapper| check_is_expired(wrapper))\n            .map(|wrapper| wrapper.value.clone()))\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()> {\n        self.map\n            .write()\n            .await\n            .insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n        Ok(())\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool> {\n        let mut lock = self.map.write().await;\n\n        // TODO: use HashMap::try_insert when stable\n        // https://github.com/rust-lang/rust/issues/82766\n        if !lock.contains_key(key) {\n            lock.insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n            return Ok(true);\n        }\n\n        Ok(false)\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()> {\n        self.map.write().await.remove(key.as_ref().as_bytes());\n\n        Ok(())\n    }\n}\n\nfn check_is_expired(vw: &ValueWrapper) -> bool {\n    vw.timer.elapsed().as_millis() <= vw.ttl.as_millis()\n}\n\n#[cfg(test)]\nmod tests {\n    use serde::{Deserialize, Serialize};\n\n    use super::{\n        super::{kv_def, CacheValue},\n        *,\n    };\n    use crate::core::cache::string_kv_def;\n\n    // Test structures\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValA(usize);\n    kv_def!(TestKeyA, TestValA);\n    impl TestKeyA {\n        fn new(id: String) -> TestKeyA {\n            TestKeyA(format!(\"SVIX_TEST_KEY_A_{id}\"))\n        }\n    }\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValB(String);\n    kv_def!(TestKeyB, TestValB);\n    impl TestKeyB {\n        fn new(id: String) -> TestKeyB {\n            TestKeyB(format!(\"SVIX_TEST_KEY_B_{id}\"))\n        }\n    }\n\n    string_kv_def!(StringTestKey);\n    impl StringTestKey {\n        fn new(id: String) -> StringTestKey {\n            StringTestKey(format!(\"SVIX_TEST_KEY_STRING_{id}\"))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_cache_crud_no_ttl() {\n        let cache = new();\n\n        let (first_key, first_val_a, first_val_b) =\n            (TestKeyA::new(\"1\".to_owned()), TestValA(1), TestValA(2));\n        let (second_key, second_val_a, second_val_b) = (\n            TestKeyB::new(\"1\".to_owned()),\n            TestValB(\"1\".to_owned()),\n            TestValB(\"2\".to_owned()),\n        );\n        let (third_key, third_val_a, third_val_b) = (\n            StringTestKey::new(\"1\".to_owned()),\n            \"1\".to_owned(),\n            \"2\".to_owned(),\n        );\n\n        // Create\n        assert!(cache\n            .set(&first_key, &first_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Read\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_a));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_a));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_a)\n        );\n\n        // Update (overwrite)\n        assert!(cache\n            .set(&first_key, &first_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Confirm update\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_b));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_b));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_b)\n        );\n\n        // Delete\n        assert!(cache.delete(&first_key).await.is_ok());\n        assert!(cache.delete(&second_key).await.is_ok());\n        assert!(cache.delete(&third_key).await.is_ok());\n\n        // Confirm deletion\n        assert_eq!(cache.get::<TestValA>(&first_key).await.unwrap(), None);\n        assert_eq!(cache.get::<TestValB>(&second_key).await.unwrap(), None);\n        assert_eq!(cache.get_string(&third_key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_ttl() {\n        let cache = new();\n        let key = TestKeyA::new(\"key\".to_owned());\n\n        assert!(cache\n            .set(&key, &TestValA(1), Duration::from_secs(1),)\n            .await\n            .is_ok());\n        tokio::time::sleep(std::time::Duration::from_millis(1200)).await;\n        assert_eq!(cache.get::<TestValA>(&key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_nx_status() {\n        let cache = new();\n        let key = TestKeyA::new(\"nx_status_test_key\".to_owned());\n\n        assert!(cache\n            .set_if_not_exists(&key, &TestValA(1), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(!cache\n            .set_if_not_exists(&key, &TestValA(2), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(cache.delete(&key).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "use std::{collections::HashMap, sync::Arc};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/memory.rs", "node_type": "use_declaration", "line_range": [4, 4]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{error, fmt, panic::Location};\n\nuse aide::OperationOutput;\nuse axum::{\n    extract::rejection::{ExtensionRejection, PathRejection},\n    response::{IntoResponse, Response},\n    Json,\n};\nuse hyper::StatusCode;\nuse schemars::JsonSchema;\nuse sea_orm::{DbErr, RuntimeErr, TransactionError};\nuse serde::Serialize;\nuse serde_json::json;\n\nuse crate::core::webhook_http_client;\n\n/// A short-hand version of a [`std::result::Result`] that defaults to Svix'es [Error].\npub type Result<T, E = Error> = std::result::Result<T, E>;\n\n/// The error type returned from the Svix API\n#[derive(Debug)]\npub struct Error {\n    // the file name and line number of the error. Used for debugging non Http errors\n    pub trace: Vec<&'static Location<'static>>,\n    pub typ: ErrorType,\n}\n\nimpl Error {\n    #[track_caller]\n    fn new(typ: ErrorType) -> Self {\n        let trace = vec![Location::caller()];\n        Self { trace, typ }\n    }\n\n    #[track_caller]\n    p<|fim_suffix|>\n    #[track_caller]\n    pub fn database(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Database(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn conflict(e: DbErr) -> Self {\n        Self::new(ErrorType::Conflict(e))\n    }\n\n    #[track_caller]\n    pub fn queue(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Queue(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn validation(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Validation(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn http(h: HttpError) -> Self {\n        Self {\n            trace: Vec::with_capacity(0), // no debugging necessary\n            typ: ErrorType::Http(h),\n        }\n    }\n\n    #[track_caller]\n    pub fn cache(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Cache(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Timeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn db_timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::DbTimeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn connection_timeout(e: DbErr) -> Self {\n        Self::new(ErrorType::ConnectionTimeout(e))\n    }\n\n    #[track_caller]\n    pub fn trace(mut self) -> Self {\n        self.trace.push(Location::caller());\n        self\n    }\n}\n\nimpl fmt::Display for Error {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        self.typ.fmt(f)\n    }\n}\n\nimpl error::Error for Error {\n    fn source(&self) -> Option<&(dyn error::Error + 'static)> {\n        None\n    }\n}\n\nimpl IntoResponse for Error {\n    fn into_response(self) -> Response {\n        let stringified: Vec<String> = self.trace.into_iter().map(ToString::to_string).collect();\n        match self.typ {\n            ErrorType::Http(s) => {\n                tracing::debug!(\"{:?}, location: {:?}\", &s, stringified);\n                s.into_response()\n            }\n            s => {\n                tracing::error!(\"type: {:?}, location: {:?}\", s, stringified);\n                (StatusCode::INTERNAL_SERVER_ERROR, Json(json!({}))).into_response()\n            }\n        }\n    }\n}\n\nimpl OperationOutput for Error {\n    type Inner = Self;\n}\n\npub trait Traceable<T> {\n    /// Pushes the current [`Location`] onto the error's trace stack\n    #[track_caller]\n    fn trace(self) -> Result<T>;\n}\n\nimpl<T> Traceable<T> for Result<T> {\n    fn trace(self) -> Result<T> {\n        // Using `map_err` would lose `#[track_caller]` information\n        match self {\n            Err(e) => Err(e.trace()),\n            ok => ok,\n        }\n    }\n}\n\nimpl From<DbErr> for Error {\n    #[track_caller]\n    fn from(err: DbErr) -> Self {\n        if is_timeout_error(&err) {\n            Error::db_timeout(err)\n        } else if is_conflict_err(&err) {\n            Error::conflict(err)\n        } else if is_connection_timeout_error(&err) {\n            Error::connection_timeout(err)\n        } else {\n            Error::database(err)\n        }\n    }\n}\n\nimpl From<redis::RedisError> for Error {\n    #[track_caller]\n    fn from(value: redis::RedisError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<omniqueue::QueueError> for Error {\n    #[track_caller]\n    fn from(value: omniqueue::QueueError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl<E: error::Error + 'static> From<bb8::RunError<E>> for Error {\n    #[track_caller]\n    fn from(value: bb8::RunError<E>) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<ExtensionRejection> for Error {\n    #[track_caller]\n    fn from(value: ExtensionRejection) -> Self {\n        Error::generic(value)\n    }\n}\n\nimpl From<PathRejection> for Error {\n    #[track_caller]\n    fn from(value: PathRejection) -> Self {\n        Error::generic(value)\n    }\n}\n\nimpl From<crate::core::cache::Error> for Error {\n    #[track_caller]\n    fn from(value: crate::core::cache::Error) -> Self {\n        Error::cache(value)\n    }\n}\n\nimpl From<TransactionError<Error>> for Error {\n    #[track_caller]\n    fn from(value: TransactionError<Error>) -> Self {\n        match value {\n            TransactionError::Connection(db_err) => Error::database(db_err),\n            TransactionError::Transaction(crate_err) => crate_err, // preserve the trace that comes from within the transaction\n        }\n    }\n}\n\nimpl From<lapin::Error> for Error {\n    #[track_caller]\n    fn from(value: lapin::Error) -> Self {\n        Error::queue(format_args!(\"{value:?}\"))\n    }\n}\n\n#[derive(Debug)]\npub enum ErrorType {\n    /// A generic error\n    Generic(String),\n    /// Database error\n    Database(String),\n    /// Queue error\n    Queue(String),\n    /// Database error\n    Validation(String),\n    /// Any kind of HttpError\n    Http(HttpError),\n    /// Cache error\n    Cache(String),\n    /// Timeout error\n    Timeout(String),\n    /// Database timeout error\n    DbTimeout(String),\n    /// Connection timeout error\n    ConnectionTimeout(DbErr),\n    /// Conflict error\n    Conflict(DbErr),\n}\n\nimpl fmt::Display for ErrorType {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Generic(s) => s.fmt(f),\n            Self::Database(s) => s.fmt(f),\n            Self::Queue(s) => s.fmt(f),\n            Self::Validation(s) => s.fmt(f),\n            Self::Http(s) => s.fmt(f),\n            Self::Cache(s) => s.fmt(f),\n            Self::Timeout(s) => s.fmt(f),\n            Self::DbTimeout(s) => s.fmt(f),\n            Self::ConnectionTimeout(s) => s.fmt(f),\n            Self::Conflict(s) => s.fmt(f),\n        }\n    }\n}\n\nimpl From<HttpError> for ErrorType {\n    fn from(e: HttpError) -> Self {\n        Self::Http(e)\n    }\n}\n\n// Python generation relies on the title of this being `HttpError`\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HttpErrorOut\", title = \"HttpError\")]\npub struct StandardHttpError {\n    code: String,\n    detail: String,\n}\n\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HTTPValidationError\")]\npub struct ValidationHttpError {\n    detail: Vec<ValidationErrorItem>,\n}\n\n#[derive(Debug, Clone, Serialize)]\n#[serde(untagged)]\npub enum HttpErrorBody {\n    Standard(StandardHttpError),\n    Validation(ValidationHttpError),\n}\n\n#[derive(Debug, Clone, Serialize, PartialEq, Eq, JsonSchema)]\n/// Validation errors have their own schema to provide context for invalid requests eg. mismatched\n/// types and out of bounds values. There may be any number of these per 422 UNPROCESSABLE ENTITY\n/// error.\npub struct ValidationErrorItem {\n    /// The location as a [`Vec`] of [`String`]s -- often in the form `[\"body\", \"field_name\"]`,\n    /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, detail: String) -> <|fim_middle|>", "completion": "pub fn generic(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Generic(s.to_string()))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "function_item", "line_range": [39, 41]}
{"prompt": "<|fim_prefix|>\"after\", after)\n        .with_optional_query_param(\"with_content\", with_content)\n        .with_optional_query_param(\"with_msg\", with_msg)\n        .with_optional_query_param(\"event_types\", event_types)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// List attempts by message ID.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days'\n    /// worth of data relative to now or, if an iterator is provided, 90\n    /// days before/after the time indicated by the iterator ID. If you\n    /// require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    pub async fn list_by_msg(\n        &self,\n        app_id: String,\n        msg_id: String,\n        options: Option<MessageAttemptListByMsgOptions>,\n    ) -> Result<ListResponseMessageAttemptOut> {\n        let MessageAttemptListByMsgOptions {\n            limit,\n            iterator,\n            status,\n            status_code_class,\n            channel,\n            tag,\n            endpoint_id,\n            before,\n            after,\n            with_content,\n            event_types,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/attempt/msg/{msg_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"msg_id\", msg_id)\n        .with_optional_query_param(\"limit\", limit)\n        .with_optional_query_param(\"iterator\", iterator)\n        .with_optional_query_param(\"status\", status)\n        .with_optional_query_param(\"status_code_class\", status_code_class)\n        .with_optional_query_param(\"channel\", channel)\n        .with_optional_query_param(\"tag\", tag)\n        .with_optional_query_param(\"endpoint_id\", endpoint_id)\n        .with_optional_query_param(\"before\", before)\n        .with_optional_query_param(\"after\", after)\n        .with_optional_query_param(\"with_content\", with_content)\n        .with_optional_query_param(\"event_types\", event_types)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// List messages for a particular endpoint. Additionally includes metadata\n    /// about the latest message attempt.\n    ///\n    /// The `before` parameter lets you filter all items created before a\n    /// certain date and is ignored if an iterator is passed.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days'\n    /// worth of data relative to now or, if an iterator is provided, 90\n    /// days before/after the time indicated by the iterator ID. If you\n    /// require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    pub async fn list_attempted_messages(\n        &self,\n        app_id: String,\n        endpoint_id: String,\n        options: Option<MessageAttemptListAttemptedMessagesOptions>,\n    ) -> Result<ListResponseEndpointMessageOut> {\n        let MessageAttemptListAttemptedMessagesOptions {\n            limit,\n            iterator,\n            channel,\n            tag,\n            status,\n            before,\n            after,\n            with_content,\n            event_types,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/endpoint/{endpoint_id}/msg\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_optional_query_param(\"limit\", limit)\n        .with_optional_query_param(\"iterator\", iterator)\n        .with_optional_query_param(\"channel\", channel)\n        .with_optional_query_param(\"tag\", tag)\n        .with_optional_query_param(\"status\", status)\n        .with_optional_query_param(\"before\", before)\n        .with_optional_query_param(\"after\", after)\n        .with_optional_query_param(\"with_content\", with_content)\n        .with_optional_query_param(\"event_types\", event_types)\n        .execute(self.cfg)\n        .await\n    }\n\n    #[deprecated = \"Use `list_by_msg` instead, setting the `endpoint_id` in `options`.\"]\n    pub async fn list_attempts_for_endpoint(\n        &self,\n        app_id: String,\n        msg_id: String,\n        endpoint_id: String,\n        options: Option<MessageAttemptListByMsgOptions>,\n    ) -> Result<ListResponseMessageAttemptEndpointOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/msg/{msg_id}/endpoint/{endpoint_id}/attempt\",\n        )\n        .with_optional_query_param(\"limit\", limit)\n        .with_optional_query_param(\"iterator\", iterator)\n        .with_optional_query_param(\"channel\", channel)\n        .with_optional_query_param(\"tag\", tag)\n        .with_optional_query_param(\"status\", status)\n        .with_optional_query_param(\"before\", before)\n        .with_optional_query_param(\"after\", after)\n        .with_optional_query_param(\"event_types\", event_types)\n        .with_path_param(\"app_id\", app_id.to_string())\n        .with_path_param(\"msg_id\", msg_id.to_string())\n        .with_path_param(\"endpoint_id\", endpoint_id.to_string())\n        .execute(self.cfg)\n        .await\n    }\n\n    /// `msg_id`: Use a message id or a message `eventId`\n    pub async fn get(\n        &self,\n        app_id: String,\n        msg_id: String,\n        attempt_id: String,\n    ) -> Result<MessageAttemptOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"msg_id\", msg_id)\n        .with_path_param(\"attempt_id\", attempt_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Deletes the given attempt's response body.\n    ///\n    /// Useful when an endpoint accidentally returned sensitive content.\n    /// The message can't be replayed or resent once its payload has been\n    /// deleted or expired.\n    pub async fn expunge_content(\n        &self,\n        app_id: String,\n        msg_id: String,\n        attempt_id: String,\n    ) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"msg_id\", msg_id)\n        .with_path_param(\"attempt_id\", attempt_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// List endpoints attempted by a given message.\n    ///\n    /// Additionally includes metadata about the latest message attempt.\n    /// By default, endpoints are listed in ascending order by ID.\n    pub async fn list_attempted_destinations(\n        &self,\n        app_id: String,\n        msg_id: String,\n        options: Option<MessageAttemptListAttemptedDestinationsOptions>,\n    ) -> Result<ListResponseMessageEndpointOut> {\n        let MessageAttemptListAttemptedDestinationsOptions { limit, iterator } =\n            options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/msg/{msg_id}/endpoint\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"msg_id\", msg_id)\n        .with_optional_query_param(\"limit\", limit)\n        .with_optional_query_param(\"iterator\", iterator)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Resend a message to the specified endpoint.\n    pub async fn resend(\n        &self,\n        app_id: String,\n        msg_id: String,\n        endpoint_id: String,\n        options: Option<MessageAttemptResendOptions>,\n    ) -> Result<EmptyResponse> {\n        let MessageAttemptResendOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/msg/{msg_id}/endpoint/{endpoint_id}/resend\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"msg_id\", msg_id)\n        .with_path_param(\"endpoint_id\", endpoint_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "let MessageAttemptListByMsgOptions {\n            iterator,\n            limit,\n            event_types,\n            before,\n            after,\n            channel,\n            tag,\n            status,\n            status_code_class: _,\n            endpoint_id: _,\n            with_content: _,\n        } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/message_attempt.rs", "node_type": "let_declaration", "line_range": [296, 308]}
{"prompt": "<|fim_prefix|>use rdkafka::{\n    error::KafkaError,\n    producer::{FutureProducer, FutureRecord},\n    util::Timeout,\n};\nuse svix_bridge_types::{async_trait, BoxError, ForwardRequest, ReceiverOutput};\n\nuse crate::config::KafkaOutputOpts;\n\n/// Forwards webhook payloads to kafka.\npub struct KafkaProducer {\n    name: String,\n    topic: String,\n    producer: FutureProducer,\n}\n\n<|fim_suffix|>\n\n#[async_trait]\nimpl ReceiverOutput for KafkaProducer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.producer\n            .send(\n                FutureRecord::<(), _>::to(&self.topic)\n                    .payload(&serde_json::to_vec(&request.payload)?),\n                Timeout::Never,\n            )\n            .await\n            .map_err(|(e, _msg)| e)?;\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "impl KafkaProducer {\n    pub fn new(name: String, opts: KafkaOutputOpts) -> Result<Self, KafkaError> {\n        let KafkaOutputOpts::Inner { topic, .. } = &opts;\n        let topic = topic.clone();\n        let producer = opts.create_producer()?;\n\n        Ok(Self {\n            name,\n            topic,\n            producer,\n        })\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/output.rs", "node_type": "impl_item", "line_range": [17, 29]}
{"prompt": "<|fim_prefix|>use std::{\n    str,\n    time::{Duration, Instant},\n};\n\nuse rdkafka::{\n    consumer::{CommitMode, Consumer as _},\n    error::KafkaError,\n    Message as _,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::api::{MessageCreateOptions, Svix},\n    CreateMessageRequest, JsObject, SenderInput, SenderOutputOpts, TransformationConfig,\n    TransformerInput, TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\n<|fim_suffix|>\n\nuse crate::{config::KafkaInputOpts, Error, Result};\n\npub struct KafkaConsumer {\n    name: String,\n    opts: KafkaInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n}\n\nimpl KafkaConsumer {\n    pub fn new(\n        name: String,\n        opts: KafkaInputOpts,\n        transformation: Option<TransformationConfig>,\n        output: SenderOutputOpts,\n    ) -> Result<Self> {\n        Ok(Self {\n            name,\n            transformation,\n            transformer_tx: None,\n            opts,\n            svix_client: match output {\n                SenderOutputOpts::Svix(output) => {\n                    Svix::new(output.token, output.options.map(Into::into))\n                }\n            },\n        })\n    }\n\n    #[tracing::instrument(skip_all)]\n    async fn process(&self, msg: &rdkafka::message::BorrowedMessage<'_>) -> Result<()> {\n        let payload = msg.payload().ok_or_else(|| Error::MissingPayload)?;\n        let payload = if let Some(transformation) = &self.transformation {\n            let input = match transformation.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload =\n                        serde_json::from_slice(payload).map_err(Error::Deserialization)?;\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    let raw_payload = str::from_utf8(payload).map_err(Error::NonUtf8Payload)?;\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n\n            let script = transformation.source().clone();\n            let object = self.transform(script, input).await?;\n            serde_json::from_value(serde_json::Value::Object(object))\n                .map_err(Error::Deserialization)?\n        } else {\n            serde_json::from_slice(payload).map_err(Error::Deserialization)?\n        };\n\n        let CreateMessageRequest { app_id, message } = payload;\n\n        let KafkaInputOpts::Inner {\n            group_id, topic, ..\n        } = &self.opts;\n\n        let options = MessageCreateOptions {\n            with_content: None,\n            // If committing the message fails or the process crashes after posting the webhook but\n            // before committing, this makes sure that the next run of this fn with the same kafka\n            // message doesn't end up creating a duplicate webhook in svix.\n            idempotency_key: Some(format!(\n                \"svix_bridge_kafka_{group_id}_{topic}_{}\",\n                msg.offset()\n            )),\n        };\n\n        self.svix_client\n            .message()\n            .create(app_id, message, Some(options))\n            .await?;\n\n        Ok(())\n    }\n\n    async fn transform(&self, script: String, input: TransformerInput) -> Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx\n            .as_ref()\n            .ok_or_else(|| Error::transformation(\"transformations not configured\"))?\n            .send(job)\n            .map_err(|e| Error::transformation(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::transformation(\"transformation rx failed\"))\n            .and_then(|x| {\n                x.map_err(|_e| Error::transformation(\"transformation execution failed\"))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => Err(Error::transformation(\n                \"transformation produced unexpected value\",\n            )),\n        }\n    }\n\n    async fn run_inner(&self) -> Result<()> {\n        let opts = self.opts.clone();\n        // `ClientConfig::create` does blocking I/O.\n        // Same for subscribe, most likely.\n        let consumer = spawn_blocking(move || {\n            let KafkaInputOpts::Inner { topic, .. } = &opts;\n            let topic = topic.clone();\n\n            let consumer = opts.create_consumer()?;\n            tracing::debug!(\"Created StreamConsumer\");\n\n            consumer.subscribe(&[&topic])?;\n            tracing::debug!(topic, \"Subscribed\");\n\n            Ok::<_, KafkaError>(consumer)\n        })\n        .await\n        .expect(\"create_consumer task panicked\")?;\n\n        loop {\n            // It's fine to pull messages one-by-one without any buffering in our own code because\n            // rdkafka buffers messages internally through a background task / thread.\n            let msg = consumer.recv().await?;\n            tracing::debug!(\"Received a message\");\n\n            let mut process_error_count = 0;\n            while let Err(e) = self.process(&msg).await {\n                match e {\n                    // If the payload is invalid, log an error and continue.\n                    // It would fail the same way if retried.\n                    Error::MissingPayload\n                    | Error::Deserialization(_)\n                    | Error::NonUtf8Payload(_) => {\n                        tracing::error!(error = &e as &dyn std::error::Error, \"invalid payload\");\n                        break;\n                    }\n\n                    // If the error is (possibly) transient, retry a few times.\n                    // After that, bubble up the error so it's logged at error level.\n                    Error::Kafka(_) | Error::SvixClient(_) | Error::Transformation { .. } => {\n                        process_error_count += 1;\n                        if process_error_count >= 3 {\n                            return Err(e);\n                        }\n\n                        tracing::warn!(\n                            error = &e as &dyn std::error::Error,\n                            \"failed to process payload from kafka\"\n                        );\n\n                        // retry\n                    }\n                }\n            }\n\n            // FIXME(jplatte): Should we introduce logic to only commit every N messages to reduce\n            // back and forth on the Kafka connection / disk writes inside Kafka?\n            //\n            // Background: messages in Kafka are not committed individually, rather what this call\n            // does is update the stored stream position for the consumer group.\n            consumer.commit_message(&msg, CommitMode::Async)?;\n        }\n    }\n}\n\n#[async_trait]\nimpl SenderInput for KafkaConsumer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        let mut fails: u64 = 0;\n        let mut last_fail = Instant::now();\n\n        let KafkaInputOpts::Inner { topic, .. } = &self.opts;\n        tracing::info!(topic, \"Starting to listen for messages\");\n\n        loop {\n            if let Err(e) = self.run_inner().await {\n                tracing::error!(\"{e}\");\n            }\n\n            if last_fail.elapsed() > Duration::from_secs(10) {\n                // reset the fail count if we didn't have a hiccup in the past short while.\n                tracing::trace!(\"been a while since last fail, resetting count\");\n                fails = 0;\n            } else {\n                fails += 1;\n            }\n\n            last_fail = Instant::now();\n            tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n        }\n    }\n}\n<|fim_middle|>", "completion": "use tokio::task::spawn_blocking;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/input.rs", "node_type": "use_declaration", "line_range": [17, 17]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct ApplicationListOptions {\n    /// Exclude applications that have no endpoints. Default is false.\n    #[arg(long)]\n    pub exclude_apps_with_no_endpoints: Option<bool>,\n    /// Exclude applications that have only disabled endpoints. Default is false.\n    #[arg(long)]\n    pub exclude_apps_with_disabled_endpoints: Option<bool>,\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n}\n\nimpl From<ApplicationListOptions> for svix::api::ApplicationListOptions {\n    fn from(value: ApplicationListOptions) -> Self {\n        let ApplicationListOptions {\n            exclude_apps_with_no_endpoints,\n            exclude_apps_with_disabled_endpoints,\n            limit,\n            iterator,\n            order,\n        } = value;\n        Self {\n            exclude_apps_with_no_endpoints,\n            exclude_apps_with_disabled_endpoints,\n            limit,\n            iterator,\n            order,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct ApplicationCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<ApplicationCreateOptions> for svix::api::ApplicationCreateOptions {\n    <|fim_suffix|>\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct ApplicationArgs {\n    #[command(subcommand)]\n    pub command: ApplicationCommands,\n}\n\n#[derive(Subcommand)]\npub enum ApplicationCommands {\n    /// List of all the organization's applications.\n    List {\n        #[clap(flatten)]\n        options: ApplicationListOptions,\n    },\n    /// Create a new application.\n    Create {\n        application_in: crate::json::JsonOf<ApplicationIn>,\n        #[clap(flatten)]\n        options: ApplicationCreateOptions,\n    },\n    /// Get an application.\n    Get { id: String },\n    /// Update an application.\n    Update {\n        id: String,\n        application_in: crate::json::JsonOf<ApplicationIn>,\n    },\n    /// Delete an application.\n    Delete { id: String },\n    /// Partially update an application.\n    Patch {\n        id: String,\n        application_patch: Option<crate::json::JsonOf<ApplicationPatch>>,\n    },\n}\n\nimpl ApplicationCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { options } => {\n                let resp = client.application().list(Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                application_in,\n                options,\n            } => {\n                let resp = client\n                    .application()\n                    .create(application_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { id } => {\n                let resp = client.application().get(id).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update { id, application_in } => {\n                let resp = client\n                    .application()\n                    .update(id, application_in.into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete { id } => {\n                client.application().delete(id).await?;\n            }\n            Self::Patch {\n                id,\n                application_patch,\n            } => {\n                let resp = client\n                    .application()\n                    .patch(id, application_patch.unwrap_or_default().into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "fn from(value: ApplicationCreateOptions) -> Self {\n        let ApplicationCreateOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/application.rs", "node_type": "function_item", "line_range": [50, 53]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct IngestSourceListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct IngestSourceCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct IngestSourceRotateTokenOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct IngestSource<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> IngestSource<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List of all the organization's Ingest Sources.\n    pub async fn list(\n        &self,\n        options: Option<IngestSourceListOptions>,\n    ) -> Result<ListResponseIngestSourceOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(http1::Method::GET, \"/ingest/api/v1/source\")\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create Ingest Source.\n    pub async fn create(\n        &self,\n        ingest_source_in: IngestSourceIn,\n        options: Option<IngestSourceCreateOptions>,\n    ) -> Result<IngestSourceOut> {\n        let IngestSourceCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/ingest/api/v1/source\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(ingest_source_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an Ingest Source by id or uid.\n    pub async fn get(&self, source_id: String) -> Result<IngestSourceOut> {\n        crate::request::Request::new(http1::Method::GET, \"/ingest/api/v1/source/{source_id}\")\n            .with_path_param(\"source_id\", source_id)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Update an Ingest Source.\n    pub async fn update(\n        &self,\n        source_id: String,\n        ingest_source_in: IngestSourceIn,\n    ) -> Result<IngestSourceOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/ingest/api/v1/source/{source_id}\")\n            .with_path_param(\"source_id\", source_id)\n            .with_body_param(ingest_source_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete an Ingest Source.\n    pub async fn delete(&self, source_id: String) -> Result<()> {\n        crate::request::Request::new(http1::Method::DELETE, \"/ingest/api/v1/source/{source_id}\")\n            .with_path_param(\"source_id\", source_id)\n            .returns_nothing()\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Rotate the Ingest Source's Url Token.\n    ///\n    /// This will rotate the ingest source's token, which is used to\n    /// construct the unique `ingestUrl` for the source. Previous tokens\n    /// will remain valid for 48 hours after rotation. The token can be\n    /// rotated a maximum of three times within the 48-hour period.\n    pub async fn rotate_token(\n        &self,\n        source_id: String,\n        options: Option<IngestSourceRotateTokenOptions>,\n    ) -> Result<RotateTokenOut> {\n        let IngestSourceRotateTokenOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/ingest/api/v1/source/{source_id}/token/rotate\",\n        )\n        .with_path_param(\"source_id\", source_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "let IngestSourceListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/ingest_source.rs", "node_type": "let_declaration", "line_range": [40, 44]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse std::fmt;\n\nuse serde::{Deserialize, Serialize};\n\n#[derive(\n    Clone, Copy, Debug, Default, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize,\n)]\npub enum BackgroundTaskStatus {\n    #[default]\n    #[serde(rename = \"running\")]\n    Running,\n    #[serde(rename = \"finished\")]\n    Finished,\n    #[serde(rename = \"failed\")]\n    Failed,\n}\n\nimpl fmt::Display for BackgroundTaskStatus {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        <|fim_suffix|>\n        f.write_str(value)\n    }\n}\n<|fim_middle|>", "completion": "let value = match self {\n            Self::Running => \"running\",\n            Self::Finished => \"finished\",\n            Self::Failed => \"failed\",\n        };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/background_task_status.rs", "node_type": "let_declaration", "line_range": [21, 25]}
{"prompt": "<|fim_prefix|>lidation on webhook dispatch. This is\n    /// a dangerous flag to set true. This value will default to false.\n    #[serde(default)]\n    pub dangerous_disable_tls_verification: bool,\n\n    /// Optional configuration for sending webhooks through a proxy.\n    #[serde(flatten)]\n    pub proxy_config: Option<ProxyConfig>,\n\n    #[serde(default = \"default_redis_pending_duration_secs\")]\n    pub redis_pending_duration_secs: u64,\n\n    #[serde(flatten)]\n    pub internal: InternalConfig,\n}\n\n#[derive(Clone, Debug, Deserialize)]\npub struct ProxyBypassCfg(pub String);\n\n#[derive(Clone, Debug, Deserialize)]\npub struct ProxyConfig {\n    /// Proxy address.\n    ///\n    /// Currently supported proxy types are:\n    /// - `socks5://`, i.e. a SOCKS5 proxy, with domain name resolution being\n    ///   done before the proxy gets involved\n    /// - `http://` or `https://` proxy, sending HTTP requests to the proxy;\n    ///   both HTTP and HTTPS targets are supported\n    #[serde(rename = \"proxy_addr\")]\n    pub addr: ProxyAddr,\n\n    #[serde(default)]\n    pub noproxy: Option<ProxyBypassCfg>,\n}\n\n#[derive(Clone, Debug)]\npub enum ProxyAddr {\n    /// A SOCKS5 proxy.\n    Socks5(http::Uri),\n    /// An HTTP / HTTPs proxy.\n    Http(http::Uri),\n}\n\nimpl ProxyAddr {\n    pub fn new(raw: impl Into<String>) -> Result<Self, Box<dyn std::error::Error>> {\n        let raw = raw.into();\n        let parsed: http::Uri = raw.parse()?;\n        match parsed.scheme_str().unwrap_or(\"\") {\n            \"socks5\" => Ok(Self::Socks5(parsed)),\n            \"http\" | \"https\" => Ok(Self::Http(parsed)),\n            _ => Err(\"Unsupported proxy scheme. \\\n                Supported schemes are `socks5://`, `http://` and `https://`.\"\n                .into()),\n        }\n    }\n}\n\nimpl<'de> Deserialize<'de> for ProxyAddr {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let raw = String::deserialize(deserializer)?;\n        Self::new(raw).map_err(serde::de::Error::custom)\n    }\n}\n\nfn validate_config_complete(config: &ConfigurationInner) -> Result<(), ValidationError> {\n    match config.cache_type {\n        CacheType::None | CacheType::Memory => {}\n        CacheType::Redis | CacheType::RedisCluster => {\n            if config.cache_dsn().is_none() {\n                return Err(ValidationError {\n                    code: Cow::from(\"missing field\"),\n                    message: Some(Cow::from(\n                        \"The redis_dsn or cache_dsn field must be set if the cache_type is `redis` or `rediscluster`\"\n                    )),\n                    params: HashMap::new(),\n                });\n            }\n        }\n        CacheType::RedisSentinel => {\n            if config.cache_dsn().is_none() {\n                return Err(ValidationError {\n                    code: Cow::from(\"missing field\"),\n                    message: Some(Cow::from(\n                        \"The redis_dsn or cache_dsn field must be set if the cache_type is `redissentinel`\"\n                    )),\n                    params: HashMap::new(),\n                });\n            }\n\n            if config.redis_sentinel_cfg.is_none() {\n                return Err(ValidationError {\n                    code: Cow::from(\"missing field\"),\n                    message: Some(Cow::from(\n                        \"sentinel_service_name must be set if the cache_type is `redissentinel`\",\n                    )),\n                    params: HashMap::new(),\n                });\n            }\n        }\n    }\n\n    match config.queue_type {\n        QueueType::Memory => {}\n        QueueType::Redis | QueueType::RedisCluster => {\n            if config.queue_dsn().is_none() {\n                return Err(ValidationError {\n                    code: Cow::from(\"missing field\"),\n                    message: Some(Cow::from(\n                        \"The redis_dsn or queue_dsn field must be set if the queue_type is `redis` or `rediscluster`\"\n                    )),\n                    params: HashMap::new(),\n                });\n            }\n        }\n        QueueType::RabbitMQ => {\n            i<|fim_suffix|>        }\n        QueueType::RedisSentinel => {\n            if config.queue_dsn().is_none() {\n                return Err(ValidationError {\n                    code: Cow::from(\"missing field\"),\n                    message: Some(Cow::from(\n                        \"The redis_dsn or queue_dsn field must be set if the queue_type is `redissentinel`\"\n                    )),\n                    params: HashMap::new(),\n                });\n            }\n\n            if config.redis_sentinel_cfg.is_none() {\n                return Err(ValidationError {\n                    code: Cow::from(\"missing field\"),\n                    message: Some(Cow::from(\n                        \"sentinel_service_name must be set if the queue_type is `redissentinel`\",\n                    )),\n                    params: HashMap::new(),\n                });\n            }\n        }\n    }\n\n    Ok(())\n}\n\nimpl ConfigurationInner {\n    pub(self) fn queue_dsn(&self) -> Option<&str> {\n        self.queue_dsn.as_deref().or(self.redis_dsn.as_deref())\n    }\n\n    pub(self) fn cache_dsn(&self) -> Option<&str> {\n        self.cache_dsn.as_deref().or(self.redis_dsn.as_deref())\n    }\n\n    /// Fetches the configured backend information for the queue. May panic is the configuration has\n    /// not been validated\n    pub fn queue_backend(&self) -> QueueBackend<'_> {\n        let err = \"Called [`queue_backend`] before validating configuration\";\n\n        match self.queue_type {\n            QueueType::Memory => QueueBackend::Memory,\n            QueueType::Redis => QueueBackend::Redis(self.queue_dsn().expect(err)),\n            QueueType::RedisCluster => QueueBackend::RedisCluster(self.queue_dsn().expect(err)),\n            QueueType::RedisSentinel => QueueBackend::RedisSentinel(\n                self.queue_dsn().expect(err),\n                self.redis_sentinel_cfg.as_ref().expect(err),\n            ),\n            QueueType::RabbitMQ => QueueBackend::RabbitMq(self.rabbit_dsn.as_ref().expect(err)),\n        }\n    }\n\n    /// Fetches the configured backend information for the cache, or `None` if the [`CacheType`] is\n    ///  `None`. May panic is the configuration has not been validated\n    pub fn cache_backend(&self) -> CacheBackend<'_> {\n        let err = \"Called [`cache_backend`] before validating configuration\";\n\n        match self.cache_type {\n            CacheType::None => CacheBackend::None,\n            CacheType::Memory => CacheBackend::Memory,\n            CacheType::Redis => CacheBackend::Redis(self.cache_dsn().expect(err)),\n            CacheType::RedisCluster => CacheBackend::RedisCluster(self.cache_dsn().expect(err)),\n            CacheType::RedisSentinel => CacheBackend::RedisSentinel(\n                self.cache_dsn().expect(err),\n                self.redis_sentinel_cfg.as_ref().expect(err),\n            ),\n        }\n    }\n}\n\n#[derive(Clone, Debug, Deserialize)]\npub struct InternalConfig {\n    /// The region to use in the Svix URL given in th dashboard access endpoint\n    #[serde(default = \"default_region\")]\n    pub region: String,\n\n    /// The base url to use for the app portal\n    #[serde(default = \"default_app_portal_url\")]\n    pub app_portal_url: String,\n}\n\nfn default_region() -> String {\n    \"self_hosted\".to_owned()\n}\n\nfn default_app_portal_url() -> String {\n    \"https://app.svix.com\".to_owned()\n}\n\n#[derive(Debug, Eq, PartialEq)]\npub enum QueueBackend<'a> {\n    Memory,\n    Redis(&'a str),\n    RedisCluster(&'a str),\n    RedisSentinel(&'a str, &'a SentinelConfig),\n    RabbitMq(&'a str),\n}\n\n#[derive(Debug, Eq, PartialEq)]\npub enum CacheBackend<'a> {\n    None,\n    Memory,\n    Redis(&'a str),\n    RedisCluster(&'a str),\n    RedisSentinel(&'a str, &'a SentinelConfig),\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogLevel {\n    Info,\n    Debug,\n    Trace,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogFormat {\n    Default,\n    Json,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum QueueType {\n    Memory,\n    Redis,\n    RedisCluster,\n    RedisSentinel,\n    RabbitMQ,\n}\n\n#[de<|fim_middle|>", "completion": "if config.rabbit_dsn.is_none() {\n                return Err(ValidationError {\n                    code: Cow::from(\"missing field\"),\n                    message: Some(Cow::from(\n                        \"The rabbit_dsn field must be set if the queue_type is `rabbitmq`\",\n                    )),\n                    params: HashMap::new(),\n                });\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/cfg.rs", "node_type": "if_expression", "line_range": [355, 363]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse aide::axum::{\n    routing::{get, get_with},\n    ApiRouter,\n};\nuse axum::{extract::State, http::StatusCode, Json};\nuse sea_orm::{query::Statement, ConnectionTrait, DatabaseBackend};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::aide_annotate;\n\nuse crate::{\n    core::cache::{kv_def, CacheBehavior, CacheKey, CacheValue},\n    queue::QueueTask,\n    v1::utils::{openapi_tag, NoContent},\n    AppState,\n};\n\nasync fn ping() -> NoContent {\n    NoContent\n}\n\n#[derive(Debug, Deserialize, Serialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum HealthStatusVariant {\n    Ok,\n    Error,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthStatus {\n    status: HealthStatusVariant,\n    // TODO: information field\n}\n\nimpl HealthStatus {\n    pub fn new_ok() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Ok,\n        }\n    }\n\n    pub fn new_error() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Error,\n        }\n    }\n\n    pub fn is_ok(&self) -> bool {\n        matches!(\n            self,\n            HealthStatus {\n                status: HealthStatusVariant::Ok,\n                ..\n            }\n        )\n    }\n}\nimpl<O, E> From<Result<O, E>> for HealthStatus {\n    fn from(res: Result<O, E>) -> Self {\n        match res {\n            Ok(_) => HealthStatus::new_ok(),\n            Err(_) => HealthStatus::new_error(),\n        }\n    }\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthReport {\n    database: HealthStatus,\n\n    queue: HealthStatus,\n    cache: HealthStatus,\n}\n\n#[derive(Deserialize, Serialize, Debug, PartialEq)]\nstruct HealthCheckCacheValue(());\nkv_def!(HealthCheckCacheKey, HealthCheckCacheValue);\n\n/// Verify the API server is up and running.\n#[aide_annotate(op_id = \"v1.health.get\")]\nasync fn health(\n    State(AppState {\n        ref db,\n        queue_tx,\n        cache,\n        ..\n    }): State<AppState>,\n) -> (StatusCode, Json<HealthReport>) {\n    // SELECT 1 FROM any table\n    let database: HealthStatus = db\n        .execute(Statement::from_string(\n            DatabaseBackend::Postgres,\n            \"SELECT 1\".to_owned(),\n        ))\n        .await\n        .into();\n\n    // Send a [`HealthCheck`] through the queue\n    let queue: HealthStatus = queue_tx.send(&QueueTask::HealthCheck, None).await.into();\n\n    // Set a cache value with an expiration to ensure it works\n    let cache: HealthStatus = cache\n        .set(\n            &HealthCheckCacheKey(\"health_check_value\".to_owned()),\n            &HealthCheckCacheValue(()),\n            // Expires after this time, so it won't pollute the DB\n            Duration::from_millis(100),\n        )\n        .await\n        .into();\n\n    l<|fim_suffix|>\n    (\n        status,\n        Json(HealthReport {\n            database,\n            queue,\n            cache,\n        }),\n    )\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Health\");\n\n    ApiRouter::new()\n        .api_route(\"/health/ping\", get(ping).head(ping))\n        .api_route_with(\n            \"/health\",\n            get_with(health, |op| op.response::<204, ()>().with(health_operation))\n                .head_with(health, health_operation),\n            tag,\n        )\n}\n<|fim_middle|>", "completion": "let status = if database.is_ok() && queue.is_ok() && cache.is_ok() {\n        StatusCode::OK\n    } else {\n        StatusCode::INTERNAL_SERVER_ERROR\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/health.rs", "node_type": "let_declaration", "line_range": [116, 120]}
{"prompt": "<|fim_prefix|>ze,\n        iter_direction,\n    )))\n}\n\n/// `msg_id`: Use a message id or a message `eventId`\n#[aide_annotate(op_id = \"v1.message-attempt.get\")]\nasync fn get_messageattempt(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgAttemptPath {\n        msg_id, attempt_id, ..\n    }): Path<ApplicationMsgAttemptPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<MessageAttemptOut>> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let attempt = messageattempt::Entity::secure_find_by_msg(msg.id)\n        .filter(messageattempt::Column::Id.eq(attempt_id))\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    Ok(Json(attempt.into()))\n}\n\n/// Resend a message to the specified endpoint.\n#[aide_annotate(op_id = \"v1.message-attempt.resend\")]\nasync fn resend_webhook(\n    State(AppState {\n        ref db, queue_tx, ..\n    }): State<AppState>,\n    Path(ApplicationMsgEndpointPath {\n        msg_id,\n        endpoint_id,\n        ..\n    }): Path<ApplicationMsgEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<NoContentWithCode<202>> {\n    let (msg, msg_content) = message::Entity::secure_find_by_id_or_uid(app.id.clone(), msg_id)\n        .find_also_related(messagecontent::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let msg_content = match msg_content {\n        Some(m) => serde_json::from_slice(&m.payload).ok(),\n        None => msg.legacy_payload,\n    };\n    if msg_content.is_none() {\n        return Err(HttpError::bad_request(\n            Some(\"missing_payload\".to_string()),\n            Some(\"Unable to resend message. Payload is missing (probably expired).\".to_string()),\n        )\n        .into());\n    }\n\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    queue_tx\n        .send(\n            &MessageTask::new_task(\n                msg.id.clone(),\n                app.id,\n                endp.id,\n                MessageAttemptTriggerType::Manual,\n            ),\n            None,\n        )\n        .await?;\n    Ok(NoContentWithCode)\n}\n\n/// Deletes the given attempt's response body. Useful when an endpoint accidentally returned sensitive content.\n#[aide_annotate(op_id = \"v1.message-attempt.expunge-content\")]\nasync fn expunge_attempt_content(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgAttemptPath {\n        msg_id, attempt_id, ..\n    }): Path<ApplicationMsgAttemptPath>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n) -> Result<StatusCode> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, Some(\"Message not found\".to_string())))?;\n\n    let mut attempt = messageattempt::Entity::secure_find_by_msg(msg.id)\n        .filter(messageattempt::Column::Id.eq(attempt_id))\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, Some(\"Message attempt not found\".to_string())))?\n        .into_active_model();\n\n    attempt.response = sea_orm::Set(\"EXPUNGED\".to_string());\n    attempt.update(db).await?;\n\n    Ok(StatusCode::NO_CONTENT)\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Message Attempt\");\n    ApiRouter::new()\n        // NOTE: [`list_messageattempts`] is deprecated\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/attempt\",\n            get_with(list_messageattempts, list_messageattempts_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/attempt/:attempt_id\",\n            get_with(get_messageattempt, get_messageattempt_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/attempt/:attempt_id/content\",\n            delete_with(expunge_attempt_content, expunge_attempt_content_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/endpoint\",\n            get_with(\n                list_attempted_destinations,\n                list_attempted_destinations_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/endpoint/:endpoint_id/resend\",\n            post_with(resend_webhook, resend_webhook_operation),\n            &tag,\n        )\n        // NOTE: [`list_attempts_for_endpoint`] is deprecated\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/endpoint/:endpoint_id/attempt\",\n            get_with(\n                list_attempts_for_endpoint,\n                list_attempts_for_endpoint_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/msg\",\n            get_with(list_attempted_messages, list_attempted_messages_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/attempt/endpoint/:endpoint_id\",\n            get_with(\n                list_attempts_by_endpoint,\n                list_attempts_by_endpoint_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/attempt/msg/:msg_id\",\n            get_with(list_attempts_by_msg, list_attempts_by_msg_operation),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{\n        AttemptListFetchQueryParams, ListAttemptedMessagesQueryParams,\n        ListAttemptsByEndpointQueryParams, ListAttemptsByMsgQueryParams,\n        ListAttemptsForEndpointQueryParams,\n    };\n\n    const INVALID_CHANNEL: &str = \"$$invalid-channel\";\n    const VALID_CHANNEL: &str = \"valid-channel\";\n    const INVALID_ENDPOINT_ID: &str = \"$$invalid-endpoint\";\n    const VALID_ENDPOINT_ID: &str = \"ep_valid-endpoint\";\n\n    #[test]\n    f<|fim_suffix|>\n    #[test]\n    fn test_list_attempts_by_endpoint_query_parameters_validation() {\n        let q: ListAttemptsByEndpointQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n    }\n\n    #[test]\n    fn test_list_attempts_by_msg_query_parameters_validation() {\n        let q: ListAttemptsByMsgQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptsByMsgQueryParams =\n            serde_json::from_value(json!({ \"endpoint_id\": INVALID_ENDPOINT_ID })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptsByMsgQueryParams = serde_json::from_value(json!(\n            {\n                \"channel\": VALID_CHANNEL,\n                \"endpoint_id\": VALID_ENDPOINT_ID\n            }\n        ))\n        .unwrap();\n        q.validate().unwrap();\n    }\n\n    #[test]\n    fn test_list_attempts_for_endpoint_query_parameters_validation() {\n        let q: ListAttemptsForEndpointQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptsForEndpointQueryParams =\n            serde_json::from_value(json!({ \"channel\": VALID_CHANNEL })).unwrap();\n        q.validate().unwrap();\n    }\n\n    #[test]\n    fn test_attempt_list_fetch_options_validation() {\n        let q: AttemptListFetchQueryParams =\n            serde_json::from_value(json!({ \"endpoint_id\": INVALID_ENDPOINT_ID })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: AttemptListFetchQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: AttemptListFetchQueryParams = serde_json::from_value(json!(\n            {\n                \"endpoint_id\": VALID_ENDPOINT_ID,\n                \"channel\": VALID_CHANNEL\n            }\n        ))\n        .unwrap();\n        q.validate().unwrap();\n    }\n}\n<|fim_middle|>", "completion": "fn test_list_attempted_messages_query_params_validation() {\n        let q: ListAttemptedMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": INVALID_CHANNEL })).unwrap();\n        assert!(q.validate().is_err());\n\n        let q: ListAttemptedMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": VALID_CHANNEL })).unwrap();\n        q.validate().unwrap();\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/attempt.rs", "node_type": "function_item", "line_range": [1133, 1141]}
{"prompt": "<|fim_prefix|>ult(),\n        },\n        Duration::from_mins(10),\n    )\n    .with_issuer(JWT_ISSUER)\n    .with_subject(management_org_id());\n\n    signing_config.generate(claims).map_err(Error::generic)\n}\n\npub fn generate_app_token(\n    keys: &JwtSigningConfig,\n    org_id: OrganizationId,\n    app_id: ApplicationId,\n    feature_flags: FeatureFlagSet,\n) -> Result<String> {\n    let claims = Claims::with_custom_claims(\n        CustomClaim {\n            organization: Some(org_id.0),\n            feature_flags,\n        },\n        Duration::from_hours(24 * 28),\n    )\n    .with_issuer(JWT_ISSUER)\n    .with_subject(app_id.0);\n\n    keys.generate(claims).map_err(Error::generic)\n}\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum JwtSigningConfig {\n    /// Variants that specify both key and algorithm to use\n    Advanced(JWTAlgorithm),\n    /// The variant used when the algorithm is not specified, defaults to HS256\n    Default {\n        #[serde(deserialize_with = \"deserialize_hs256\")]\n        jwt_secret: HS256Key,\n    },\n}\n\n/// A wrapper for the available JWT signing algorithms exposed by `jwt-simple`\n#[derive(Deserialize)]\n#[serde(tag = \"jwt_algorithm\", content = \"jwt_secret\")]\npub enum JWTAlgorithm {\n    #[serde(deserialize_with = \"deserialize_hs256\")]\n    HS256(HS256Key),\n    #[serde(deserialize_with = \"deserialize_hs384\")]\n    HS384(HS384Key),\n    #[serde(deserialize_with = \"deserialize_hs512\")]\n    HS512(HS512Key),\n    #[serde(deserialize_with = \"deserialize_rs256\")]\n    RS256(RS256),\n    #[serde(deserialize_with = \"deserialize_rs384\")]\n    RS384(RS384),\n    #[serde(deserialize_with = \"deserialize_rs512\")]\n    RS512(RS512),\n    #[serde(deserialize_with = \"deserialize_eddsa\")]\n    EdDSA(EdDSA),\n}\n\npub enum RS256 {\n    Public(RS256PublicKey),\n    Pair(Box<RS256KeyPair>),\n}\n\npub enum RS384 {\n    Public(RS384PublicKey),\n    Pair(Box<RS384KeyPair>),\n}\n\npub enum RS512 {\n    Public(RS512PublicKey),\n    Pair(Box<RS512KeyPair>),\n}\n\npub enum EdDSA {\n    Public(Ed25519PublicKey),\n    Pair(Box<Ed25519KeyPair>),\n}\n\nimpl JwtSigningConfig {\n    pub fn generate(&self, claims: JWTClaims<CustomClaim>) -> Result<String, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.authenticate(claims),\n                JWTAlgorithm::HS384(key) => key.authenticate(claims),\n                JWTAlgorithm::HS512(key) => key.authenticate(claims),\n                JWTAlgorithm::RS256(kind) => match kind {\n                    RS256::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS256::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS384::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS512::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::EdDSA(kind) => match kind {\n                    EdDSA::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    EdDSA::Pair(key) => key.sign(claims),\n                },\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.authenticate(claims),\n        }\n    }\n\n    pub fn verify_token(\n        &self,\n        token: &str,\n        options: Option<VerificationOptions>,\n    ) -> Result<JWTClaims<CustomClaim>, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS384(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS512(key) => key.verify_token(token, options),\n                JWTAlgorithm::RS256(kind) => match kind {\n                    RS256::Public(key) => key.verify_token(token, options),\n                    RS256::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(key) => key.verify_token(token, options),\n                    RS384::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(key) => key.verify_token(token, options),\n                    RS512::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::EdDSA(kind) => match kind {\n                    EdDSA::Public(key) => key.verify_token(token, options),\n                    EdDSA::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.verify_token(token, options),\n        }\n    }\n}\n\nimpl Debug for JwtSigningConfig {\n    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"{}\",\n            match self {\n                JwtSigningConfig::Advanced(a) => {\n                    match a {\n                        JWTAlgorithm::HS256(_) => \"HS256\",\n                        JWTAlgorithm::HS384(_) => \"HS384\",\n                        JWTAlgorithm::HS512(_) => \"HS512\",\n                        JWTAlgorithm::RS256(_) => \"RS256\",\n                        JWTAlgorithm::RS384(_) => \"RS384\",\n                        JWTAlgorithm::RS512(_) => \"RS512\",\n                        JWTAlgorithm::EdDSA(_) => \"EdDSA\",\n                    }\n                }\n                JwtSigningConfig::Default { .. } => {\n                    \"HS256\"\n                }\n            }\n        )\n    }\n}\n\nfn deserialize_hs256<'de, D>(deserializer: D) -> Result<HS256Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS256Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs384<'de, D>(deserializer: D) -> Result<HS384Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS384Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs512<'de, D>(deserializer: D) -> Result<HS512Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS512Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_rs256<'de, D>(deserializer: D) -> Result<RS256, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS256KeyPair::from_pem(&key) {\n        Ok(RS256::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS256PublicKey::from_pem(&key) {\n        Ok(RS256::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_rs384<'de, D>(deserializer: D) -> Result<RS384, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS384KeyPair::from_pem(&key) {\n        Ok(RS384::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS384PublicKey::from_pem(&key) {\n        Ok(RS384::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_rs512<'de, D>(deserializer: D) -> Result<RS512, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    i<|fim_suffix|>}\n\nfn deserialize_eddsa<'de, D>(deserializer: D) -> Result<EdDSA, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = Ed25519KeyPair::from_pem(&key) {\n        Ok(EdDSA::Pair(Box::new(pair)))\n    } else if let Ok(public) = Ed25519PublicKey::from_pem(&key) {\n        Ok(EdDSA::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n<|fim_middle|>", "completion": "if let Ok(pair) = RS512KeyPair::from_pem(&key) {\n        Ok(RS512::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS512PublicKey::from_pem(&key) {\n        Ok(RS512::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/security.rs", "node_type": "if_expression", "line_range": [418, 424]}
{"prompt": "<|fim_prefix|>use anyhow::Context;\nuse clap::Args;\nuse rand::{rngs::StdRng, seq::SliceRandom, SeedableRng};\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse svix::api::*;\n\n#[derive(Args)]\nstruct SeedOptions {\n    /// Will clear out all the applications and event types\n    #[arg(long, default_value = \"false\")]\n    pub reset: bool,\n\n    /// The number of endpoints to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=10) , default_value = \"2\")]\n    pub endpoint_count: u8,\n\n    /// The number of messages to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=100) , default_value = \"10\")]\n    pub message_count: u8,\n}\n\n#[derive(Args)]\npub struct SeedArgs {\n    #[clap(flatten)]\n    options: SeedOptions,\n}\n\n#[derive(Debug, Serialize, Default)]\n#[serde(rename_all = \"camelCase\")]\nstruct SeedOut {\n    application: ApplicationOut,\n    endpoints: Vec<String>,\n    event_types: Vec<String>,\n    messages: Vec<String>,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct PlayTokenOut {\n    token: String,\n}\n\nconst PLAY_TOKEN_GENERATE_URL: &str = \"https://api.play.svix.com/api/v1/token/generate/\";\nconst USER_EVENT_TYPES: [&str; 4] = [\"signup\", \"signin\", \"signout\", \"deleted\"];\n\npub async fn exec(\n    client: &Svix,\n    args: SeedArgs,\n    color_mode: colored_json::ColorMode,\n) -> anyhow::Result<()> {\n    let mut seed_out = SeedOut {\n        ..Default::default()\n    };\n\n    if args.options.reset {\n        let confirmation = dialoguer::Confirm::new()\n         .with_prompt(\"This will clear out all the applications and event types! Do you want to continue? \")\n         .interact()\n         .unwrap_or(false);\n\n        if confirmation {\n            reset_application(client).await?;\n            reset_event_type(client).await?;\n        } else {\n            return Ok(());\n        }\n    }\n\n    let application_in = ApplicationIn {\n        name: \"Test application\".to_string(),\n        ..Default::default()\n    };\n    let application_out = client.application().create(application_in, None).await?;\n\n    seed_out.application = application_out.clone();\n\n    let app_id = application_out.id;\n\n    let mut handles = Vec::new();\n\n    for _ in 0..args.options.endpoint_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(async move {\n            create_endpoint(client, app_id).await\n        }))\n    }\n\n    for h in handles {\n        let eo = h.await??;\n        seed_out.endpoints.push(eo.url);\n    }\n\n    for typ in USER_EVENT_TYPES {\n        let event_type_in = EventTypeIn {\n            name: format!(\"user.{typ}\"),\n            description: \"\".to_string(),\n            schemas: Some(json!(schema_example())),\n            ..Default::default()\n        };\n        let res = client.event_type().create(event_type_in, None).await;\n\n        match res {\n            Ok(event_type_out) => {\n                seed_out.event_types.push(event_type_out.name);\n            }\n            Err(err) => {\n                eprintln!(\"Failed to create event type: {err}\");\n                continue;\n            }\n        }\n    }\n    let mut handles = Vec::new();\n\n    <|fim_suffix|>\n\n    for h in handles {\n        let message_out = h.await??;\n        seed_out.messages.push(message_out.id);\n    }\n\n    let summary = format!(\n        \"Seeded {} endpoints, {} event types, {} messages to application \\\"{}\\\"\",\n        seed_out.endpoints.len(),\n        seed_out.event_types.len(),\n        seed_out.messages.len(),\n        seed_out.application.name\n    );\n\n    crate::json::print_json_output(&seed_out, color_mode)?;\n    println!(\"{summary}\");\n\n    Ok(())\n}\n\nasync fn create_endpoint(client: Svix, app_id: String) -> anyhow::Result<EndpointOut> {\n    let req_client = reqwest::Client::new();\n\n    let resp = req_client\n        .post(PLAY_TOKEN_GENERATE_URL)\n        .send()\n        .await?\n        .json::<PlayTokenOut>()\n        .await\n        .context(\"Failed to get token from public api\")?;\n\n    let endpoint_in = EndpointIn {\n        url: format!(\"https://play.svix.com/in/{}/\", resp.token),\n        ..Default::default()\n    };\n    let endpoint_out = client.endpoint().create(app_id, endpoint_in, None).await?;\n    Ok(endpoint_out)\n}\n\nasync fn create_message(client: Svix, app_id: String) -> anyhow::Result<MessageOut> {\n    let mut rng = StdRng::from_entropy();\n\n    let event_type = USER_EVENT_TYPES\n        .choose(&mut rng)\n        .context(\"Couldn't pick a random event type while creating a message\")?;\n\n    let message_in = MessageIn {\n        event_type: event_type.to_string(),\n        payload: json!({\n            \"userId\": \"41376126-35bf-4eda-81ef-83d741b0e026\",\n            \"firstName\": \"John\",\n            \"lastName\": \"Doe\",\n        }),\n        ..Default::default()\n    };\n\n    let message_out = client.message().create(app_id, message_in, None).await?;\n    Ok(message_out)\n}\n\nasync fn reset_application(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.application().list(None).await?;\n    let mut handles = Vec::new();\n\n    for app_out in resp.data {\n        let client = client.clone();\n\n        handles.push(tokio::spawn(async move {\n            if let Err(err) = client.application().delete(app_out.id.clone()).await {\n                eprintln!(\"Failed to delete application {}: {}\", app_out.id, err);\n            }\n        }));\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\nasync fn reset_event_type(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.event_type().list(None).await?;\n    let mut handles = Vec::new();\n\n    for event_type_out in resp.data {\n        let client = client.clone();\n\n        let handle = tokio::spawn(async move {\n            let _ = client\n                .event_type()\n                .delete(\n                    event_type_out.name,\n                    Some(EventTypeDeleteOptions {\n                        expunge: Some(true),\n                    }),\n                )\n                .await;\n        });\n        handles.push(handle);\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\npub fn schema_example() -> serde_json::Value {\n    serde_json::json!({\n        \"1\": {\n            \"description\": \"A user signed up\",\n            \"properties\": {\n                \"userId\": {\n                    \"description\": \"The user id\",\n                    \"type\": \"string\"\n                },\n                \"firstName\": {\n                    \"description\": \"User first name\",\n                    \"type\": \"string\"\n                },\n                \"lastName\": {\n                    \"description\": \"User last name\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"userId\",\n                \"firstName\",\n                \"lastName\",\n            ],\n            \"title\": \"User signed up Event\",\n            \"type\": \"object\",\n        }\n    })\n}\n<|fim_middle|>", "completion": "for _ in 0..args.options.message_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(\n            async move { create_message(client, app_id).await },\n        ))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/seed.rs", "node_type": "for_expression", "line_range": [117, 124]}
{"prompt": "<|fim_prefix|>erminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n#[derive(Parser)]\npub struct Args {\n    #[arg(long, env = \"SVIX_BRIDGE_CFG_FILE\", help = \"Path to the config file.\")]\n    cfg_file: Option<PathBuf>,\n    #[arg(\n        long,\n        env = \"SVIX_BRIDGE_CFG\",\n        help = \"Config data as a string (instead of a file on disk).\",\n        conflicts_with = \"cfg_file\"\n    )]\n    cfg: Option<String>,\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let mut config_search_paths = vec![];\n\n    if let Some(fp) = args.cfg_file {\n        config_search_paths.push(fp)\n    } else {\n        for name in [\"svix-bridge.yaml\", \"svix-bridge.yml\", \"svix-bridge.json\"] {\n            config_search_paths.push(std::env::current_dir().expect(\"current dir\").join(name));\n        }\n    }\n\n    // Clap will ensure we have only one or the other (cfg and cfg_file can't be specified together).\n    let cfg_source = match args.cfg {\n        Some(cfg_source) => cfg_source,\n        None => {\n            let fp = config_search_paths\n                .into_iter()\n                .find(|x| x.exists())\n                .expect(\"config file path\");\n            std::fs::read_to_string(&fp).map_err(|e| {\n                let p = fp.into_os_string().into_string().expect(\"config file path\");\n                Error::other(format!(\"Failed to read {p}: {e}\"))\n            })\n        }?,\n    };\n\n    let vars = std::env::vars().collect();\n    <|fim_suffix|>\n    setup_tracing(&cfg);\n    let _metrics = setup_metrics(&cfg);\n    tracing::info!(\"starting\");\n\n    tokio::spawn(async move {\n        let mut interval = tokio::time::interval(Duration::from_secs(15));\n        let metrics = CommonMetrics::new(&opentelemetry::global::meter(\"svix.com\"));\n        match get_allocator_stat_mibs() {\n            Ok(mibs) => {\n                tracing::debug!(\"Common Metrics Collection: Started\");\n\n                loop {\n                    interval.tick().await;\n\n                    if let Ok(Some((allocated, resident))) = get_allocator_stats(true, &mibs) {\n                        metrics.record_mem_allocated(allocated as _);\n                        metrics.record_mem_resident(resident as _);\n                    }\n                }\n            }\n            Err(e) => tracing::error!(\"Unable to get allocator stats mibs: {e}\"),\n        }\n    });\n\n    let (xform_tx, mut xform_rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n\n    // XXX: this is a bit nasty, but might be okay to start.\n    // The nested spawns are needed to make sure we can saturate the\n    // threadpool (otherwise we'd run each job serially).\n    //\n    // Another approach would be to do what og-ingester did: give each plugin a clone of the\n    // `TpHandle`, but this would likely mean moving the runtime module over to the `-types` crate.\n    // I'd rather not do this, mostly to help keep things more unit test friendly; channels can\n    // help keep the coupling more loose, with less stateful baggage.\n    // Starting with this just to keep the JS executor stuff here in the binary.\n    tokio::spawn(async move {\n        tracing::info!(\n            \"Starting JS Transformation Workers: {}\",\n            cfg.transformation_worker_count\n        );\n\n        deno_core::JsRuntime::init_platform(None, false);\n        let pooler: runtime::JsPooler = runtime::JsPooler::new(cfg.transformation_worker_count);\n\n        while let Some(TransformerJob {\n            input,\n            script,\n            callback_tx,\n        }) = xform_rx.recv().await\n        {\n            let tp = pooler.clone();\n            tokio::spawn(async move {\n                let out = tp.run_script(input, script).await;\n                // FIXME: seeing this Err case come up during load testing.\n                //   Seems like we shouldn't be hitting this so easily while the process is not terminating.\n                //   Regularly there are group error log lines that show up right at the end of an\n                //   `oha` run, POSTing to receivers. Need to investigate why.\n                if callback_tx\n                    .send(out.map_err(|e| tracing::error!(\"{:?}\", e)))\n                    .is_err()\n                {\n                    // If the callback fails, the plugin is likely unwinding/dropping.\n                    // Not a whole lot we can do about that.\n                    tracing::error!(\"failed to send js output back to caller\");\n                }\n            });\n        }\n    });\n\n    let mut senders = Vec::with_capacity(cfg.senders.len());\n    for sc in cfg.senders {\n        let mut sender: Box<dyn SenderInput> = sc.try_into().map_err(Error::other)?;\n        sender.set_transformer(Some(xform_tx.clone()));\n        senders.push(sender);\n    }\n    if senders.is_empty() {\n        tracing::warn!(\"No senders configured.\")\n    }\n    let senders_fut = supervise_senders(senders);\n\n    if cfg.receivers.is_empty() {\n        tracing::warn!(\"No receivers configured.\")\n    }\n    let (webhook_receivers, poller_receivers): (\n        Vec<WebhookReceiverConfig>,\n        Vec<PollerReceiverConfig>,\n    ) = cfg\n        .receivers\n        .into_iter()\n        .partition_map(|either| match either {\n            EitherReceiver::Webhook(x) => Either::Left(x),\n            EitherReceiver::Poller(y) => Either::Right(y),\n        });\n\n    let webhook_receivers_fut =\n        webhook_receiver::run(cfg.http_listen_address, webhook_receivers, xform_tx.clone());\n\n    let mut pollers: Vec<Box<dyn PollerInput>> = Vec::with_capacity(poller_receivers.len());\n    for poller_cfg in <|fim_middle|>", "completion": "let cfg = Config::from_src(&cfg_source, Some(vars).as_ref())?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "let_declaration", "line_range": [277, 277]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{collections::HashMap, sync::Arc};\n\nuse axum::async_trait;\nuse tokio::{\n    sync::RwLock,\n    task,\n    time::{sleep, Duration, Instant},\n};\n\nuse super::{Cache, CacheBehavior, CacheKey, Result};\n\n#[derive(Debug)]\nstruct ValueWrapper {\n    value: Vec<u8>,\n    ttl: Duration,\n    timer: Instant,\n}\n\nimpl ValueWrapper {\n    fn new(value: Vec<u8>, ttl: Duration) -> ValueWrapper {\n        ValueWrapper {\n            value,\n            ttl,\n            timer: Instant::now(),\n        }\n    }\n}\n\ntype State = HashMap<Vec<u8>, ValueWrapper>;\ntype SharedState = Arc<RwLock<State>>;\n\npub fn new() -> Cache {\n    let shared_state = Arc::new(RwLock::new(State::new()));\n\n    let shared_state_clone = shared_state.clone();\n    task::spawn(async move {\n        loop {\n            sleep(Duration::from_secs(60 * 5)).await;\n            shared_state_clone\n                .write()\n                .await\n                .retain(|_, v| check_is_expired(v))\n        }\n    });\n\n    MemoryCache { map: shared_state }.into()\n}\n\n#[derive(Clone)]\npub struct MemoryCache {\n    map: SharedState,\n}\n\n#[async_trait]\nimpl CacheBehavior for MemoryCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(self\n            .map\n            .read()\n            .await\n            .get(key)\n            .filter(|wrapper| check_is_expired(wrapper))\n            .map(|wrapper| wrapper.value.clone()))\n    }\n\n    a<|fim_suffix|>\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool> {\n        let mut lock = self.map.write().await;\n\n        // TODO: use HashMap::try_insert when stable\n        // https://github.com/rust-lang/rust/issues/82766\n        if !lock.contains_key(key) {\n            lock.insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n            return Ok(true);\n        }\n\n        Ok(false)\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()> {\n        self.map.write().await.remove(key.as_ref().as_bytes());\n\n        Ok(())\n    }\n}\n\nfn check_is_expired(vw: &ValueWrapper) -> bool {\n    vw.timer.elapsed().as_millis() <= vw.ttl.as_millis()\n}\n\n#[cfg(test)]\nmod tests {\n    use serde::{Deserialize, Serialize};\n\n    use super::{\n        super::{kv_def, CacheValue},\n        *,\n    };\n    use crate::core::cache::string_kv_def;\n\n    // Test structures\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValA(usize);\n    kv_def!(TestKeyA, TestValA);\n    impl TestKeyA {\n        fn new(id: String) -> TestKeyA {\n            TestKeyA(format!(\"SVIX_TEST_KEY_A_{id}\"))\n        }\n    }\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValB(String);\n    kv_def!(TestKeyB, TestValB);\n    impl TestKeyB {\n        fn new(id: String) -> TestKeyB {\n            TestKeyB(format!(\"SVIX_TEST_KEY_B_{id}\"))\n        }\n    }\n\n    string_kv_def!(StringTestKey);\n    impl StringTestKey {\n        fn new(id: String) -> StringTestKey {\n            StringTestKey(format!(\"SVIX_TEST_KEY_STRING_{id}\"))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_cache_crud_no_ttl() {\n        let cache = new();\n\n        let (first_key, first_val_a, first_val_b) =\n            (TestKeyA::new(\"1\".to_owned()), TestValA(1), TestValA(2));\n        let (second_key, second_val_a, second_val_b) = (\n            TestKeyB::new(\"1\".to_owned()),\n            TestValB(\"1\".to_owned()),\n            TestValB(\"2\".to_owned()),\n        );\n        let (third_key, third_val_a, third_val_b) = (\n            StringTestKey::new(\"1\".to_owned()),\n            \"1\".to_owned(),\n            \"2\".to_owned(),\n        );\n\n        // Create\n        assert!(cache\n            .set(&first_key, &first_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Read\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_a));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_a));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_a)\n        );\n\n        // Update (overwrite)\n        assert!(cache\n            .set(&first_key, &first_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Confirm update\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_b));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_b));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_b)\n        );\n\n        // Delete\n        assert!(cache.delete(&first_key).await.is_ok());\n        assert!(cache.delete(&second_key).await.is_ok());\n        assert!(cache.delete(&third_key).await.is_ok());\n\n        // Confirm deletion\n        assert_eq!(cache.get::<TestValA>(&first_key).await.unwrap(), None);\n        assert_eq!(cache.get::<TestValB>(&second_key).await.unwrap(), None);\n        assert_eq!(cache.get_string(&third_key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_ttl() {\n        let cache = new();\n        let key = TestKeyA::new(\"key\".to_owned());\n\n        assert!(cache\n            .set(&key, &TestValA(1), Duration::from_secs(1),)\n            .await\n            .is_ok());\n        tokio::time::sleep(std::time::Duration::from_millis(1200)).await;\n        assert_eq!(cache.get::<TestValA>(&key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_nx_status() {\n        let cache = new();\n        let key = TestKeyA::new(\"nx_status_test_key\".to_owned());\n\n        assert!(cache\n            .set_if_not_exists(&key, &TestValA(1), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(!cache\n            .set_if_not_exists(&key, &TestValA(2), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(cache.delete(&key).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()> {\n        self.map\n            .write()\n            .await\n            .insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n        Ok(())\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/memory.rs", "node_type": "function_item", "line_range": [73, 79]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse chrono::Utc;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set};\n\nuse crate::core::types::{BaseId, EndpointId, MessageEndpointId, MessageId, MessageStatus};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"messagedestination\")]\npub struct Model {\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: MessageEndpointId,\n    pub created_at: DateTimeWithTimeZone,\n    pub updated_at: DateTimeWithTimeZone,\n    pub msg_id: MessageId,\n    pub endp_id: EndpointId,\n    pub status: MessageStatus,\n    pub next_attempt: Option<DateTimeWithTimeZone>,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]\npub enum Relation {}\n\n#[axum::async_trait]\nimpl ActiveModelBehavior for ActiveModel {\n    f<|fim_suffix|>\n    async fn before_save<C>(mut self, _db: &C, _insert: bool) -> Result<Self, DbErr>\n    where\n        C: ConnectionTrait,\n    {\n        self.updated_at = Set(Utc::now().into());\n        Ok(self)\n    }\n}\n\nimpl Entity {\n    pub fn secure_find_by_msg(msg_id: MessageId) -> Select<Entity> {\n        Self::find().filter(Column::MsgId.eq(msg_id))\n    }\n\n    pub fn secure_find_by_endpoint(endp_id: EndpointId) -> Select<Entity> {\n        Self::find().filter(Column::EndpId.eq(endp_id))\n    }\n}\n<|fim_middle|>", "completion": "fn new() -> Self {\n        let timestamp = Utc::now();\n        Self {\n            id: Set(MessageEndpointId::new(timestamp.into(), None)),\n            created_at: Set(timestamp.into()),\n            updated_at: Set(timestamp.into()),\n            ..ActiveModelTrait::default()\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/messagedestination.rs", "node_type": "function_item", "line_range": [27, 35]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct IngestSourceListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n}\n\nimpl From<IngestSourceListOptions> for svix::api::IngestSourceListOptions {\n    fn from(value: IngestSourceListOptions) -> Self {\n        let IngestSourceListOptions {\n            limit,\n            iterator,\n            order,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            order,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct IngestSourceCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<IngestSourceCreateOptions> for svix::api::IngestSourceCreateOptions {\n    fn from(value: IngestSourceCreateOptions) -> Self {\n        let IngestSourceCreateOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct IngestSourceRotateTokenOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<IngestSourceRotateTokenOptions> for svix::api::IngestSourceRotateTokenOptions {\n    fn from(value: IngestSourceRotateTokenOptions) -> Self {\n        let IngestSourceRotateTokenOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct IngestSourceArgs {\n    #[command(subcommand)]\n    pub command: IngestSourceCommands,\n}\n\n#[derive(Subcommand)]\npub enum IngestSourceCommands {\n    /// List of all the organization's Ingest Sources.\n    List {\n        #[clap(flatten)]\n        options: IngestSourceListOptions,\n    },\n    /// Create Ingest Source.\n    Create {\n        ingest_source_in: crate::json::JsonOf<IngestSourceIn>,\n        #[clap(flatten)]\n        options: IngestSourceCreateOptions,\n    },\n    /// Get an Ingest Source by id or uid.\n    Get { source_id: String },\n    /// Update an Ingest Source.\n    Update {\n        source_id: String,\n        ingest_source_in: crate::json::JsonOf<IngestSourceIn>,\n    },\n    /// Delete an Ingest Source.\n    Delete { source_id: String },\n    /// Rotate the Ingest Source's Url Token.\n    ///\n    /// This will rotate the ingest source's token, which is used to\n    /// construct the unique `ingestUrl` for the source. Previous tokens\n    /// will remain valid for 48 hours after rotation. The token can be\n    /// rotated a maximum of three times within the 48-hour period.\n    RotateToken {\n        source_id: String,\n        #[clap(flatten)]\n        options: IngestSourceRotateTokenOptions,\n    },\n}\n\nimpl IngestSourceCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { options } => {\n                let resp = client.ingest().source().list(Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                ingest_source_in,\n                options,\n            } => {\n                <|fim_suffix|>\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { source_id } => {\n                let resp = client.ingest().source().get(source_id).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                source_id,\n                ingest_source_in,\n            } => {\n                let resp = client\n                    .ingest()\n                    .source()\n                    .update(source_id, ingest_source_in.into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete { source_id } => {\n                client.ingest().source().delete(source_id).await?;\n            }\n            Self::RotateToken { source_id, options } => {\n                let resp = client\n                    .ingest()\n                    .source()\n                    .rotate_token(source_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let resp = client\n                    .ingest()\n                    .source()\n                    .create(ingest_source_in.into_inner(), Some(options.into()))\n                    .await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/ingest_source.rs", "node_type": "let_declaration", "line_range": [116, 120]}
{"prompt": "<|fim_prefix|>use anyhow::Context;\nuse clap::Args;\nuse rand::{rngs::StdRng, seq::SliceRandom, SeedableRng};\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse svix::api::*;\n\n#[derive(Args)]\nstruct SeedOptions {\n    /// Will clear out all the applications and event types\n    #[arg(long, default_value = \"false\")]\n    pub reset: bool,\n\n    /// The number of endpoints to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=10) , default_value = \"2\")]\n    pub endpoint_count: u8,\n\n    /// The number of messages to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=100) , default_value = \"10\")]\n    pub message_count: u8,\n}\n\n#[derive(Args)]\npub struct SeedArgs {\n    #[clap(flatten)]\n    options: SeedOptions,\n}\n\n#[derive(Debug, Serialize, Default)]\n#[serde(rename_all = \"camelCase\")]\nstruct SeedOut {\n    application: ApplicationOut,\n    endpoints: Vec<String>,\n    event_types: Vec<String>,\n    messages: Vec<String>,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct PlayTokenOut {\n    token: String,\n}\n\nconst PLAY_TOKEN_GENERATE_URL: &str = \"https://api.play.svix.com/api/v1/token/generate/\";\nconst USER_EVENT_TYPES: [&str; 4] = [\"signup\", \"signin\", \"signout\", \"deleted\"];\n\npub async fn exec(\n    client: &Svix,\n    args: SeedArgs,\n    color_mode: colored_json::ColorMode,\n) -> anyhow::Result<()> {\n    let mut seed_out = SeedOut {\n        ..Default::default()\n    };\n\n    if args.options.reset {\n        let confirmation = dialoguer::Confirm::new()\n         .with_prompt(\"This will clear out all the applications and event types! Do you want to continue? \")\n         .interact()\n         .unwrap_or(false);\n\n        if confirmation {\n            reset_application(client).await?;\n            reset_event_type(client).await?;\n        } else {\n            return Ok(());\n        }\n    }\n\n    let application_in = ApplicationIn {\n        name: \"Test application\".to_string(),\n        ..Default::default()\n    };\n    let application_out = client.application().create(application_in, None).await?;\n\n    seed_out.application = application_out.clone();\n\n    let app_id = application_out.id;\n\n    let mut handles = Vec::new();\n\n    <|fim_suffix|>\n\n    for h in handles {\n        let eo = h.await??;\n        seed_out.endpoints.push(eo.url);\n    }\n\n    for typ in USER_EVENT_TYPES {\n        let event_type_in = EventTypeIn {\n            name: format!(\"user.{typ}\"),\n            description: \"\".to_string(),\n            schemas: Some(json!(schema_example())),\n            ..Default::default()\n        };\n        let res = client.event_type().create(event_type_in, None).await;\n\n        match res {\n            Ok(event_type_out) => {\n                seed_out.event_types.push(event_type_out.name);\n            }\n            Err(err) => {\n                eprintln!(\"Failed to create event type: {err}\");\n                continue;\n            }\n        }\n    }\n    let mut handles = Vec::new();\n\n    for _ in 0..args.options.message_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(\n            async move { create_message(client, app_id).await },\n        ))\n    }\n\n    for h in handles {\n        let message_out = h.await??;\n        seed_out.messages.push(message_out.id);\n    }\n\n    let summary = format!(\n        \"Seeded {} endpoints, {} event types, {} messages to application \\\"{}\\\"\",\n        seed_out.endpoints.len(),\n        seed_out.event_types.len(),\n        seed_out.messages.len(),\n        seed_out.application.name\n    );\n\n    crate::json::print_json_output(&seed_out, color_mode)?;\n    println!(\"{summary}\");\n\n    Ok(())\n}\n\nasync fn create_endpoint(client: Svix, app_id: String) -> anyhow::Result<EndpointOut> {\n    let req_client = reqwest::Client::new();\n\n    let resp = req_client\n        .post(PLAY_TOKEN_GENERATE_URL)\n        .send()\n        .await?\n        .json::<PlayTokenOut>()\n        .await\n        .context(\"Failed to get token from public api\")?;\n\n    let endpoint_in = EndpointIn {\n        url: format!(\"https://play.svix.com/in/{}/\", resp.token),\n        ..Default::default()\n    };\n    let endpoint_out = client.endpoint().create(app_id, endpoint_in, None).await?;\n    Ok(endpoint_out)\n}\n\nasync fn create_message(client: Svix, app_id: String) -> anyhow::Result<MessageOut> {\n    let mut rng = StdRng::from_entropy();\n\n    let event_type = USER_EVENT_TYPES\n        .choose(&mut rng)\n        .context(\"Couldn't pick a random event type while creating a message\")?;\n\n    let message_in = MessageIn {\n        event_type: event_type.to_string(),\n        payload: json!({\n            \"userId\": \"41376126-35bf-4eda-81ef-83d741b0e026\",\n            \"firstName\": \"John\",\n            \"lastName\": \"Doe\",\n        }),\n        ..Default::default()\n    };\n\n    let message_out = client.message().create(app_id, message_in, None).await?;\n    Ok(message_out)\n}\n\nasync fn reset_application(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.application().list(None).await?;\n    let mut handles = Vec::new();\n\n    for app_out in resp.data {\n        let client = client.clone();\n\n        handles.push(tokio::spawn(async move {\n            if let Err(err) = client.application().delete(app_out.id.clone()).await {\n                eprintln!(\"Failed to delete application {}: {}\", app_out.id, err);\n            }\n        }));\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\nasync fn reset_event_type(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.event_type().list(None).await?;\n    let mut handles = Vec::new();\n\n    for event_type_out in resp.data {\n        let client = client.clone();\n\n        let handle = tokio::spawn(async move {\n            let _ = client\n                .event_type()\n                .delete(\n                    event_type_out.name,\n                    Some(EventTypeDeleteOptions {\n                        expunge: Some(true),\n                    }),\n                )\n                .await;\n        });\n        handles.push(handle);\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\npub fn schema_example() -> serde_json::Value {\n    serde_json::json!({\n        \"1\": {\n            \"description\": \"A user signed up\",\n            \"properties\": {\n                \"userId\": {\n                    \"description\": \"The user id\",\n                    \"type\": \"string\"\n                },\n                \"firstName\": {\n                    \"description\": \"User first name\",\n                    \"type\": \"string\"\n                },\n                \"lastName\": {\n                    \"description\": \"User last name\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"userId\",\n                \"firstName\",\n                \"lastName\",\n            ],\n            \"title\": \"User signed up Event\",\n            \"type\": \"object\",\n        }\n    })\n}\n<|fim_middle|>", "completion": "for _ in 0..args.options.endpoint_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(async move {\n            create_endpoint(client, app_id).await\n        }))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/seed.rs", "node_type": "for_expression", "line_range": [82, 89]}
{"prompt": "<|fim_prefix|>mod cluster;\nmod sentinel;\n\nuse std::{sync::Arc, time::Duration};\n\nuse bb8::{Pool, RunError};\nuse bb8_redis::RedisConnectionManager;\nuse redis::{\n    aio::ConnectionManagerConfig, sentinel::SentinelNodeConnectionInfo, AsyncConnectionConfig,\n    ProtocolVersion, RedisConnectionInfo, RedisError, TlsMode,\n};\nuse sentinel::RedisSentinelConnectionManager;\nuse tokio::sync::Mutex;\n\npub use self::cluster::RedisClusterConnectionManager;\nuse crate::cfg::{CacheBackend, QueueBackend, SentinelConfig};\n\npub const REDIS_CONN_TIMEOUT: Duration = Duration::from_secs(2);\n\npub enum RedisVariant<'a> {\n    Clustered,\n    NonClustered,\n    Sentinel(&'a SentinelConfig),\n}\n\n#[derive(Clone)]\npub enum RedisManager {\n    Clustered(Pool<RedisClusterConnectionManager>),\n    NonClustered(Pool<RedisConnectionManager>),\n    Sentinel(Pool<crate::redis::sentinel::RedisSentinelConnectionManager>),\n    ClusteredUnpooled(redis::cluster_async::ClusterConnection),\n    NonClusteredUnpooled(redis::aio::ConnectionManager),\n    SentinelUnpooled(Arc<Mutex<redis::sentinel::SentinelClient>>),\n}\n\nimpl RedisManager {\n    async fn new_pooled(dsn: &str, variant: RedisVariant<'_>, max_conns: u16) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let mgr = RedisClusterConnectionManager::new(dsn)\n                    .expect(\"Error initializing redis cluster client\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis cluster connection pool\");\n                RedisManager::Clustered(pool)\n            }\n            RedisVariant::NonClustered => {\n                let mgr =\n                    RedisConnectionManager::new(dsn).expect(\"Error initializing redis client\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::NonClustered(pool)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);\n                <|fim_suffix|>\n                let mgr = RedisSentinelConnectionManager::new(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                )\n                .expect(\"Error initializing RedisSentinelConnectionManager\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::Sentinel(pool)\n            }\n        }\n    }\n\n    async fn new_unpooled(dsn: &str, variant: RedisVariant<'_>) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let cli = redis::cluster::ClusterClient::builder(vec![dsn])\n                    .retries(1)\n                    .connection_timeout(REDIS_CONN_TIMEOUT)\n                    .build()\n                    .expect(\"Error initializing redis-unpooled cluster client\");\n                let con = cli\n                    .get_async_connection()\n                    .await\n                    .expect(\"Failed to get redis-cluster-unpooled connection\");\n                RedisManager::ClusteredUnpooled(con)\n            }\n            RedisVariant::NonClustered => {\n                let cli =\n                    redis::Client::open(dsn).expect(\"Error initializing redis unpooled client\");\n                let con = redis::aio::ConnectionManager::new_with_config(\n                    cli,\n                    ConnectionManagerConfig::new()\n                        .set_number_of_retries(1)\n                        .set_connection_timeout(REDIS_CONN_TIMEOUT),\n                )\n                .await\n                .expect(\"Failed to get redis-unpooled connection manager\");\n                RedisManager::NonClusteredUnpooled(con)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);\n                let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };\n                let cli = redis::sentinel::SentinelClient::build(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                    redis::sentinel::SentinelServerType::Master,\n                )\n                .expect(\"Failed to build sentinel client\");\n\n                RedisManager::SentinelUnpooled(Arc::new(Mutex::new(cli)))\n            }\n        }\n    }\n\n    pub async fn from_cache_backend(cache_backend: &CacheBackend<'_>) -> Self {\n        match cache_backend {\n            CacheBackend::Redis(dsn) => Self::new_unpooled(dsn, RedisVariant::NonClustered).await,\n            CacheBackend::RedisCluster(dsn) => {\n                Self::new_unpooled(dsn, RedisVariant::Clustered).await\n            }\n            CacheBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_unpooled(dsn, RedisVariant::Sentinel(cfg)).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn from_queue_backend(queue_backend: &QueueBackend<'_>, max_conns: u16) -> Self {\n        match queue_backend {\n            QueueBackend::Redis(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::NonClustered, max_conns).await\n            }\n            QueueBackend::RedisCluster(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::Clustered, max_conns).await\n            }\n            QueueBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_pooled(dsn, RedisVariant::Sentinel(cfg), max_conns).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn get(&self) -> Result<RedisConnection<'_>, RunError<RedisError>> {\n        match self {\n            Self::Clustered(pool) => Ok(RedisConnection::Clustered(pool.get().await?)),\n            Self::NonClustered(pool) => Ok(RedisConnection::NonClustered(pool.get().await?)),\n            Self::Sentinel(pool) => Ok(RedisConnection::SentinelPooled(pool.get().await?)),\n            Self::ClusteredUnpooled(conn) => Ok(RedisConnection::ClusteredUnpooled(conn.clone())),\n            Self::NonClusteredUnpooled(conn) => {\n                Ok(RedisConnection::NonClusteredUnpooled(conn.clone()))\n            }\n            Self::SentinelUnpooled(conn) => {\n                let mut conn = conn.lock().await;\n                let con = conn\n                    .get_async_connection_with_config(\n                        &AsyncConnectionConfig::new().set_response_timeout(REDIS_CONN_TIMEOUT),\n                    )\n                    .await?;\n                Ok(RedisConnection::SentinelUnpooled(con))\n            }\n        }\n    }\n}\n\npub enum RedisConnection<'a> {\n    Clustered(bb8::PooledConnection<'a, RedisClusterConnectionManager>),\n    NonClustered(bb8::PooledConnection<'a, RedisConnectionManager>),\n    SentinelPooled(bb8::PooledConnection<'a, RedisSentinelConnectionMan<|fim_middle|>", "completion": "let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/redis/mod.rs", "node_type": "let_declaration", "line_range": [61, 65]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse axum::async_trait;\n\nuse super::{Cache, CacheBehavior, CacheKey, CacheValue, Result, StringCacheKey};\n\npub fn new() -> Cache {\n    tracing::warn!(\"Running with caching disabled will negatively affect performance. Idempotency is not supported without a cache.\");\n    NoCache {}.into()\n}\n\n#[derive(Clone)]\npub struct NoCache;\n\n#[async_trait]\nimpl CacheBehavior for NoCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    a<|fim_suffix|>\n    async fn get_raw(&self, _key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(None)\n    }\n\n    async fn get_string<T: StringCacheKey>(&self, _key: &T) -> Result<Option<String>> {\n        Ok(None)\n    }\n\n    async fn set<T: CacheValue>(&self, _key: &T::Key, _value: &T, _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_raw(&self, _key: &[u8], _value: &[u8], _ttl: Duration) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_string<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<()> {\n        Ok(())\n    }\n\n    async fn delete<T: CacheKey>(&self, _key: &T) -> Result<()> {\n        Ok(())\n    }\n\n    async fn set_if_not_exists<T: CacheValue>(\n        &self,\n        _key: &T::Key,\n        _value: &T,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n\n    async fn set_raw_if_not_exists(\n        &self,\n        _key: &[u8],\n        _value: &[u8],\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n\n    async fn set_string_if_not_exists<T: StringCacheKey>(\n        &self,\n        _key: &T,\n        _value: &str,\n        _ttl: Duration,\n    ) -> Result<bool> {\n        Ok(false)\n    }\n}\n<|fim_middle|>", "completion": "async fn get<T: CacheValue>(&self, _key: &T::Key) -> Result<Option<T>> {\n        Ok(None)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/none.rs", "node_type": "function_item", "line_range": [24, 26]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::fmt;\n\nuse http_body_util::BodyExt;\nuse hyper::body::Incoming;\n\nuse crate::http1_to_02_status_code;\n\npub type Result<T> = std::result::Result<T, Error>;\n\n/// The error type returned from the Svix API\n#[derive(Debug, Clone)]\npub enum Error {\n    /// A generic error\n    Generic(String),\n    /// Http Error\n    Http(HttpErrorContent<crate::models::HttpErrorOut>),\n    /// Http Validation Error\n    Validation(HttpErrorContent<crate::models::HttpValidationError>),\n}\n\nimpl Error {\n    pub(crate) fn generic(err: impl std::error::Error) -> Self {\n        Self::Generic(format!(\"{err:?}\"))\n    }\n\n    pub(crate) async fn from_response(status_code: http1::StatusCode, body: Incoming) -> Self {\n        match body.collect().await {\n            Ok(collected) => {\n                l<|fim_suffix|>                if status_code == http1::StatusCode::UNPROCESSABLE_ENTITY {\n                    Self::Validation(HttpErrorContent {\n                        status: http02::StatusCode::UNPROCESSABLE_ENTITY,\n                        payload: serde_json::from_slice(&bytes).ok(),\n                    })\n                } else {\n                    Error::Http(HttpErrorContent {\n                        status: http1_to_02_status_code(status_code),\n                        payload: serde_json::from_slice(&bytes).ok(),\n                    })\n                }\n            }\n            Err(e) => Self::Generic(e.to_string()),\n        }\n    }\n}\n\n// TODO: Remove for v2.0 of the library (very uncommon impl for an error type)\nimpl From<Error> for String {\n    fn from(err: Error) -> Self {\n        err.to_string()\n    }\n}\n\nimpl fmt::Display for Error {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        match self {\n            Error::Generic(s) => s.fmt(f),\n            Error::Http(e) => format!(\"Http error (status={}) {:?}\", e.status, e.payload).fmt(f),\n            Error::Validation(e) => format!(\"Validation error {:?}\", e.payload).fmt(f),\n        }\n    }\n}\n\nimpl std::error::Error for Error {}\n\n#[derive(Debug, Clone)]\npub struct HttpErrorContent<T> {\n    pub status: http02::StatusCode,\n    pub payload: Option<T>,\n}\n<|fim_middle|>", "completion": "let bytes = collected.to_bytes();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/error.rs", "node_type": "let_declaration", "line_range": [32, 32]}
{"prompt": "<|fim_prefix|>use anyhow::Context;\nuse axum::http::{header, HeaderMap, HeaderName, HeaderValue};\nuse indexmap::IndexMap;\nuse serde::Deserialize;\nuse svix_bridge_types::{async_trait, BoxError, ForwardRequest, ReceiverOutput};\nuse url::Url;\n\nconst BRIDGE_USER_AGENT: HeaderValue =\n    HeaderValue::from_static(concat!(\"Svix-Bridge/\", env!(\"CARGO_PKG_VERSION\")));\n\n#[derive(Deserialize)]\n#[serde(tag = \"type\")]\npub enum HttpOutputOpts {\n    // Single-variant enum so we can require the \"type\": \"http\" field in deserialization\n    #[serde(rename = \"http\")]\n    Inner {\n        url: Url,\n        #[serde(default)]\n        headers: IndexMap<String, String>,\n    },\n}\n\npub(crate) struct HttpOutput {\n    client: reqwest::Client,\n    url: Url,\n    headers: HeaderMap,\n    name: String,\n}\n\nimpl HttpOutputOpts {\n    pub(crate) fn into_receiver_output(\n        self,\n        name: String,\n    ) -> anyhow::Result<Box<dyn ReceiverOutput>> {\n        let Self::Inner { url, headers } = self;\n        let mut headers: HeaderMap = headers\n            .into_iter()\n            .map(|(k, v)| {\n                Ok((\n                    HeaderName::try_from(k.as_str())\n                        .with_context(|| format!(\"invalid header name `{k}`\"))?,\n                    HeaderValue::try_from(v.as_str())\n                        .with_context(|| format!(\"invalid header value `{v}`\"))?,\n                ))\n            })\n            .collect::<anyhow::Result<_>>()?;\n        headers\n            .entry(header::USER_AGENT)\n            .or_insert(BRIDGE_USER_AGENT);\n\n        let client = reqwest::Client::new();\n        Ok(Box::new(HttpOutput {\n            client,\n            url,\n            headers,\n            name,\n        }))\n    }\n}\n\n#[async_trait]\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl ReceiverOutput for HttpOutput {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.client\n            .post(self.url.clone())\n            .headers(self.headers.clone())\n            .json(&request.payload)\n            .send()\n            .await?;\n        Ok(())\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/http_output.rs", "node_type": "impl_item", "line_range": [62, 76]}
{"prompt": "<|fim_prefix|>use anyhow::Result;\nuse clap::{Parser, Subcommand};\nuse clap_complete::Shell;\nuse colored_json::{ColorMode, Output};\nuse concolor_clap::{Color, ColorChoice};\nuse svix::api::SvixOptions;\n\nuse self::{\n    cmds::{\n        api::{\n            application::ApplicationArgs, authentication::AuthenticationArgs,\n            endpoint::EndpointArgs, environment::EnvironmentArgs, event_type::EventTypeArgs,\n            ingest::IngestArgs, integration::IntegrationArgs, message::MessageArgs,\n            message_attempt::MessageAttemptArgs, operational_webhook::OperationalWebhookArgs,\n            streaming::StreamingArgs,\n        },\n        listen::ListenArgs,\n        open::OpenArgs,\n        seed::SeedArgs,\n        signature::SignatureArgs,\n    },\n    config::Config,\n};\nuse crate::cmds::api::connector::ConnectorArgs;\n\nmod cmds;\nmod config;\nmod json;\nmod relay;\n\nconst VERSION: &str = env!(\"CARGO_PKG_VERSION\");\nconst BIN_NAME: &str = env!(\"CARGO_BIN_NAME\");\n\n#[derive(Parser)]\n#[command(version, about, long_about = None, bin_name = BIN_NAME)]\n#[clap(color = concolor_clap::color_choice())]\nstruct Cli {\n    #[command(flatten)]\n    color: Color,\n    #[arg(\n        short,\n        long,\n        action = clap::ArgAction::Count,\n        help = \"Log more. This option may be repeated up to 3 times\"\n    )]\n    verbose: u8,\n    #[command(subcommand)]\n    command: RootCommands,\n}\n\nimpl Cli {\n    /// Converts the selected `ColorChoice` from the CLI to a `ColorMode` as used by the JSON printer.\n    ///\n    /// When the color choice is \"auto\", this considers whether stdout is a tty or not so that\n    /// color codes are only produced when actually writing directly to a terminal.\n    <|fim_suffix|>\n\n    fn log_level(&self) -> tracing::Level {\n        match self.verbose {\n            3.. => tracing::Level::TRACE,\n            2 => tracing::Level::DEBUG,\n            1 => tracing::Level::INFO,\n            0 => tracing::Level::WARN,\n        }\n    }\n}\n\n// N.b Ordering matters here for how clap presents the help.\n#[derive(Subcommand)]\nenum RootCommands {\n    /// List, create & modify applications\n    Application(ApplicationArgs),\n    /// Manage authentication tasks such as getting dashboard URLs\n    Authentication(AuthenticationArgs),\n    /// Generate the autocompletion script for the specified shell\n    Completion { shell: Shell },\n    /// List, create & modify connectors\n    Connector(ConnectorArgs),\n    /// List, create & modify endpoints\n    Endpoint(EndpointArgs),\n    /// Import or export environments\n    Environment(EnvironmentArgs),\n    /// List, create & modify event types\n    EventType(EventTypeArgs),\n    /// List, create & modify Svix Ingest sources and endpoints\n    Ingest(IngestArgs),\n    /// List integrations by app id\n    Integration(IntegrationArgs),\n    /// Forward webhook requests to a local url\n    Listen(ListenArgs),\n    /// Interactively configure your Svix API credentials\n    Login,\n    /// List & create messages\n    Message(MessageArgs),\n    /// List, lookup & resend message attempts\n    MessageAttempt(MessageAttemptArgs),\n    /// Quickly open Svix pages in your browser\n    Open(OpenArgs),\n    /// List, create & modify operational webhook endpoints\n    OperationalWebhook(OperationalWebhookArgs),\n    /// List, create & modify Svix Stream resources\n    Streaming(StreamingArgs),\n    /// Generate a test application with sample endpoints and event types\n    Seed(SeedArgs),\n    /// Verifying and signing webhooks with the Svix signature scheme\n    Signature(SignatureArgs),\n    /// Get the version of the Svix CLI\n    Version,\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let cli = Cli::parse();\n    let color_mode = cli.color_mode();\n\n    tracing_subscriber::fmt()\n        .with_max_level(cli.log_level())\n        .with_timer(tracing_subscriber::fmt::time::LocalTime::rfc_3339())\n        .init();\n\n    // rustls requires a crypto backend (\"provider\") choice to be made explicitly\n    // The Svix SDK uses the default provider if a default is not installed, but\n    // we use reqwest directly in some code paths, which does not do this.\n    _ = rustls::crypto::aws_lc_rs::default_provider().install_default();\n\n    // XXX: cfg can give an Err in certain situations.\n    // Assigning the variable here since several match arms need a `&Config` but the rest of them\n    // won't care/are still usable if the config doesn't exist.\n    // To this, the `?` is deferred until the point inside a given match arm needs the config value.\n    let cfg = Config::load();\n    match cli.command {\n        // Local-only things\n        RootCommands::Version => println!(\"{VERSION}\"),\n        RootCommands::Signature(args) => args.command.exec().await?,\n        RootCommands::Open(args) => args.command.exec().await?,\n        // Remote API calls\n        RootCommands::Application(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Authentication(args) => {\n            let cfg = cfg?;\n            let client = get_client(&cfg)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Connector(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::EventType(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Endpoint(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Environment(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Message(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::MessageAttempt(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Ingest(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Integration(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::OperationalWebhook(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n        RootCommands::Streaming(args) => {\n            let client = get_client(&cfg?)?;\n            args.command.exec(&client, color_mode).await?;\n        }\n\n        RootCommands::Listen(args) => args.exec(&cfg?).await?,\n        RootCommands::Login => cmds::login::prompt(&cfg?).await?,\n        RootCommands::Completion { shell } => cmds::completion::generate(&shell)?,\n        RootCommands::Seed(args) => {\n            let client = get_client(&cfg?)?;\n            cmds::seed::exec(&client, args, color_mode).await?;\n        }\n    }\n\n    Ok(())\n}\n\nfn get_client(cfg: &Config) -> Result<svix::api::Svix> {\n    let token = cfg.auth_token.clone().ok_or_else(|| {\n        anyhow::anyhow!(\"No auth token set. Try running `{BIN_NAME} login` to get started.\")\n    })?;\n    let opts = get_client_options(cfg)?;\n    Ok(svix::api::Svix::new(token, Some(opts)))\n}\n\nfn get_client_options(cfg: &Config) -> Result<svix::api::SvixOptions> {\n    Ok(svix::api::SvixOptions {\n        debug: false,\n        server_url: cfg.server_url().map(Into::into),\n        timeout: None,\n        ..SvixOptions::default()\n    })\n}\n<|fim_middle|>", "completion": "fn color_mode(&self) -> ColorMode {\n        match self.color.color {\n            ColorChoice::Auto => ColorMode::Auto(Output::StdOut),\n            ColorChoice::Always => ColorMode::On,\n            ColorChoice::Never => ColorMode::Off,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/main.rs", "node_type": "function_item", "line_range": [56, 62]}
{"prompt": "<|fim_prefix|>use serde::{Deserialize, Serialize};\n\nuse super::message_attempt_endpoint_out::MessageAttemptEndpointOut;\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ListResponseMessageAttemptEndpointOut {\n    pub data: Vec<MessageAttemptEndpointOut>,\n\n    pub done: bool,\n\n    pub iterator: String,\n\n    #[serde(rename = \"prevIterator\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub prev_iterator: Option<String>,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl ListResponseMessageAttemptEndpointOut {\n    pub fn new(data: Vec<MessageAttemptEndpointOut>, done: bool, iterator: String) -> Self {\n        Self {\n            data,\n            done,\n            iterator,\n            prev_iterator: None,\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/list_response_message_attempt_endpoint_out.rs", "node_type": "impl_item", "line_range": [18, 27]}
{"prompt": "<|fim_prefix|>r ID. If you\n    /// require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    pub async fn list(\n        &self,\n        app_id: String,\n        options: Option<MessageListOptions>,\n    ) -> Result<ListResponseMessageOut> {\n        let MessageListOptions {\n            limit,\n            iterator,\n            channel,\n            before,\n            after,\n            with_content,\n            tag,\n            event_types,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/msg\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"channel\", channel)\n            .with_optional_query_param(\"before\", before)\n            .with_optional_query_param(\"after\", after)\n            .with_optional_query_param(\"with_content\", with_content)\n            .with_optional_query_param(\"tag\", tag)\n            .with_optional_query_param(\"event_types\", event_types)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Creates a new message and dispatches it to all of the application's\n    /// endpoints.\n    ///\n    /// The `eventId` is an optional custom unique ID. It's verified to be\n    /// unique only up to a day, after that no verification will be made. If\n    /// a message with the same `eventId` already exists for the application, a\n    /// 409 conflict error will be returned.\n    ///\n    /// The `eventType` indicates the type and schema of the event. All messages\n    /// of a certain `eventType` are expected to have the same schema. Endpoints\n    /// can choose to only listen to specific event types. Messages can also\n    /// have `channels`, which similar to event types let endpoints filter by\n    /// them. Unlike event types, messages can have multiple channels, and\n    /// channels don't imply a specific message content or schema.\n    ///\n    /// The `payload` property is the webhook's body (the actual webhook\n    /// message). Svix supports payload sizes of up to 1MiB, though it's\n    /// generally a good idea to keep webhook payloads small, probably no larger\n    /// than 40kb.\n    pub async fn create(\n        &self,\n        app_id: String,\n        message_in: MessageIn,\n        options: Option<MessageCreateOptions>,\n    ) -> Result<MessageOut> {\n        let MessageCreateOptions {\n            with_content,\n            idempotency_key,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/app/{app_id}/msg\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"with_content\", with_content)\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(message_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete all message payloads for the application.\n    ///\n    /// This operation is only available in the <a href=\"https://svix.com/pricing\" target=\"_blank\">Enterprise</a> plan.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"application.purge_content\",\n    ///   \"data\": {\n    ///     \"messagesPurged\": 150\n    ///   }\n    /// }\n    /// ```\n    pub async fn expunge_all_contents(\n        &self,\n        app_id: String,\n        options: Option<MessageExpungeAllContentsOptions>,\n    ) -> Result<ExpungeAllContentsOut> {\n        let MessageExpungeAllContentsOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/msg/expunge-all-contents\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get a message by its ID or eventID.\n    pub async fn get(\n        &self,\n        app_id: String,\n        msg_id: String,\n        options: Option<MessageGetOptions>,\n    ) -> Result<MessageOut> {\n        <|fim_suffix|>\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/msg/{msg_id}\")\n            .with_path_param(\"app_id\", app_id)\n            .with_path_param(\"msg_id\", msg_id)\n            .with_optional_query_param(\"with_content\", with_content)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Delete the given message's payload.\n    ///\n    /// Useful in cases when a message was accidentally sent with sensitive\n    /// content. The message can't be replayed or resent once its payload\n    /// has been deleted or expired.\n    pub async fn expunge_content(&self, app_id: String, msg_id: String) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/app/{app_id}/msg/{msg_id}/content\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"msg_id\", msg_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    #[cfg(feature = \"svix_beta\")]\n    pub async fn events(\n        &self,\n        params: V1MessageEventsParams,\n    ) -> Result<crate::models::MessageEventsOut> {\n        let V1MessageEventsParams {\n            app_id,\n            limit,\n            iterator,\n            event_types,\n            channels,\n            after,\n        } = params;\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/events\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"event_types\", event_types)\n            .with_optional_query_param(\"channels\", channels)\n            .with_optional_query_param(\"after\", after)\n            .execute(self.cfg)\n            .await\n    }\n\n    #[cfg(feature = \"svix_beta\")]\n    pub async fn events_subscription(\n        &self,\n        params: V1MessageEventsSubscriptionParams,\n    ) -> Result<crate::models::MessageEventsOut> {\n        let V1MessageEventsSubscriptionParams {\n            app_id,\n            subscription_id,\n            limit,\n            iterator,\n            event_types,\n            channels,\n            after,\n        } = params;\n\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/events/subscription/{subscription_id}\",\n        )\n        .with_path_param(\"app_id\", app_id.to_string())\n        .with_path_param(\"subscription_id\", subscription_id.to_string())\n        .with_optional_query_param(\"limit\", limit)\n        .with_optional_query_param(\"iterator\", iterator)\n        .with_optional_query_param(\"event_types\", event_types)\n        .with_optional_query_param(\"channels\", channels)\n        .with_optional_query_param(\"after\", after)\n        .execute(self.cfg)\n        .await\n    }\n}\n\n#[cfg(feature = \"svix_beta\")]\n#[derive(Clone, Debug)]\npub struct V1MessageEventsParams {\n    /// The app's ID or UID\n    pub app_id: String,\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n    /// Filter response based on the event type\n    pub event_types: Option<Vec<String>>,\n    /// Filter response based on the event type.\n    pub channels: Option<Vec<String>>,\n    pub after: Option<String>,\n}\n\n#[cfg(feature = \"svix_beta\")]\n#[derive(Clone, Debug)]\npub struct V1MessageEventsSubscriptionParams {\n    /// The app's ID or UID\n    pub app_id: String,\n    /// The esub's ID or UID\n    pub subscription_id: String,\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n    /// Filter response based on the event type\n    pub event_types: Option<Vec<String>>,\n    /// Filter response based on the event type.\n    pub channels: Option<Vec<String>>,\n    pub after: Option<String>,\n}\n<|fim_middle|>", "completion": "let MessageGetOptions { with_content } = options.unwrap_or_default();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/message.rs", "node_type": "let_declaration", "line_range": [187, 187]}
{"prompt": "<|fim_prefix|>oIter, ResolveError, Resolver, TokioResolver};\nuse http::{header::HeaderName, HeaderMap, HeaderValue, Method, StatusCode, Version};\nuse http_body_util::Full;\nuse hyper::{body::Incoming, ext::HeaderCaseMap, Uri};\nuse hyper_openssl::client::legacy::{HttpsConnector, MaybeHttpsStream};\nuse hyper_util::{\n    client::{\n        legacy::{\n            connect::{\n                dns::Name,\n                proxy::{SocksV5, Tunnel},\n                HttpConnector,\n            },\n            Client,\n        },\n        proxy::matcher::Matcher,\n    },\n    rt::{TokioExecutor, TokioIo},\n};\nuse ipnet::IpNet;\nuse openssl::ssl::{SslConnector, SslConnectorBuilder, SslMethod, SslVerifyMode};\nuse serde::Serialize;\nuse thiserror::Error;\nuse tokio::{net::TcpStream, sync::Mutex};\nuse tower::Service;\n\nuse crate::cfg::{ProxyAddr, ProxyBypassCfg, ProxyConfig};\n\npub type CaseSensitiveHeaderMap = HashMap<String, HeaderValue>;\n\n#[derive(Debug, Error)]\npub enum Error {\n    #[error(\"failure response: {0}\")]\n    FailureStatus(StatusCode),\n\n    #[error(\"requests to this IP range are blocked (see the server configuration)\")]\n    BlockedIp,\n    #[error(\"error resolving name: {0}\")]\n    Resolve(#[from] ResolveError),\n\n    #[error(\"request timed out\")]\n    TimedOut,\n\n    #[error(\"error forming request: {0}\")]\n    InvalidHttpRequest(http::Error),\n    #[error(\"error making request: {0}\")]\n    FailedRequest(hyper_util::client::legacy::Error),\n}\n\nimpl From<hyper_util::client::legacy::Error> for Error {\n    fn from(e: hyper_util::client::legacy::Error) -> Self {\n        let mut dyn_e = &e as &dyn std::error::Error;\n        loop {\n            if dyn_e\n                .to_string()\n                .contains(\"requests to this IP range are blocked\")\n            {\n                return Error::BlockedIp;\n            }\n\n            match dyn_e.source() {\n                Some(source) => dyn_e = source,\n                None => return Error::FailedRequest(e),\n            }\n        }\n    }\n}\n\n#[derive(Clone)]\npub struct WebhookClient {\n    client: Client<SvixHttpsConnector, Full<Bytes>>,\n    whitelist_nets: Arc<Vec<IpNet>>,\n}\n\nfn ssl_builder(disable_tls_verification: bool) -> SslConnectorBuilder {\n    // Openssl is required here -- in practice, rustls does not support many\n    // ciphers that we encounter on a regular basis:\n    let mut ssl = SslConnector::builder(SslMethod::tls()).expect(\"SslConnector build failed\");\n    if disable_tls_verification {\n        ssl.set_verify(SslVerifyMode::NONE);\n    }\n    ssl\n}\n\nimpl WebhookClient {\n    pub fn new(\n        whitelist_nets: Option<Arc<Vec<IpNet>>>,\n        whitelist_names: Option<Arc<Vec<String>>>,\n        dangerous_disable_tls_verification: bool,\n        proxy_config: Option<&ProxyConfig>,\n    ) -> Self {\n        let whitelist_nets = whitelist_nets.unwrap_or_else(|| Arc::new(Vec::new()));\n        let whitelist_names = whitelist_names.unwrap_or_else(|| Arc::new(Vec::new()));\n\n        let dns_resolver = NonLocalDnsResolver::new(whitelist_nets.clone(), whitelist_names);\n        let mut http = HttpConnector::new_with_resolver(dns_resolver);\n        http.enforce_http(false);\n\n        if dangerous_disable_tls_verification {\n            tracing::warn!(\"TLS certificate verification has been disabled by the configuration.\");\n        }\n        let https = SvixHttpsConnector::new(http, proxy_config, dangerous_disable_tls_verification)\n            .expect(\"SvixHttpsConnector build failed\");\n\n        let client = hyper_util::client::legacy::Client::builder(TokioExecutor::new())\n            .http1_ignore_invalid_headers_in_responses(true)\n            .http1_title_case_headers(true)\n            .build(https);\n\n        Self {\n            client,\n            whitelist_nets,\n        }\n    }\n\n    pub async fn execute(&self, request: Request) -> Result<Response, Error> {\n        let resp = self.execute_inner(request, true).await?;\n        Ok(resp.map(Body::new))\n    }\n\n    pub fn execute_inner(\n        &self,\n        request: Request,\n        retry: bool,\n    ) -> BoxFuture<'_, Result<Response<Incoming>, Error>> {\n        async move {\n            <|fim_suffix|>\n            if let Some(auth) = request.uri.authority() {\n                if let Ok(ip) = auth.host().parse::<IpAddr>() {\n                    if !is_allowed(ip)\n                        && !self\n                            .whitelist_nets\n                            .iter()\n                            .any(|subnet| subnet.contains(&ip))\n                    {\n                        return Err(Error::BlockedIp);\n                    }\n                }\n            }\n\n            let mut req = if let Some(body) = request.body {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::from(body))\n                    .map_err(Error::InvalidHttpRequest)?\n            } else {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::default())\n                    .map_err(Error::InvalidHttpRequest)?\n            };\n\n            *req.headers_mut() = request.headers;\n\n            if let Some(header_names) = request.header_names {\n                req.extensions_mut().insert(header_names);\n            }\n\n            let start = Instant::now();\n            let res = if let Some(timeout) = request.timeout {\n                match tokio::time::timeout(timeout, self.client.request(req)).await {\n                    Ok(Ok(resp)) => Ok(resp),\n                    Ok(Err(e)) => Err(e.into()),\n                    Err(_to) => Err(Error::TimedOut),\n                }\n            } else {\n                self.client.request(req).await.map_err(Into::into)\n            };\n\n            if !retry {\n                return res;\n            }\n\n            match res {\n                Err(Error::FailedRequest(e)) if start.elapsed() < Duration::from_millis(1000) => {\n                    tracing::info!(\"Insta-retrying: {e}\");\n                    self.execute_inner(org_req, false).await\n                }\n                res => res,\n            }\n        }\n        .boxed()\n    }\n}\n\n#[derive(Clone)]\npub struct Request {\n    method: Method,\n    uri: Uri,\n    headers: HeaderMap,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    timeout: Option<Duration>,\n    version: Version,\n}\n\npub struct RequestBuilder {\n    method: Option<Method>,\n    uri: Option<Uri>,\n    accept: Option<HeaderValue>,\n    user_agent: Option<HeaderValue>,\n    headers: Option<HeaderMap>,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    version: Option<Version>,\n    timeout: Option<Duration>,\n    basic_auth: Option<Vec<u8>>,\n\n    // Derived from body\n    content_type: Option<HeaderValue>,\n}\n\n#[derive(Debug)]\npub struct RequestBuildError(pub Vec<BuildError>);\n\nimpl std::fmt::Display for RequestBuildError {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        let mut iter = self.0.iter();\n\n        f.write_str(\"Build failed\")?;\n\n        if let Some(first) = iter.next() {\n            write!(f, \": {first}\")?;\n\n            for err in iter {\n                write!(f, \"; {err}\")?;\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Error)]\npub enum BuildError {\n    #[error(\"uri missing\")]\n    UriMissing,\n    #[error(\"version missing\")]\n    VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub fn new() -> Self {\n        Self {\n            method: None,\n            uri: None,\n            accept: None,\n            user_agent: None,\n            headers: None,\n            header_names: None,\n            body: None,\n            version: None,\n            timeout: None,\n            content_type: None,\n            basic_auth: None,\n        }\n    }\n\n    pub fn method(mut self, method: Method) -> Self {\n        self.method = Some(method)<|fim_middle|>", "completion": "let org_req = request.clone();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [142, 142]}
{"prompt": "<|fim_prefix|>//! Requires a rabbitmq node to be running on localhost:5672 (the default port) and using the\n//! default guest/guest credentials.\n//! Try using the `testing-docker-compose.yml` in the repo root to get this going.\n\nuse std::time::Duration;\n\nuse rdkafka::{\n    producer::{FutureProducer, FutureRecord},\n    util::Timeout,\n    ClientConfig,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_kafka::{KafkaConsumer, KafkaInputOpts};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse tracing::info;\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nuse crate::{create_topic, delete_topic, kafka_admin_client, BROKER_HOST};\n\n#[ctor::ctor]\nfn test_setup() {\n    use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n\n    tracing_subscriber::registry()\n        .with(\n            tracing_subscriber::EnvFilter::try_from_default_env().unwrap_or_else(|_| {\n                // Output is only printed for failing tests, but still we shouldn't overload\n                // the output with unnecessary info. When debugging a specific test, it's easy\n                // to override this default by setting the `RUST_LOG` environment variable.\n                \"info,svix_bridge=debug\".into()\n            }),\n        )\n        .with(tracing_subscriber::fmt::layer().with_test_writer())\n        .init();\n}\n\n/// Time to wait for the plugin to connect.\nconst CONNECT_WAIT_TIME: Duration = Duration::from_secs(10);\n/// Time to wait for the plugin to receive a message sent by a test.\nconst CONSUME_WAIT_TIME: Duration = Duration::from_secs(1);\n\nfn get_test_plugin(\n    svix_url: String,\n    topic: &str,\n    use_transformation: Option<TransformerInputFormat>,\n) -> KafkaConsumer {\n    KafkaConsumer::new(\n        \"test\".into(),\n        KafkaInputOpts::Inner {\n            bootstrap_brokers: BROKER_HOST.to_owned(),\n            // All tests use different topics, so it's fine to have only one consumer group ID\n            group_id: \"svix_bridge_test_group_id\".to_owned(),\n            topic: topic.to_owned(),\n            security_protocol: svix_bridge_plugin_kafka::KafkaSecurityProtocol::Plaintext,\n            debug_contexts: None,\n        },\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n    .unwrap()\n}\n\nfn kafka_producer() -> FutureProducer {\n    // create does block I/O, but we don't care in tests\n    ClientConfig::new()\n        .set(\"bootstrap.servers\", BROKER_HOST)\n        .create()\n        .unwrap()\n}\n\nasync fn publish(producer: &FutureProducer, topic: &str, payload: &[u8]) {\n    info!(topic, \"publishing message\");\n    producer\n        .send(\n            FutureRecord::<(), _>::to(topic).payload(payload),\n            Timeout::After(Duration::from_secs(3)),\n        )\n        .await\n        .unwrap();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let topic = unique_topic_name!();\n\n    let admin_client = kafka_admin_client();\n    create_topic(&admin_client, topic).await;\n\n    <|fim_suffix|>\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), topic, None);\n\n    let handle = tokio::spawn(async move {\n        plugin.run().await;\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(CONNECT_WAIT_TIME).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&producer, topic, &serde_json::to_vec(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(CONSUME_WAIT_TIME).await;\n\n    handle.abort();\n    delete_topic(&admin_client, topic).await;\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let topic = unique_topic_name!();\n\n    let admin_client = kafka_admin_client();\n    create_topic(&admin_client, topic).await;\n\n    let producer = kafka_producer();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(mock_server.uri(), topic, Some(TransformerInputFormat::Json));\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        plugin.run().await;\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(CONNECT_WAIT_TIME).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&producer, topic, &serde_json::to_vec(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(CONSUME_WAIT_TIME).await;\n\n    handle.abort();\n    delete_topic(&admin_client, topic).await;\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_string_ok() {\n    let topic = unique_topic_name!();\n\n    let admin_client = kafka_admin_client();\n    create_topic(&admin_client, topic).await;\n\n    let producer = kafka_producer();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 <|fim_middle|>", "completion": "let producer = kafka_producer();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/tests/it/kafka_consumer.rs", "node_type": "let_declaration", "line_range": [107, 107]}
{"prompt": "<|fim_prefix|>trics::CommonMetrics,\n};\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\n#[global_allocator]\nstatic GLOBAL: Jemalloc = Jemalloc;\n\n#[cfg(all(target_env = \"msvc\", feature = \"jemalloc\"))]\ncompile_error!(\"jemalloc cannot be enabled on msvc\");\n\n// Seems like it would be useful to be able to configure this.\n// In some docker setups, hostname is sometimes the container id, and advertising this can be\n// helpful.\nstatic INSTANCE_ID: Lazy<String> = Lazy::new(|| KsuidMs::new(None, None).to_string());\n\nfn get_svc_identifiers(cfg: &Config) -> opentelemetry_sdk::Resource {\n    opentelemetry_sdk::Resource::new(vec![\n        opentelemetry::KeyValue::new(\n            \"service.name\",\n            cfg.opentelemetry\n                .as_ref()\n                .and_then(|x| x.service_name.as_deref())\n                .unwrap_or(\"svix-bridge\")\n                .to_owned(),\n        ),\n        opentelemetry::KeyValue::new(\"instance_id\", INSTANCE_ID.to_owned()),\n    ])\n}\n\nfn setup_tracing(cfg: &Config) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        const CRATE_NAME: &str = env!(\"CARGO_CRATE_NAME\");\n        let level = cfg.log_level.to_string();\n        let var = [\n            format!(\"{CRATE_NAME}={level}\"),\n            // XXX: Assuming this applies to the Producer side (aka `og-ingester`) when we fold it back in.\n            format!(\"tower_http={level}\"),\n        ];\n        var.join(\",\")\n    });\n\n    let otel_layer = cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        // Configure the OpenTelemetry tracing layer\n        opentelemetry::global::set_text_map_propagator(\n            opentelemetry_sdk::propagation::TraceContextPropagator::new(),\n        );\n\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        let tracer = opentelemetry_otlp::new_pipeline()\n            .tracing()\n            .with_exporter(exporter)\n            .with_trace_config(\n                opentelemetry_sdk::trace::Config::default()\n                    .with_sampler(\n                        otel_cfg\n                            .sample_ratio\n                            .map(opentelemetry_sdk::trace::Sampler::TraceIdRatioBased)\n                            .unwrap_or(opentelemetry_sdk::trace::Sampler::AlwaysOn),\n                    )\n                    .with_resource(get_svc_identifiers(cfg)),\n            )\n            .install_batch(Tokio)\n            .unwrap()\n            .tracer(\"svix_bridge\");\n\n        tracing_opentelemetry::layer().with_tracer(tracer)\n    });\n\n    // Then create a subscriber with an additional layer printing to stdout.\n    // This additional layer is either formatted normally or in JSON format.\n    match cfg.log_format {\n        config::LogFormat::Default => {\n            let stdout_layer = tracing_subscriber::fmt::layer();\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n        config::LogFormat::Json => {\n            let fmt = tracing_subscriber::fmt::format().json().flatten_event(true);\n            let json_fields = tracing_subscriber::fmt::format::JsonFields::new();\n\n            let stdout_layer = tracing_subscriber::fmt::layer()\n                .event_format(fmt)\n                .fmt_fields(json_fields);\n\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n    };\n}\n\n/// Delta temporality selector as recommended by upstream:\n/// https://github.com/open-telemetry/opentelemetry-rust/discussions/1511#discussioncomment-8386721\nstruct DeltaTemporalitySelector;\n\nimpl TemporalitySelector for DeltaTemporalitySelector {\n    <|fim_suffix|>\n}\n\npub fn setup_metrics(cfg: &Config) -> Option<SdkMeterProvider> {\n    cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        opentelemetry_otlp::new_pipeline()\n            .metrics(Tokio)\n            .with_temporality_selector(DeltaTemporalitySelector)\n            .with_exporter(exporter)\n            .with_resource(get_svc_identifiers(cfg))\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn supervise_senders(inputs: Vec<Box<dyn SenderInput>>) -> Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"sender input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n#[derive(Parser)]\npub struct Args {\n    #[arg(long, env = \"SVIX_BRIDGE_CFG_FILE\", help = \"Path to the config file.\")]\n    cfg_file: Option<PathBuf>,\n    #[arg(\n        long,\n        env = \"SVIX_BRIDGE_CFG\",\n        help = \"Config data as a string (instead of a file on disk).\",\n        conflicts_with = \"cfg_file\"\n    )]\n    cfg: Option<String>,\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let args = Args::parse(<|fim_middle|>", "completion": "fn temporality(&self, kind: InstrumentKind) -> Temporality {\n        match kind {\n            InstrumentKind::UpDownCounter => Temporality::Cumulative,\n            InstrumentKind::ObservableUpDownCounter => Temporality::Cumulative,\n            _ => Temporality::Delta,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "function_item", "line_range": [142, 148]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct StreamingEventTypeListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n    /// Include archived (deleted but not expunged) items in the response.\n    #[arg(long)]\n    pub include_archived: Option<bool>,\n}\n\nimpl From<StreamingEventTypeListOptions> for svix::api::StreamingEventTypeListOptions {\n    fn from(value: StreamingEventTypeListOptions) -> Self {\n        let StreamingEventTypeListOptions {\n            limit,\n            iterator,\n            order,\n            include_archived,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            order,\n            include_archived,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct StreamingEventTypeCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<StreamingEventTypeCreateOptions> for svix::api::StreamingEventTypeCreateOptions {\n    fn from(value: StreamingEventTypeCreateOptions) -> Self {\n        let StreamingEventTypeCreateOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct StreamingEventTypeDeleteOptions {\n    /// By default, event types are archived when \"deleted\". With this flag, they are deleted entirely.\n    #[arg(long)]\n    pub expunge: Option<bool>,\n}\n\nimpl From<StreamingEventTypeDeleteOptions> for svix::api::StreamingEventTypeDeleteOptions {\n    fn from(value: StreamingEventTypeDeleteOptions) -> Self {\n        let StreamingEventTypeDeleteOptions { expunge } = value;\n        Self { expunge }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct StreamingEventTypeArgs {\n    #[command(subcommand)]\n    pub command: StreamingEventTypeCommands,\n}\n\n#[derive(Subcommand)]\npub enum StreamingEventTypeCommands {\n    /// List of all the organization's event types for streaming.\n    List {\n        #[clap(flatten)]\n        options: StreamingEventTypeListOptions,\n    },\n    /// Create an event type for Streams.\n    Create {\n        stream_event_type_in: crate::json::JsonOf<StreamEventTypeIn>,\n        #[clap(flatten)]\n        options: StreamingEventTypeCreateOptions,\n    },\n    /// Get an event type.\n    Get { name: String },\n    /// Update or create a event type for Streams.\n    Update {\n        name: String,\n        stream_event_type_in: crate::json::JsonOf<StreamEventTypeIn>,\n    },\n    /// Delete an event type.\n    Delete {\n        name: String,\n        #[clap(flatten)]\n        options: StreamingEventTypeDeleteOptions,\n    },\n    /// Patch an event type for Streams.\n    Patch {\n        name: String,\n        stream_event_type_patch: Option<crate::json::JsonOf<StreamEventTypePatch>>,\n    },\n}\n\nimpl StreamingEventTypeCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { options } => {\n                <|fim_suffix|>\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                stream_event_type_in,\n                options,\n            } => {\n                let resp = client\n                    .streaming()\n                    .event_type()\n                    .create(stream_event_type_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { name } => {\n                let resp = client.streaming().event_type().get(name).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                name,\n                stream_event_type_in,\n            } => {\n                let resp = client\n                    .streaming()\n                    .event_type()\n                    .update(name, stream_event_type_in.into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete { name, options } => {\n                client\n                    .streaming()\n                    .event_type()\n                    .delete(name, Some(options.into()))\n                    .await?;\n            }\n            Self::Patch {\n                name,\n                stream_event_type_patch,\n            } => {\n                let resp = client\n                    .streaming()\n                    .event_type()\n                    .patch(\n                        name,\n                        stream_event_type_patch.unwrap_or_default().into_inner(),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let resp = client\n                    .streaming()\n                    .event_type()\n                    .list(Some(options.into()))\n                    .await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/streaming_event_type.rs", "node_type": "let_declaration", "line_range": [113, 117]}
{"prompt": "<|fim_prefix|>ate_limit: model.rate_limit.map(|x| x as u16),\n            uid: model.uid,\n            url: model.url,\n            version: model.version as u16,\n            disabled: model.disabled,\n            event_types_ids: model.event_types_ids,\n            channels: model.channels,\n            created_at: model.created_at.into(),\n            updated_at: model.updated_at.into(),\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointOut {\n    #[serde(flatten)]\n    pub ep: EndpointOutCommon,\n    pub id: EndpointId,\n    pub metadata: Metadata,\n}\n\n// FIXME: This can and should be a derive macro\nimpl From<(endpoint::Model, Metadata)> for EndpointOut {\n    fn from((endp, metadata): (endpoint::Model, Metadata)) -> Self {\n        Self {\n            id: endp.id.clone(),\n            ep: endp.into(),\n            metadata,\n        }\n    }\n}\n\n#[derive(Default, Clone, Debug, PartialEq, Eq, Validate, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointSecretRotateIn {\n    #[validate]\n    #[serde(default)]\n    key: Option<EndpointSecret>,\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointSecretOut {\n    pub key: EndpointSecret,\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Validate, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct RecoverIn {\n    pub since: DateTime<Utc>,\n    pub until: Option<DateTime<Utc>>,\n}\n\nfn endpoint_headers_example() -> HashMap<&'static str, &'static str> {\n    HashMap::from([(\"X-Example\", \"123\"), (\"X-Foobar\", \"Bar\")])\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Validate, Deserialize, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointHeadersIn {\n    #[schemars(example = \"endpoint_headers_example\")]\n    pub headers: EndpointHeaders,\n}\n\nimpl ModelIn for EndpointHeadersIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointHeadersIn { headers } = self;\n        model.headers = Set(Some(headers));\n    }\n}\n\nfn sensitive_headers_example() -> HashSet<String> {\n    HashSet::from([\"Authorization\".to_string()])\n}\n\n/// The value of the headers is returned in the `headers` field.\n///\n/// Sensitive headers that have been redacted are returned in the sensitive field.\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Serialize, Default, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointHeadersOut {\n    #[schemars(example = \"endpoint_headers_example\")]\n    pub headers: HashMap<String, String>,\n    #[schemars(example = \"sensitive_headers_example\")]\n    pub sensitive: HashSet<String>,\n}\n\nimpl EndpointHeadersOut {\n    const SENSITIVE_HEADERS: &'static [&'static str] = &[\n        \"x-auth-token\",\n        \"x-api-key\",\n        \"www-authenticate\",\n        \"authorization\",\n        \"proxy-authenticate\",\n        \"proxy-authorization\",\n    ];\n}\n\nimpl From<EndpointHeaders> for EndpointHeadersOut {\n    fn from(hdr: EndpointHeaders) -> Self {\n        let (sens, remaining) = hdr.0.into_iter().partition(|(k, _)| {\n            let k = k.to_lowercase();\n            Self::SENSITIVE_HEADERS.iter().any(|&x| x == k)\n        });\n\n        Self {\n            headers: remaining,\n            sensitive: sens.into_keys().collect(),\n        }\n    }\n}\n\nfn endpoint_headers_patch_example() -> EndpointHeadersPatch {\n    EndpointHeadersPatch(HashMap::from([\n        (\"X-Example\".to_string(), Some(\"123\".to_string())),\n        (\"X-Foobar\".to_string(), Some(\"Bar\".to_string())),\n    ]))\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Validate, Deserialize, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointHeadersPatchIn {\n    #[validate]\n    #[schemars(example = \"endpoint_headers_patch_example\")]\n    pub headers: EndpointHeadersPatch,\n}\n\nimpl ModelIn for EndpointHeadersPatchIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        l<|fim_suffix|>\n        model.headers = if let Some(Some(mut hdrs)) = model.headers.take() {\n            for (k, v) in headers.0 {\n                if let Some(v) = v {\n                    hdrs.0.insert(k, v);\n                } else {\n                    hdrs.0.remove(&k);\n                }\n            }\n            Set(Some(hdrs))\n        } else {\n            let headers: HashMap<String, String> = headers\n                .0\n                .into_iter()\n                .filter_map(|(k, v)| v.map(|v| (k, v)))\n                .collect();\n            Set(Some(EndpointHeaders(headers)))\n        };\n    }\n}\n\n#[derive(Deserialize, JsonSchema)]\nstruct EndpointStatsRange {\n    since: Option<DateTime<Utc>>,\n    until: Option<DateTime<Utc>>,\n}\n\nimpl EndpointStatsRange {\n    fn validate_unwrap_or_default(self) -> error::Result<(DateTime<Utc>, DateTime<Utc>)> {\n        let until = self.until.unwrap_or_else(Utc::now);\n\n        if until > Utc::now() {\n            return Err(HttpError::bad_request(\n                Some(\"invalid_range\".into()),\n                Some(\"'until' cannot be in the future\".into()),\n            )\n            .into());\n        }\n\n        let since = self.since.unwrap_or(until - Duration::days(28));\n\n        // Add five minutes so that people can easily just do `now() - 28 days`\n        // without having to worry about clock sync\n        if until - since > (Duration::days(28) + Duration::minutes(5)) {\n            return Err(HttpError::bad_request(\n                Some(\"invalid_range\".into()),\n                Some(format!(\n                    \"'since' cannot be more than 28 days prior to {until}\"\n                )),\n            )\n            .into());\n        }\n\n        Ok((since, until))\n    }\n}\n\n#[derive(Deserialize, Serialize, JsonSchema)]\n#[schemars(rename = \"EndpointStats\")]\npub struct EndpointStatsOut {\n    pub success: i64,\n    pub pending: i64,\n    pub sending: i64,\n    pub fail: i64,\n}\n\n#[derive(Debug, FromQueryResult)]\npub struct EndpointStatsQueryOut {\n    status: MessageStatus,\n    count: i64,\n}\n\n/// Get basic statistics for the endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.get-stats\")]\nasync fn endpoint_stats(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    Query(range): Query<EndpointStatsRange>,\n    permissions::Application { app }: permissions::Application,\n) -> error::Result<Json<EndpointStatsOut>> {\n    let (since, until) = range.validate_unwrap_or_default()?;\n\n    let endpoint = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?\n        .id;\n\n    let query_out: Vec<EndpointStatsQueryOut> =\n        messageattempt::Entity::secure_find_by_endpoint(endpoint)\n            .after(since)\n            .before(until)\n            .select_only()\n            .column(messageattempt::Column::Status)\n            .column_as(messageattempt::Column::Status.count(), \"count\")\n            .group_by(messageattempt::Column::Status)\n            .into_model::<EndpointStatsQueryOut>()\n            .all(db)\n            .await?;\n    let mut query_out = query_out\n        .into_iter()\n        .map(|EndpointStatsQueryOut { status, count }| (status, count))\n        .collect::<HashMap<_, _>>();\n\n    Ok(Json(EndpointStatsOut {\n        success: query_out.remove(&MessageStatus::Success).unwrap_or(0),\n        pending: query_out.remove(&MessageStatus::Pending).unwrap_or(0),\n        fail: query_out.remove(&MessageStatus::Fail).unwrap_or(0),\n        sending: query_out.remove(&MessageStatus::Sending).unwrap_or(0),\n    }))\n}\n\n#[derive(Deserialize, JsonSchema, Validate)]\n#[serde(rename_all = \"camelCase\")]\nstruct EventExampleIn {\n    event_type: EventTypeName,\n}\n\nconst SVIX_PING_EVENT_TYPE_NAME: &str = \"svix.ping\";\nconst SVIX_PING_EVENT_TYPE_PAYLOAD: &str = r#\"{\"success\": true}\"#;\n\n/// Send an example message for an event\n#[aide_annotate(\n    op_id = \"v1.endpoint.send-example\",\n    op_summary = \"Send Event Type Example Message\"\n)]\nasync fn send_example(\n    state<|fim_middle|>", "completion": "let EndpointHeadersPatchIn { headers } = self;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "let_declaration", "line_range": [624, 624]}
{"prompt": "<|fim_prefix|>s.push(\n            create_test_endpoint(&client, &app.id, &receiver.endpoint)\n                .await\n                .unwrap(),\n        );\n    }\n\n    let mut messages = Vec::new();\n    for i in 1..=5usize {\n        messages.push(\n            create_test_message(\n                &client,\n                &app.id,\n                serde_json::json!({\n                    \"test\": i,\n                }),\n            )\n            .await\n            .unwrap(),\n        );\n    }\n    messages.push(\n        create_test_msg_with(\n            &client,\n            &app.id,\n            serde_json::json!({\"test\": \"data6\"}),\n            \"balloon.popped\",\n            [\"news\"],\n        )\n        .await,\n    );\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        for endp_id in eps.iter().map(|ep| &ep.id) {\n            let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{}/attempt/endpoint/{endp_id}/\", app.id),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if list.data.len() != 6 {\n                anyhow::bail!(\"list len {}, not 6\", list.data.len());\n            }\n\n            let list_filtered: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\n                        \"api/v1/app/{}/attempt/endpoint/{endp_id}/?channel=news\",\n                        app.id\n                    ),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n            assert_eq!(list_filtered.data.len(), 1);\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // By message\n    for msg in &messages {\n        let all_attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/msg/{}/\", app.id, msg.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        // Test Limit\n        let first_three: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/msg/{}/?limit=3\", app.id, msg.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(all_attempts.data.len(), 6);\n        assert_eq!(first_three.data.len(), 3);\n\n        assert_eq!(&all_attempts.data[0..3], first_three.data.as_slice());\n\n        // Forward iterator\n        let last_three_manual: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=3&iterator={}\",\n                    app.id, msg.id, all_attempts.data[2].id\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let last_three_iter_field: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=3&iterator={}\",\n                    app.id,\n                    msg.id,\n                    first_three.iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(last_three_manual.data, last_three_iter_field.data);\n\n        assert_eq!(last_three_manual.data.len(), 3);\n        assert_eq!(&all_attempts.data[3..6], last_three_manual.data.as_slice());\n        assert!(last_three_manual.done);\n\n        // `prev` iterator\n        let two_and_three: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=2&iterator={}\",\n                    app.id,\n                    msg.id,\n                    last_three_manual.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(two_and_three.data.len(), 2);\n        assert_eq!(&all_attempts.data[1..3], two_and_three.data.as_slice());\n        assert!(!two_and_three.done);\n\n        l<|fim_suffix|>\n        assert_eq!(one.data.len(), 1);\n        assert_eq!(all_attempts.data[0], one.data[0]);\n        assert!(one.done);\n\n        // Because messages are dispatched so quickly, a different approach than above needs to be tested\n        // for checking by time.\n\n        // `after` field\n        let all_six_by_time: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?after={}\",\n                    app.id,\n                    msg.id,\n                    sub_5ms(all_attempts.data[5].created_at)\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        assert_eq!(all_attempts.data, all_six_by_time.data);\n\n        let none_by_time: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?after={}\",\n                    app.id,\n                    msg.id,\n                    add_5ms(all_attempts.data[0].created_at)\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        assert!(none_by_time.data.is_empty());\n\n        // `before field`\n        let all_six_by_time: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?before={}\",\n                    app.id,\n                    msg.id,\n                    add_5ms(all_attempts.data[0].created_at),\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        assert_eq!(all_attempts.data, all_six_by_time.data);\n\n        let none_by_time: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?before={}\",\n                    app.id,\n                    msg.id,\n                    sub_5ms(all_attempts.data[5].created_at),\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        assert!(none_by_time.data.is_empty());\n    }\n\n    /// Adds 5ms to a [`chrono::DateTime`] for testing `before` and `after`\n    fn add_5ms<T: chrono::TimeZone>(ts: chrono::DateTime<T>) -> chrono::DateTime<T> {\n        ts + chrono::Duration::from_std(std::time::Duration::from_millis(5)).unwrap()\n    }\n\n    /// Subtracts 5ms to a [`chrono::DateTime`] for testing `before` and `after`\n    fn sub_5ms<T: chrono::TimeZone>(ts: chrono::DateTime<T>) -> chrono::DateTime<T> {\n        ts - chrono::Duration::from_std(std::time::Duration::from_millis(5)).unwrap()\n    }\n}\n\n#[tokio::test]\nasync fn test_pagination_forward_and_back() {\n    let (client, _) = start_svix_server().await;\n\n    let app = create_test_app(&client, \"test_app\").await.unwrap();\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ep = create_test_endpoint(&client, &app.id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let mut messages = Vec::new();\n    for i in 1..=28usize {\n        messages.push(\n            create_test_message(\n                &client,\n                &app.id,\n                serde_json::json!({\n                    \"test\": i,\n                }),\n            )\n            .await\n            .unwrap(),\n        );\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 28 {\n            anyhow::bail!(\"list len {}, not 28\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // Go forward\n    let mut forward_msgs = Vec::new();\n    let mut done = false;\n    let mut prev_iterator = None;\n    let mut iterator = None;\n\n    while !done {\n        let iter_suffix = if let Some(iter) = iterator {\n            format!(\"&iterator={iter}\")\n        } else {\n          <|fim_middle|>", "completion": "let one: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=2&iterator={}\",\n                    app.id,\n                    msg.id,\n                    two_and_three.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [1359, 1370]}
{"prompt": "<|fim_prefix|>one,\n        payload_retention_period: 90,\n        extra_params: None,\n        application: None,\n    };\n\n    let create_message = create_message_inner(\n        db,\n        queue_tx,\n        cache,\n        false,\n        Some(endpoint.id),\n        msg_in,\n        app.org_id,\n        ApplicationIdOrUid(app.id.0),\n    )\n    .await?;\n\n    Ok(Json(create_message))\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Endpoint\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app/:app_id/endpoint\",\n            post_with(crud::create_endpoint, crud::create_endpoint_operation)\n                .get_with(crud::list_endpoints, crud::list_endpoints_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id\",\n            get_with(crud::get_endpoint, crud::get_endpoint_operation)\n                .put_with(crud::update_endpoint, crud::update_endpoint_operation)\n                .patch_with(crud::patch_endpoint, crud::patch_endpoint_operation)\n                .delete_with(crud::delete_endpoint, crud::delete_endpoint_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret\",\n            get_with(\n                secrets::get_endpoint_secret,\n                secrets::get_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret/rotate\",\n            post_with(\n                secrets::rotate_endpoint_secret,\n                secrets::rotate_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/stats\",\n            get_with(endpoint_stats, endpoint_stats_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/send-example\",\n            post_with(send_example, send_example_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/recover\",\n            post_with(\n                recovery::recover_failed_webhooks,\n                recovery::recover_failed_webhooks_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/headers\",\n            get_with(\n                headers::get_endpoint_headers,\n                headers::get_endpoint_headers_operation,\n            )\n            .patch_with(\n                headers::patch_endpoint_headers,\n                headers::patch_endpoint_headers_operation,\n            )\n            .put_with(\n                headers::update_endpoint_headers,\n                headers::update_endpoint_headers_operation,\n            ),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use std::collections::{HashMap, HashSet};\n\n    use reqwest::Url;\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{validate_url, EndpointHeadersOut, EndpointHeadersPatchIn, EndpointIn};\n    use crate::core::types::EndpointHeaders;\n\n    const URL_VALID: &str = \"https://www.example.com\";\n    const URL_INVALID: &str = \"invalid url\";\n    const VERSION_VALID: u16 = 1;\n    const VERSION_INVALID: u16 = 0;\n    const RATE_LIMIT_VALID: u16 = 1;\n    const RATE_LIMIT_INVALID: u16 = 0;\n    const EVENT_TYPES_INVALID: &[&str] = &[\"valid-event-type\", \"&&invalid-event-type\"];\n    const EVENT_TYPES_VALID: &[&str] = &[\"valid-event-type1\", \"valid-event-type2\"];\n    const EVENT_CHANNELS_INVALID: &[&str] = &[\"valid-event-channel\", \"&&invalid-event-channel\"];\n    const EVENT_CHANNELS_VALID: &[&str] = &[\"valid-event-channel1\", \"valid-event-channel2\"];\n    const ENDPOINT_ID_INVALID: &str = \"$$invalid-endpoint\";\n    const ENDPOINT_ID_VALID: &str = \"valid-endpoint\";\n\n    #[allow(deprecated)]\n    #[test]\n    fn test_endpoint_in_validation() {\n        let invalid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_INVALID,\n             \"url\": URL_VALID\n        }))\n        .unwrap();\n\n        let invalid_2: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"channels\": EVENT_CHANNELS_INVALID\n        }))\n        .unwrap();\n\n        let invalid_3: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_INVALID\n        }))\n        .unwrap();\n\n        let invalid_4: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"uid\": ENDPOINT_ID_INVALID\n        }))\n        .unwrap();\n\n        l<|fim_suffix|>\n        let invalid_6: Result<EndpointIn, _> = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_INVALID\n        }));\n        assert!(invalid_6.is_err());\n\n        for e in [invalid_1, invalid_2, invalid_3, invalid_4, invalid_5] {\n            assert!(e.validate().is_err());\n        }\n\n        let valid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_1.validate().unwrap();\n\n        let valid_2: EndpointIn = serde_json::from_value(json!({\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_2.validate().unwrap();\n        assert_eq!(1, valid_2.version.unwrap());\n    }\n\n    #[test]\n    fn test_endpoint_headers_sensitive() {\n        let headers = EndpointHeaders(HashMap::from([\n            (\"foo\".to_string(), \"1\".to_string()),\n            (\"authorization\".to_string(), \"test\".to_string()),\n            (\"X-Auth-Token\".to_string(), \"test2\".to_string()),\n        ]));\n\n        let headers_out: EndpointHeadersOut = headers.into();\n\n        assert_eq!(\n            headers_out.headers,\n            HashMap::from([(\"foo\".to_string(), \"1\".to_string())])\n        );\n        assert_eq!(\n            headers_out.sensitive,\n            HashSet::from([\"authorization\".to_string(), \"X-Auth-Token\".to_string()])\n        );\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_in_validation() {\n        let headers_valid = HashMap::from([(\"x-valid\", \"1\")]);\n        let headers_invalid = HashMap::from([(\"x-invalid???\", \"1\")]);\n\n        let invalid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_invalid })).unwrap();\n        assert!(invalid.validate().is_err());\n\n        let valid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_valid })).unwrap();\n        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_url_validation() {\n        let valid_https = Url::parse(\"https://test.url\").unwrap();\n        let valid_http = Url::parse(\"http://test.url\").unwrap();\n        let invalid_scheme = Url::parse(\"anythingelse://test.url\").unwrap();\n        let invalid_format = \"http://[:::1]\";\n\n        assert!(validate_url(&valid_https).is_ok());\n        assert!(validate_url(&valid_http).is_ok());\n        assert!(validate_url(&invalid_scheme).is_err());\n\n        let valid_https: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_https, \"version\": 1})).unwrap();\n        let valid_http: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_http, \"version\": 1})).unwrap();\n        let invalid_scheme: EndpointIn =\n            serde_json::from_value(json!({\"url\": invalid_scheme, \"version\": 1})).unwrap();\n        let invalid_format: Result<EndpointIn, _> =\n            serde_json::from_value(json!({\"url\": invalid_format, \"version\": 1}));\n\n        assert!(valid_https.validate().is_ok());\n        assert!(valid_http.validate().is_ok());\n        assert!(invalid_scheme.validate().is_err());\n        assert!(invalid_format.is_err());\n    }\n}\n<|fim_middle|>", "completion": "let invalid_5: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"filterTypes\": EVENT_TYPES_INVALID\n        }))\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "let_declaration", "line_range": [939, 944]}
{"prompt": "<|fim_prefix|>// Modified version of the file openapi-generator would usually put in\n// apis/request.rs\n\nuse std::{collections::HashMap, time::Duration};\n\nuse http1::header::{HeaderValue, AUTHORIZATION, CONTENT_LENGTH, CONTENT_TYPE, USER_AGENT};\nuse http_body_util::{BodyExt as _, Full};\nuse hyper::body::Bytes;\nuse itertools::Itertools as _;\nuse percent_encoding::{utf8_percent_encode, AsciiSet, CONTROLS};\nuse rand::Rng;\nuse serde::de::DeserializeOwned;\n\nuse crate::{error::Error, models, Configuration};\n\n#[allow(dead_code)]\npub(crate) enum Auth {\n    None,\n    Bearer,\n}\n\n/// If the authorization type is unspecified then it will be automatically\n/// detected based on the configuration. This functionality is useful when the\n/// OpenAPI definition does not include an authorization scheme.\n#[derive(Clone)]\npub(crate) struct Request {\n    method: http1::Method,\n    path: &'static str,\n    query_params: HashMap<&'static str, String>,\n    no_return_type: bool,\n    path_params: HashMap<&'static str, String>,\n    header_params: HashMap<&'static str, String>,\n    // TODO: multiple body params are possible technically, but not supported here.\n    serialized_body: Option<String>,\n}\n\nimpl Request {\n    pub fn new(method: http1::Method, path: &'static str) -> Self {\n        Request {\n            method,\n            path,\n            query_params: HashMap::new(),\n            path_params: HashMap::new(),\n            header_params: HashMap::new(),\n            serialized_body: None,\n            no_return_type: false,\n        }\n    }\n\n    pub fn with_body_param<T: serde::Serialize>(mut self, param: T) -> Self {\n        self.serialized_body = Some(serde_json::to_string(&param).unwrap());\n        self\n    }\n\n    pub fn with_optional_header_param(\n        mut self,\n        basename: &'static str,\n        param: Option<String>,\n    ) -> Self {\n        <|fim_suffix|>\n        self\n    }\n\n    pub fn with_query_param(mut self, basename: &'static str, param: impl QueryParamValue) -> Self {\n        self.query_params.insert(basename, param.encode());\n        self\n    }\n\n    pub fn with_optional_query_param<T: QueryParamValue>(\n        mut self,\n        basename: &'static str,\n        param: Option<T>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.query_params.insert(basename, value.encode());\n        }\n        self\n    }\n\n    pub fn with_path_param(mut self, basename: &'static str, param: String) -> Self {\n        self.path_params.insert(basename, param);\n        self\n    }\n\n    pub fn returns_nothing(mut self) -> Self {\n        self.no_return_type = true;\n        self\n    }\n\n    pub async fn execute<T: DeserializeOwned>(self, conf: &Configuration) -> Result<T, Error> {\n        match self.execute_with_backoff(conf).await? {\n            // This is a hack; if there's no_ret_type, T is (), but serde_json gives an\n            // error when deserializing \"\" into (), so deserialize 'null' into it\n            // instead.\n            // An alternate option would be to require T: Default, and then return\n            // T::default() here instead since () implements that, but then we'd\n            // need to impl default for all models.\n            None => Ok(serde_json::from_str(\"null\").expect(\"serde null value\")),\n            Some(bytes) => Ok(serde_json::from_slice(&bytes).map_err(Error::generic)?),\n        }\n    }\n\n    async fn execute_with_backoff(mut self, conf: &Configuration) -> Result<Option<Bytes>, Error> {\n        let no_return_type = self.no_return_type;\n        if self.method == http1::Method::POST && !self.header_params.contains_key(\"idempotency-key\")\n        {\n            self.header_params\n                .insert(\"idempotency-key\", format!(\"auto_{}\", uuid::Uuid::new_v4()));\n        }\n\n        const MAX_BACKOFF: Duration = Duration::from_secs(5);\n\n        let retry_schedule = match &conf.retry_schedule {\n            Some(schedule) => schedule,\n            None => &std::iter::successors(Some(Duration::from_millis(20)), |last_backoff| {\n                Some(MAX_BACKOFF.min(*last_backoff * 2))\n            })\n            .take(conf.num_retries as usize)\n            .collect(),\n        };\n        let mut retries = retry_schedule.iter();\n\n        let mut request = self.build_request(conf)?;\n        request\n            .headers_mut()\n            .insert(\"svix-req-id\", rand::rng().random::<u32>().into());\n\n        let mut retry_count = 0;\n\n        let execute_request = async |request| {\n            let response = conf.client.request(request).await.map_err(Error::generic)?;\n\n            let status = response.status();\n            if !status.is_success() {\n                Err(Error::from_response(status, response.into_body()).await)\n            } else if no_return_type {\n                Ok(None)\n            } else {\n                let bytes = response\n                    .into_body()\n                    .collect()\n                    .await\n                    .map_err(Error::generic)?\n                    .to_bytes();\n                Ok(Some(bytes))\n            }\n        };\n\n        loop {\n            let request_fut = execute_request(request.clone());\n            let res = if let Some(duration) = conf.timeout {\n                tokio::time::timeout(duration, request_fut)\n                    .await\n                    .map_err(Error::generic)?\n            } else {\n                request_fut.await\n            };\n\n            let next_backoff = retries.next().copied();\n\n            match res {\n                Ok(result) => return Ok(result),\n                e @ Err(Error::Validation(_)) => return e,\n                Err(Error::Http(err)) if err.status.as_u16() < 500 => return Err(Error::Http(err)),\n                e @ Err(_) => {\n                    if next_backoff.is_none() {\n                        return e;\n                    }\n                }\n            }\n\n            tokio::time::sleep(next_backoff.expect(\"next_backoff is always Some\")).await;\n            retry_count += 1;\n\n            request\n                .headers_mut()\n                .insert(\"svix-retry-count\", retry_count.into());\n        }\n    }\n\n    fn build_request(self, conf: &Configuration) -> Result<http1::Request<Full<Bytes>>, Error> {\n        const FRAGMENT: &AsciiSet = &CONTROLS.add(b' ').add(b'\"').add(b'<').add(b'>').add(b'`');\n        const PATH: &AsciiSet = &FRAGMENT.add(b'#').add(b'?').add(b'{').add(b'}');\n        const PATH_SEGMENT: &AsciiSet = &PATH.add(b'/').add(b'%');\n\n        let mut path = self.path.to_owned();\n        for (k, v) in self.path_params {\n            // replace {id} with the value of the id path param\n            let percent_encoded_path_param_value =\n                utf8_percent_encode(&v, PATH_SEGMENT).to_string();\n            path = path.replace(&format!(\"{{{k}}}\"), &percent_encoded_path_param_value);\n        }\n\n        let mut uri = format!(\"{}{}\", conf.base_path, path);\n\n        let mut query_string = url::form_urlencoded::Serializer::new(\"\".to_owned());\n        for (key, val) in self.query_params {\n            query_string.append_pair(key, &val);\n        }\n\n        let query_string_str = query_string.finish();\n        if !query_string_str.is_empty() {\n            uri += \"?\";\n            uri += &query_string_str;\n        }\n\n        let uri = http1::Uri::try_from(uri).map_err(Error::generic)?;\n        let mut req_builder = http1::Request::builder().uri(uri).method(self.method);\n\n        let mut request = if let Some(body) = self.serialized_body {\n            let req_headers = req_builder.headers_mut().unwrap();\n            req_headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n            req_headers.insert(CONTENT_LENGTH, body.len().into());\n            req_builder.body(Full::from(body)).map_err(Error::generic)?\n        } else {\n            req_builder.body(Full::default()).map_err(Error::generic)?\n        };\n\n        let request_headers = request.headers_mut();\n\n        // Detect the authorization type if it hasn't been set.\n        let auth = if conf.bearer_access_token.is_some() {\n            Auth::Bearer\n        } else {\n            Auth::None\n        };\n        match auth {\n            Auth::Bearer => {\n                if let Some(token) = &conf.be<|fim_middle|>", "completion": "if let Some(value) = param {\n            self.header_params.insert(basename, value);\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/request.rs", "node_type": "if_expression", "line_range": [60, 62]}
{"prompt": "<|fim_prefix|>// Modified version of the file openapi-generator would usually put in\n// apis/request.rs\n\nuse std::{collections::HashMap, time::Duration};\n\n<|fim_suffix|>\nuse http_body_util::{BodyExt as _, Full};\nuse hyper::body::Bytes;\nuse itertools::Itertools as _;\nuse percent_encoding::{utf8_percent_encode, AsciiSet, CONTROLS};\nuse rand::Rng;\nuse serde::de::DeserializeOwned;\n\nuse crate::{error::Error, models, Configuration};\n\n#[allow(dead_code)]\npub(crate) enum Auth {\n    None,\n    Bearer,\n}\n\n/// If the authorization type is unspecified then it will be automatically\n/// detected based on the configuration. This functionality is useful when the\n/// OpenAPI definition does not include an authorization scheme.\n#[derive(Clone)]\npub(crate) struct Request {\n    method: http1::Method,\n    path: &'static str,\n    query_params: HashMap<&'static str, String>,\n    no_return_type: bool,\n    path_params: HashMap<&'static str, String>,\n    header_params: HashMap<&'static str, String>,\n    // TODO: multiple body params are possible technically, but not supported here.\n    serialized_body: Option<String>,\n}\n\nimpl Request {\n    pub fn new(method: http1::Method, path: &'static str) -> Self {\n        Request {\n            method,\n            path,\n            query_params: HashMap::new(),\n            path_params: HashMap::new(),\n            header_params: HashMap::new(),\n            serialized_body: None,\n            no_return_type: false,\n        }\n    }\n\n    pub fn with_body_param<T: serde::Serialize>(mut self, param: T) -> Self {\n        self.serialized_body = Some(serde_json::to_string(&param).unwrap());\n        self\n    }\n\n    pub fn with_optional_header_param(\n        mut self,\n        basename: &'static str,\n        param: Option<String>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.header_params.insert(basename, value);\n        }\n        self\n    }\n\n    pub fn with_query_param(mut self, basename: &'static str, param: impl QueryParamValue) -> Self {\n        self.query_params.insert(basename, param.encode());\n        self\n    }\n\n    pub fn with_optional_query_param<T: QueryParamValue>(\n        mut self,\n        basename: &'static str,\n        param: Option<T>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.query_params.insert(basename, value.encode());\n        }\n        self\n    }\n\n    pub fn with_path_param(mut self, basename: &'static str, param: String) -> Self {\n        self.path_params.insert(basename, param);\n        self\n    }\n\n    pub fn returns_nothing(mut self) -> Self {\n        self.no_return_type = true;\n        self\n    }\n\n    pub async fn execute<T: DeserializeOwned>(self, conf: &Configuration) -> Result<T, Error> {\n        match self.execute_with_backoff(conf).await? {\n            // This is a hack; if there's no_ret_type, T is (), but serde_json gives an\n            // error when deserializing \"\" into (), so deserialize 'null' into it\n            // instead.\n            // An alternate option would be to require T: Default, and then return\n            // T::default() here instead since () implements that, but then we'd\n            // need to impl default for all models.\n            None => Ok(serde_json::from_str(\"null\").expect(\"serde null value\")),\n            Some(bytes) => Ok(serde_json::from_slice(&bytes).map_err(Error::generic)?),\n        }\n    }\n\n    async fn execute_with_backoff(mut self, conf: &Configuration) -> Result<Option<Bytes>, Error> {\n        let no_return_type = self.no_return_type;\n        if self.method == http1::Method::POST && !self.header_params.contains_key(\"idempotency-key\")\n        {\n            self.header_params\n                .insert(\"idempotency-key\", format!(\"auto_{}\", uuid::Uuid::new_v4()));\n        }\n\n        const MAX_BACKOFF: Duration = Duration::from_secs(5);\n\n        let retry_schedule = match &conf.retry_schedule {\n            Some(schedule) => schedule,\n            None => &std::iter::successors(Some(Duration::from_millis(20)), |last_backoff| {\n                Some(MAX_BACKOFF.min(*last_backoff * 2))\n            })\n            .take(conf.num_retries as usize)\n            .collect(),\n        };\n        let mut retries = retry_schedule.iter();\n\n        let mut request = self.build_request(conf)?;\n        request\n            .headers_mut()\n            .insert(\"svix-req-id\", rand::rng().random::<u32>().into());\n\n        let mut retry_count = 0;\n\n        let execute_request = async |request| {\n            let response = conf.client.request(request).await.map_err(Error::generic)?;\n\n            let status = response.status();\n            if !status.is_success() {\n                Err(Error::from_response(status, response.into_body()).await)\n            } else if no_return_type {\n                Ok(None)\n            } else {\n                let bytes = response\n                    .into_body()\n                    .collect()\n                    .await\n                    .map_err(Error::generic)?\n                    .to_bytes();\n                Ok(Some(bytes))\n            }\n        };\n\n        loop {\n            let request_fut = execute_request(request.clone());\n            let res = if let Some(duration) = conf.timeout {\n                tokio::time::timeout(duration, request_fut)\n                    .await\n                    .map_err(Error::generic)?\n            } else {\n                request_fut.await\n            };\n\n            let next_backoff = retries.next().copied();\n\n            match res {\n                Ok(result) => return Ok(result),\n                e @ Err(Error::Validation(_)) => return e,\n                Err(Error::Http(err)) if err.status.as_u16() < 500 => return Err(Error::Http(err)),\n                e @ Err(_) => {\n                    if next_backoff.is_none() {\n                        return e;\n                    }\n                }\n            }\n\n            tokio::time::sleep(next_backoff.expect(\"next_backoff is always Some\")).await;\n            retry_count += 1;\n\n            request\n                .headers_mut()\n                .insert(\"svix-retry-count\", retry_count.into());\n        }\n    }\n\n    fn build_request(self, conf: &Configuration) -> Result<http1::Request<Full<Bytes>>, Error> {\n        const FRAGMENT: &AsciiSet = &CONTROLS.add(b' ').add(b'\"').add(b'<').add(b'>').add(b'`');\n        const PATH: &AsciiSet = &FRAGMENT.add(b'#').add(b'?').add(b'{').add(b'}');\n        const PATH_SEGMENT: &AsciiSet = &PATH.add(b'/').add(b'%');\n\n        let mut path = self.path.to_owned();\n        for (k, v) in self.path_params {\n            // replace {id} with the value of the id path param\n            let percent_encoded_path_param_value =\n                utf8_percent_encode(&v, PATH_SEGMENT).to_string();\n            path = path.replace(&format!(\"{{{k}}}\"), &percent_encoded_path_param_value);\n        }\n\n        let mut uri = format!(\"{}{}\", conf.base_path, path);\n\n        let mut query_string = url::form_urlencoded::Serializer::new(\"\".to_owned());\n        for (key, val) in self.query_params {\n            query_string.append_pair(key, &val);\n        }\n\n        let query_string_str = query_string.finish();\n        if !query_string_str.is_empty() {\n            uri += \"?\";\n            uri += &query_string_str;\n        }\n\n        let uri = http1::Uri::try_from(uri).map_err(Error::generic)?;\n        let mut req_builder = http1::Request::builder().uri(uri).method(self.method);\n\n        let mut request = if let Some(body) = self.serialized_body {\n            let req_headers = req_builder.headers_mut().unwrap();\n            req_headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n            req_headers.insert(CONTENT_LENGTH, body.len().into());\n            req_builder.body(Full::from(body)).map_err(Error::generic)?\n        } else {\n            req_builder.body(Full::default()).map_err(Error::generic)?\n        };\n\n        let request_headers = request.headers_mut();\n\n        // Detect the authorization type if it hasn't been set.\n        let auth = if conf.bearer_access_token.is_some() {\n            Auth::Bearer\n        } else {\n            Auth::None\n        };\n        match auth {\n            Auth::Bearer => {\n                if let Some(token) = &con<|fim_middle|>", "completion": "use http1::header::{HeaderValue, AUTHORIZATION, CONTENT_LENGTH, CONTENT_TYPE, USER_AGENT};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/request.rs", "node_type": "use_declaration", "line_range": [6, 6]}
{"prompt": "<|fim_prefix|>rror, HttpError, Result, ValidationErrorItem},\n    queue::{MessageTaskBatch, TaskQueueProducer},\n    v1::{\n        endpoints::application::{create_app_from_app_in, ApplicationIn},\n        utils::{\n            filter_and_paginate_time_limited, openapi_tag, validation_error, validation_errors,\n            ApplicationMsgPath, ApplicationPath, EventTypesQueryParams, JsonStatus, ListResponse,\n            ModelIn, ModelOut, PaginationDescending, PaginationLimit, ReversibleIterator,\n            ValidatedJson, ValidatedQuery,\n        },\n    },\n    AppState,\n};\n\npub fn validate_channels_msg(channels: &EventChannelSet) -> Result<(), ValidationError> {\n    let len = channels.0.len();\n    if !(1..=5).contains(&len) {\n        Err(validation_error(\n            Some(\"channels\"),\n            Some(\"Channels must have at least 1 and at most 5 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, Deserialize, Serialize)]\npub struct RawPayload(pub Box<RawValue>);\n\nimpl JsonSchema for RawPayload {\n    fn schema_name() -> String {\n        \"RawPayload\".to_string()\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        serde_json::Value::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\nimpl std::fmt::Display for RawPayload {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        f.write_str(self.0.get())\n    }\n}\n\nimpl Eq for RawPayload {}\n\nimpl PartialEq for RawPayload {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.get() == other.0.get()\n    }\n}\n\nimpl RawPayload {\n    pub fn from_string(val: String) -> serde_json::Result<Self> {\n        Ok(Self(RawValue::from_string(val)?))\n    }\n}\n\npub fn validate_raw_payload_is_object(payload: &RawPayload) -> Result<(), ValidationError> {\n    // Verify it's an object/map\n    if payload.0.get().starts_with('{') {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"payload\"),\n            Some(\"Payload must be an object.\"),\n        ))\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Serialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageIn {\n    /// Optional unique identifier for the message\n    #[validate]\n    #[serde(rename = \"eventId\", skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<MessageUid>,\n    #[validate]\n    pub event_type: EventTypeName,\n    #[validate(custom = \"validate_raw_payload_is_object\")]\n    #[serde(alias = \"payload\", alias = \"data\")]\n    #[schemars(example = \"example_payload\")]\n    pub payload: RawPayload,\n    /// List of free-form identifiers that endpoints can filter by\n    #[validate(custom = \"validate_channels_msg\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 5))]\n    pub channels: Option<EventChannelSet>,\n    #[validate(range(min = 5, max = 90))]\n    #[serde(default = \"default_90\")]\n    #[schemars(example = \"default_90\")]\n    pub payload_retention_period: i64,\n    #[serde(rename = \"transformationsParams\")]\n    #[schemars(skip)]\n    pub extra_params: Option<MessageInExtraParams>,\n\n    /// Optionally creates a new application alongside the message.\n    ///\n    /// If the application id or uid that is used in the path already exists,\n    /// this argument is ignored.\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub application: Option<ApplicationIn>,\n}\n\nimpl MessageIn {\n    fn payload(&self) -> Vec<u8> {\n        if let Some(params) = &self.extra_params {\n            if let Some(raw_payload) = &params.raw_payload {\n                return raw_payload.as_bytes().to_owned();\n            }\n        }\n\n        self.payload.0.get().as_bytes().to_owned()\n    }\n}\n\n#[derive(Clone, Debug, Default, PartialEq, Eq, Deserialize, Serialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageInExtraParams {\n    raw_payload: Option<String>,\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nf<|fim_suffix|>\n// FIXME: This can and should be a derive macro\nimpl ModelIn for MessageIn {\n    type ActiveModel = message::ActiveModel;\n\n    fn update_model(self, model: &mut message::ActiveModel) {\n        let MessageIn {\n            uid,\n            event_type,\n            channels,\n            payload_retention_period,\n            ..\n        } = self;\n\n        let expiration = Utc::now() + Duration::days(payload_retention_period);\n\n        model.uid = Set(uid);\n        model.event_type = Set(event_type);\n        model.expiration = Set(expiration.with_timezone(&Utc).into());\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageOut {\n    /// Optional unique identifier for the message\n    #[serde(rename = \"eventId\")]\n    pub uid: Option<MessageUid>,\n    pub event_type: EventTypeName,\n    #[schemars(example = \"example_payload\")]\n    pub payload: RawPayload,\n    /// List of free-form identifiers that endpoints can filter by\n    #[schemars(length(min = 1, max = 5), example = \"example_channel_set\")]\n    pub channels: Option<EventChannelSet>,\n    pub id: MessageId,\n    #[serde(rename = \"timestamp\")]\n    pub created_at: DateTime<Utc>,\n}\n\nimpl MessageOut {\n    pub fn from_msg_and_payload(\n        model: message::Model,\n        content: Option<Vec<u8>>,\n        with_content: bool,\n    ) -> Self {\n        let payload = if with_content {\n            let payload = content\n                .and_then(|p| match serde_json::from_slice(&p) {\n                    Ok(v) => Some(v),\n                    Err(e) => {\n                        tracing::error!(\"Failed to parse content: {e}\");\n                        None\n                    }\n                })\n                .or(model.legacy_payload);\n            RawPayload::from_string(match payload {\n                Some(payload) => serde_json::to_string(&payload).expect(\"Can never fail\"),\n                None => r#\"{\"expired\":true}\"#.to_string(),\n            })\n            .expect(\"Can never fail\")\n        } else {\n            RawPayload::from_string(\"{}\".to_string()).expect(\"Can never fail\")\n        };\n\n        Self {\n            uid: model.uid,\n            event_type: model.event_type,\n            payload,\n            channels: model.channels,\n            id: model.id,\n            created_at: model.created_at.into(),\n        }\n    }\n}\n\nfn default_true() -> bool {\n    true\n}\n\nfn default_90() -> i64 {\n    90\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct ListMessagesQueryParams {\n    /// Filter response based on the channel\n    #[validate]\n    channel: Option<EventChannel>,\n    /// Only include items created before a certain date\n    before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    after: Option<DateTime<Utc>>,\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// List all of the application's messages.\n///\n/// The `before` parameter lets you filter all items created before a certain date and is ignored if an iterator is passed.\n/// The `after` parameter lets you filter all items created after a certain date and is ignored if an iterator is passed.\n/// `before` and `after` cannot be used simultaneously.\n#[aide_annotate(op_id = \"v1.message.list\")]\nasync fn list_messages(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<PaginationDescending<ReversibleIterator<MessageId>>>,\n    ValidatedQuery(ListMessagesQueryParams {\n        channel,\n        with_content,\n        before,\n        after,\n    }): ValidatedQuery<ListMessagesQueryParams>,\n    EventTypesQueryParams(event_types): EventTypesQueryParams,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<ListResponse<MessageOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n\n    let mut query = message::Entity::secure_find(app.id);\n\n    if let Some(EventTypeNameSet(event_types)) = event_types {\n     <|fim_middle|>", "completion": "fn example_payload() -> serde_json::Value {\n    serde_json::json!({\n        \"email\": \"test@example.com\",\n        \"username\": \"test_user\"\n    })\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/message.rs", "node_type": "function_item", "line_range": [166, 171]}
{"prompt": "<|fim_prefix|>mod cluster;\nmod sentinel;\n\nuse std::{sync::Arc, time::Duration};\n\nuse bb8::{Pool, RunError};\nuse bb8_redis::RedisConnectionManager;\nuse redis::{\n    aio::ConnectionManagerConfig, sentinel::SentinelNodeConnectionInfo, AsyncConnectionConfig,\n    ProtocolVersion, RedisConnectionInfo, RedisError, TlsMode,\n};\nuse sentinel::RedisSentinelConnectionManager;\nuse tokio::sync::Mutex;\n\npub use self::cluster::RedisClusterConnectionManager;\nuse crate::cfg::{CacheBackend, QueueBackend, SentinelConfig};\n\npub const REDIS_CONN_TIMEOUT: Duration = Duration::from_secs(2);\n\npub enum RedisVariant<'a> {\n    Clustered,\n    NonClustered,\n    Sentinel(&'a SentinelConfig),\n}\n\n#[derive(Clone)]\npub enum RedisManager {\n    Clustered(Pool<RedisClusterConnectionManager>),\n    NonClustered(Pool<RedisConnectionManager>),\n    Sentinel(Pool<crate::redis::sentinel::RedisSentinelConnectionManager>),\n    ClusteredUnpooled(redis::cluster_async::ClusterConnection),\n    NonClusteredUnpooled(redis::aio::ConnectionManager),\n    SentinelUnpooled(Arc<Mutex<redis::sentinel::SentinelClient>>),\n}\n\nimpl RedisManager {\n    async fn new_pooled(dsn: &str, variant: RedisVariant<'_>, max_conns: u16) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let mgr = RedisClusterConnectionManager::new(dsn)\n                    .expect(\"Error initializing redis cluster client\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis cluster connection pool\");\n                RedisManager::Clustered(pool)\n            }\n            RedisVariant::NonClustered => {\n                let mgr =\n                    RedisConnectionManager::new(dsn).expect(\"Error initializing redis client\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::NonClustered(pool)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);\n                let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };\n                let mgr = RedisSentinelConnectionManager::new(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                )\n                .expect(\"Error initializing RedisSentinelConnectionManager\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::Sentinel(pool)\n            }\n        }\n    }\n\n    async fn new_unpooled(dsn: &str, variant: RedisVariant<'_>) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let cli = redis::cluster::ClusterClient::builder(vec![dsn])\n                    .retries(1)\n                    .connection_timeout(REDIS_CONN_TIMEOUT)\n                    .build()\n                    .expect(\"Error initializing redis-unpooled cluster client\");\n                <|fim_suffix|>\n                RedisManager::ClusteredUnpooled(con)\n            }\n            RedisVariant::NonClustered => {\n                let cli =\n                    redis::Client::open(dsn).expect(\"Error initializing redis unpooled client\");\n                let con = redis::aio::ConnectionManager::new_with_config(\n                    cli,\n                    ConnectionManagerConfig::new()\n                        .set_number_of_retries(1)\n                        .set_connection_timeout(REDIS_CONN_TIMEOUT),\n                )\n                .await\n                .expect(\"Failed to get redis-unpooled connection manager\");\n                RedisManager::NonClusteredUnpooled(con)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);\n                let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };\n                let cli = redis::sentinel::SentinelClient::build(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                    redis::sentinel::SentinelServerType::Master,\n                )\n                .expect(\"Failed to build sentinel client\");\n\n                RedisManager::SentinelUnpooled(Arc::new(Mutex::new(cli)))\n            }\n        }\n    }\n\n    pub async fn from_cache_backend(cache_backend: &CacheBackend<'_>) -> Self {\n        match cache_backend {\n            CacheBackend::Redis(dsn) => Self::new_unpooled(dsn, RedisVariant::NonClustered).await,\n            CacheBackend::RedisCluster(dsn) => {\n                Self::new_unpooled(dsn, RedisVariant::Clustered).await\n            }\n            CacheBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_unpooled(dsn, RedisVariant::Sentinel(cfg)).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn from_queue_backend(queue_backend: &QueueBackend<'_>, max_conns: u16) -> Self {\n        match queue_backend {\n            QueueBackend::Redis(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::NonClustered, max_conns).await\n            }\n            QueueBackend::RedisCluster(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::Clustered, max_conns).await\n            }\n            QueueBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_pooled(dsn, RedisVariant::Sentinel(cfg), max_conns).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn get(&self) -> Result<RedisConnection<'_>, RunError<RedisError>> {\n        match self {\n            Self::Clustered(pool) => Ok(RedisConnection::Clustered(pool.get().await?)),\n            Self::NonClustered(pool) => Ok(RedisConnection::NonClustered(pool.get().await?)),\n            Self::Sentinel(pool) => Ok(RedisConnection::SentinelPooled(pool.get().await?)),\n            Self::ClusteredUnpooled(conn) => Ok(RedisConnection::ClusteredUnpooled(conn.clone())),\n            Self::NonClusteredUnpooled(conn) => {\n                Ok(RedisConnection::NonClusteredUnpooled(conn.clone()))\n            }\n            Self::SentinelUnpooled(conn) => {\n                let mut conn = conn.lock().await;\n                let con = conn\n                    .get_async_connection_with_config(\n                        &AsyncConnectionConfig::new().set_response_timeout(REDIS_CONN_TIMEOUT),\n                    )\n                    .await?;\n                Ok(RedisConnection::SentinelUnpooled(con))\n            }\n        }\n    }\n}\n\npub enum RedisConnection<'a> {\n    Clustered(bb8::PooledConnection<'a, RedisClusterConnectionManager>),\n    NonClustered(bb8::PooledConnection<'a, RedisConnectionManager>),\n    SentinelPooled(bb8::PooledConnection<'a, RedisSentinelConn<|fim_middle|>", "completion": "let con = cli\n                    .get_async_connection()\n                    .await\n                    .expect(\"Failed to get redis-cluster-unpooled connection\");", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/redis/mod.rs", "node_type": "let_declaration", "line_range": [98, 101]}
{"prompt": "<|fim_prefix|>'s two separate functions in the libs.\n    #[serde(default = \"default_as_false\")]\n    get_if_exists: bool,\n}\n\n/// Create a new application.\n#[aide_annotate(op_id = \"v1.application.create\")]\nasync fn create_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    query: ValidatedQuery<CreateApplicationQueryParams>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<ApplicationIn>,\n) -> Result<JsonStatusUpsert<ApplicationOut>> {\n    if let Some(ref uid) = data.uid {\n        if let Some((app, metadata)) =\n            application::Model::fetch_with_metadata(db, org_id.clone(), uid.clone().into())\n                .await\n                .trace()?\n        {\n            if query.get_if_exists {\n                // Technically not updated, but it fits.\n                return Ok(JsonStatusUpsert::Updated((app, metadata).into()));\n            }\n            return Err(HttpError::conflict(\n                None,\n                Some(\"An application with that id or uid already exists\".into()),\n            )\n            .into());\n        };\n    }\n\n    let (app, metadata) = create_app_from_app_in(db, data, org_id).await?;\n\n    Ok(JsonStatusUpsert::Created((app, metadata).into()))\n}\n\npub async fn create_app_from_app_in(\n    db: &DatabaseConnection,\n    app_in: ApplicationIn,\n    org_id: OrganizationId,\n) -> Result<(application::Model, applicationmetadata::Model)> {\n    let app = application::ActiveModel::new(org_id);\n    let metadata = applicationmetadata::ActiveModel::new(app.id.clone().unwrap(), None);\n\n    let mut model = (app, metadata);\n    app_in.update_model(&mut model);\n    let (app, metadata) = model;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            async move {\n                let app_result = app.insert(txn).await.map_err(http_error_on_conflict)?;\n                let metadata = metadata.upsert_or_delete(txn).await.trace()?;\n                Ok((app_result, metadata))\n            }\n            .boxed()\n        })\n        .await?;\n\n    Ok((app, metadata))\n}\n\n/// Get an application.\n#[aide_annotate(op_id = \"v1.application.get\")]\nasync fn get_application(\n    permissions::ApplicationWithMetadata { app, metadata }: permissions::ApplicationWithMetadata,\n) -> Result<Json<ApplicationOut>> {\n    Ok(Json((app, metadata).into()))\n}\n\n/// Update an application.\n#[aide_annotate(op_id = \"v1.application.update\")]\nasync fn update_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationPath { app_id }): Path<ApplicationPath>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<ApplicationIn>,\n) -> Result<JsonStatusUpsert<ApplicationOut>> {\n    let (app, metadata, create_models) = if let Some((app, metadata)) =\n        application::Model::fetch_with_metadata(db, org_id.clone(), app_id)\n            .await\n            .trace()?\n    {\n        (app.into(), metadata.into(), false)\n    } else {\n        let app = application::ActiveModel::new(org_id);\n        let metadata = applicationmetadata::ActiveModel::new(app.id.clone().unwrap(), None);\n        (app, metadata, true)\n    };\n\n    let mut models = (app, metadata);\n    data.update_model(&mut models);\n    let (app, metadata) = models;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            async move {\n                let app = if create_models {\n                    app.insert(txn).await.map_err(http_error_on_conflict)?\n                } else {\n                    app.update(txn).await.map_err(http_error_on_conflict)?\n                };\n                let metadata = metadata.upsert_or_delete(txn).await?;\n                Ok((app, metadata))\n            }\n            .boxed()\n        })\n        .await?;\n\n    if create_models {\n        Ok(JsonStatusUpsert::Created((app, metadata).into()))\n    } else {\n        Ok(JsonStatusUpsert::Updated((app, metadata).into()))\n    }\n}\n\n/// Partially update an application.\n#[aide_annotate]\nasync fn patch_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n    ValidatedJson(data): ValidatedJson<ApplicationPatch>,\n) -> Result<Json<ApplicationOut>> {\n    let metadata = app.fetch_or_create_metadata(db).await.trace()?;\n    let app: application::ActiveModel = app.into();\n\n    let mut model = (app, metadata);\n    data.update_model(&mut model);\n    let (app, metadata) = model;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            async move {\n                let app = app.update(txn).await.map_err(http_error_on_conflict)?;\n                let metadata = metadata.upsert_or_delete(txn).await.trace()?;\n                Ok((app, metadata))\n            }\n            .boxed()\n        })\n        .await?;\n\n    Ok(Json((app, metadata).into()))\n}\n\n/// Delete an application.\n#[aide_annotate(op_id = \"v1.application.delete\")]\nasync fn delete_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n) -> Result<NoContent> {\n    let mut app: application::ActiveModel = app.into();\n    app.deleted = Set(true);\n    app.uid = Set(None); // We don't want deleted UIDs to clash\n    app.update(db).await?;\n    Ok(NoContent)\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Application\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app\",\n            post_with(create_application, create_application_operation)\n                .get_with(list_applications, list_applications_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id\",\n            get_with(get_application, get_application_operation)\n                .put_with(update_application, update_application_operation)\n                .patch_with(patch_application, patch_application_operation)\n                .delete_with(delete_application, delete_application_operation),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{ApplicationIn, ApplicationPatch};\n\n    const APP_NAME_INVALID: &str = \"\";\n    const APP_NAME_VALID: &str = \"test-app\";\n    const RATE_LIMIT_INVALID: u16 = 0;\n    const RATE_LIMIT_VALID: u16 = 1;\n    const UID_INVALID: &str = \"$$invalid-uid\";\n    const UID_VALID: &str = \"valid-uid\";\n\n    #[test]\n    fn test_application_in_validation() {\n        let invalid_1: ApplicationIn =\n            serde_json::from_value(json!({ \"name\": APP_NAME_INVALID })).unwrap();\n        let invalid_2: ApplicationIn = serde_json::from_value(json!({\n                    \"name\": APP_NAME_VALID,\n                    \"rateLimit\": RATE_LIMIT_INVALID }))\n        .unwrap();\n        let invalid_3: ApplicationIn = serde_json::from_value(json!({\n                    \"name\": APP_NAME_VALID,\n                    \"uid\": UID_INVALID }))\n        .unwrap();\n\n        for a in [invalid_1, invalid_2, invalid_3] {\n            assert!(a.validate().is_err());\n        }\n\n        let valid: ApplicationIn = serde_json::from_value(json!({\n            \"name\": APP_NAME_VALID,\n            \"rateLimit\": RATE_LIMIT_VALID,\n            \"uid\": UID_VALID,\n        }))\n        .unwrap();\n        valid.validate().unwrap();\n    }\n\n    // FIXME: How to eliminate the repetition here?\n    #[test]\n    fn test_application_patch_validation() {\n        l<|fim_suffix|>        let invalid_2: ApplicationPatch = serde_json::from_value(json!({\n                    \"name\": APP_NAME_VALID,\n                    \"rateLimit\": RATE_LIMIT_INVALID }))\n        .unwrap();\n        let invalid_3: ApplicationPatch = serde_json::from_value(json!({\n                    \"name\": APP_NAME_VALID,\n                    \"uid\": UID_INVALID }))\n        .unwrap();\n\n        for a in [invalid_1, invalid_2, invalid_3] {\n            assert!(a.validate().is_err());\n        }\n\n        let valid: ApplicationPatch = serde_json::from_value(json!({\n            \"name\": APP_NAME_VALID,\n            \"rateLimit\": RATE_LIMIT_VALID,\n            \"uid\": UID_VALID,\n        }))\n        .unwrap();\n        valid.validate().unwrap();\n    }\n}\n<|fim_middle|>", "completion": "let invalid_1: ApplicationPatch =\n            serde_json::from_value(json!({ \"name\": APP_NAME_INVALID })).unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/application.rs", "node_type": "let_declaration", "line_range": [461, 462]}
{"prompt": "<|fim_prefix|>           .unwrap(),\n        );\n        headers\n    }\n\n    fn get_unbranded_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(UNBRANDED_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    #[test]\n    fn test_sign() {\n        let wh = Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap();\n        assert_eq!(\n            \"v1,tZ1I4/hDygAJgO5TYxiSd6Sd0kDW6hPenDe+bTa3Kkw=\".to_owned(),\n            wh.sign(\n                \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\",\n                1649367553,\n                br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#\n            )\n            .unwrap()\n        );\n    }\n\n    #[test]\n    fn test_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            wh.verify(payload, &headers).unwrap();\n        }\n    }\n\n    #[test]\n    fn test_no_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = \"v1,R3PTzyfHASBKHH98a7yexTwaJ4yNIcGhFQc1yuN+BPU=\".to_owned();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_partial_signature() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        // Just `v1,`\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = format!(\n                \"{},\",\n                signature.split(',').collect::<Vec<&str>>().first().unwrap()\n            );\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n\n        // Non-empty but still partial signature (first few bytes)\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = &signature[0..8];\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_incorrect_timestamp() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        // Checks that timestamps that are in the future or too old are rejected by\n        // `verify` but okay for `verify_ignoring_timestamp`.\n        for ts in [\n            OffsetDateTime::now_utc().unix_timestamp() - (super::TOLERANCE_IN_SECONDS + 1),\n            OffsetDateTime::now_utc().unix_timestamp() + (super::TOLERANCE_IN_SECONDS + 1),\n        ] {\n            let signature = wh.sign(msg_id, ts, payload).unwrap();\n            let mut headers = get_svix_headers(msg_id, &signature);\n            headers.insert(\n                super::SVIX_MSG_TIMESTAMP_KEY,\n                ts.to_string().parse().unwrap(),\n            );\n\n            assert!(wh.verify(payload, &headers,).is_err());\n            // Timestamp tolerance is not considered in this case.\n            assert!(wh.verify_ignoring_timestamp(payload, &headers,).is_ok());\n        }\n\n        l<|fim_suffix|>        let signature = wh.sign(msg_id, ts, payload).unwrap();\n        let mut headers = get_svix_headers(msg_id, &signature);\n        headers.insert(\n            super::SVIX_MSG_TIMESTAMP_KEY,\n            // Timestamp mismatch!\n            (ts + 1).to_string().parse().unwrap(),\n        );\n\n        // Both versions should reject the timestamp if it's not the same one used to\n        // produce the signature.\n        assert!(wh.verify(payload, &headers,).is_err());\n        assert!(wh.verify_ignoring_timestamp(payload, &headers,).is_err());\n    }\n\n    #[test]\n    fn test_verify_with_multiple_signatures() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        let multi_sig = format!(\n            \"{} {} {} {}\",\n            \"v1,tFtCZ5RDCPxzWQRWXWPgrCgE2frDBe9gjpbWQxnVfsQ=\",\n            \"v1,Mm7xgUVICxZfQ3bgf0h0Dof65L/IFx+PnZvnDWPCX6Q=\",\n            signature,\n            \"v1,9DfC1c3eeOrXB6w/5dIDydLNQaEyww5KalE5jLBZucE=\",\n        );\n\n        let headers = get_svix_headers(msg_id, &multi_sig);\n\n        wh.verify(payload, &headers).unwrap();\n    }\n\n    #[test]\n    fn test_no_verify_with_multiple_signatures() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let missing_sig = format!(\n            \"{} {} {}\",\n            \"v1,tFtCZ5RDCPxzWQRWXWPgrCgE2frDBe9gjpbWQxnVfsQ=\",\n            \"v1,Mm7xgUVICxZfQ3bgf0h0Dof65L/IFx+PnZvnDWPCX6Q=\",\n            \"v1,9DfC1c3eeOrXB6w/5dIDydLNQaEyww5KalE5jLBZucE=\",\n        );\n\n        let headers = get_svix_headers(msg_id, &missing_sig);\n\n        assert!(wh.verify(payload, &headers).is_err());\n    }\n\n    #[test]\n    fn test_missing_headers() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for (mut hdr_map, hdrs) in [\n            (\n                get_svix_headers(msg_id, &signature),\n                [\n                    SVIX_MSG_ID_KEY,\n                    SVIX_MSG_SIGNATURE_KEY,\n                    SVIX_MSG_TIMESTAMP_KEY,\n                ],\n            ),\n            (\n                get_unbranded_headers(msg_id, &signature),\n                [\n                    UNBRANDED_MSG_ID_KEY,\n                    UNBRANDED_MSG_SIGNATURE_KEY,\n                    UNBRANDED_MSG_TIMESTAMP_KEY,\n                ],\n            ),\n        ] {\n            for hdr in hdrs {\n                hdr_map.remove(hdr);\n                assert!(wh.verify(payload, &hdr_map).is_err());\n            }\n        }\n    }\n}\n<|fim_middle|>", "completion": "let ts = OffsetDateTime::now_utc().unix_timestamp();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/webhooks.rs", "node_type": "let_declaration", "line_range": [357, 357]}
{"prompt": "<|fim_prefix|>use rdkafka::{\n    error::KafkaError,\n    producer::{FutureProducer, FutureRecord},\n    util::Timeout,\n};\nuse svix_bridge_types::{async_trait, BoxError, ForwardRequest, ReceiverOutput};\n\n<|fim_suffix|>\n\n/// Forwards webhook payloads to kafka.\npub struct KafkaProducer {\n    name: String,\n    topic: String,\n    producer: FutureProducer,\n}\n\nimpl KafkaProducer {\n    pub fn new(name: String, opts: KafkaOutputOpts) -> Result<Self, KafkaError> {\n        let KafkaOutputOpts::Inner { topic, .. } = &opts;\n        let topic = topic.clone();\n        let producer = opts.create_producer()?;\n\n        Ok(Self {\n            name,\n            topic,\n            producer,\n        })\n    }\n}\n\n#[async_trait]\nimpl ReceiverOutput for KafkaProducer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.producer\n            .send(\n                FutureRecord::<(), _>::to(&self.topic)\n                    .payload(&serde_json::to_vec(&request.payload)?),\n                Timeout::Never,\n            )\n            .await\n            .map_err(|(e, _msg)| e)?;\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "use crate::config::KafkaOutputOpts;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/output.rs", "node_type": "use_declaration", "line_range": [8, 8]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse aide::axum::{\n    routing::{get, get_with},\n    ApiRouter,\n};\nuse axum::{extract::State, http::StatusCode, Json};\nuse sea_orm::{query::Statement, ConnectionTrait, DatabaseBackend};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::aide_annotate;\n\nuse crate::{\n    core::cache::{kv_def, CacheBehavior, CacheKey, CacheValue},\n    queue::QueueTask,\n    v1::utils::{openapi_tag, NoContent},\n    AppState,\n};\n\nasync fn ping() -> NoContent {\n    NoContent\n}\n\n#[derive(Debug, Deserialize, Serialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum HealthStatusVariant {\n    Ok,\n    Error,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthStatus {\n    status: HealthStatusVariant,\n    // TODO: information field\n}\n\nimpl HealthStatus {\n    pub fn new_ok() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Ok,\n        }\n    }\n\n    pub fn new_error() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Error,\n        }\n    }\n\n    pub fn is_ok(&self) -> bool {\n        matches!(\n            self,\n            HealthStatus {\n                status: HealthStatusVariant::Ok,\n                ..\n            }\n        )\n    }\n}\nimpl<O, E> From<Result<O, E>> for HealthStatus {\n    fn from(res: Result<O, E>) -> Self {\n        match res {\n            Ok(_) => HealthStatus::new_ok(),\n            Err(_) => HealthStatus::new_error(),\n        }\n    }\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthReport {\n    database: HealthStatus,\n\n    queue: HealthStatus,\n    cache: HealthStatus,\n}\n\n#[derive(Deserialize, Serialize, Debug, PartialEq)]\nstruct HealthCheckCacheValue(());\nkv_def!(HealthCheckCacheKey, HealthCheckCacheValue);\n\n/// Verify the API server is up and running.\n#[aide_annotate(op_id = \"v1.health.get\")]\nasync fn health(\n    State(AppState {\n        ref db,\n        queue_tx,\n        cache,\n        ..\n    }): State<AppState>,\n) -> (StatusCode, Json<HealthReport>) {\n    // SELECT 1 FROM any table\n    let database: HealthStatus = db\n        .execute(Statement::from_string(\n            DatabaseBackend::Postgres,\n            \"SELECT 1\".to_owned(),\n        ))\n        .await\n        .into();\n\n    // Send a [`HealthCheck`] through the queue\n    let queue: HealthStatus = queue_tx.send(&QueueTask::HealthCheck, None).await.into();\n\n    // Set a cache value with an expiration to ensure it works\n    let cache: HealthStatus = cache\n        .set(\n            &HealthCheckCacheKey(\"health_check_value\".to_owned()),\n            &HealthCheckCacheValue(()),\n            // Expires after this time, so it won't pollute the DB\n            Duration::from_millis(100),\n        )\n        .await\n        .into();\n\n    let status = if database.is_ok() && queue.is_ok() && cache.is_ok() {\n        StatusCode::OK\n    } else {\n        StatusCode::INTERNAL_SERVER_ERROR\n    };\n\n    (\n        status,\n        Json(HealthReport {\n            database,\n            queue,\n            cache,\n        }),\n    )\n}\n\np<|fim_suffix|><|fim_middle|>", "completion": "pub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Health\");\n\n    ApiRouter::new()\n        .api_route(\"/health/ping\", get(ping).head(ping))\n        .api_route_with(\n            \"/health\",\n            get_with(health, |op| op.response::<204, ()>().with(health_operation))\n                .head_with(health, health_operation),\n            tag,\n        )\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/health.rs", "node_type": "function_item", "line_range": [132, 143]}
{"prompt": "<|fim_prefix|>//! Module that test the dashboard-access endpoint and associated JWT tokens. This module will test\n//! that the tokens returned by the endpoint have restricted functionality and that the response\n//! from the endpoint is valid in the process.\n\nuse rand::distributions::DistString;\nuse reqwest::StatusCode;\nuse serde::de::IgnoredAny;\nuse serde_json::{json, Value};\nuse svix_server::{\n    core::{\n        security::{INVALID_TOKEN_ERR, JWT_SECRET_ERR},\n        types::ApplicationId,\n    },\n    v1::endpoints::application::ApplicationOut,\n};\n\nuse crate::utils::{\n    common_calls::{app_portal_access, application_in},\n    get_default_test_config, start_svix_server,\n};\n\n#[tokio::test]\n/// Users with application-level tokens should only be allowed to read the information related to\n/// their one application. All other endpoints should error.\nasync fn test_restricted_application_access() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n    let app_id_2: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME_2\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let client = app_portal_access(&client, &app_id, Default::default()).await;\n\n    // CREATE, UPDATE, DELETE, and LIST ops\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::FORBIDDEN,\n        )\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/\"),\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::FORBIDDEN,\n        )\n        .await\n        .unwrap();\n    client\n        .delete(&format!(\"api/v1/app/{app_id}/\"), StatusCode::FORBIDDEN)\n        .await\n        .unwrap();\n    let _: IgnoredAny = client\n        .get(\"api/v1/app/\", StatusCode::FORBIDDEN)\n        .await\n        .unwrap();\n\n    // READ should succeed when accessing the app_id the token is authorized for but no others\n    let _: IgnoredAny = client\n        .get(&format!(\"api/v1/app/{app_id_2}/\"), StatusCode::NOT_FOUND)\n        .await\n        .unwrap();\n    let _: ApplicationOut = client\n        .get(&format!(\"api/v1/app/{app_id}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_dashboard_access_without_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    // We just need to ensure we get an OK response without a body.\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/dashboard-access/{app_id}/\"),\n            (),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_invalid_auth_error_detail() {\n    let (mut client, _jh) = start_svix_server().await;\n    let cfg = get_default_test_config();\n    let jwt_secret = match cfg.jwt_signing_config.as_ref() {\n        svix_server::core::security::JwtSigningConfig::Default { jwt_secret } => {\n            std::str::from_utf8(&jwt_secret.to_bytes())\n                .unwrap()\n                .to_owned()\n        }\n\n        _ => return,\n    };\n\n    client.set_auth_header(\"some-nonsense-key\".to_string());\n    match client\n        .post::<_, Value>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::UNAUTHORIZED,\n        )\n        .await\n    {\n        Ok(Value::Object(i)) => {\n            assert_eq!(i.get(\"detail\").unwrap(), INVALID_TOKEN_ERR);\n        }\n        _ => {\n            panic!(\"Unexpected response\");\n        }\n    }\n    client.set_auth_header(jwt_secret);\n    match client\n        .post::<_, Value>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::UNAUTHORIZED,\n        )\n        .await\n    {\n        Ok(Value::Object(i)) => {\n            assert_eq!(i.get(\"detail\").unwrap(), JWT_SECRET_ERR);\n        }\n        _ => {\n            panic!(\"Unexpected response\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_app_portal_access_with_application() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_uid = format!(\n        \"app-created-in-portal-{}\",\n        rand::distributions::Alphanumeric.sample_string(&mut rand::thread_rng(), 15)\n    );\n\n    // app-portal-access without the application field fails\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": []\n            }),\n            StatusCode::NOT_FOUND,\n        )\n        .await\n        .unwrap();\n\n    // app-portal-access with application\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App Created With Portal Access\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // app was created\n    <|fim_suffix|>\n\n    assert_eq!(app[\"uid\"], app_uid);\n    assert_eq!(app[\"name\"], \"Test App Created With Portal Access\");\n\n    // Access portal again with application field - should be ignored since app exists\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/auth/app-portal-access/{app_uid}/\"),\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Updated name will be ignored\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Verify the app name didn't change\n    let app_after: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(app_after[\"name\"], \"Test App Created With Portal Access\");\n\n    // UID in path must match UID in body\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/auth/app-portal-access/different-uid/\",\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App\",\n                    \"uid\": app_uid,  // This doesn't match the path\n                }\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    // UID must be set in body when creating\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/auth/app-portal-access/new-app-uid/\",\n            json!({\n                \"featureFlags\": [],\n                \"application\": {\n                    \"name\": \"Test App Without UID\",\n                    // Missing uid field\n                }\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n}\n<|fim_middle|>", "completion": "let app: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_auth.rs", "node_type": "let_declaration", "line_range": [197, 200]}
{"prompt": "<|fim_prefix|>         }),\n                    }),\n                )\n                .expect(\"Error initializing RedisSentinelConnectionManager\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::Sentinel(pool)\n            }\n        }\n    }\n\n    async fn new_unpooled(dsn: &str, variant: RedisVariant<'_>) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let cli = redis::cluster::ClusterClient::builder(vec![dsn])\n                    .retries(1)\n                    .connection_timeout(REDIS_CONN_TIMEOUT)\n                    .build()\n                    .expect(\"Error initializing redis-unpooled cluster client\");\n                let con = cli\n                    .get_async_connection()\n                    .await\n                    .expect(\"Failed to get redis-cluster-unpooled connection\");\n                RedisManager::ClusteredUnpooled(con)\n            }\n            RedisVariant::NonClustered => {\n                let cli =\n                    redis::Client::open(dsn).expect(\"Error initializing redis unpooled client\");\n                let con = redis::aio::ConnectionManager::new_with_config(\n                    cli,\n                    ConnectionManagerConfig::new()\n                        .set_number_of_retries(1)\n                        .set_connection_timeout(REDIS_CONN_TIMEOUT),\n                )\n                .await\n                .expect(\"Failed to get redis-unpooled connection manager\");\n                RedisManager::NonClusteredUnpooled(con)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);\n                let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };\n                let cli = redis::sentinel::SentinelClient::build(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                    redis::sentinel::SentinelServerType::Master,\n                )\n                .expect(\"Failed to build sentinel client\");\n\n                RedisManager::SentinelUnpooled(Arc::new(Mutex::new(cli)))\n            }\n        }\n    }\n\n    pub async fn from_cache_backend(cache_backend: &CacheBackend<'_>) -> Self {\n        match cache_backend {\n            CacheBackend::Redis(dsn) => Self::new_unpooled(dsn, RedisVariant::NonClustered).await,\n            CacheBackend::RedisCluster(dsn) => {\n                Self::new_unpooled(dsn, RedisVariant::Clustered).await\n            }\n            CacheBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_unpooled(dsn, RedisVariant::Sentinel(cfg)).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn from_queue_backend(queue_backend: &QueueBackend<'_>, max_conns: u16) -> Self {\n        match queue_backend {\n            QueueBackend::Redis(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::NonClustered, max_conns).await\n            }\n            QueueBackend::RedisCluster(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::Clustered, max_conns).await\n            }\n            QueueBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_pooled(dsn, RedisVariant::Sentinel(cfg), max_conns).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn get(&self) -> Result<RedisConnection<'_>, RunError<RedisError>> {\n        match self {\n            Self::Clustered(pool) => Ok(RedisConnection::Clustered(pool.get().await?)),\n            Self::NonClustered(pool) => Ok(RedisConnection::NonClustered(pool.get().await?)),\n            Self::Sentinel(pool) => Ok(RedisConnection::SentinelPooled(pool.get().await?)),\n            Self::ClusteredUnpooled(conn) => Ok(RedisConnection::ClusteredUnpooled(conn.clone())),\n            Self::NonClusteredUnpooled(conn) => {\n                Ok(RedisConnection::NonClusteredUnpooled(conn.clone()))\n            }\n            Self::SentinelUnpooled(conn) => {\n                let mut conn = conn.lock().await;\n                let con = conn\n                    .get_async_connection_with_config(\n                        &AsyncConnectionConfig::new().set_response_timeout(REDIS_CONN_TIMEOUT),\n                    )\n                    .await?;\n                Ok(RedisConnection::SentinelUnpooled(con))\n            }\n        }\n    }\n}\n\npub enum RedisConnection<'a> {\n    Clustered(bb8::PooledConnection<'a, RedisClusterConnectionManager>),\n    NonClustered(bb8::PooledConnection<'a, RedisConnectionManager>),\n    SentinelPooled(bb8::PooledConnection<'a, RedisSentinelConnectionManager>),\n    ClusteredUnpooled(redis::cluster_async::ClusterConnection),\n    NonClusteredUnpooled(redis::aio::ConnectionManager),\n    SentinelUnpooled(redis::aio::MultiplexedConnection),\n}\n\nimpl redis::aio::ConnectionLike for RedisConnection<'_> {\n    fn req_packed_command<'a>(\n        &'a mut self,\n        cmd: &'a redis::Cmd,\n    ) -> redis::RedisFuture<'a, redis::Value> {\n        match self {\n            RedisConnection::Clustered(conn) => conn.req_packed_command(cmd),\n            RedisConnection::NonClustered(conn) => conn.req_packed_command(cmd),\n            RedisConnection::ClusteredUnpooled(conn) => conn.req_packed_command(cmd),\n            RedisConnection::NonClusteredUnpooled(conn) => conn.req_packed_command(cmd),\n            RedisConnection::SentinelPooled(conn) => conn.req_packed_command(cmd),\n            RedisConnection::SentinelUnpooled(conn) => conn.req_packed_command(cmd),\n        }\n    }\n\n    fn req_packed_commands<'a>(\n        &'a mut self,\n        cmd: &'a redis::Pipeline,\n        offset: usize,\n        count: usize,\n    ) -> redis::RedisFuture<'a, Vec<redis::Value>> {\n        match self {\n            RedisConnection::Clustered(conn) => conn.req_packed_commands(cmd, offset, count),\n            RedisConnection::NonClustered(conn) => conn.req_packed_commands(cmd, offset, count),\n            RedisConnection::ClusteredUnpooled(conn) => {\n                conn.req_packed_commands(cmd, offset, count)\n            }\n            RedisConnection::NonClusteredUnpooled(conn) => {\n                conn.req_packed_commands(cmd, offset, count)\n            }\n            RedisConnection::SentinelPooled(conn) => conn.req_packed_commands(cmd, offset, count),\n            RedisConnection::SentinelUnpooled(conn) => conn.req_packed_commands(cmd, offset, count),\n        }\n    }\n\n    fn get_db(&self) -> i64 {\n        match self {\n            RedisConnection::Clustered(conn) => conn.get_db(),\n            RedisConnection::NonClustered(conn) => conn.get_db(),\n            RedisConnection::ClusteredUnpooled(conn) => conn.get_db(),\n            RedisConnection::NonClusteredUnpooled(conn) => conn.get_db(),\n            RedisConnection::SentinelPooled(conn) => conn.get_db(),\n            RedisConnection::SentinelUnpooled(conn) => conn.get_db(),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use redis::AsyncCommands;\n\n    use super::RedisManager;\n\n    // Ensure basic set/get works -- should test sharding as well:\n    #[tokio::test]\n    // run with `cargo test -- --ignored redis` only when redis is up and configured\n    #[ignore]\n    async fn test_set_read_random_keys() {\n        dotenvy::dotenv().ok();\n        let cfg = crate::cfg::load().unwrap();\n\n        let mgr = RedisManager::from_cache_backend(&cfg.cache_backend()).await;\n        let mut conn = mgr.get().await.unwrap();\n\n        <|fim_suffix|>\n    }\n}\n<|fim_middle|>", "completion": "for (val, key) in \"abcdefghijklmnopqrstuvwxyz\".chars().enumerate() {\n            let key = key.to_string();\n            let _: () = conn.set(key.clone(), val).await.unwrap();\n            assert_eq!(conn.get::<_, usize>(&key).await.unwrap(), val);\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/redis/mod.rs", "node_type": "for_expression", "line_range": [268, 272]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::fmt::Debug;\n\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chacha20poly1305::{\n    aead::{Aead, KeyInit},\n    Key, XChaCha20Poly1305, XNonce,\n};\nuse ed25519_compact::*;\nuse rand::Rng;\n\nuse crate::error::Result;\n\n// Asymmetric Signature keys\n#[derive(Clone, Eq)]\npub struct AsymmetricKey(pub KeyPair);\n\nimpl AsymmetricKey {\n    pub fn generate() -> AsymmetricKey {\n        AsymmetricKey(KeyPair::from_seed(Seed::generate()))\n    }\n\n    pub fn from_slice(bytes: &[u8]) -> Result<Self> {\n        Ok(AsymmetricKey(KeyPair::from_slice(bytes).map_err(|_| {\n            crate::error::Error::generic(\"Failed parsing key.\")\n        })?))\n    }\n\n    pub fn from_base64(b64: &str) -> Result<Self> {\n        let bytes = STANDARD\n            .decode(b64)\n            .map_err(|_| crate::error::Error::generic(\"Failed parsing base64\"))?;\n\n        Self::from_slice(bytes.as_slice())\n    }\n\n    pub fn pubkey(&self) -> &[u8] {\n        &self.0.pk[..]\n    }\n}\n\nimpl Debug for AsymmetricKey {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"<AsymmetricKey sk=*** pk={}>\",\n            STANDARD.encode(self.0.pk.as_slice())\n        )\n    }\n}\n\nimpl PartialEq for AsymmetricKey {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.as_slice() == other.0.as_slice()\n    }\n}\n\n#[derive(Clone, Debug)]\npub struct Encryption(Option<Key>);\n\nimpl Encryption {\n    const NONCE_SIZE: usize = 24;\n\n    pub fn new_noop() -> Self {\n        Self(None)\n    }\n\n    pub fn new(key: [u8; 32]) -> Self {\n        Self(Some(Key::from_slice(&key).to_owned()))\n    }\n\n    pub fn encrypt(&self, data: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce: [u8; Self::NONCE_SIZE] = rand::thread_rng().gen();\n            let nonce = XNonce::from_slice(&nonce);\n            let mut ciphertext = cipher\n                .encrypt(nonce, data)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))?;\n            let mut ret = nonce.to_vec();\n            ret.append(&mut ciphertext);\n            Ok(ret)\n        } else {\n            Ok(data.to_vec())\n        }\n    }\n\n    pub fn decrypt(&self, ciphertext: &[u8]) -> Result<Vec<u8>> {\n        i<|fim_suffix|>    }\n\n    pub fn enabled(&self) -> bool {\n        self.0.is_some()\n    }\n}\n\nimpl Default for Encryption {\n    fn default() -> Self {\n        Self::new_noop()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::Encryption;\n\n    #[test]\n    fn test_encryption() {\n        let clear = b\"hello world\";\n        let encryption = Encryption::new([1; 32]);\n        let ciphertext = encryption.encrypt(clear).unwrap();\n        let clear2 = encryption.decrypt(&ciphertext).unwrap();\n        assert_eq!(&clear[..], &clear2[..]);\n    }\n}\n<|fim_middle|>", "completion": "if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce = &ciphertext[..Self::NONCE_SIZE];\n            let ciphertext = &ciphertext[Self::NONCE_SIZE..];\n            cipher\n                .decrypt(XNonce::from_slice(nonce), ciphertext)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))\n        } else {\n            Ok(ciphertext.to_vec())\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cryptography.rs", "node_type": "if_expression", "line_range": [91, 100]}
{"prompt": "<|fim_prefix|>//! Requires a rabbitmq node to be running on localhost:5672 (the default port) and using the\n//! default guest/guest credentials.\n//! Try using the `testing-docker-compose.yml` in the repo root to get this going.\n\nuse std::time::Duration;\n\nuse rdkafka::{\n    producer::{FutureProducer, FutureRecord},\n    util::Timeout,\n    ClientConfig,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_kafka::{KafkaConsumer, KafkaInputOpts};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse tracing::info;\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nuse crate::{create_topic, delete_topic, kafka_admin_client, BROKER_HOST};\n\n#[ctor::ctor]\nfn test_setup() {\n    use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n\n    tracing_subscriber::registry()\n        .with(\n            tracing_subscriber::EnvFilter::try_from_default_env().unwrap_or_else(|_| {\n                // Output is only printed for failing tests, but still we shouldn't overload\n                // the output with unnecessary info. When debugging a specific test, it's easy\n                // to override this default by setting the `RUST_LOG` environment variable.\n                \"info,svix_bridge=debug\".into()\n            }),\n        )\n        .with(tracing_subscriber::fmt::layer().with_test_writer())\n        .init();\n}\n\n/// Time to wait for the plugin to connect.\nconst CONNECT_WAIT_TIME: Duration = Duration::from_secs(10);\n/// Time to wait for the plugin to receive a message sent by a test.\nconst CONSUME_WAIT_TIME: Duration = Duration::from_secs(1);\n\nfn get_test_plugin(\n    svix_url: String,\n    topic: &str,\n    use_transformation: Option<TransformerInputFormat>,\n) -> KafkaConsumer {\n    KafkaConsumer::new(\n        \"test\".into(),\n        KafkaInputOpts::Inner {\n            bootstrap_brokers: BROKER_HOST.to_owned(),\n            // All tests use different topics, so it's fine to have only one consumer group ID\n            group_id: \"svix_bridge_test_group_id\".to_owned(),\n            topic: topic.to_owned(),\n            security_protocol: svix_bridge_plugin_kafka::KafkaSecurityProtocol::Plaintext,\n            debug_contexts: None,\n        },\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n    .unwrap()\n}\n\n<|fim_suffix|>\n\nasync fn publish(producer: &FutureProducer, topic: &str, payload: &[u8]) {\n    info!(topic, \"publishing message\");\n    producer\n        .send(\n            FutureRecord::<(), _>::to(topic).payload(payload),\n            Timeout::After(Duration::from_secs(3)),\n        )\n        .await\n        .unwrap();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let topic = unique_topic_name!();\n\n    let admin_client = kafka_admin_client();\n    create_topic(&admin_client, topic).await;\n\n    let producer = kafka_producer();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), topic, None);\n\n    let handle = tokio::spawn(async move {\n        plugin.run().await;\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(CONNECT_WAIT_TIME).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&producer, topic, &serde_json::to_vec(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(CONSUME_WAIT_TIME).await;\n\n    handle.abort();\n    delete_topic(&admin_client, topic).await;\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let topic = unique_topic_name!();\n\n    let admin_client = kafka_admin_client();\n    create_topic(&admin_client, topic).await;\n\n    let producer = kafka_producer();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(mock_server.uri(), topic, Some(TransformerInputFormat::Json));\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        plugin.run().await;\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(CONNECT_WAIT_TIME).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&producer, topic, &serde_json::to_vec(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(CONSUME_WAIT_TIME).await;\n\n    handle.abort();\n    delete_topic(&admin_client, topic).await;\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_string_ok() {\n    let topic = unique_topic_name!();\n\n    let admin_client = kafka_admin_client();\n    create_topic(&admin_client, topic).await;\n\n    let producer = kafka_producer();\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(\n   <|fim_middle|>", "completion": "fn kafka_producer() -> FutureProducer {\n    // create does block I/O, but we don't care in tests\n    ClientConfig::new()\n        .set(\"bootstrap.servers\", BROKER_HOST)\n        .create()\n        .unwrap()\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/tests/it/kafka_consumer.rs", "node_type": "function_item", "line_range": [79, 85]}
{"prompt": "<|fim_prefix|> = \"UnrequiredField::is_absent\")]\n    #[validate(\n        custom = \"validate_name_length_patch\",\n        custom = \"validate_no_control_characters_unrequired\"\n    )]\n    pub name: UnrequiredField<String>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredNullableField::is_absent\")]\n    #[validate(custom = \"validate_rate_limit_patch\")]\n    pub rate_limit: UnrequiredNullableField<u16>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredNullableField::is_absent\")]\n    #[validate]\n    pub uid: UnrequiredNullableField<ApplicationUid>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredField::is_absent\")]\n    pub metadata: UnrequiredField<Metadata>,\n}\n\nimpl ModelIn for ApplicationPatch {\n    type ActiveModel = (application::ActiveModel, applicationmetadata::ActiveModel);\n\n    fn update_model(self, (app, app_metadata): &mut Self::ActiveModel) {\n        let ApplicationPatch {\n            name,\n            rate_limit,\n            uid,\n            metadata,\n        } = self;\n\n        // `model`'s version of `rate_limit` is an i32, while `self`'s is a u16.\n        let rate_limit_map = |x: u16| -> i32 { x.into() };\n        let data = metadata;\n\n        patch_field_non_nullable!(app, name);\n        patch_field_nullable!(app, rate_limit, rate_limit_map);\n        patch_field_nullable!(app, uid);\n        patch_field_non_nullable!(app_metadata, data);\n    }\n}\n\nfn validate_name_length_patch(name: &UnrequiredField<String>) -> Result<(), ValidationError> {\n    match name {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(s) => {\n            if s.is_empty() {\n                Err(validation_error(\n                    Some(\"length\"),\n                    Some(\"Application names must be at least one character\"),\n                ))\n            } else {\n                Ok(())\n            }\n        }\n    }\n}\n\nfn validate_rate_limit_patch(\n    rate_limit: &UnrequiredNullableField<u16>,\n) -> Result<(), ValidationError> {\n    match rate_limit {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(rate_limit) => {\n            if *rate_limit > 0 {\n                Ok(())\n            } else {\n                Err(validation_error(\n                    Some(\"range\"),\n                    Some(\"Application rate limits must be at least 1 if set\"),\n                ))\n            }\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct ApplicationOut {\n    // FIXME: Do we want to use serde(flatten) or just duplicate the keys?\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<ApplicationUid>,\n    #[schemars(example = \"application_name_example\")]\n    pub name: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    pub id: ApplicationId,\n    pub created_at: DateTime<Utc>,\n    pub updated_at: DateTime<Utc>,\n    pub metadata: Metadata,\n}\n\nimpl From<(application::Model, applicationmetadata::Model)> for ApplicationOut {\n    fn from((app, metadata): (application::Model, applicationmetadata::Model)) -> Self {\n        Self {\n            uid: app.uid,\n            name: app.name,\n            rate_limit: app.rate_limit.map(|x| x as u16),\n            id: app.id,\n            created_at: app.created_at.into(),\n            updated_at: app.updated_at.into(),\n            metadata: metadata.metadata(),\n        }\n    }\n}\n\n/// List of all the organization's applications.\n#[aide_annotate(op_id = \"v1.application.list\")]\nasync fn list_applications(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<Pagination<ReversibleIterator<ApplicationId>>>,\n    permissions::Organization { org_id }: permissions::Organization,\n) -> Result<Json<ListResponse<ApplicationOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    let iterator = pagination.iterator;\n    let iter_direction = iterator\n        .as_ref()\n        .map_or(IteratorDirection::Normal, |iter| iter.direction());\n\n    l<|fim_suffix|>\n    let results: Vec<ApplicationOut> = query\n        .find_also_related(applicationmetadata::Entity)\n        .all(db)\n        .await?\n        .into_iter()\n        .map(|(app, metadata)| {\n            let metadata =\n                metadata.unwrap_or_else(|| applicationmetadata::Model::new(app.id.clone()));\n            (app, metadata)\n        })\n        .map(ApplicationOut::from)\n        .collect();\n\n    Ok(Json(ApplicationOut::list_response(\n        results,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\nfn default_as_false() -> bool {\n    false\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct CreateApplicationQueryParams {\n    /// Get an existing application, or create a new one if doesn't exist. It's two separate functions in the libs.\n    #[serde(default = \"default_as_false\")]\n    get_if_exists: bool,\n}\n\n/// Create a new application.\n#[aide_annotate(op_id = \"v1.application.create\")]\nasync fn create_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    query: ValidatedQuery<CreateApplicationQueryParams>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<ApplicationIn>,\n) -> Result<JsonStatusUpsert<ApplicationOut>> {\n    if let Some(ref uid) = data.uid {\n        if let Some((app, metadata)) =\n            application::Model::fetch_with_metadata(db, org_id.clone(), uid.clone().into())\n                .await\n                .trace()?\n        {\n            if query.get_if_exists {\n                // Technically not updated, but it fits.\n                return Ok(JsonStatusUpsert::Updated((app, metadata).into()));\n            }\n            return Err(HttpError::conflict(\n                None,\n                Some(\"An application with that id or uid already exists\".into()),\n            )\n            .into());\n        };\n    }\n\n    let (app, metadata) = create_app_from_app_in(db, data, org_id).await?;\n\n    Ok(JsonStatusUpsert::Created((app, metadata).into()))\n}\n\npub async fn create_app_from_app_in(\n    db: &DatabaseConnection,\n    app_in: ApplicationIn,\n    org_id: OrganizationId,\n) -> Result<(application::Model, applicationmetadata::Model)> {\n    let app = application::ActiveModel::new(org_id);\n    let metadata = applicationmetadata::ActiveModel::new(app.id.clone().unwrap(), None);\n\n    let mut model = (app, metadata);\n    app_in.update_model(&mut model);\n    let (app, metadata) = model;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            async move {\n                let app_result = app.insert(txn).await.map_err(http_error_on_conflict)?;\n                let metadata = metadata.upsert_or_delete(txn).await.trace()?;\n                Ok((app_result, metadata))\n            }\n            .boxed()\n        })\n        .await?;\n\n    Ok((app, metadata))\n}\n\n/// Get an application.\n#[aide_annotate(op_id = \"v1.application.get\")]\nasync fn get_application(\n    permissions::ApplicationWithMetadata { app, metadata }: permissions::ApplicationWithMetadata,\n) -> Result<Json<ApplicationOut>> {\n    Ok(Json((app, metadata).into()))\n}\n\n/// Update an application.\n#[aide_annotate(op_id = \"v1.application.update\")]\nasync fn update_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationPath { app_id }): Path<ApplicationPath>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<ApplicationIn>,\n) -> Result<JsonStatusUpsert<ApplicationOut>> {\n    let (app, metadata, create_models) = if let Some((app, metadata)) =\n        application::Model::fetch_with_metadata(db, org_id.clone(), app_id)\n            .await\n            .trace()?\n    {\n        (app.into(), metadata.into(), false)\n    } else {\n        let app = application::ActiveModel::new(org_id);\n        let metadata = applicationmetadata::ActiveModel::new(app.id.clone().unwrap(), None);\n        (app, metadata, true)\n    };\n\n    let mut models = (app, metadata);\n    data.update_model(&mut models);\n    let (app, metadata) = models;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            <|fim_middle|>", "completion": "let query = apply_pagination(\n        application::Entity::secure_find(org_id),\n        application::Column::Id,\n        limit,\n        iterator,\n        pagination.order.unwrap_or(Ordering::Ascending),\n    );", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/application.rs", "node_type": "let_declaration", "line_range": [208, 214]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nu<|fim_suffix|>\nuse reqwest::StatusCode;\nuse serde::de::IgnoredAny;\nuse svix_server::{\n    core::types::{ApplicationId, EventTypeName, FeatureFlag, FeatureFlagSet},\n    db::models::eventtype::Schema,\n    v1::{\n        endpoints::{\n            application::ApplicationOut,\n            event_type::{EventTypeIn, EventTypeOut},\n        },\n        utils::ListResponse,\n    },\n};\n\nuse crate::utils::{\n    common_calls::{app_portal_access, application_in, common_test_list, event_type_in},\n    start_svix_server,\n};\n\n#[tokio::test]\nasync fn test_patch() {\n    let (client, _jh) = start_svix_server().await;\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\n                \"test-event-type\",\n                serde_json::json!({\n                    \"1\": {\n                        \"type\": \"object\",\n                        \"title\": \"Longitude and Latitude Values\",\n                        \"description\": \"A geographical coordinate.\",\n                        \"required\": [\"latitude\", \"longitude\"],\n                        \"properties\": {\n                        \"latitude\": {\"type\": \"number\", \"minimum\": -90, \"maximum\": 90},\n                        \"longitude\": {\"type\": \"number\", \"minimum\": -180, \"maximum\": 180},\n                        },\n                    }\n                }),\n            )\n            .unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that PUT with invalid ID creates an event type\n    let _: EventTypeOut = client\n        .put(\n            \"api/v1/event-type/fake-id/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that description may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"description\": \"updated_description\",\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.schemas, et.schemas);\n\n    // Test that schemas may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": {},\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, Some(Schema::default()));\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that schemas may be unset while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": null,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, None);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that deleted may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"archived\": true,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert!(out.deleted);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.schemas, None);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n}\n\n#[tokio::test]\nasync fn test_event_type_create_read_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        client\n            .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n            .await\n            .unwrap(),\n        et\n    );\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/?with_content=true\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&et));\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&EventTypeOut {\n        schemas: None,\n        ..et\n    }));\n}\n\n#[tokio::test]\nasync fn test_event_type_feature_flags() {\n    let (client, _jh) = start_svix_server().await;\n\n    let feature = FeatureFlag(\"foo-feature\".into());\n    let another_feature = FeatureFlag(\"bar-feature\".into());\n    let (features, other_features, union) = {\n        let mut s1 = HashSet::new();\n        s1.insert(feature.clone());\n        let mut s2 = HashSet::new();\n        s2.insert(another_feature);\n        let union: FeatureFlagSet = s1.union(&s2).cloned().collect();\n\n        (s1, s2, union)\n    };\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"event-type-with-flag\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: Some(feature),\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"no-flag-event\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: None,\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let app: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let path = format!(\"api/v1/event-type/{}/\", et.name);\n\n    for (flag_set, should_see) in [\n        (FeatureFlagSet::default(), false),\n        (other_features, false),\n        (union.clone(), true),\n        (features.clone(), true),\n    ] {\n        let client = app_portal_access(&client, &app, flag_set).await;\n\n        let list: ListResponse<EventTypeOut> = client\n            .get(\"api/v1/event-type/\", StatusCode::OK)\n            .await\n            .unwrap();\n\n        if should_see {\n            // If the client is expected to see both event types it should be able to retrieve it\n            let got_et: EventTypeOut = client.get(&path, StatusCode::OK).await.unwrap();\n            assert_eq!(et, got_et);\n\n            // ... and see it in the list.\n            assert_eq!(list.data.len(), 2);\n            assert!(list.data.<|fim_middle|>", "completion": "use std::collections::HashSet;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_event_type.rs", "node_type": "use_declaration", "line_range": [4, 4]}
{"prompt": "<|fim_prefix|>dpointHeadersIn>,\n    },\n    /// Partially set the additional headers to be sent with the webhook.\n    PatchHeaders {\n        app_id: String,\n        id: String,\n        endpoint_headers_patch_in: crate::json::JsonOf<EndpointHeadersPatchIn>,\n    },\n    /// Resend all failed messages since a given time.\n    ///\n    /// Messages that were sent successfully, even if failed initially, are not resent.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"endpoint.recover\",\n    ///   \"data\": {\n    ///     \"messagesSent\": 2\n    ///   }\n    /// }\n    /// ```\n    Recover {\n        app_id: String,\n        id: String,\n        recover_in: crate::json::JsonOf<RecoverIn>,\n        #[clap(flatten)]\n        options: EndpointRecoverOptions,\n    },\n    /// Replays messages to the endpoint.\n    ///\n    /// Only messages that were created after `since` will be sent.\n    /// Messages that were previously sent to the endpoint are not resent.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"endpoint.replay\",\n    ///   \"data\": {\n    ///     \"messagesSent\": 2\n    ///   }\n    /// }\n    /// ```\n    ReplayMissing {\n        app_id: String,\n        id: String,\n        replay_in: crate::json::JsonOf<ReplayIn>,\n        #[clap(flatten)]\n        options: EndpointReplayMissingOptions,\n    },\n    /// Get the endpoint's signing secret.\n    ///\n    /// This is used to verify the authenticity of the webhook.\n    /// For more information please refer to [the consuming webhooks docs](https://docs.svix.com/consuming-webhooks/).\n    GetSecret { app_id: String, id: String },\n    /// Rotates the endpoint's signing secret.\n    ///\n    /// The previous secret will remain valid for the next 24 hours.\n    RotateSecret {\n        app_id: String,\n        id: String,\n        endpoint_secret_rotate_in: Option<crate::json::JsonOf<EndpointSecretRotateIn>>,\n        #[clap(flatten)]\n        options: EndpointRotateSecretOptions,\n    },\n    /// Send an example message for an event.\n    SendExample {\n        app_id: String,\n        id: String,\n        event_example_in: crate::json::JsonOf<EventExampleIn>,\n        #[clap(flatten)]\n        options: EndpointSendExampleOptions,\n    },\n    /// Get basic statistics for the endpoint.\n    GetStats {\n        app_id: String,\n        id: String,\n        #[clap(flatten)]\n        options: EndpointGetStatsOptions,\n    },\n    /// Get the transformation code associated with this endpoint.\n    TransformationGet { app_id: String, id: String },\n    /// Set or unset the transformation code associated with this endpoint.\n    PatchTransformation {\n        app_id: String,\n        id: String,\n        endpoint_transformation_patch: Option<crate::json::JsonOf<EndpointTransformationPatch>>,\n    },\n    /// This operation was renamed to `set-transformation`.\n    TransformationPartialUpdate {\n        app_id: String,\n        id: String,\n        endpoint_transformation_in: Option<crate::json::JsonOf<EndpointTransformationIn>>,\n    },\n}\n\nimpl EndpointCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { app_id, options } => {\n                let resp = client.endpoint().list(app_id, Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                app_id,\n                endpoint_in,\n                options,\n            } => {\n                let resp = client\n                    .endpoint()\n                    .create(app_id, endpoint_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { app_id, id } => {\n                <|fim_suffix|>\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                app_id,\n                id,\n                endpoint_update,\n            } => {\n                let resp = client\n                    .endpoint()\n                    .update(app_id, id, endpoint_update.into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete { app_id, id } => {\n                client.endpoint().delete(app_id, id).await?;\n            }\n            Self::Patch {\n                app_id,\n                id,\n                endpoint_patch,\n            } => {\n                let resp = client\n                    .endpoint()\n                    .patch(app_id, id, endpoint_patch.unwrap_or_default().into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::GetHeaders { app_id, id } => {\n                let resp = client.endpoint().get_headers(app_id, id).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::UpdateHeaders {\n                app_id,\n                id,\n                endpoint_headers_in,\n            } => {\n                client\n                    .endpoint()\n                    .update_headers(app_id, id, endpoint_headers_in.into_inner())\n                    .await?;\n            }\n            Self::PatchHeaders {\n                app_id,\n                id,\n                endpoint_headers_patch_in,\n            } => {\n                client\n                    .endpoint()\n                    .patch_headers(app_id, id, endpoint_headers_patch_in.into_inner())\n                    .await?;\n            }\n            Self::Recover {\n                app_id,\n                id,\n                recover_in,\n                options,\n            } => {\n                let resp = client\n                    .endpoint()\n                    .recover(app_id, id, recover_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ReplayMissing {\n                app_id,\n                id,\n                replay_in,\n                options,\n            } => {\n                let resp = client\n                    .endpoint()\n                    .replay_missing(app_id, id, replay_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::GetSecret { app_id, id } => {\n                let resp = client.endpoint().get_secret(app_id, id).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::RotateSecret {\n                app_id,\n                id,\n                endpoint_secret_rotate_in,\n                options,\n            } => {\n                client\n                    .endpoint()\n                    .rotate_secret(\n                        app_id,\n                        id,\n                        endpoint_secret_rotate_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n            Self::SendExample {\n                app_id,\n                id,\n                event_example_in,\n                options,\n            } => {\n                let resp = client\n                    .endpoint()\n                    .send_example(\n                        app_id,\n                        id,\n                        event_example_in.into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::GetStats {\n                app_id,\n                id,\n                options,\n            } => {\n                let resp = client\n                    .endpoint()\n              <|fim_middle|>", "completion": "let resp = client.endpoint().get(app_id, id).await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/endpoint.rs", "node_type": "let_declaration", "line_range": [286, 286]}
{"prompt": "<|fim_prefix|>er to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        // The transformed bit of the payload\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        subscription.id(),\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&topic, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_string_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        // The transformed bit of the payload\n        .and(body_partial_json(\n            json!({ \"payload\": { \"hello\": \"world\" } }),\n        ))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    <|fim_suffix|>\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let input = match x.input {\n                TransformerInput::String(input) => input,\n                _ => unreachable!(),\n            };\n            // Build a create-message-compatible object, using the string input as a field in the payload.\n            let out = json!({\n                \"appId\": \"app_1234\",\n                \"message\": {\n                    \"eventType\": \"testing.things\",\n                    \"payload\": {\n                        \"hello\": input,\n                    }\n                }\n            });\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    publish(&topic, \"world\").await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n#[tokio::test]\nasync fn test_missing_app_id_nack() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    let mock = Mock::given(method(\"POST\"))\n        // The response doesn't really matter, but we need to define it to be able to `expect(0)`.\n        .respond_with(ResponseTemplate::new(400))\n        .named(\"create_message\")\n        // No requests should be made when the event type or app id are missing.\n        .expect(0);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), subscription.id(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    publish(\n        &topic,\n        &serde_json::to_string(&json!({\n            // No app id\n            \"message\": {\n                \"eventType\": \"testing.things\",\n                \"payload\": {\n                    \"hi\": \"there\",\n                }\n            },\n\n        }))\n        .unwrap(),\n    )\n    .await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n#[tokio::test]\nasync fn test_missing_event_type_nack() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    let mock = Mock::given(method(\"POST\"))\n        // The response doesn't really matter, but we need to define it to be able to `expect(0)`.\n        .respond_with(ResponseTemplate::new(400))\n        .named(\"create_message\")\n        // No requests should be made when the event type or app id are missing.\n        .expect(0);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), subscription.id(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    publish(\n        &topic,\n        &serde_json::to_string(&json!({\n            \"appId\": \"app_1234\",\n            \"message\": {\n                // No event type\n                \"payload\": {\n                    \"hi\": \"there\",\n                }\n            },\n        }))\n        .unwrap(),\n    )\n    .await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n    handl<|fim_middle|>", "completion": "let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        subscription.id(),\n        Some(TransformerInputFormat::String),\n    );", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/gcp_pubsub_consumer.rs", "node_type": "let_declaration", "line_range": [277, 281]}
{"prompt": "<|fim_prefix|>      let lock = \"{test}_ack_delayed_lock\";\n        let dlq = \"{test}_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(100);\n\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n        p.send(&mt, None).await.unwrap();\n\n        let recv = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv.task, mt);\n        recv.ack().await.unwrap();\n\n        if let Ok(recv) = timeout(delay, c.receive_all(TEST_RECV_DEADLINE)).await {\n            panic!(\"Received unexpected QueueTask {:?}\", recv.unwrap()[0].task);\n        }\n\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n        // And assert that the task has been deleted\n        assert!(conn\n            .xread::<_, _, StreamReadReply>(&[main_queue], &[0])\n            .await\n            .unwrap()\n            .keys\n            .is_empty());\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_nack() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        let main_queue = \"{test}_nack\";\n        let delayed = \"{test}_nack_delayed\";\n        let lock = \"{test}_nack_delayed_lock\";\n        let dlq = \"{test}_nack_delayed_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(100);\n\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test\".to_owned()),\n            app_id: ApplicationId(\"test\".to_owned()),\n            endpoint_id: EndpointId(\"test\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n        p.send(&mt, None).await.unwrap();\n\n        let recv = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv.task, mt);\n        recv.nack().await.unwrap();\n\n        let recv = timeout(\n            Duration::from_millis(500) + delay,\n            c.receive_all(TEST_RECV_DEADLINE),\n        )\n        .await\n        .expect(\"Expected QueueTask\");\n        assert_eq!(*recv.unwrap().pop().unwrap().task, mt);\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_delay() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        let main_queue = \"{test}_delay\";\n        let delayed = \"{test}_delay_delayed\";\n        let lock = \"{test}_delay_delayed_lock\";\n        let dlq = \"{test}_delay_delayed_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(500);\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt1 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test1\".to_owned()),\n            app_id: ApplicationId(\"test1\".to_owned()),\n            endpoint_id: EndpointId(\"test1\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Scheduled,\n            attempt_count: 0,\n        });\n        let mt2 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n\n        p.send(&mt1, Some(Duration::from_millis(2000)))\n            .await\n            .unwrap();\n        p.send(&mt2, None).await.unwrap();\n\n        <|fim_suffix|>\n        assert_eq!(*recv2.task, mt2);\n        recv2.ack().await.unwrap();\n\n        let recv1 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv1.task, mt1);\n        recv1.ack().await.unwrap();\n    }\n\n    fn to_redis_key(id: &str, task: &QueueTask) -> String {\n        format!(\"{id}|{}\", serde_json::to_string(task).unwrap())\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_migrations() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        // Test queue name constants\n        let v1_main = \"{test}_migrations_main_v1\";\n        let v2_main = \"{test}_migrations_main_v2\";\n        let v3_main = \"{test}_migrations_main_v3\";\n\n        let v1_processing = \"{test}_migrations_processing_v1\";\n        let v2_processing = \"{test}_migrations_processing_v2\";\n        // v3_processing is the stream pending queue for v3_main\n\n        let v1_delayed = \"{test}_migrations_delayed_v1\";\n        let v2_delayed = \"{test}_migrations_delayed_v2\";\n        let v2_delayed_lock = \"{test}_migrations_delayed_lock_v2\";\n        // v3_delayed doesn not yet exist\n\n        {\n            let mut conn = pool.get().await.unwrap();\n\n            // Clear test keys\n            let _: () = conn\n                .del(&[\n                    v1_main,\n                    v2_main,\n                    v3_main,\n                    v1_processing,\n                    v2_processing,\n                    v1_delayed,\n                    v2_delayed,\n                ])\n                .await\n                .unwrap();\n\n            // Add v3 consumer group\n            let _: () = conn\n                .xgroup_create_mkstream(v3_main, super::WORKERS_GROUP, 0i8)\n                .await\n                .unwrap();\n\n            // Add v1 data\n            for num in 1..=10 {\n                let _: () = conn\n                    .rpush(\n                        v1_main,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            for num in 11..=15 {\n                let _: () = conn\n                    .zadd(\n                        v1_delayed,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                        Utc::now().timestamp() + 2,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // Move the first five of v1_main to v1_processing\n            for _ in 0..5 {\n                let _: () = conn\n                    .blmove(\n                        v1_main,\n                        v1_processing,\n                        Direction::Left,\n                        Direction::Right,\n                        0.0,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // v1 to v2\n            migrate_list(&mut conn, v1_main, v2_main).await.unwrap();\n            migrate_list(&<|fim_middle|>", "completion": "let recv2 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [688, 693]}
{"prompt": "<|fim_prefix|> MessageAttemptListByEndpointOptions {\n            limit,\n            iterator,\n            status,\n            status_code_class,\n            channel,\n            tag,\n            before,\n            after,\n            with_content,\n            with_msg,\n            event_types,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            status,\n            status_code_class,\n            channel,\n            tag,\n            before: before.map(|dt| dt.to_rfc3339()),\n            after: after.map(|dt| dt.to_rfc3339()),\n            with_content,\n            with_msg,\n            event_types,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageAttemptListByMsgOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filter response based on the status of the attempt: Success (0), Pending (1), Failed (2), or Sending (3)\n    #[arg(long)]\n    pub status: Option<MessageStatus>,\n    /// Filter response based on the HTTP status code\n    #[arg(long)]\n    pub status_code_class: Option<StatusCodeClass>,\n    /// Filter response based on the channel\n    #[arg(long)]\n    pub channel: Option<String>,\n    /// Filter response based on the tag\n    #[arg(long)]\n    pub tag: Option<String>,\n    /// Filter the attempts based on the attempted endpoint\n    #[arg(long)]\n    pub endpoint_id: Option<String>,\n    /// Only include items created before a certain date\n    #[arg(long)]\n    pub before: Option<chrono::DateTime<chrono::Utc>>,\n    /// Only include items created after a certain date\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n    /// When `true` attempt content is included in the response\n    #[arg(long)]\n    pub with_content: Option<bool>,\n    /// Filter response based on the event type\n    #[arg(long)]\n    pub event_types: Option<Vec<String>>,\n}\n\nimpl From<MessageAttemptListByMsgOptions> for svix::api::MessageAttemptListByMsgOptions {\n    fn from(value: MessageAttemptListByMsgOptions) -> Self {\n        let MessageAttemptListByMsgOptions {\n            limit,\n            iterator,\n            status,\n            status_code_class,\n            channel,\n            tag,\n            endpoint_id,\n            before,\n            after,\n            with_content,\n            event_types,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            status,\n            status_code_class,\n            channel,\n            tag,\n            endpoint_id,\n            before: before.map(|dt| dt.to_rfc3339()),\n            after: after.map(|dt| dt.to_rfc3339()),\n            with_content,\n            event_types,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageAttemptListAttemptedMessagesOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filter response based on the channel\n    #[arg(long)]\n    pub channel: Option<String>,\n    /// Filter response based on the message tags\n    #[arg(long)]\n    pub tag: Option<String>,\n    /// Filter response based on the status of the attempt: Success (0), Pending (1), Failed (2), or Sending (3)\n    #[arg(long)]\n    pub status: Option<MessageStatus>,\n    /// Only include items created before a certain date\n    #[arg(long)]\n    pub before: Option<chrono::DateTime<chrono::Utc>>,\n    /// Only include items created after a certain date\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n    /// When `true` message payloads are included in the response\n    #[arg(long)]\n    pub with_content: Option<bool>,\n    /// Filter response based on the event type\n    #[arg(long)]\n    pub event_types: Option<Vec<String>>,\n}\n\nimpl From<MessageAttemptListAttemptedMessagesOptions>\n    for svix::api::MessageAttemptListAttemptedMessagesOptions\n{\n    fn from(value: MessageAttemptListAttemptedMessagesOptions) -> Self {\n        <|fim_suffix|>\n        Self {\n            limit,\n            iterator,\n            channel,\n            tag,\n            status,\n            before: before.map(|dt| dt.to_rfc3339()),\n            after: after.map(|dt| dt.to_rfc3339()),\n            with_content,\n            event_types,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageAttemptListAttemptedDestinationsOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n}\n\nimpl From<MessageAttemptListAttemptedDestinationsOptions>\n    for svix::api::MessageAttemptListAttemptedDestinationsOptions\n{\n    fn from(value: MessageAttemptListAttemptedDestinationsOptions) -> Self {\n        let MessageAttemptListAttemptedDestinationsOptions { limit, iterator } = value;\n        Self { limit, iterator }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageAttemptResendOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<MessageAttemptResendOptions> for svix::api::MessageAttemptResendOptions {\n    fn from(value: MessageAttemptResendOptions) -> Self {\n        let MessageAttemptResendOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct MessageAttemptArgs {\n    #[command(subcommand)]\n    pub command: MessageAttemptCommands,\n}\n\n#[derive(Subcommand)]\npub enum MessageAttemptCommands {\n    /// List attempts by endpoint id\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    ListByEndpoint {\n        app_id: String,\n        endpoint_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptListByEndpointOptions,\n    },\n    /// List attempts by message ID.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    ListByMsg {\n        app_id: String,\n        msg_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptListByMsgOptions,\n    },\n    /// List messages for a particular endpoint. Additionally includes metadata about the latest message attempt.\n    ///\n    /// The `before` parameter lets you filter all items created before a certain date and is ignored if an iterator is passed.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    ListAttemptedMessages {\n        app_id: String,\n        endpoint_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptListAttemptedMessagesOptions,\n    },\n    /// `msg_id`: Use a message id or a message `eventId`\n    Get {\n        app_id: String,\n        msg_id: String,\n        attempt_id: String,\n    },\n    /// Deletes the given attempt's response body.\n    ///\n    /// Useful when an endpoint accidentally returned sensitive content.\n    /// The message can't be replayed or resent once its payload has been deleted or expired.\n    ExpungeContent {\n        app_id: String,\n        msg_id: String,\n        attempt_id: String,\n    },\n    /// List endpoints attempted by a given message.\n    ///\n    /// Additionally includes metadata about the latest message attempt.\n    /// By default, endpoints are listed in ascending order by ID.\n    Li<|fim_middle|>", "completion": "let MessageAttemptListAttemptedMessagesOptions {\n            limit,\n            iterator,\n            channel,\n            tag,\n            status,\n            before,\n            after,\n            with_content,\n            event_types,\n        } = value;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/message_attempt.rs", "node_type": "let_declaration", "line_range": [176, 186]}
{"prompt": "<|fim_prefix|>_ISSUER)\n    .with_subject(management_org_id());\n\n    signing_config.generate(claims).map_err(Error::generic)\n}\n\npub fn generate_app_token(\n    keys: &JwtSigningConfig,\n    org_id: OrganizationId,\n    app_id: ApplicationId,\n    feature_flags: FeatureFlagSet,\n) -> Result<String> {\n    let claims = Claims::with_custom_claims(\n        CustomClaim {\n            organization: Some(org_id.0),\n            feature_flags,\n        },\n        Duration::from_hours(24 * 28),\n    )\n    .with_issuer(JWT_ISSUER)\n    .with_subject(app_id.0);\n\n    keys.generate(claims).map_err(Error::generic)\n}\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum JwtSigningConfig {\n    /// Variants that specify both key and algorithm to use\n    Advanced(JWTAlgorithm),\n    /// The variant used when the algorithm is not specified, defaults to HS256\n    Default {\n        #[serde(deserialize_with = \"deserialize_hs256\")]\n        jwt_secret: HS256Key,\n    },\n}\n\n/// A wrapper for the available JWT signing algorithms exposed by `jwt-simple`\n#[derive(Deserialize)]\n#[serde(tag = \"jwt_algorithm\", content = \"jwt_secret\")]\npub enum JWTAlgorithm {\n    #[serde(deserialize_with = \"deserialize_hs256\")]\n    HS256(HS256Key),\n    #[serde(deserialize_with = \"deserialize_hs384\")]\n    HS384(HS384Key),\n    #[serde(deserialize_with = \"deserialize_hs512\")]\n    HS512(HS512Key),\n    #[serde(deserialize_with = \"deserialize_rs256\")]\n    RS256(RS256),\n    #[serde(deserialize_with = \"deserialize_rs384\")]\n    RS384(RS384),\n    #[serde(deserialize_with = \"deserialize_rs512\")]\n    RS512(RS512),\n    #[serde(deserialize_with = \"deserialize_eddsa\")]\n    EdDSA(EdDSA),\n}\n\npub enum RS256 {\n    Public(RS256PublicKey),\n    Pair(Box<RS256KeyPair>),\n}\n\npub enum RS384 {\n    Public(RS384PublicKey),\n    Pair(Box<RS384KeyPair>),\n}\n\npub enum RS512 {\n    Public(RS512PublicKey),\n    Pair(Box<RS512KeyPair>),\n}\n\npub enum EdDSA {\n    Public(Ed25519PublicKey),\n    Pair(Box<Ed25519KeyPair>),\n}\n\nimpl JwtSigningConfig {\n    pub fn generate(&self, claims: JWTClaims<CustomClaim>) -> Result<String, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.authenticate(claims),\n                JWTAlgorithm::HS384(key) => key.authenticate(claims),\n                JWTAlgorithm::HS512(key) => key.authenticate(claims),\n                JWTAlgorithm::RS256(kind) => match kind {\n                    RS256::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS256::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS384::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    RS512::Pair(key) => key.sign(claims),\n                },\n                JWTAlgorithm::EdDSA(kind) => match kind {\n                    EdDSA::Public(_) => {\n                        Err(jwt_simple::Error::msg(\"cannot sign JWT with public key\"))\n                    }\n                    EdDSA::Pair(key) => key.sign(claims),\n                },\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.authenticate(claims),\n        }\n    }\n\n    pub fn verify_token(\n        &self,\n        token: &str,\n        options: Option<VerificationOptions>,\n    ) -> Result<JWTClaims<CustomClaim>, jwt_simple::Error> {\n        match self {\n            JwtSigningConfig::Advanced(a) => match a {\n                JWTAlgorithm::HS256(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS384(key) => key.verify_token(token, options),\n                JWTAlgorithm::HS512(key) => key.verify_token(token, options),\n                JWTAlgorithm::RS256(kind) => m<|fim_suffix|>\n                JWTAlgorithm::RS384(kind) => match kind {\n                    RS384::Public(key) => key.verify_token(token, options),\n                    RS384::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::RS512(kind) => match kind {\n                    RS512::Public(key) => key.verify_token(token, options),\n                    RS512::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n                JWTAlgorithm::EdDSA(kind) => match kind {\n                    EdDSA::Public(key) => key.verify_token(token, options),\n                    EdDSA::Pair(pair) => pair.public_key().verify_token(token, options),\n                },\n            },\n            JwtSigningConfig::Default { jwt_secret } => jwt_secret.verify_token(token, options),\n        }\n    }\n}\n\nimpl Debug for JwtSigningConfig {\n    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"{}\",\n            match self {\n                JwtSigningConfig::Advanced(a) => {\n                    match a {\n                        JWTAlgorithm::HS256(_) => \"HS256\",\n                        JWTAlgorithm::HS384(_) => \"HS384\",\n                        JWTAlgorithm::HS512(_) => \"HS512\",\n                        JWTAlgorithm::RS256(_) => \"RS256\",\n                        JWTAlgorithm::RS384(_) => \"RS384\",\n                        JWTAlgorithm::RS512(_) => \"RS512\",\n                        JWTAlgorithm::EdDSA(_) => \"EdDSA\",\n                    }\n                }\n                JwtSigningConfig::Default { .. } => {\n                    \"HS256\"\n                }\n            }\n        )\n    }\n}\n\nfn deserialize_hs256<'de, D>(deserializer: D) -> Result<HS256Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS256Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs384<'de, D>(deserializer: D) -> Result<HS384Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS384Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_hs512<'de, D>(deserializer: D) -> Result<HS512Key, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    Ok(HS512Key::from_bytes(\n        String::deserialize(deserializer)?.as_bytes(),\n    ))\n}\n\nfn deserialize_rs256<'de, D>(deserializer: D) -> Result<RS256, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS256KeyPair::from_pem(&key) {\n        Ok(RS256::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS256PublicKey::from_pem(&key) {\n        Ok(RS256::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_rs384<'de, D>(deserializer: D) -> Result<RS384, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS384KeyPair::from_pem(&key) {\n        Ok(RS384::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS384PublicKey::from_pem(&key) {\n        Ok(RS384::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_rs512<'de, D>(deserializer: D) -> Result<RS512, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = RS512KeyPair::from_pem(&key) {\n        Ok(RS512::Pair(Box::new(pair)))\n    } else if let Ok(public) = RS512PublicKey::from_pem(&key) {\n        Ok(RS512::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n\nfn deserialize_eddsa<'de, D>(deserializer: D) -> Result<EdDSA, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    if let Ok(pair) = Ed25519KeyPair::from_pem(&key) {\n        Ok(EdDSA::Pair(Box::new(pair)))\n    } else if let Ok(public) = Ed25519PublicKey::from_pem(&key) {\n        Ok(EdDSA::Public(public))\n    } else {\n        Err(serde::de::Error::custom(\"could not deserialize key\"))\n    }\n}\n<|fim_middle|>", "completion": "match kind {\n                    RS256::Public(key) => key.verify_token(token, options),\n                    RS256::Pair(pair) => pair.public_key().verify_token(token, options),\n                }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/security.rs", "node_type": "match_expression", "line_range": [311, 314]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Configuration-dependent queue tests. This will depend on the set environment variables as with\n//! the e2e tests such as to allow testing multiple queue backends via the test script.\n\nuse std::{str::FromStr, time::Duration};\n\nuse http::StatusCode;\nuse redis::AsyncCommands as _;\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::Configuration,\n    core::types::{\n        ApplicationId, BaseId, EndpointId, MessageAttemptTriggerType, MessageId, OrganizationId,\n    },\n    queue::{\n        new_pair, MessageTask, QueueTask, TaskQueueConsumer, TaskQueueDelivery, TaskQueueProducer,\n    },\n    redis::RedisManager,\n    v1::endpoints::message::MessageOut,\n};\nuse tokio::time::timeout;\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id_and_prefix,\n};\n\n// TODO: Don't copy this from the Redis queue test directly, place the fn somewhere both can access\nasync fn get_pool(cfg: &Configuration) -> RedisManager {\n    RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await\n}\n\nfn task_queue_delivery_to_u16(tqd: &TaskQueueDelivery) -> u16 {\n    match &*tqd.task {\n        QueueTask::HealthCheck => panic!(\"Health check in test\"),\n        QueueTask::MessageBatch(batch) => u16::from_str(batch.msg_id.as_str()).unwrap(),\n        QueueTask::MessageV1(task) => u16::from_str(task.msg_id.as_str()).unwrap(),\n    }\n}\n\nasync fn test_many_queue_consumers_inner(prefix: &str, delay: Option<Duration>) {\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n\n    // This test assumes an empty queue, so load Redis and delete the test key\n    {\n        let pool = get_pool(&cfg).await;\n        let mut conn = pool.get().await.unwrap();\n\n        let _: () = conn\n            .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n            .await\n            .unwrap();\n    }\n\n    // Make 20 producers and 20 consumers using the same configuration\n    let mut producers_and_consumers: Vec<(TaskQueueProducer, TaskQueueConsumer)> = Vec::new();\n    for _ in 0..20 {\n        producers_and_consumers.push(new_pair(&cfg, Some(prefix)).await);\n    }\n\n    // Add 200 test messagesÂ¹ with unique message IDs to each producer for a\n    // total of 4000 unique messages\n    //\n    // Â¹ it is important for this number to be no smaller than MAX_MESSAGES in\n    //   TaskQueueConsumer::receive_all\n    for (index, (p, _c)) in producers_and_consumers.iter().enumerate() {\n        for num in 0..200 {\n            p.send(\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"{}\", index * 200 + num)),\n                    app_id: ApplicationId(\"TestApplicationId\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointId\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                }),\n                delay,\n            )\n            .await\n            .unwrap();\n        }\n    }\n\n    let mut join_handles = Vec::new();\n    // Producers need to stay alive for the remainder of the test for in-memory queue which uses\n    // [`tokio::mpsc`]s, so add them to this [`Vec`]\n    let mut producers = Vec::new();\n\n    // Ensure that consumers run on separate OS threads and receive messages until 500ms has passed\n    // without any messages\n    for (p, mut c) in producers_and_consumers {\n        producers.push(p);\n        let handle = tokio::runtime::Handle::current();\n        join_handles.push(std::thread::spawn(move || {\n            handle.block_on(async move {\n                let mut out = Vec::new();\n                let mut read = 0;\n\n                while let Ok(recv) = timeout(\n                    Duration::from_secs(1),\n                    c.receive_all(Duration::from_secs(5)),\n                )\n                .await\n                {\n                    let recv = recv.unwrap();\n                    read += recv.len();\n                    for<|fim_suffix|>              }\n\n                (out, read)\n            })\n        }));\n    }\n\n    // Create a Vec with all the threads' outputs\n    let mut out = Vec::new();\n    for jh in join_handles {\n        let (mut jh_out, read): (Vec<u16>, usize) = jh.join().unwrap();\n        out.append(&mut jh_out);\n\n        if read < 20 {\n            panic!(\"Consumer starved, only read {read} messages\");\n        }\n    }\n\n    // Sort it by the message ID\n    out.sort();\n\n    // Then assert that all the messages are there\n    assert_eq!(out.len(), 4000);\n    for (idx, &num) in out.iter().enumerate() {\n        assert_eq!(idx, num as usize);\n    }\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n// run with `cargo test -- --ignored redis` only when redis is up and configured\n#[ignore]\nasync fn test_many_queue_consumers() {\n    test_many_queue_consumers_inner(\"test_many_queue_consumers_\", None).await;\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n#[ignore]\nasync fn test_many_queue_consumers_delayed() {\n    test_many_queue_consumers_inner(\n        \"test_many_queue_consumers_delayed_\",\n        Some(Duration::from_millis(500)),\n    )\n    .await;\n}\n\n#[tokio::test]\n#[ignore]\nasync fn test_redis_streams_dlq() {\n    let mut cfg = get_default_test_config();\n    cfg.worker_enabled = false;\n    cfg.redis_pending_duration_secs = 1;\n\n    let cfg = std::sync::Arc::new(cfg);\n    let prefix = svix_ksuid::Ksuid::new(None, None).to_string();\n\n    let pool = get_pool(&cfg).await;\n    let mut conn = pool.get().await.unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n        .await\n        .unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_dlq\"))\n        .await\n        .unwrap();\n\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id_and_prefix(\n        &cfg,\n        OrganizationId::new(None, None),\n        prefix.clone(),\n    )\n    .await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let _message_1: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({\"test\": \"value\"})).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let (_p, mut c) = new_pair(&cfg, Some(&prefix)).await;\n\n    let wait_time = std::time::Duration::from_millis(1_500);\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n\n    tokio::time::sleep(wait_time).await;\n\n    // Redrive\n    client\n        .post_without_response(\n            \"/api/v1/admin/redrive-dlq\",\n            serde_json::Value::Null,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n}\n<|fim_middle|>", "completion": "for r in recv {\n                        out.push(task_queue_delivery_to_u16(&r));\n                        r.ack().await.unwrap();\n                    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/redis_queue.rs", "node_type": "for_expression", "line_range": [109, 112]}
{"prompt": "<|fim_prefix|>use std::{\n    io::{Error, Result},\n    path::PathBuf,\n    time::Duration,\n};\n\nuse clap::Parser;\n<|fim_suffix|>\nuse once_cell::sync::Lazy;\nuse opentelemetry::trace::TracerProvider as _;\nuse opentelemetry_otlp::WithExportConfig;\nuse opentelemetry_sdk::{\n    metrics::{data::Temporality, reader::TemporalitySelector, InstrumentKind, SdkMeterProvider},\n    runtime::Tokio,\n};\nuse svix_bridge_types::{PollerInput, SenderInput, TransformerJob};\nuse svix_ksuid::{KsuidLike as _, KsuidMs};\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\nuse tikv_jemallocator::Jemalloc;\nuse tracing_subscriber::{layer::SubscriberExt as _, util::SubscriberInitExt as _};\n\nuse self::config::Config;\n\nmod allocator;\nmod config;\nmod http_output;\nmod metrics;\nmod runtime;\nmod webhook_receiver;\n\nuse crate::{\n    allocator::{get_allocator_stat_mibs, get_allocator_stats},\n    config::{EitherReceiver, PollerReceiverConfig, WebhookReceiverConfig},\n    metrics::CommonMetrics,\n};\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\n#[global_allocator]\nstatic GLOBAL: Jemalloc = Jemalloc;\n\n#[cfg(all(target_env = \"msvc\", feature = \"jemalloc\"))]\ncompile_error!(\"jemalloc cannot be enabled on msvc\");\n\n// Seems like it would be useful to be able to configure this.\n// In some docker setups, hostname is sometimes the container id, and advertising this can be\n// helpful.\nstatic INSTANCE_ID: Lazy<String> = Lazy::new(|| KsuidMs::new(None, None).to_string());\n\nfn get_svc_identifiers(cfg: &Config) -> opentelemetry_sdk::Resource {\n    opentelemetry_sdk::Resource::new(vec![\n        opentelemetry::KeyValue::new(\n            \"service.name\",\n            cfg.opentelemetry\n                .as_ref()\n                .and_then(|x| x.service_name.as_deref())\n                .unwrap_or(\"svix-bridge\")\n                .to_owned(),\n        ),\n        opentelemetry::KeyValue::new(\"instance_id\", INSTANCE_ID.to_owned()),\n    ])\n}\n\nfn setup_tracing(cfg: &Config) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        const CRATE_NAME: &str = env!(\"CARGO_CRATE_NAME\");\n        let level = cfg.log_level.to_string();\n        let var = [\n            format!(\"{CRATE_NAME}={level}\"),\n            // XXX: Assuming this applies to the Producer side (aka `og-ingester`) when we fold it back in.\n            format!(\"tower_http={level}\"),\n        ];\n        var.join(\",\")\n    });\n\n    let otel_layer = cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        // Configure the OpenTelemetry tracing layer\n        opentelemetry::global::set_text_map_propagator(\n            opentelemetry_sdk::propagation::TraceContextPropagator::new(),\n        );\n\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        let tracer = opentelemetry_otlp::new_pipeline()\n            .tracing()\n            .with_exporter(exporter)\n            .with_trace_config(\n                opentelemetry_sdk::trace::Config::default()\n                    .with_sampler(\n                        otel_cfg\n                            .sample_ratio\n                            .map(opentelemetry_sdk::trace::Sampler::TraceIdRatioBased)\n                            .unwrap_or(opentelemetry_sdk::trace::Sampler::AlwaysOn),\n                    )\n                    .with_resource(get_svc_identifiers(cfg)),\n            )\n            .install_batch(Tokio)\n            .unwrap()\n            .tracer(\"svix_bridge\");\n\n        tracing_opentelemetry::layer().with_tracer(tracer)\n    });\n\n    // Then create a subscriber with an additional layer printing to stdout.\n    // This additional layer is either formatted normally or in JSON format.\n    match cfg.log_format {\n        config::LogFormat::Default => {\n            let stdout_layer = tracing_subscriber::fmt::layer();\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n        config::LogFormat::Json => {\n            let fmt = tracing_subscriber::fmt::format().json().flatten_event(true);\n            let json_fields = tracing_subscriber::fmt::format::JsonFields::new();\n\n            let stdout_layer = tracing_subscriber::fmt::layer()\n                .event_format(fmt)\n                .fmt_fields(json_fields);\n\n            tracing_subscriber::Registry::default()\n                .with(otel_layer)\n                .with(stdout_layer)\n                .with(tracing_subscriber::EnvFilter::new(filter_directives))\n                .init()\n        }\n    };\n}\n\n/// Delta temporality selector as recommended by upstream:\n/// https://github.com/open-telemetry/opentelemetry-rust/discussions/1511#discussioncomment-8386721\nstruct DeltaTemporalitySelector;\n\nimpl TemporalitySelector for DeltaTemporalitySelector {\n    fn temporality(&self, kind: InstrumentKind) -> Temporality {\n        match kind {\n            InstrumentKind::UpDownCounter => Temporality::Cumulative,\n            InstrumentKind::ObservableUpDownCounter => Temporality::Cumulative,\n            _ => Temporality::Delta,\n        }\n    }\n}\n\npub fn setup_metrics(cfg: &Config) -> Option<SdkMeterProvider> {\n    cfg.opentelemetry.as_ref().map(|otel_cfg| {\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(&otel_cfg.address);\n\n        opentelemetry_otlp::new_pipeline()\n            .metrics(Tokio)\n            .with_temporality_selector(DeltaTemporalitySelector)\n            .with_exporter(exporter)\n            .with_resource(get_svc_identifiers(cfg))\n            .build()\n            .unwrap()\n    })\n}\n\nasync fn supervise_senders(inputs: Vec<Box<dyn SenderInput>>) -> Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"sender input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are p<|fim_middle|>", "completion": "use itertools::{Either, Itertools};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "use_declaration", "line_range": [8, 8]}
{"prompt": "<|fim_prefix|><|fim_suffix|>\nuse syn::{parse_macro_input, parse_quote, DeriveInput, GenericParam, Generics, ItemFn};\n\nmod aide;\n\nuse self::aide::{expand_aide_annotate, AideAnnotateArgumentList};\n\n#[proc_macro_derive(ModelIn)]\npub fn derive_model_in(input: proc_macro::TokenStream) -> proc_macro::TokenStream {\n    // Parse the input tokens into a syntax tree.\n    let input = parse_macro_input!(input as DeriveInput);\n\n    // Used in the quasi-quotation below as `#name`.\n    let name = input.ident;\n\n    let expanded = quote! {\n        impl From<#name> for <#name as crate::v1::utils::ModelIn>::ActiveModel {\n            fn from(data: #name) -> Self {\n                let mut ret = Self {\n                    ..Default::default()\n                };\n                data.update_model(&mut ret);\n                ret\n            }\n        }\n\n    };\n\n    // Hand the output tokens back to the compiler.\n    proc_macro::TokenStream::from(expanded)\n}\n\n#[proc_macro_derive(ModelOut)]\npub fn derive_model_out(input: proc_macro::TokenStream) -> proc_macro::TokenStream {\n    // Parse the input tokens into a syntax tree.\n    let input = parse_macro_input!(input as DeriveInput);\n\n    // Used in the quasi-quotation below as `#name`.\n    let name = input.ident;\n\n    // Add a bound `T: BaseId` to every type parameter T.\n    let generics = add_trait_bounds(input.generics);\n    let (impl_generics, ty_generics, where_clause) = generics.split_for_impl();\n\n    let expanded = if name == \"EventTypeOut\" {\n        // We want to use name as the id in this case\n        quote! {\n            impl #impl_generics crate::v1::utils::ModelOut for #name #ty_generics #where_clause {\n                fn id_copy(&self) -> String {\n                    self.name.0.clone()\n                }\n            }\n        }\n    } else {\n        quote! {\n            impl #impl_generics crate::v1::utils::ModelOut for #name #ty_generics #where_clause {\n                fn id_copy(&self) -> String {\n                    self.id.0.clone()\n                }\n            }\n        }\n    };\n\n    // Hand the output tokens back to the compiler.\n    proc_macro::TokenStream::from(expanded)\n}\n\n#[proc_macro_attribute]\n/// Generate an aide operation transform for an axum handler function.\n///\n/// The generated function has the same name as the handler, suffixed with\n/// `_operation`. It automatically sets the operation ID, summary and\n/// description.\n///\n/// # Example\n/// ```ignore\n/// /// This is foo!\n/// #[aide_annotate]\n/// fn foo() {\n/// }\n///\n/// /// This is bar, with a custom op ID and summary\n/// #[aide_annotate(op_id = \"custom_id\", op_summary = \"Bar Operation!\")]\n/// fn bar() {\n/// }\n/// ```\npub fn aide_annotate(\n    args: proc_macro::TokenStream,\n    input: proc_macro::TokenStream,\n) -> proc_macro::TokenStream {\n    let args = parse_macro_input!(args with AideAnnotateArgumentList::parse_terminated);\n    let item = parse_macro_input!(input as ItemFn);\n\n    expand_aide_annotate(args, item)\n        .unwrap_or_else(syn::Error::into_compile_error)\n        .into()\n}\n\n// Add a bound `T: HeapSize` to every type parameter T.\nfn add_trait_bounds(mut generics: Generics) -> Generics {\n    for param in &mut generics.params {\n        if let GenericParam::Type(ref mut type_param) = *param {\n            type_param.bounds.push(parse_quote!(heapsize::HeapSize));\n        }\n    }\n    generics\n}\n<|fim_middle|>", "completion": "use quote::quote;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server-derive/src/lib.rs", "node_type": "use_declaration", "line_range": [1, 1]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::collections::HashMap;\n\nuse chrono::Utc;\nuse jsonschema::Draft;\nuse schemars::JsonSchema;\nuse sea_orm::{entity::prelude::*, ActiveValue::Set};\nuse serde::{Deserialize, Serialize};\n\nuse crate::{\n    core::types::{\n        BaseId, EventTypeId, EventTypeName, FeatureFlag, FeatureFlagSet, OrganizationId,\n    },\n    json_wrapper,\n};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"eventtype\")]\npub struct Model {\n    pub created_at: DateTimeWithTimeZone,\n    pub updated_at: DateTimeWithTimeZone,\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: EventTypeId,\n    pub org_id: OrganizationId,\n    pub description: String,\n    pub deleted: bool,\n    pub deprecated: bool,\n    pub schemas: Option<Schema>,\n    pub name: EventTypeName,\n    pub feature_flag: Option<FeatureFlag>,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter)]\npub enum Relation {}\n\nimpl RelationTrait for Relation {\n    fn def(&self) -> RelationDef {\n        panic!(\"No RelationDef\")\n    }\n}\n\n#[axum::async_trait]\nimpl ActiveModelBehavior for ActiveModel {\n    fn new() -> Self {\n        let timestamp = Utc::now();\n        Self {\n            id: Set(EventTypeId::new(timestamp.into(), None)),\n            created_at: Set(timestamp.into()),\n            updated_at: Set(timestamp.into()),\n            deleted: Set(false),\n            ..ActiveModelTrait::default()\n        }\n    }\n\n    async fn before_save<C>(mut self, _db: &C, _insert: bool) -> Result<Self, DbErr>\n    where\n        C: ConnectionTrait,\n    {\n        self.updated_at = Set(Utc::now().into());\n        Ok(self)\n    }\n}\n\nimpl Entity {\n    pub fn secure_find(org_id: OrganizationId) -> Select<Entity> {\n        Self::find().filter(Column::OrgId.eq(org_id))\n    }\n\n    pub fn secure_find_by_name(org_id: OrganizationId, name: EventTypeName) -> Select<Entity> {\n        Self::secure_find(org_id).filter(Column::Name.eq(name))\n    }\n\n    pub fn filter_feature_flags(query: Select<Self>, flags: FeatureFlagSet) -> Select<Self> {\n        query.filter(\n            sea_orm::Condition::any()\n                .add(Column::FeatureFlag.is_in(flags))\n                .add(Column::FeatureFlag.is_null()),\n        )\n    }\n}\n\npub fn schema_example() -> serde_json::Value {\n    serde_json::json!({\n        \"description\": \"An invoice was paid by a user\",\n        \"properties\": {\n            \"invoiceId\": {\n                \"description\": \"The invoice id\",\n                \"type\": \"string\"\n            },\n            \"userId\": {\n                \"description\": \"The user id\",\n                \"type\": \"string\"\n            }\n        },\n        \"required\": [\n            \"invoiceId\",\n            \"userId\"\n        ],\n        \"title\": \"Invoice Paid Event\",\n        \"type\": \"object\"\n    })\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Default)]\npub struct Schema(HashMap<String, Json>);\njson_wrapper!(Schema);\n\nimpl Schema {\n    pub fn example(&self) -> Option<&serde_json::Value> {\n        self.0\n            .get(\"1\")\n            .and_then(|version| match version {\n                serde_json::Value::Object(obj) => obj.get(\"examples\"),\n                _ => None,\n            })\n            .and_then(|examples| match examples {\n                serde_json::Value::Array(arr) => arr.iter().next(),\n                _ => None,\n            })\n    }\n}\n\nimpl JsonSchema for Schema {\n    f<|fim_suffix|>\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        let mut schema = gen.subschema_for::<HashMap<String, Json>>();\n\n        if let schemars::schema::Schema::Object(obj) = &mut schema {\n            obj.extensions\n                .insert(\"example\".to_string(), schema_example());\n        }\n\n        schema\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\nimpl<'de> Deserialize<'de> for Schema {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let inner: HashMap<String, Json> = Deserialize::deserialize(deserializer)?;\n\n        // JSONSchema doesn't implement (De)Serialize, so we have to\n        // manually enforce the values are valid JSON schemas\n\n        let opts = jsonschema::Validator::options().with_draft(Draft::Draft7);\n        if let Some(error) = inner.values().find_map(|schema| opts.build(schema).err()) {\n            return Err(serde::de::Error::custom(error));\n        }\n\n        Ok(Self(inner))\n    }\n}\n<|fim_middle|>", "completion": "fn schema_name() -> String {\n        stringify!(Schema).to_string()\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/eventtype.rs", "node_type": "function_item", "line_range": [126, 128]}
{"prompt": "<|fim_prefix|>//! Redis stream-based queue implementation\n//!\n//! # Redis Streams in Brief\n//! Redis has a built-in queue called streams. With consumer groups and consumers, messages in this\n//! queue will automatically be put into a pending queue when read and deleted when acknowledged.\n//!\n//! # The Implementation\n//! This implementation uses this to allow worker instances to race for messages to dispatch which\n//! are then, ideally, acknowledged. If a message is processing for more than 45 seconds, it is\n//! reinserted at the back of the queue to be tried again.\n//!\n//! This implementation uses the following data structures:\n//! - A \"tasks to be processed\" stream - which is what the consumer listens to for tasks.\n//!   AKA: Main\n//! - A ZSET for delayed tasks with the sort order being the time-to-be-delivered\n//!   AKA: Delayed\n//!\n//! - Tasks in the delayed queue are prefixed with a ksuid so that we can know the timestamp of when\n//!   they should be executed.\n//!\n//! The implementation spawns an additional worker that monitors both the zset delayed tasks and\n//! the tasks currently processing. It monitors the zset task set for tasks that should be\n//! processed now, and the currently processing queue for tasks that have timed out and should be\n//! put back on the main queue.\n\n// This lint warns on `let _: () = ...` which is used throughout this file for Redis commands which\n// have generic return types. This is cleaner than the turbofish operator in my opinion.\n#![allow(clippy::let_unit_value)]\n\nuse std::{num::NonZeroUsize, sync::Arc, time::Duration};\n\nuse omniqueue::backends::{redis::DeadLetterQueueConfig, RedisBackend, RedisConfig};\nuse redis::{AsyncCommands as _, RedisResult};\n\nuse super::{QueueTask, TaskQueueConsumer, TaskQueueProducer};\n<|fim_suffix|>\n\n/// This is the key of the main queue. As a KV store, redis places the entire stream under this key.\n/// Confusingly, each message in the queue may have any number of KV pairs.\nconst MAIN: &str = \"{queue}_svix_v3_main\";\n\n/// The key for the DELAYED queue in which scheduled messages are placed. This is the same DELAYED\n/// queue as v2 of the queue implementation.\nconst DELAYED: &str = \"{queue}_svix_delayed\";\n\n/// The key for the lock guarding the delayed queue background task.\nconst DELAYED_LOCK: &str = \"{queue}_svix_delayed_lock\";\n\n/// The key for the DLQ\nconst DLQ: &str = \"{queue}_svix_dlq\";\n\n// v2 KEY CONSTANTS\nconst LEGACY_V2_MAIN: &str = \"{queue}_svix_main\";\nconst LEGACY_V2_PROCESSING: &str = \"{queue}_svix_processing\";\n\n// v1 KEY CONSTANTS\nconst LEGACY_V1_MAIN: &str = \"svix_queue_main\";\nconst LEGACY_V1_PROCESSING: &str = \"svix_queue_processing\";\nconst LEGACY_V1_DELAYED: &str = \"svix_queue_delayed\";\n\n/// Consumer group name constant -- each consumer group is able to read and acknowledge messages\n/// from the queue, and messages are read by all consumer groups.\nconst WORKERS_GROUP: &str = \"svix_workers_group\";\n/// Consumer group consumer name constant -- consumer groups contain consumers which receive\n/// messages in a round-robin manner. Every worker uses the same consumer name such that they race\n/// for messages instead of having them evenly distributed.\nconst WORKER_CONSUMER: &str = \"svix_workers_consumer\";\n\n/// Special ID for XADD command's which generates a stream ID automatically\nconst GENERATE_STREAM_ID: &str = \"*\";\n\n/// Each queue item has a set of KV pairs associated with it, for simplicity a sing key, \"data\" is\n/// used with the entire [`QueueTask`] as the value in serialized JSON\nconst QUEUE_KV_KEY: &str = \"data\";\n\n/// Generates a [`TaskQueueProducer`] and a [`TaskQueueConsumer`] backed by Redis.\npub async fn new_pair(\n    cfg: &Configuration,\n    prefix: Option<&str>,\n) -> (TaskQueueProducer, TaskQueueConsumer) {\n    new_pair_inner(\n        cfg,\n        Duration::from_secs(cfg.redis_pending_duration_secs),\n        prefix.unwrap_or_default(),\n        MAIN,\n        DELAYED,\n        DELAYED_LOCK,\n        DLQ,\n    )\n    .await\n}\n\n/// Runs Redis queue migrations with the given delay schedule. Migrations are run on this schedule\n/// such that if an old instance of the server is online after the migrations are made, that no data\n/// will be lost assuming the old server is taken offline before the last scheduled delay.\nasync fn run_migration_schedule(delays: &[Duration], pool: RedisManager) {\n    let mut conn = pool\n        .get()\n        .await\n        .expect(\"Error retrieving connection from Redis pool\");\n\n    for delay in delays {\n        // drain legacy queues:\n        if let Err(e) = migrate_v1_to_v2_queues(&mut conn).await {\n            tracing::error!(\"Error migrating queue: {}\", e);\n            tokio::time::sleep(*delay).await;\n            continue;\n        }\n        if let Err(e) = migrate_v2_to_v3_queues(&mut conn).await {\n            tracing::error!(\"Error migrating queue: {}\", e);\n            tokio::time::sleep(*delay).await;\n            continue;\n        }\n\n        tokio::time::sleep(*delay).await;\n    }\n}\n\n/// An inner function allowing key constants to be variable for testing purposes\nasync fn new_pair_inner(\n    cfg: &Configuration,\n    pending_duration: Duration,\n    queue_prefix: &str,\n    main_queue_name: &'static str,\n    delayed_queue_name: &'static str,\n    delayed_lock_name: &'static str,\n    dlq_name: &'static str,\n) -> (TaskQueueProducer, TaskQueueConsumer) {\n    let main_queue_name = format!(\"{queue_prefix}{main_queue_name}\");\n    let delayed_queue_name = format!(\"{queue_prefix}{delayed_queue_name}\");\n    let delayed_lock_name = format!(\"{queue_prefix}{delayed_lock_name}\");\n    let dlq_name = format!(\"{queue_prefix}{dlq_name}\");\n\n    // This fn is only called from\n    // - `queue::new_pair` if the queue type is redis and a DSN is set\n    // - redis tests that only makes sense to run with the DSN set\n    let dsn = cfg.redis_dsn.as_deref().unwrap();\n    let pool =\n        RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await;\n\n    // Create the stream and consumer group for the MAIN queue should it not already exist. The\n    // consumer is created automatically upon use so it does not have to be created here.\n    {\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n\n        let consumer_group_resp: RedisResult<()> = conn\n            .xgroup_create_mkstream(&main_queue_name, WORKERS_GROUP, 0i8)\n            .await;\n\n        // If the error is a BUSYGROUP error, then the stream or consumer group already exists. This does\n        // not impact functionality, so continue as usual.\n        if let Err(e) = consumer_group_resp {\n            if !e.to_string().contains(\"BUSYGROUP\") {\n                panic!(\n                    \"error creating consumer group or stream: {:?}, {:?}, {:?}, {:?}, {e:?}\",\n                    e.kind(),\n                    e.detail(),\n                    e.code(),\n                    e.category()\n                )\n            };\n        }\n    }\n\n    // Redis durations are given in integer numbers of milliseconds, so the pending_duration (the\n    // time in which a task is allowed to be processing before being restarted) must be converted to\n    // one.\n    let pending_duration: i64 = pending_duration\n        .as_millis()\n        .try_into()\n        .expect(\"Pending duration out of bounds\");\n\n    // Migrate v1 queues to v2 and v2 queues to v3 on a loop with exponential backoff.\n    tokio::spawn({\n        let pool = pool.clone();\n\n        async move {\n            let delays = [\n                // 11.25 min\n                Duration::from_secs(60 * 11 + 15),\n                // 22.5 min\n                Duration::from_secs(60 * 22 + 30),\n                // 45 min\n                Duration::from_secs(60 * 45),\n                // 1.5 hours\n                Duration::from_secs(60 * 30 * 3),\n                // 3 hours\n                Duration::from_secs(60 * 60 * 3),\n                // 6 hours\n                Duration::from_secs(60 * 60 * 6),\n                // 12 hours\n                Duration::from_secs(60 * 60 * 12),\n                // 24 hours\n                Duration::from_secs(60 * 60 * 24),\n            ];\n\n            run_migration_schedule(&delays, p<|fim_middle|>", "completion": "use crate::{\n    cfg::{Configuration, QueueType},\n    error::Result,\n    metrics::RedisQueueType,\n    redis::{RedisConnection, RedisManager},\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "use_declaration", "line_range": [36, 41]}
{"prompt": "<|fim_prefix|>gitude and Latitude Values\",\n                        \"description\": \"A geographical coordinate.\",\n                        \"required\": [\"latitude\", \"longitude\"],\n                        \"properties\": {\n                        \"latitude\": {\"type\": \"number\", \"minimum\": -90, \"maximum\": 90},\n                        \"longitude\": {\"type\": \"number\", \"minimum\": -180, \"maximum\": 180},\n                        },\n                    }\n                }),\n            )\n            .unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that PUT with invalid ID creates an event type\n    let _: EventTypeOut = client\n        .put(\n            \"api/v1/event-type/fake-id/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Test that description may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"description\": \"updated_description\",\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.schemas, et.schemas);\n\n    // Test that schemas may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": {},\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, Some(Schema::default()));\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that schemas may be unset while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"schemas\": null,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(out.schemas, None);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.deleted, et.deleted);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n\n    // Test that deleted may be set while the rest are omitted\n    let _: EventTypeOut = client\n        .patch(\n            &format!(\"api/v1/event-type/{}/\", et.name),\n            serde_json::json!({\n                \"archived\": true,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert that the change was made\n    let out = client\n        .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert!(out.deleted);\n\n    // Assert the other fields remain unchanged\n    assert_eq!(out.schemas, None);\n    assert_eq!(out.description, \"updated_description\".to_owned());\n}\n\n#[tokio::test]\nasync fn test_event_type_create_read_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"test-event-type\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        client\n            .get::<EventTypeOut>(&format!(\"api/v1/event-type/{}/\", &et.name), StatusCode::OK)\n            .await\n            .unwrap(),\n        et\n    );\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/?with_content=true\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&et));\n\n    let list: ListResponse<EventTypeOut> = client\n        .get(\"api/v1/event-type/\", StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list.data.len(), 1);\n    assert!(list.data.contains(&EventTypeOut {\n        schemas: None,\n        ..et\n    }));\n}\n\n#[tokio::test]\nasync fn test_event_type_feature_flags() {\n    let (client, _jh) = start_svix_server().await;\n\n    let feature = FeatureFlag(\"foo-feature\".into());\n    let another_feature = FeatureFlag(\"bar-feature\".into());\n    let (features, other_features, union) = {\n        let mut s1 = HashSet::new();\n        s1.insert(feature.clone());\n        let mut s2 = HashSet::new();\n        s2.insert(another_feature);\n        let union: FeatureFlagSet = s1.union(&s2).cloned().collect();\n\n        (s1, s2, union)\n    };\n\n    let et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"event-type-with-flag\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: Some(feature),\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            EventTypeIn {\n                name: EventTypeName(\"no-flag-event\".to_owned()),\n                description: \"test-event-description\".to_owned(),\n                deleted: false,\n                deprecated: false,\n                schemas: None,\n                feature_flag: None,\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let app: ApplicationId = client\n        .post::<_, ApplicationOut>(\n            \"api/v1/app/\",\n            application_in(\"TEST_APP_NAME\"),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let path = format!(\"api/v1/event-type/{}/\", et.name);\n\n    for (flag_set, should_see) in [\n        (FeatureFlagSet::default(), false),\n        (other_features, false),\n        (union.clone(), true),\n        (features.clone(), true),\n    ] {\n        let client = app_portal_access(&client, &app, flag_set).await;\n\n        let list: ListResponse<EventTypeOut> = client\n            .get(\"api/v1/event-type/\", StatusCode::OK)\n            .await\n            .unwrap();\n\n        if should_see {\n            // If the client is expected to see both event types it should be able to retrieve it\n            let got_et: EventTypeOut = client.get(&path, StatusCode::OK).await.unwrap();\n            assert_eq!(et, got_et);\n\n            // ... and see it in the list.\n            assert_eq!(list.data.len(), 2);\n            assert!(list.data.contains(&et));\n        } else {\n            // If the client is not supposed to see it it shouldn't be able to retrieve it\n            let _: IgnoredAny = client.get(&path, StatusCode::NOT_FOUND).await.unwrap();\n\n            // ... and it shouldn't be in the list.\n            assert_eq!(list.data.len(), 1);\n            assert!(!list.data.contains(&et));\n        };\n    }\n}\n\n#[tokio::test]\na<|fim_suffix|>\n#[tokio::test]\nasync fn test_schema() {\n    let (client, _jh) = start_svix_server().await;\n    let _: serde_json::Value = client\n        .post(\n            \"api/v1/event-type/\",\n            serde_json::json!({\n                            \"name\": \"bad-schema\",\n                            \"description\": \"I have a bad schema\",\n                            \"schemas\": {\n                                \"1\": {\"readOnly\": 15},\n                            },\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n}\n<|fim_middle|>", "completion": "async fn test_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    common_test_list::<EventTypeOut, EventTypeIn>(\n        &client,\n        \"api/v1/event-type/\",\n        |i| event_type_in(&format!(\"test-event-type-{i}\"), None).unwrap(),\n        true,\n        false,\n    )\n    .await\n    .unwrap();\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_event_type.rs", "node_type": "function_item", "line_range": [291, 303]}
{"prompt": "<|fim_prefix|>use std::{collections::HashSet, mem};\n\nuse axum::{\n    extract::{Path, State},\n    Json,\n};\nuse sea_orm::{entity::prelude::*, ActiveValue::Set, QuerySelect, TransactionTrait};\nuse svix_server_derive::aide_annotate;\nuse url::Url;\n\nuse self::hack::EventTypeNameResult;\nuse super::{EndpointIn, EndpointOut, EndpointPatch, EndpointUpdate};\nuse crate::{\n    cfg::Configuration,\n    core::{\n        operational_webhooks::{EndpointEvent, OperationalWebhook, OperationalWebhookSender},\n        permissions,\n        types::{EndpointId, EventTypeName, EventTypeNameSet, OrganizationId},\n    },\n    db::models::{application, endpoint, endpointmetadata, eventtype},\n    error::{http_error_on_conflict, HttpError, Result, Traceable, ValidationErrorItem},\n    v1::utils::{\n        apply_pagination,\n        patch::{patch_field_non_nullable, UnrequiredField, UnrequiredNullableField},\n        ApplicationEndpointPath, ApplicationPath, IteratorDirection, JsonStatus, JsonStatusUpsert,\n        ListResponse, ModelIn, ModelOut, NoContent, Ordering, Pagination, PaginationLimit,\n        ReversibleIterator, ValidatedJson, ValidatedQuery,\n    },\n    AppState,\n};\n\n/// List the application's endpoints.\n#[aide_annotate(op_id = \"v1.endpoint.list\")]\npub(super) async fn list_endpoints(\n    State(AppState { ref db, .. }): State<AppState>,\n    _: Path<ApplicationPath>,\n    ValidatedQuery(pagination): ValidatedQuery<Pagination<ReversibleIterator<EndpointId>>>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<ListResponse<EndpointOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    let iterator = pagination.iterator;\n    let iter_direction = iterator\n        .as_ref()\n        .map_or(IteratorDirection::Normal, |iter| iter.direction());\n\n    let query = apply_pagination(\n        endpoint::Entity::secure_find(app.id),\n        endpoint::Column::Id,\n        limit,\n        iterator,\n        pagination.order.unwrap_or(Ordering::Descending),\n    );\n\n    let results = query\n        .find_also_related(endpointmetadata::Entity)\n        .all(db)\n        .await?\n        .into_iter()\n        .map(|(endp, metadata)| {\n            let metadata = metadata.map(|m| m.data).unwrap_or_default();\n            (endp, metadata).into()\n        })\n        .collect();\n\n    Ok(Json(EndpointOut::list_response(\n        results,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\nasync fn create_endp_from_data(\n    db: &DatabaseConnection,\n    cfg: &Configuration,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    mut data: EndpointIn,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let key = data.key_take_or_generate(&cfg.encryption, &cfg.default_signature_type)?;\n\n    let mut endp = endpoint::ActiveModel::new(app.id, key);\n    <|fim_suffix|>\n    data.update_model(&mut endp);\n\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.insert(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointCreated(EndpointEvent::new(app.uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Create a new endpoint for the application.\n///\n/// When `secret` is `null` the secret is automatically generated (recommended)\n#[aide_annotate(op_id = \"v1.endpoint.create\")]\npub(super) async fn create_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        op_webhooks,\n        ..\n    }): State<AppState>,\n    _: Path<ApplicationPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointIn>,\n) -> Result<JsonStatus<201, EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let (endp, metadata) = create_endp_from_data(db, cfg, &op_webhooks, app, data)\n        .await\n        .trace()?;\n\n    Ok(JsonStatus((endp, metadata.data).into()))\n}\n\n/// Get an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.get\")]\npub(super) async fn get_endpoint(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointOut>> {\n    let (endp, metadata) = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .find_also_related(endpointmetadata::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let metadata = metadata.map(|m| m.data).unwrap_or_default();\n\n    Ok(Json((endp, metadata).into()))\n}\n\nasync fn update_endp_from_data(\n    db: &DatabaseConnection,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    endp: endpoint::ActiveModel,\n    metadata: endpointmetadata::ActiveModel,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.update(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    let app_uid = app.uid;\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointUpdated(EndpointEvent::new(app_uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Update an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.update\")]\npub(super) async fn update_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(mut data): ValidatedJson<EndpointUpdate>,\n) -> Result<JsonStatusUpsert<EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let models = endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n        .await\n        .trace()?;\n\n    if let Some((mut endp, mut metadata)) = models {\n        metadata.data = Set(mem::take(&mut data.metadata));\n        data.update_model(&mut endp);\n        let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Updated((endp, metadata.data).into()))\n    } else {\n        let data = data.into_in_with_default_key();\n        let (endp, metadata) = create_endp_from_data(db, cfg, op_webhooks, app, data)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Created((endp, metadata.data).into()))\n    }\n}\n\n/// Partially update an endpoint.\n#[aide_annotate]\npub(super) async fn patch_endpoint(\n    State(AppState {\n        ref db,\n        cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointPatch>,\n) -> Result<Json<EndpointOut>> {\n    if let UnrequiredNullableField::Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    if let UnrequiredField::Some(url) = &data.url {\n        validate_endpoint_url(url, cfg.endpoint_https_only)?;\n    }\n\n    let (mut endp, mut metadata) =\n        endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut patch_data = data; // need to alias so we can use data for `patch_field_non_nullable!`\n\n    let data = mem::take(&mut patch_data<|fim_middle|>", "completion": "let metadata =\n        endpointmetadata::ActiveModel::new(endp.id.clone().unwrap(), mem::take(&mut data.metadata));", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/crud.rs", "node_type": "let_declaration", "line_range": [82, 83]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::{connector_kind::ConnectorKind, connector_product::ConnectorProduct};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct ConnectorIn {\n    #[serde(rename = \"allowedEventTypes\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub allowed_event_types: Option<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub description: Option<String>,\n\n    #[serde(rename = \"featureFlags\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub feature_flags: Option<Vec<String>>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub instructions: Option<String>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub kind: Option<ConnectorKind>,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub logo: Option<String>,\n\n    pub name: String,\n\n    #[serde(rename = \"productType\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub product_type: Option<ConnectorProduct>,\n\n    pub transformation: String,\n\n    /// The Connector's UID.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<String>,\n}\n\nimpl ConnectorIn {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(name: String, transformation: String) -> Self {\n        Self {\n            allowed_event_types: None,\n            description: None,\n            feature_flags: None,\n            instructions: None,\n            kind: None,\n            logo: None,\n            name,\n            product_type: None,\n            transformation,\n            uid: None,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/connector_in.rs", "node_type": "function_item", "line_range": [42, 55]}
{"prompt": "<|fim_prefix|>ing>,\n    /// Filter response based on the tag\n    #[arg(long)]\n    pub tag: Option<String>,\n    /// Filter the attempts based on the attempted endpoint\n    #[arg(long)]\n    pub endpoint_id: Option<String>,\n    /// Only include items created before a certain date\n    #[arg(long)]\n    pub before: Option<chrono::DateTime<chrono::Utc>>,\n    /// Only include items created after a certain date\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n    /// When `true` attempt content is included in the response\n    #[arg(long)]\n    pub with_content: Option<bool>,\n    /// Filter response based on the event type\n    #[arg(long)]\n    pub event_types: Option<Vec<String>>,\n}\n\nimpl From<MessageAttemptListByMsgOptions> for svix::api::MessageAttemptListByMsgOptions {\n    fn from(value: MessageAttemptListByMsgOptions) -> Self {\n        let MessageAttemptListByMsgOptions {\n            limit,\n            iterator,\n            status,\n            status_code_class,\n            channel,\n            tag,\n            endpoint_id,\n            before,\n            after,\n            with_content,\n            event_types,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            status,\n            status_code_class,\n            channel,\n            tag,\n            endpoint_id,\n            before: before.map(|dt| dt.to_rfc3339()),\n            after: after.map(|dt| dt.to_rfc3339()),\n            with_content,\n            event_types,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageAttemptListAttemptedMessagesOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filter response based on the channel\n    #[arg(long)]\n    pub channel: Option<String>,\n    /// Filter response based on the message tags\n    #[arg(long)]\n    pub tag: Option<String>,\n    /// Filter response based on the status of the attempt: Success (0), Pending (1), Failed (2), or Sending (3)\n    #[arg(long)]\n    pub status: Option<MessageStatus>,\n    /// Only include items created before a certain date\n    #[arg(long)]\n    pub before: Option<chrono::DateTime<chrono::Utc>>,\n    /// Only include items created after a certain date\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n    /// When `true` message payloads are included in the response\n    #[arg(long)]\n    pub with_content: Option<bool>,\n    /// Filter response based on the event type\n    #[arg(long)]\n    pub event_types: Option<Vec<String>>,\n}\n\nimpl From<MessageAttemptListAttemptedMessagesOptions>\n    for svix::api::MessageAttemptListAttemptedMessagesOptions\n{\n    fn from(value: MessageAttemptListAttemptedMessagesOptions) -> Self {\n        let MessageAttemptListAttemptedMessagesOptions {\n            limit,\n            iterator,\n            channel,\n            tag,\n            status,\n            before,\n            after,\n            with_content,\n            event_types,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            channel,\n            tag,\n            status,\n            before: before.map(|dt| dt.to_rfc3339()),\n            after: after.map(|dt| dt.to_rfc3339()),\n            with_content,\n            event_types,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageAttemptListAttemptedDestinationsOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n}\n\nimpl From<MessageAttemptListAttemptedDestinationsOptions>\n    for svix::api::MessageAttemptListAttemptedDestinationsOptions\n{\n    fn from(value: MessageAttemptListAttemptedDestinationsOptions) -> Self {\n        let MessageAttemptListAttemptedDestinationsOptions { limit, iterator } = value;\n        Self { limit, iterator }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageAttemptResendOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\n<|fim_suffix|>\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct MessageAttemptArgs {\n    #[command(subcommand)]\n    pub command: MessageAttemptCommands,\n}\n\n#[derive(Subcommand)]\npub enum MessageAttemptCommands {\n    /// List attempts by endpoint id\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    ListByEndpoint {\n        app_id: String,\n        endpoint_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptListByEndpointOptions,\n    },\n    /// List attempts by message ID.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    ListByMsg {\n        app_id: String,\n        msg_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptListByMsgOptions,\n    },\n    /// List messages for a particular endpoint. Additionally includes metadata about the latest message attempt.\n    ///\n    /// The `before` parameter lets you filter all items created before a certain date and is ignored if an iterator is passed.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    ListAttemptedMessages {\n        app_id: String,\n        endpoint_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptListAttemptedMessagesOptions,\n    },\n    /// `msg_id`: Use a message id or a message `eventId`\n    Get {\n        app_id: String,\n        msg_id: String,\n        attempt_id: String,\n    },\n    /// Deletes the given attempt's response body.\n    ///\n    /// Useful when an endpoint accidentally returned sensitive content.\n    /// The message can't be replayed or resent once its payload has been deleted or expired.\n    ExpungeContent {\n        app_id: String,\n        msg_id: String,\n        attempt_id: String,\n    },\n    /// List endpoints attempted by a given message.\n    ///\n    /// Additionally includes metadata about the latest message attempt.\n    /// By default, endpoints are listed in ascending order by ID.\n    ListAttemptedDestinations {\n        app_id: String,\n        msg_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptListAttemptedDestinationsOptions,\n    },\n    /// Resend a message to the specified endpoint.\n    Resend {\n        app_id: String,\n        msg_id: String,\n        endpoint_id: String,\n        #[clap(flatten)]\n        options: MessageAttemptResendOptions,\n    },\n}\n\nimpl MessageAttemptCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::ListByEndpoint {\n                app_id,\n                endpoint_id,\n                options,\n            } => {\n                let resp = client\n                    .message_attempt()\n                    .list_by_endpoint(app_id, endpoint_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ListByMsg {\n                app_id,\n                msg_id,\n                options,\n            } => {\n                let resp = client\n                    .message_attempt()\n                    .list_by_msg(app_id, msg_id, Some(options.into()))\n                    .await?;\n       <|fim_middle|>", "completion": "impl From<MessageAttemptResendOptions> for svix::api::MessageAttemptResendOptions {\n    fn from(value: MessageAttemptResendOptions) -> Self {\n        let MessageAttemptResendOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/message_attempt.rs", "node_type": "impl_item", "line_range": [226, 231]}
{"prompt": "<|fim_prefix|>use deno_core::JsRuntime;\nuse serde_json::json;\nuse svix_bridge_types::{TransformerInput, TransformerOutput};\n\nuse super::{run_script_inner, validate_script};\n\nfn get_test_rt() -> JsRuntime {\n    JsRuntime::new(Default::default())\n}\n\n// Really just trying to figure out if the deno runtime is working the way I hope.\n#[test]\nfn test_happy_fn() {\n    let src = r#\"\n    function handler(input) {\n        return { \"x\": 123, ...input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({ \"y\": 456 }).into(), src).unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n            assert_eq!(v[\"y\"].as_i64(), Some(456));\n        }\n        TransformerOutput::Invalid => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\nfn test_invalid_output_bool() {\n    let src = r#\"\n    function handler(input) {\n        return false;\n    }\n    \"#\n    .to_string();\n\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\n// FIXME: serde decodes arrays with keys like \"0\", \"1\"... in this situation, failing the test.\n#[ignore]\n<|fim_suffix|>\n\n/// Receives a string input, parses as JSON in js, then returns the result back to rust.\n#[test]\nfn test_string_input() {\n    let src = r#\"\n    function handler(input) {\n        return JSON.parse(input);\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(r#\"{\"x\": 123}\"#)),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n/// Take the string input and just add it to a field in the returned object.\n/// The string should make it through, back to rust, as-is.\n#[test]\nfn test_string_input2() {\n    let src = r#\"\n    function handler(input) {\n        return { \"payload\": input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(\"Hello World\")),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"payload\"].as_str(), Some(\"Hello World\"));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n#[test]\nfn test_validate_script_bad_syntax_is_err() {\n    assert!(validate_script(\"let 123 = ';\").is_err());\n}\n\n#[test]\nfn test_validate_script_empty_handler_is_ok() {\n    assert!(validate_script(\"function handler() { }\").is_ok());\n}\n\n#[test]\nfn test_validate_script_arrow_fn_is_ok() {\n    assert!(validate_script(\"const handler = () => ({ a: 123 })\").is_ok());\n}\n\n/// Technically, this should be legal though the utility is questionable.\n#[test]\nfn test_validate_script_empty_is_ok() {\n    assert!(validate_script(\"\").is_ok());\n    assert!(validate_script(\"    \").is_ok());\n}\n<|fim_middle|>", "completion": "fn test_invalid_output_array() {\n    let src = r#\"\n    function handler(input) {\n        return [1, 2];\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => {\n            panic!(\"got unexpected return value\");\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/tests.rs", "node_type": "function_item", "line_range": [51, 66]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct EventTypeListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n    /// When `true` archived (deleted but not expunged) items are included in the response.\n    #[arg(long)]\n    pub include_archived: Option<bool>,\n    /// When `true` the full item (including the schema) is included in the response.\n    #[arg(long)]\n    pub with_content: Option<bool>,\n}\n\nimpl From<EventTypeListOptions> for svix::api::EventTypeListOptions {\n    fn from(value: EventTypeListOptions) -> Self {\n        let EventTypeListOptions {\n            limit,\n            iterator,\n            order,\n            include_archived,\n            with_content,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            order,\n            include_archived,\n            with_content,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct EventTypeCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EventTypeCreateOptions> for svix::api::EventTypeCreateOptions {\n    fn from(value: EventTypeCreateOptions) -> Self {\n        <|fim_suffix|>\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct EventTypeImportOpenapiOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EventTypeImportOpenapiOptions> for svix::api::EventTypeImportOpenapiOptions {\n    fn from(value: EventTypeImportOpenapiOptions) -> Self {\n        let EventTypeImportOpenapiOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct EventTypeDeleteOptions {\n    /// By default event types are archived when \"deleted\". Passing this to `true` deletes them entirely.\n    #[arg(long)]\n    pub expunge: Option<bool>,\n}\n\nimpl From<EventTypeDeleteOptions> for svix::api::EventTypeDeleteOptions {\n    fn from(value: EventTypeDeleteOptions) -> Self {\n        let EventTypeDeleteOptions { expunge } = value;\n        Self { expunge }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct EventTypeArgs {\n    #[command(subcommand)]\n    pub command: EventTypeCommands,\n}\n\n#[derive(Subcommand)]\npub enum EventTypeCommands {\n    /// Return the list of event types.\n    List {\n        #[clap(flatten)]\n        options: EventTypeListOptions,\n    },\n    /// Create new or unarchive existing event type.\n    ///\n    /// Unarchiving an event type will allow endpoints to filter on it and messages to be sent with it.\n    /// Endpoints filtering on the event type before archival will continue to filter on it.\n    /// This operation does not preserve the description and schemas.\n    Create {\n        event_type_in: crate::json::JsonOf<EventTypeIn>,\n        #[clap(flatten)]\n        options: EventTypeCreateOptions,\n    },\n    /// Given an OpenAPI spec, create new or update existing event types.\n    /// If an existing `archived` event type is updated, it will be unarchived.\n    ///\n    /// The importer will convert all webhooks found in the either the `webhooks` or `x-webhooks`\n    /// top-level.\n    ImportOpenapi {\n        event_type_import_open_api_in: Option<crate::json::JsonOf<EventTypeImportOpenApiIn>>,\n        #[clap(flatten)]\n        options: EventTypeImportOpenapiOptions,\n    },\n    /// Get an event type.\n    Get { event_type_name: String },\n    /// Update an event type.\n    Update {\n        event_type_name: String,\n        event_type_update: crate::json::JsonOf<EventTypeUpdate>,\n    },\n    /// Archive an event type.\n    ///\n    /// Endpoints already configured to filter on an event type will continue to do so after archival.\n    /// However, new messages can not be sent with it and endpoints can not filter on it.\n    /// An event type can be unarchived with the\n    /// [create operation](#operation/create_event_type_api_v1_event_type__post).\n    Delete {\n        event_type_name: String,\n        #[clap(flatten)]\n        options: EventTypeDeleteOptions,\n    },\n    /// Partially update an event type.\n    Patch {\n        event_type_name: String,\n        event_type_patch: Option<crate::json::JsonOf<EventTypePatch>>,\n    },\n}\n\nimpl EventTypeCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { options } => {\n                let resp = client.event_type().list(Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                event_type_in,\n                options,\n            } => {\n                let resp = client\n                    .event_type()\n                    .create(event_type_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ImportOpenapi {\n                event_type_import_open_api_in,\n                options,\n            } => {\n                let resp = client\n                    .event_type()\n                    .import_openapi(\n                        event_type_import_open_api_in\n                            .unwrap_or_default()\n                            .into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { event_type_name } => {\n                let resp = client.event_type().get(event_type_name).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                event_type_name,\n                event_type_update,\n            } => {\n                let resp = client\n                    .event_type()\n                    .update(event_type_name, event_type_update.into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete {\n                event_type_name,\n                options,\n            } => {\n                client\n                    .event_type()\n                    .delete(event_type_name, Some(options.into()))\n                    .await?;\n            }\n            Self::Patch {\n                event_type_name,\n                event_type_patch,\n            } => {\n                let resp = client\n                    .event_type()\n                    .patch(\n                        event_type_name,\n                        event_type_patch.unwrap_or_default().into_inner(),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let EventTypeCreateOptions { idempotency_key } = value;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/event_type.rs", "node_type": "let_declaration", "line_range": [51, 51]}
{"prompt": "<|fim_prefix|>        )\n        })?\n    };\n\n    let msg_in = MessageIn {\n        channels: None,\n        event_type: data.event_type,\n        payload: RawPayload::from_string(example).unwrap(),\n        uid: None,\n        payload_retention_period: 90,\n        extra_params: None,\n        application: None,\n    };\n\n    let create_message = create_message_inner(\n        db,\n        queue_tx,\n        cache,\n        false,\n        Some(endpoint.id),\n        msg_in,\n        app.org_id,\n        ApplicationIdOrUid(app.id.0),\n    )\n    .await?;\n\n    Ok(Json(create_message))\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Endpoint\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app/:app_id/endpoint\",\n            post_with(crud::create_endpoint, crud::create_endpoint_operation)\n                .get_with(crud::list_endpoints, crud::list_endpoints_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id\",\n            get_with(crud::get_endpoint, crud::get_endpoint_operation)\n                .put_with(crud::update_endpoint, crud::update_endpoint_operation)\n                .patch_with(crud::patch_endpoint, crud::patch_endpoint_operation)\n                .delete_with(crud::delete_endpoint, crud::delete_endpoint_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret\",\n            get_with(\n                secrets::get_endpoint_secret,\n                secrets::get_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/secret/rotate\",\n            post_with(\n                secrets::rotate_endpoint_secret,\n                secrets::rotate_endpoint_secret_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/stats\",\n            get_with(endpoint_stats, endpoint_stats_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/send-example\",\n            post_with(send_example, send_example_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/recover\",\n            post_with(\n                recovery::recover_failed_webhooks,\n                recovery::recover_failed_webhooks_operation,\n            ),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/endpoint/:endpoint_id/headers\",\n            get_with(\n                headers::get_endpoint_headers,\n                headers::get_endpoint_headers_operation,\n            )\n            .patch_with(\n                headers::patch_endpoint_headers,\n                headers::patch_endpoint_headers_operation,\n            )\n            .put_with(\n                headers::update_endpoint_headers,\n                headers::update_endpoint_headers_operation,\n            ),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use std::collections::{HashMap, HashSet};\n\n    use reqwest::Url;\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{validate_url, EndpointHeadersOut, EndpointHeadersPatchIn, EndpointIn};\n    use crate::core::types::EndpointHeaders;\n\n    const URL_VALID: &str = \"https://www.example.com\";\n    const URL_INVALID: &str = \"invalid url\";\n    const VERSION_VALID: u16 = 1;\n    const VERSION_INVALID: u16 = 0;\n    const RATE_LIMIT_VALID: u16 = 1;\n    const RATE_LIMIT_INVALID: u16 = 0;\n    const EVENT_TYPES_INVALID: &[&str] = &[\"valid-event-type\", \"&&invalid-event-type\"];\n    const EVENT_TYPES_VALID: &[&str] = &[\"valid-event-type1\", \"valid-event-type2\"];\n    const EVENT_CHANNELS_INVALID: &[&str] = &[\"valid-event-channel\", \"&&invalid-event-channel\"];\n    const EVENT_CHANNELS_VALID: &[&str] = &[\"valid-event-channel1\", \"valid-event-channel2\"];\n    const ENDPOINT_ID_INVALID: &str = \"$$invalid-endpoint\";\n    const ENDPOINT_ID_VALID: &str = \"valid-endpoint\";\n\n    #[allow(deprecated)]\n    #[test]\n    fn test_endpoint_in_validation() {\n        l<|fim_suffix|>\n        let invalid_2: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"channels\": EVENT_CHANNELS_INVALID\n        }))\n        .unwrap();\n\n        let invalid_3: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_INVALID\n        }))\n        .unwrap();\n\n        let invalid_4: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"uid\": ENDPOINT_ID_INVALID\n        }))\n        .unwrap();\n\n        let invalid_5: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"filterTypes\": EVENT_TYPES_INVALID\n        }))\n        .unwrap();\n\n        let invalid_6: Result<EndpointIn, _> = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_INVALID\n        }));\n        assert!(invalid_6.is_err());\n\n        for e in [invalid_1, invalid_2, invalid_3, invalid_4, invalid_5] {\n            assert!(e.validate().is_err());\n        }\n\n        let valid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_VALID,\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_1.validate().unwrap();\n\n        let valid_2: EndpointIn = serde_json::from_value(json!({\n             \"url\": URL_VALID,\n             \"rateLimit\": RATE_LIMIT_VALID,\n             \"uid\": ENDPOINT_ID_VALID,\n             \"filterTypes\": EVENT_TYPES_VALID,\n             \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid_2.validate().unwrap();\n        assert_eq!(1, valid_2.version.unwrap());\n    }\n\n    #[test]\n    fn test_endpoint_headers_sensitive() {\n        let headers = EndpointHeaders(HashMap::from([\n            (\"foo\".to_string(), \"1\".to_string()),\n            (\"authorization\".to_string(), \"test\".to_string()),\n            (\"X-Auth-Token\".to_string(), \"test2\".to_string()),\n        ]));\n\n        let headers_out: EndpointHeadersOut = headers.into();\n\n        assert_eq!(\n            headers_out.headers,\n            HashMap::from([(\"foo\".to_string(), \"1\".to_string())])\n        );\n        assert_eq!(\n            headers_out.sensitive,\n            HashSet::from([\"authorization\".to_string(), \"X-Auth-Token\".to_string()])\n        );\n    }\n\n    #[test]\n    fn test_endpoint_headers_patch_in_validation() {\n        let headers_valid = HashMap::from([(\"x-valid\", \"1\")]);\n        let headers_invalid = HashMap::from([(\"x-invalid???\", \"1\")]);\n\n        let invalid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_invalid })).unwrap();\n        assert!(invalid.validate().is_err());\n\n        let valid: EndpointHeadersPatchIn =\n            serde_json::from_value(json!({ \"headers\": headers_valid })).unwrap();\n        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_url_validation() {\n        let valid_https = Url::parse(\"https://test.url\").unwrap();\n        let valid_http = Url::parse(\"http://test.url\").unwrap();\n        let invalid_scheme = Url::parse(\"anythingelse://test.url\").unwrap();\n        let invalid_format = \"http://[:::1]\";\n\n        assert!(validate_url(&valid_https).is_ok());\n        assert!(validate_url(&valid_http).is_ok());\n        assert!(validate_url(&invalid_scheme).is_err());\n\n        let valid_https: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_https, \"version\": 1})).unwrap();\n        let valid_http: EndpointIn =\n            serde_json::from_value(json!({\"url\": valid_http, \"version\": 1})).unwrap();\n        let invalid_scheme: EndpointIn =\n            serde_json::from_value(json!({\"url\": invalid_scheme, \"version\": 1})).unwrap();\n        let invalid_format: Result<EndpointIn, _> =\n            serde_json::from_value(json!({<|fim_middle|>", "completion": "let invalid_1: EndpointIn = serde_json::from_value(json!({\n             \"version\": VERSION_INVALID,\n             \"url\": URL_VALID\n        }))\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "let_declaration", "line_range": [912, 916]}
{"prompt": "<|fim_prefix|>alent to [`UnrequiredNullableField`].\n///\n/// This is effectively an [`Option`] with the additional context that any field\n/// which uses this type is a member of a PATCH request model and that the field\n/// may be absent, meaning it is not to be updated. In comparison, [`Option`]s\n/// are used in other [`ModelIn`]s to define a field, that when absent, is\n/// `null`.\n///\n/// NOTE: You must tag these fields with `#[serde(default)]` in order for the\n/// serialization to work correctly.\n#[derive(Debug, Default)]\npub enum UnrequiredField<T> {\n    #[default]\n    Absent,\n    Some(T),\n}\n\nimpl<T> UnrequiredNullableField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredNullableField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredNullableField<U> {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> UnrequiredField<T> {\n    pub fn is_absent(&self) -> bool {\n        matches!(self, UnrequiredField::Absent)\n    }\n\n    pub fn map<U>(self, f: impl Fn(T) -> U) -> UnrequiredField<U> {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(f(v)),\n        }\n    }\n}\n\nimpl<T> From<Option<T>> for UnrequiredNullableField<T> {\n    fn from(opt: Option<T>) -> Self {\n        match opt {\n            Some(v) => UnrequiredNullableField::Some(v),\n            None => UnrequiredNullableField::None,\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredNullableField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n            UnrequiredNullableField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Validate> Validate for UnrequiredField<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            UnrequiredField::Absent => Ok(()),\n            UnrequiredField::Some(v) => v.validate(),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredNullableField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredNullableField::Absent => UnrequiredNullableField::Absent,\n            UnrequiredNullableField::None => UnrequiredNullableField::None,\n            UnrequiredNullableField::Some(v) => UnrequiredNullableField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone> Clone for UnrequiredField<T> {\n    fn clone(&self) -> Self {\n        match self {\n            UnrequiredField::Absent => UnrequiredField::Absent,\n            UnrequiredField::Some(v) => UnrequiredField::Some(v.clone()),\n        }\n    }\n}\n\nimpl<T: Clone + Copy> Copy for UnrequiredNullableField<T> {}\nimpl<T: Clone + Copy> Copy for UnrequiredField<T> {}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredNullableField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        Option::deserialize(deserializer).map(Into::into)\n    }\n}\n\nimpl<'de, T> Deserialize<'de> for UnrequiredField<T>\nwhere\n    T: Deserialize<'de>,\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        T::deserialize(deserializer).map(UnrequiredField::Some)\n    }\n}\n\nimpl<T> Serialize for UnrequiredNullableField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredNullableField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredNullableField must skip serializing if field is absent\",\n            )),\n            UnrequiredNullableField::None => serializer.serialize_none(),\n            UnrequiredNullableField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\nimpl<T> Serialize for UnrequiredField<T>\nwhere\n    T: Serialize,\n{\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        match self {\n            UnrequiredField::Absent => Err(serde::ser::Error::custom(\n                \"UnrequiredField must skip serializing if field is absent\",\n            )),\n            UnrequiredField::Some(v) => v.serialize(serializer),\n        }\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"Unrequired_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<T>()\n    }\n}\n\nimpl<T: JsonSchema> JsonSchema for UnrequiredNullableField<T> {\n    fn is_referenceable() -> bool {\n        false\n    }\n\n    fn schema_name() -> String {\n        format!(\"UnrequiredNullable_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        gen.subschema_for::<Option<T>>()\n    }\n}\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`Option`] is `Some`, but\n/// performs no operation in the case it is `None`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The nullable equivalent which is used for [`UnrequiredNullableField`] is [`patch_field_nullable`].\nmacro_rules! patch_field_non_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredField::Some(v) => $model.$member = Set(v),\n            UnrequiredField::Absent => {}\n        }\n    };\n}\npub(crate) use patch_field_non_nullable;\n\n/// Macro that simplifies updating a field on an [`ActiveModel`] for use in a [`ModelIn`]\n/// implementation. This macro expands to setting the field when the [`UnrequiredNullableField`] is\n/// `Some` and unsetting the field when it is `None`, but performs no operation in the case it is\n///  `Absent`.\n///\n/// The input for this macro is three identifiers meant to be `self`, the `model` in a [`ModelIn`]\n/// implementation, and the member that `self`, and `model` share that is being modified.\n///\n/// Optionally, a fourth identifier may be given which is meant to be a closure that takes the type\n/// of self's version of the member being modified and returns model's version of the member being\n/// modified. This is applied via [`UnrequiredNullableField::map`] such that  basic type conversions may\n/// be made.\n///\n/// The non-nullable equivalent which is used for [`Option`] is [`patch_field_non_nullable`].\nmacro_rules! patch_field_nullable {\n    ($model:ident, $member:ident) => {\n        match $member {\n            UnrequiredNullableField::Some(v) => $model.$member = Set(Some(v)),\n            UnrequiredNullableField::None => $model.$member = Set(None),\n            UnrequiredNullableField::Absent => {}\n        }\n    };\n\n    ($model:ident, $member:ident, $f:ident) => {\n        let mapped = $member.map($f);\n        match mapped {\n            UnrequiredNullableField::Some(v) => $model.$member = Set(Some(v)),\n            UnrequiredNullableField::None => $model.$member = Set(None),\n            UnrequiredNullableField::Absent => {}\n        }\n    };\n}\np<|fim_suffix|><|fim_middle|>", "completion": "pub(crate) use patch_field_nullable;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/patch.rs", "node_type": "use_declaration", "line_range": [269, 269]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::fmt::Debug;\n\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chacha20poly1305::{\n    aead::{Aead, KeyInit},\n    Key, XChaCha20Poly1305, XNonce,\n};\nuse ed25519_compact::*;\nuse rand::Rng;\n\nuse crate::error::Result;\n\n// Asymmetric Signature keys\n#[derive(Clone, Eq)]\npub struct AsymmetricKey(pub KeyPair);\n\nimpl AsymmetricKey {\n    pub fn generate() -> AsymmetricKey {\n        AsymmetricKey(KeyPair::from_seed(Seed::generate()))\n    }\n\n    pub fn from_slice(bytes: &[u8]) -> Result<Self> {\n        Ok(AsymmetricKey(KeyPair::from_slice(bytes).map_err(|_| {\n            crate::error::Error::generic(\"Failed parsing key.\")\n        })?))\n    }\n\n    pub fn from_base64(b64: &str) -> Result<Self> {\n        let bytes = STANDARD\n            .decode(b64)\n            .map_err(|_| crate::error::Error::generic(\"Failed parsing base64\"))?;\n\n        Self::from_slice(bytes.as_slice())\n    }\n\n    pub fn pubkey(&self) -> &[u8] {\n        &self.0.pk[..]\n    }\n}\n\nimpl Debug for AsymmetricKey {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"<AsymmetricKey sk=*** pk={}>\",\n            STANDARD.encode(self.0.pk.as_slice())\n        )\n    }\n}\n\nimpl PartialEq for AsymmetricKey {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.as_slice() == other.0.as_slice()\n    }\n}\n\n#[derive(Clone, Debug)]\npub struct Encryption(Option<Key>);\n\nimpl Encryption {\n    const NONCE_SIZE: usize = 24;\n\n    pub fn new_noop() -> Self {\n        Self(None)\n    }\n\n    pub fn new(key: [u8; 32]) -> Self {\n        Self(Some(Key::from_slice(&key).to_owned()))\n    }\n\n    pub fn encrypt(&self, data: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce: [u8; Self::NONCE_SIZE] = rand::thread_rng().gen();\n            l<|fim_suffix|>            let mut ciphertext = cipher\n                .encrypt(nonce, data)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))?;\n            let mut ret = nonce.to_vec();\n            ret.append(&mut ciphertext);\n            Ok(ret)\n        } else {\n            Ok(data.to_vec())\n        }\n    }\n\n    pub fn decrypt(&self, ciphertext: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce = &ciphertext[..Self::NONCE_SIZE];\n            let ciphertext = &ciphertext[Self::NONCE_SIZE..];\n            cipher\n                .decrypt(XNonce::from_slice(nonce), ciphertext)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))\n        } else {\n            Ok(ciphertext.to_vec())\n        }\n    }\n\n    pub fn enabled(&self) -> bool {\n        self.0.is_some()\n    }\n}\n\nimpl Default for Encryption {\n    fn default() -> Self {\n        Self::new_noop()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::Encryption;\n\n    #[test]\n    fn test_encryption() {\n        let clear = b\"hello world\";\n        let encryption = Encryption::new([1; 32]);\n        let ciphertext = encryption.encrypt(clear).unwrap();\n        let clear2 = encryption.decrypt(&ciphertext).unwrap();\n        assert_eq!(&clear[..], &clear2[..]);\n    }\n}\n<|fim_middle|>", "completion": "let nonce = XNonce::from_slice(&nonce);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cryptography.rs", "node_type": "let_declaration", "line_range": [78, 78]}
{"prompt": "<|fim_prefix|>use std::{\n    io::Write,\n    path::{Path, PathBuf},\n};\n\nuse anyhow::{Context as _, Result};\nuse figment::{\n    providers::{Env, Format, Toml},\n    Figment,\n};\nuse fs_err::{self as fs, File};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, Deserialize, Serialize)]\npub struct Config {\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub auth_token: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    server_url: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    debug_url: Option<String>,\n\n    // Relay stuff relates to the `listen` command.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub relay_token: Option<String>,\n    #[serde(alias = \"relay_debug_url\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub relay_debug_hostname: Option<String>,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub relay_disable_security: Option<bool>,\n}\n\nfn create_config_file(path: &Path) -> Result<File> {\n    let dir = path\n        .parent()\n        .context(\"config file path must not be empty\")?;\n    fs::create_dir_all(dir)?;\n\n    let mut opts = File::options();\n    opts.create(true).truncate(true).write(true);\n\n    #[cfg(unix)]\n    {\n        use std::os::unix::fs::OpenOptionsExt;\n\n        const FILE_MODE: u32 = 0o600;\n        opts.options_mut().mode(FILE_MODE);\n    }\n\n    Ok(opts.open(path)?)\n}\n\nimpl Config {\n    pub fn load() -> Result<Config> {\n        let cfg_file = get_config_file_path()?;\n        let config: Config = Figment::new()\n            .merge(Toml::file(cfg_file))\n            .merge(Env::prefixed(\"SVIX_\"))\n            .extract()?;\n        Ok(config)\n    }\n\n    pub fn save_to_disk(&self, path: &Path) -> Result<()> {\n        let mut fh = create_config_file(path)?;\n        <|fim_suffix|>\n        fh.write_all(source.as_bytes())?;\n        Ok(())\n    }\n\n    /// Gives the `server_url` for a Svix client with fallback to the legacy `SVIX_DEBUG_URL` variable/config.\n    pub fn server_url(&self) -> Option<&str> {\n        match self.server_url.as_deref() {\n            Some(s) if s.trim().is_empty() => self.debug_url.as_deref(),\n            server_url @ Some(_) => server_url,\n            None => self.debug_url.as_deref(),\n        }\n    }\n}\n\nconst FILE_NAME: &str = \"config.toml\";\n\nfn get_folder() -> Result<PathBuf> {\n    Ok(dirs::config_dir()\n        .context(\"unable to find config path\")?\n        .join(\"svix\"))\n}\n\npub fn get_config_file_path() -> Result<PathBuf> {\n    Ok(get_folder()?.join(FILE_NAME))\n}\n<|fim_middle|>", "completion": "let source = &toml::to_string_pretty(self)?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/config.rs", "node_type": "let_declaration", "line_range": [65, 65]}
{"prompt": "<|fim_prefix|>:<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    // Assert that no other changes were made\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the rate limit may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"rateLimit\": 1 }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.rate_limit, Some(1));\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the rate limit may be unset\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"rateLimit\": null,\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.rate_limit, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the UID may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"uid\": \"some\" }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.uid, Some(EndpointUid(\"some\".to_owned())));\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test the UID may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"uid\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.uid, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.url, \"http://bad.url/\".to_owned());\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the URL may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"url\": \"http://bad.url2\" }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.version, 1);\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that the version may be set\n    l<|fim_suffix|>\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.version, 2);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert!(!out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that disabled may be set\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"disabled\": true }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert!(out.ep.disabled);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert_eq!(out.ep.event_types_ids, None);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that event type IDs may be set\n\n    // But first make an event type to set it to\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            json!({\n                \"description\": \"a test event type\",\n                \"name\": \"test\",\n            }),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"filterTypes\": [\"test\"],\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(\n        out.ep.event_types_ids,\n        Some(EventTypeNameSet(HashSet::from([EventTypeName(\n            \"test\".to_owned()\n        )])))\n    );\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that event type IDs may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"filterTypes\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.event_types_ids, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that channels may be set\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"channels\": [\"test\"],\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(\n        out.ep.channels,\n        Some(EventChannelSet(HashSet::from([EventChannel(\n            \"test\".to_owned()\n        )])))\n    );\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n\n    // Test that channels may be unset\n    let _: Endpo<|fim_middle|>", "completion": "let _: EndpointOut = client\n        .patch(&url, json!({ \"version\": 2 }), StatusCode::OK)\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [282, 285]}
{"prompt": "<|fim_prefix|>ocketAddr,\n    #[serde(default = \"default_transformation_worker_count\")]\n    pub transformation_worker_count: NonZeroUsize,\n}\n\nimpl Config {\n    /// Build a Config from yaml source.\n    /// Optionally accepts a map to perform variable substitution with.\n    pub fn from_src(\n        raw_src: &str,\n        vars: Option<&HashMap<String, String>>,\n    ) -> std::io::Result<Self> {\n        let src = if let Some(vars) = vars {\n            let context = |key: &str| -> Result<Option<Cow<'_, str>>, LookupError<Infallible>> {\n                Ok(vars.get(key).map(Cow::from))\n            };\n            shellexpand::env_with_context(raw_src, context).map_err(|e: LookupError<_>| {\n                Error::other(format!(\"Variable substitution failed: {e}\"))\n            })?\n        } else {\n            Cow::Borrowed(raw_src)\n        };\n        let cfg: Self = serde_yaml::from_str(&src)\n            .map_err(|e| Error::other(format!(\"Failed to parse config: {e}\")))?;\n\n        for sc in &cfg.senders {\n            if let Some(tc) = sc.transformation() {\n                crate::runtime::validate_script(tc.source().as_str()).map_err(|e| {\n                    Error::other(format!(\n                        \"failed to parse transformation for sender `{}`: {e:?}\",\n                        &sc.name(),\n                    ))\n                })?;\n            }\n        }\n\n        for (name, tc) in cfg.receivers.iter().filter_map(|either| match either {\n            EitherReceiver::Webhook(receiver) => receiver\n                .transformation\n                .as_ref()\n                .map(|tc| (&receiver.name, tc)),\n            EitherReceiver::Poller(receiver) => receiver\n                .transformation\n                .as_ref()\n                .map(|tc| (&receiver.name, tc)),\n        }) {\n            crate::runtime::validate_script(tc.source().as_str()).map_err(|e| {\n                Error::other(format!(\n                    \"failed to parse transformation for receiver `{name}`: {e:?}\"\n                ))\n            })?;\n        }\n\n        Ok(cfg)\n    }\n}\n\nfn default_http_listen_address() -> SocketAddr {\n    \"0.0.0.0:5000\".parse().expect(\"default http listen address\")\n}\n\nfn default_transformation_worker_count() -> NonZeroUsize {\n    NonZeroUsize::new(4).expect(\"4 is greater than 0\")\n}\n\n#[derive(Deserialize)]\npub struct OtelExporterConfig {\n    /// The OpenTelemetry service name to use\n    pub service_name: Option<String>,\n    /// The OpenTelemetry address to send events to if given.\n    pub address: String,\n    /// The ratio at which to sample spans when sending to OpenTelemetry. When not given it defaults\n    /// to always sending. If the OpenTelemetry address is not set, this will do nothing.\n    pub sample_ratio: Option<f64>,\n}\n\n#[derive(Clone, Debug, Default, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogLevel {\n    #[default]\n    Info,\n    Debug,\n    Trace,\n}\n\nimpl fmt::Display for LogLevel {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Info => Level::INFO,\n            Self::Debug => Level::DEBUG,\n            Self::Trace => Level::TRACE,\n        }\n        .fmt(f)\n    }\n}\n\n#[derive(Clone, Debug, Default, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogFormat {\n    #[default]\n    Default,\n    Json,\n}\n\n/// Config for reading messages from plugins and forwarding to Svix.\n#[derive(Deserialize)]\npub struct WebhookSenderConfig {\n    pub name: String,\n    pub input: SenderInputOpts,\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: SenderOutputOpts,\n}\n\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum SenderInputOpts {\n    #[cfg(feature = \"kafka\")]\n    Kafka(KafkaInputOpts),\n    Queue(QueueInputOpts),\n}\n\nimpl WebhookSenderConfig {\n    pub fn into_sender_input(self) -> anyhow::Result<Box<dyn SenderInput>> {\n        Ok(match self.input {\n            #[cfg(feature = \"kafka\")]\n            SenderInputOpts::Kafka(input_opts) => svix_bridge_plugin_kafka::into_sender_input(\n                self.name,\n                input_opts,\n                self.transformation,\n                self.output,\n            )?,\n            SenderInputOpts::Queue(input_opts) => svix_bridge_plugin_queue::into_sender_input(\n                self.name,\n                input_opts,\n                self.transformation,\n                self.output,\n            )\n            .map_err(|e| anyhow!(\"{e}\"))?,\n        })\n    }\n}\n\n<|fim_suffix|>\n\nimpl TryFrom<WebhookSenderConfig> for Box<dyn SenderInput> {\n    type Error = anyhow::Error;\n\n    fn try_from(value: WebhookSenderConfig) -> Result<Self, Self::Error> {\n        value.into_sender_input()\n    }\n}\n\n/// Config for receiving webhooks and forwarding them to plugins.\n#[derive(Deserialize)]\npub struct WebhookReceiverConfig {\n    pub name: String,\n    pub input: ReceiverInputOpts,\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: ReceiverOutputOpts,\n}\n\n#[derive(Deserialize)]\n#[allow(clippy::large_enum_variant)] // we're talking a couple hundred bytes only\n#[serde(untagged)]\npub enum ReceiverOutputOpts {\n    Http(HttpOutputOpts),\n    #[cfg(feature = \"kafka\")]\n    Kafka(KafkaOutputOpts),\n    Queue(QueueOutputOpts),\n}\n\nimpl WebhookReceiverConfig {\n    pub async fn into_receiver_output(self) -> anyhow::Result<Box<dyn ReceiverOutput>> {\n        match self.output {\n            ReceiverOutputOpts::Http(opts) => opts.into_receiver_output(self.name),\n            #[cfg(feature = \"kafka\")]\n            ReceiverOutputOpts::Kafka(opts) => {\n                svix_bridge_plugin_kafka::into_receiver_output(self.name, opts).map_err(Into::into)\n            }\n            ReceiverOutputOpts::Queue(x) => svix_bridge_plugin_queue::into_receiver_output(\n                self.name.clone(),\n                x,\n                self.transformation.as_ref(),\n            )\n            .await\n            .map_err(Into::into),\n        }\n    }\n}\n\n#[derive(Clone, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"kebab-case\")]\npub enum PollerInputOpts {\n    SvixMessagePoller {\n        /// Identifies this client, allowing the server to track progress during iteration.\n        /// Processes should not share a consumer id. Only exclusive access is permitted.\n        consumer_id: String,\n        /// The app portion of the polling endpoint, e.g. `app_XXXX`\n        app_id: String,\n        /// The poller portion of the polling endpoint, e.g. `poll_XXXX`\n        sink_id: String,\n        /// The auth token for the polling endpoint\n        token: String,\n        #[serde(default)]\n        svix_options: Option<SvixOptions>,\n    },\n}\n\nimpl PollerInputOpts {\n    pub fn svix_client(&self) -> Option<Svix> {\n        match self {\n            PollerInputOpts::SvixMessagePoller {\n                token,\n                svix_options,\n                ..\n            } => Some(Svix::new(\n                token.clone(),\n                svix_options.clone().map(Into::into),\n            )),\n        }\n    }\n}\n\n/// Config for fetching from HTTP endpoints and forwarding them to plugins.\n#[derive(Deserialize)]\npub struct PollerReceiverConfig {\n    pub name: String,\n    pub input: PollerInputOpts,\n    // FIXME: add a configurable polling schedule or interval\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: ReceiverOutputOpts,\n}\n\nimpl PollerReceiverConfig {\n    // FIXME: duplicate from WebhookReceiverConfig. Extract/refactor as TryFrom ReceiverOutputOpts?\n    pub async fn into_receiver_output(self) -> anyhow::Result<Box<dyn ReceiverOutput>> {\n        match self.output {\n            ReceiverOutputOpts::Http(opts) => opts.into_receiver_output(self.name),\n            #[cfg(feature = \"kafka\")]\n            ReceiverOutputOpts::Kafka(opts) => {\n                svix_bridge_plugin_kafka::into_receiver_output(self.name, opts).map_err(Into::into)\n            }\n            ReceiverOutputOpts::Queue(x) => svix_bridge_plugin_queue::into_receiver_output(\n                self.name.clone(),\n                x,\n                self.transformation.as_ref(),\n            )\n            .await\n            .map_err(Into::into),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests;\n<|fim_middle|>", "completion": "impl WebhookSenderConfig {\n    pub fn name(&self) -> &str {\n        &self.name\n    }\n\n    pub fn transformation(&self) -> Option<&TransformationConfig> {\n        self.transformation.as_ref()\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/config/mod.rs", "node_type": "impl_item", "line_range": [189, 197]}
{"prompt": "<|fim_prefix|>use std::sync::Arc;\n\nuse axum::{\n    body::Body,\n    http::{Request, StatusCode},\n};\nuse serde_json::json;\nuse svix_bridge_types::{\n    async_trait, svix::webhooks::Webhook, BoxError, ForwardRequest, ReceiverOutput,\n    TransformationConfig, TransformerInput, TransformerInputFormat, TransformerJob,\n    TransformerOutput,\n};\nuse tower::{Service, ServiceExt};\n\nuse super::router;\nuse crate::webhook_receiver::{\n    types::{IntegrationState, InternalState},\n    verification::{NoVerifier, SvixVerifier},\n};\n\nstruct FakeReceiverOutput {\n    tx: tokio::sync::mpsc::UnboundedSender<serde_json::Value>,\n}\n\n<|fim_suffix|>\n\n#[async_trait]\nimpl ReceiverOutput for FakeReceiverOutput {\n    fn name(&self) -> &str {\n        \"fake output\"\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.tx.send(request.payload)?;\n        Ok(())\n    }\n}\n\n#[tokio::test]\nasync fn test_forwarding_no_verification() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n}\n\n/// Registers 2 receivers and sends 1 request to each.\n#[tokio::test]\nasync fn test_forwarding_multiple_receivers() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"a\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: None,\n            },\n        ),\n        (\n            \"b\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/a\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n\n    let request = Request::builder()\n        .uri(\"/webhook/b\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n/// Registers 2 receivers, one with a transformation and one without. Sends 1 request to each.\n#[tokio::test]\nasync fn test_transformation_json() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let mut input = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            input.insert(\"__TRANSFORMED__\".into(), json!(true));\n            let out = json!({ \"payload\": input });\n\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"transformed\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: Some(\n                    \"handler = (x) => ({ payload: {__TRANSFORMED__: true, ...x }})\".into(),\n                ),\n            },\n        ),\n        (\n            \"as-is\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The `__TRANSFORMED__` key should have been added\n    assert_eq!(\n        json!(forwarded),\n        json!({\"a\": true, \"__TRANSFORMED__\": true})\n    );\n\n    let request = Request::builder()\n        .uri(\"/webhook/as-is\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    // The same payload should come through, without any transformation.\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_transformation_string() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let out = match x.input {\n                TransformerInput::String(input) => json!({\"payload\": { \"got\": input }})\n                    .as_object()\n                    .cloned()\n                    .unwrap(),\n                _ => unreachable!(),\n            };\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"transformed\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: Some(TransformationConfig::Explicit {\n                format: TransformerInputFormat::String,\n                src: String::from(\"handler = (x) => ({ payload: { got: x }})\"),\n            }),\n        },\n    )]\n    .into_iter()\n    .c<|fim_middle|>", "completion": "impl FakeReceiverOutput {\n    pub fn new() -> (\n        Self,\n        tokio::sync::mpsc::UnboundedReceiver<serde_json::Value>,\n    ) {\n        let (tx, rx) = tokio::sync::mpsc::unbounded_channel();\n        (Self { tx }, rx)\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/tests.rs", "node_type": "impl_item", "line_range": [25, 33]}
{"prompt": "<|fim_prefix|>\n        false,\n        true,\n    )\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_endpoint_list_ordering() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"App1\").await.unwrap().id;\n\n    for i in 0..5 {\n        create_test_endpoint(&client, &app_id, &format!(\"https://test.url/{i}\"))\n            .await\n            .unwrap();\n        // Sleep to account for ksuid 4ms resolution\n        tokio::time::sleep(Duration::from_millis(5)).await;\n    }\n\n    let first_list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/?limit=2\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // First iterate through in order\n    assert_eq!(\n        first_list.data.first().unwrap().ep.url,\n        \"https://test.url/4\"\n    );\n    assert_eq!(first_list.data.last().unwrap().ep.url, \"https://test.url/3\");\n    assert!(!first_list.done);\n\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?limit=2&iterator={}\",\n                first_list.iterator.unwrap()\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/2\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/1\");\n    assert!(!list.done);\n\n    // Iterate with previous iterator\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?iterator={}\",\n                list.prev_iterator.unwrap()\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/4\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/3\");\n    assert!(list.done);\n\n    // Iterate in ascending order\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{}/endpoint/?limit=3&order=ascending\", &app_id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/0\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/2\");\n    assert!(!list.done);\n\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?limit=3&order=ascending&iterator={}\",\n                list.iterator.unwrap(),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/3\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/4\");\n    assert!(list.done);\n\n    // Previous iterator on descending order\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?limit=2&order=ascending&iterator={}\",\n                list.prev_iterator.unwrap(),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/1\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/2\");\n}\n\n/// Tests that there is at most one endpoint with a single UID for all endpoints associated with\n/// any application\n#[tokio::test]\nasync fn test_uid() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1: &str = \"v1EndpointUidTestApp1\";\n    const APP_NAME_2: &str = \"v1EndpointUidTestApp2\";\n\n    const EP_URI_APP_1_EP_1: &str = \"http://v1EndpointUidTestApp1Ep1.test\";\n    const EP_URI_APP_1_EP_2: &str = \"http://v1EndpointUidTestApp1Ep2.test\";\n    const EP_URI_APP_2: &str = \"http://v1EndpointUidTestApp2Ep1.test\";\n\n    const DUPLICATE_UID: &str = \"test_uid\";\n\n    // Same App\n\n    // Double Create -- on creation, it should return an error if identical UIDs are used for\n    // endpoints in the same app\n    let app_id = create_test_app(&client, APP_NAME_1).await.unwrap().id;\n    l<|fim_suffix|>\n    let mut ep_1 = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1.uid = Some(uid.clone());\n\n    let mut ep_2 = endpoint_in(EP_URI_APP_1_EP_2);\n    ep_2.uid = Some(uid.clone());\n\n    let ep_1 = post_endpoint(&client, &app_id, ep_1).await.unwrap();\n\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_2,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Update One to Existing -- on update it should return an error if attempting to change\n    // the UID to that of an existing endpoint associated with the same app\n    let ep_2 = create_test_endpoint(&client, &app_id, EP_URI_APP_1_EP_2)\n        .await\n        .unwrap();\n\n    let mut ep_2_with_duplicate_uid = endpoint_in(EP_URI_APP_1_EP_2);\n    ep_2_with_duplicate_uid.uid = Some(uid.clone());\n\n    client\n        .put::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_2.id),\n            ep_2_with_duplicate_uid,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Update One to Identical -- however it should not return an error if updating the\n    // existing endpoint to one with the same UID\n    let mut ep_1_with_duplicate_id = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1_with_duplicate_id.uid = Some(uid.clone());\n\n    let ep_1_updated = client\n        .put::<_, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_1.id),\n            ep_1_with_duplicate_id,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(ep_1.id, ep_1_updated.id);\n    assert_eq!(ep_1.ep.uid, ep_1_updated.ep.uid);\n\n    // Delete One then Create One -- UIDs may be reused after deletion\n    delete_endpoint(&client, &app_id, &ep_1.id).await.unwrap();\n    delete_endpoint(&client, &app_id, &ep_2.id).await.unwrap();\n\n    let mut ep_1 = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1.uid = Some(uid.clone());\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{}/endpoint/\", &app_id),\n            ep_1,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    delete_test_app(&client, app_id).await.unwrap();\n\n    // Different App -- however if they are associated with different applications, identical\n    // UIDs are valid\n    let app_1 = create_test_app(&client, APP_NAME_1).await.unwrap().id;\n    let app_2 = create_test_app(&client, APP_NAME_2).await.unwrap().id;\n\n    let mut ep_1 = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1.uid = Some(uid.clone());\n\n    let mut ep_2 = endpoint_in(EP_URI_APP_2);\n    ep_2.uid = Some(uid.clone());\n\n    let _ = post_endpoint(&client, &app_1, ep_1).await.unwrap();\n    let _ = post_endpoint(&client, &app_2, ep_2).await.unwrap();\n}\n\n// Simply tests that upon rotating an endpoint secret that it differs from the prior one\n#[tokio::test]\nasync fn test_endpoint_secret_get_and_rotation() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME: &str = \"v1EndpointSecretRotationTestApp\";\n    const EP_URI: &str = \"http://v1EndpointSecretRotationTestEp.test\";\n\n    let app_id = create_test_app(&client, APP_NAME).await.unwrap().id;\n\n    let ep = create_test_endpoint(&client, &app_id, EP_URI)\n        .await\n        .unwrap();\n\n    let former_secret: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    assert_ne!(\n        former_secret,\n        client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                StatusCode::OK\n            )\n            .await\n            .unwrap()\n    );\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            &former_secre<|fim_middle|>", "completion": "let uid = EndpointUid(DUPLICATE_UID.to_owned());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [761, 761]}
{"prompt": "<|fim_prefix|>use std::{\n    collections::HashMap,\n    fmt::{Debug, Display, Formatter},\n    time::Duration,\n};\n\nuse anyhow::{Context, Result};\nuse futures_util::{\n    stream::{SplitSink, SplitStream},\n    SinkExt, StreamExt,\n};\nuse http::{HeaderMap, HeaderName, HeaderValue};\n<|fim_suffix|>\nuse message::{MessageIn, MessageInEvent};\nuse tokio::{\n    net::TcpStream,\n    sync::mpsc::{UnboundedReceiver, UnboundedSender},\n    task::JoinSet,\n    time::Instant,\n};\nuse tokio_tungstenite::{\n    connect_async,\n    tungstenite::{\n        client::IntoClientRequest,\n        protocol::{frame::coding::CloseCode::Policy, CloseFrame, Message},\n        Bytes, Utf8Bytes,\n    },\n    MaybeTlsStream, WebSocketStream,\n};\n\nuse crate::relay::{\n    message::{MessageOut, MessageOutEvent, MessageOutStart},\n    token::generate_token,\n};\n\nmod message;\npub mod token;\n\n// Defaults\nconst DEFAULT_API_HOST: &str = \"api.relay.svix.com\";\nconst API_PREFIX: &str = \"api/v1\";\nconst DEFAULT_TIMEOUT: Duration = Duration::from_secs(30);\nconst WRITE_WAIT: Duration = Duration::from_secs(10);\n\n/// How often the server sends a ping, expecting a pong from us.\n///\n/// The client will send a pong automatically, but if we don't see a ping from the server at\n/// around this interval, we may need to reconnect.\nconst SERVER_PING_PERIOD: Duration = Duration::from_secs(\n    // The actual frequency seems to be 20s on the dot, but travel time means the message often\n    // arrives just a little later.\n    21,\n);\n\n/// When multiple clients try to connect to the Relay server using the same token, one will \"win\"\n/// and the others will get a Close frame with this message as the reason.\nconst SOCKET_IN_USE_REASON: Utf8Bytes = Utf8Bytes::from_static(\"This socket is already in use\");\n\ntype HttpClient = reqwest::Client;\ntype LocalServerResponse = reqwest::Response;\n\nstruct Client {\n    token: String,\n    websocket_url: url::Url,\n    local_url: url::Url,\n    http_client: HttpClient,\n}\n\n/// Special handling for the errors during establishing a websocket connection.\n///\n/// In a situation where a relay token is already in use, the server will send a `Close` frame.\n/// When this happens, the caller of `Client::connect` may want to try again with a different token.\n///\n/// For all other error cases, we report/propagate in the same way as we ever have.\nstruct TokenInUse;\n\nimpl Debug for TokenInUse {\n    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        f.write_str(\"TokenInUse\")\n    }\n}\n\nimpl Display for TokenInUse {\n    fn fmt(&self, f: &mut Formatter<'_>) -> std::fmt::Result {\n        f.write_str(\"TokenInUse\")\n    }\n}\n\nimpl std::error::Error for TokenInUse {}\n\nimpl Client {\n    async fn connect(&mut self, show_welcome_message: bool) -> Result<()> {\n        let mut set = JoinSet::new();\n        let conn = WsConnection::new(&self.websocket_url).await?;\n        let (mut ws_tx, mut ws_rx) = conn.stream.split();\n\n        let (remote_tx, remote_rx) = tokio::sync::mpsc::unbounded_channel::<MessageOut>();\n\n        match tokio::time::timeout(\n            WRITE_WAIT,\n            ws_tx.send(Message::Text(\n                serde_json::to_string(&MessageOut::Start {\n                    version: message::VERSION,\n                    data: MessageOutStart {\n                        token: self.token.clone(),\n                    },\n                })?\n                .into(),\n            )),\n        )\n        .await\n        {\n            Ok(Ok(_)) => { /* nothing to do  */ }\n            // The outer Result is for the timeout, the inner is for if there was some other failure\n            // during `send`.\n            Ok(Err(_)) | Err(_) => {\n                anyhow::bail!(\"failed to complete handshake with Webhook Relay server: remote didn't accept start message\");\n            }\n        }\n\n        // The assumption is the very first message we get from the websocket reader will be the\n        // response to our `MessageOut::Start` but it could also be any number of control messages.\n        // Keep reading until we see a `MessageIn::Start` or give up after some attempts.\n        const MAX_ATTEMPTS: u8 = 10;\n        let mut attempts = 0;\n        let start_response = loop {\n            if attempts > MAX_ATTEMPTS {\n                anyhow::bail!(\"failed to complete handshake with Webhook Relay server: no response from remote\");\n            }\n            attempts += 1;\n\n            match tokio::time::timeout(SERVER_PING_PERIOD, ws_rx.next()).await {\n                Err(_timeout) => continue,\n                Ok(None) => {\n                    anyhow::bail!(\"no response from server for start message\");\n                }\n                Ok(Some(msg)) => {\n                    let data = match msg? {\n                        // Control messages.\n                        Message::Close(Some(CloseFrame { code, reason }))\n                            if code == Policy && reason == SOCKET_IN_USE_REASON =>\n                        {\n                            return Err(TokenInUse.into())\n                        }\n                        Message::Close(_) => {\n                            anyhow::bail!(\"Relay server refused connection\");\n                        }\n                        Message::Ping(_) | Message::Pong(_) | Message::Frame(_) => continue,\n\n                        // Messages that carry data we care to process.\n                        Message::Text(s) => s.into(),\n                        Message::Binary(bytes) => bytes,\n                    };\n\n                    match serde_json::from_slice::<MessageIn>(&data)? {\n                        // This is what we're waiting to see. A `MessageOut::Start` sent to the writer\n                        // should result in a `MessageInStart` coming back on the reader.\n                        MessageIn::Start { data, .. } => break data,\n                        MessageIn::Event { .. } => continue,\n                    };\n                }\n            }\n        };\n\n        if show_welcome_message {\n            printdoc!(\n                r#\"\n\n                Webhook Relay is now listening at:\n                {}\n\n                All requests on this endpoint will be forwarded to your local URL:\n                {}\n\n                View logs and debug information at:\n                {}\n\n                \"#,\n                receive_url(&start_response.token),\n                self.local_url,\n                view_url(&self.token),\n            );\n        } else {\n            // Shows that a reconnection attempt succeeded after some failing initial attempts.\n            println!(\"Connected!\");\n        }\n\n        set.spawn({\n            let local_url = self.local_url.clone();\n            let http_client = self.http_client.clone();\n            async move {\n                read_from_ws_loop(ws_rx, remote_tx, local_url.clone(), http_client.clone())\n                    .await\n                    .inspect_err(|e| eprintln!(\"read loop terminated: {e:#}\"))\n            }\n        });\n\n        set.spawn(async move {\n            send_to_ws_loop(remote_rx, ws_tx)\n                .await\n                .inspect_err(|e| eprintln!(\"write loop terminated: {e:#}\"))\n        });\n\n        // If any task terminates, trash the rest so we can reconnect.\n        if set.join_next().await.is_some() {\n            set.shutdown().await;\n        }\n\n        Ok(())\n    }\n}\n\npub async fn listen(\n    local_url: url::Url,\n    relay_token: String,\n    relay_debug_url: Option<&str>,\n    relay_disable_security: bool,\n    disable_tls_verification: bool,\n) -> Result<()> {\n    let scheme = if relay_disable_security { \"ws\" } else { \"wss\" };\n    let api_host = relay_debug_url.unwrap_or(DEFAULT_API_HOST);\n    let token = format!(\"c_{relay_token}\");\n\n    let websocket_url = format!(\"{scheme}://{api_host}/{API_PREFIX}/listen/\").parse()?;\n\n    let http_client = HttpClient::builder()\n        .danger_accept_invalid_certs(disable_tls_verification)\n        .build()?;\n\n    let mut client = Client {\n        token,\n        websocket_url,\n        local_url,\n        http_client,\n    };\n\n    const MAX_BACKOFF: Duration = Duration::from_millis(5000);\n    let backoff_schedule = [\n        Duration::ZERO,\n        Duration::from_millis(100),\n        Duration::from_millis(1000),\n    <|fim_middle|>", "completion": "use indoc::printdoc;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/mod.rs", "node_type": "use_declaration", "line_range": [13, 13]}
{"prompt": "<|fim_prefix|>// Dead code is allowed because not everything is used in all of the tests\n#![allow(dead_code)]\n\nuse std::{\n    future::Future,\n    net::TcpListener,\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse anyhow::{Context, Result};\nuse axum::response::IntoResponse;\nuse http::HeaderMap;\nuse reqwest::{Client, RequestBuilder, StatusCode};\nuse serde::{de::DeserializeOwned, Serialize};\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::ConfigurationInner,\n    core::{\n        security::generate_org_token,\n        types::{BaseId, OrganizationId},\n    },\n    setup_tracing,\n};\nuse tokio::sync::mpsc;\nuse tracing::instrument::WithSubscriber;\n\npub mod common_calls;\n\n#[derive(Clone)]\npub struct TestClient {\n    base_uri: String,\n    auth_header: String,\n    client: Client,\n}\n\nimpl TestClient {\n    pub fn set_auth_header(&mut self, auth_header: String) {\n        self.auth_header = format!(\"Bearer {auth_header}\");\n    }\n}\n\nimpl TestClient {\n    pub fn new(base_uri: String, auth_token: &str) -> TestClient {\n        TestClient {\n            base_uri,\n            auth_header: format!(\"Bearer {auth_token}\"),\n            client: Client::new(),\n        }\n    }\n\n    fn build_uri(&self, endpoint: &str) -> String {\n        format!(\"{}/{endpoint}\", self.base_uri)\n    }\n\n    fn add_headers(&self, request: RequestBuilder) -> RequestBuilder {\n        request.header(\"Authorization\", &self.auth_header)\n    }\n\n    pub async fn get<O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.get(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        <|fim_suffix|>\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn get_without_response(\n        &self,\n        endpoint: &str,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.get(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn post<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                resp.json()\n                    .await\n                    .context(\"error receiving/parsing response\")\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await;\n        match resp {\n            Ok(resp) => {\n                if resp.status() != expected_code {\n                    anyhow::bail!(\n                        \"assertion failed: expected status {}, actual status {}\",\n                        expected_code,\n                        resp.status()\n                    );\n                }\n\n                Ok(())\n            }\n            Err(e) => {\n                println!(\"Unexpected request error: {e:?}\");\n                Err(e.into())\n            }\n        }\n    }\n\n    pub async fn post_with_idempotency<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        idempotency_key: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.post(self.build_uri(endpoint));\n        req = self\n            .add_headers(req)\n            .header(\"idempotency-key\", idempotency_key)\n            .json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json().await.context(\"error receiving/paring response\")\n    }\n\n    pub async fn put<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn put_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.put(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn delete(&self, endpoint: &str, expected_code: StatusCode) -> Result<()> {\n        let mut req = self.client.delete(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        if expected_code == StatusCode::NO_CONTENT {\n            let res_body = resp.text().await.context(\"error receiving response\")?;\n            anyhow::ensure!(res_body.is_empty());\n        }\n\n        Ok(())\n    }\n\n    pub async fn patch<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn patch_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = <|fim_middle|>", "completion": "if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/mod.rs", "node_type": "if_expression", "line_range": [70, 76]}
{"prompt": "<|fim_prefix|>.len() + 1,\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, MessageStatus::Fail);\n        assert_eq!(i.response_status_code, 0);\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n}\n\n#[tokio::test]\nasync fn test_message_attempts_empty_retry_schedule() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![];\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let (status_code, msg_status, attempt_count) =\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None);\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(status_code);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, msg_status);\n        println!(\"{} {status_code}\", i.response_status_code);\n        assert_eq!(\n            i.response_status_code,\n            TryInto::<i16>::try_into(status_code.as_u16()).unwrap()\n        );\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_combined_before_after_filtering() {\n    let (client, _) = start_svix_server().await;\n\n    let app = create_test_app(&client, \"test_app\").await.unwrap();\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ep = create_test_endpoint(&client, &app.id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    // Send a first message\n    create_test_message(\n        &client,\n        &app.id,\n        serde_json::json!({\n            \"test\": 1,\n        }),\n    )\n    .await\n    .unwrap();\n\n    // Wait until attempt was made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let ts1 = chrono::Utc::now();\n\n    // Send another two messages\n    for i in 1..=2 {\n        create_test_message(\n            &client,\n            &app.id,\n            serde_json::json!({\n                \"test\": i + 1,\n            }),\n        )\n        .await\n        .unwrap();\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 3 {\n            anyhow::bail!(\"list len {}, not 3\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n    let ts2 = chrono::Utc::now();\n\n    // Send another three messages\n    for i in 1..=3 {\n        create_test_message(\n            &client,\n            &app.id,\n            serde_json::json!({\n                \"test\": i + 3,\n            }),\n        )\n        .await\n        .unwrap();\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 6 {\n            anyhow::bail!(\"list len {}, not 6\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // No timestamp-based filtering should yield all 6 messages\n    l<|fim_suffix|>\n    assert!(out.done);\n    assert_eq!(out.data.len(), 6);\n\n    // Limiting the time to the second batch should only yield those two messages\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/\\\n                 ?limit=10&before={ts2}&after={ts1}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // We got all the data there is for the filters we supplied..\n    assert!(out.done);\n    assert_eq!(out.data.len(), 2);\n\n    // .. but we can still iterate from here when loosening filters.\n    let prev_iter = out.prev_iterator.unwrap();\n    let iter = out.iterator.unwrap();\n\n    // Can get the older three messages via pagination\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/?iterator={prev_iter}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 3);\n\n    // Can get the earlier message via pagination\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/?iterator={iter}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 1);\n}\n\n#[tokio::test]\nasync fn test_pagination_by_endpoint() {\n    let (client, _jh) = start_svix_server().await;\n\n    // Setup six endpoints and six messages so there's a sufficient number to test pagination\n    let app = create_test_app(&client, \"app1\").await.unwrap();\n\n    let mut receivers = Vec::new();\n    for _ in 0..6 {\n        receivers.push(TestReceiver::start(StatusCode::OK));\n    }\n\n    let mut eps = Vec::new();\n    for receiver in &receivers {\n        eps.push(\n            create_test_endpoint(&client, &app.id, &receiver.endpoint)\n                .await\n                .unwrap(),\n        );\n    }\n\n    let mut messages = Vec::new();\n    for i in 1..=6usize {\n        messages.push(\n            async {\n                // the requests that depend on time (ie, `before` and `after`) can flake if too many\n                // messages are created too close together.\n                // This short sleep aims to separate them a little so we can get clean counts.\n                tokio::time::sleep(Duration::from_millis(10)).await;\n                create_test_message(\n                    &client,\n                    &app.id,\n                    serde_json::json!({\n                        \"test\": i,\n                    }),\n                )\n                .await\n                .unwrap()\n            }\n            .await,\n        );\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        for endp_id in eps.iter().map(|ep| &ep.id) {\n            let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{}/attempt/endpoint/{endp_id}/\", app.id),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if list.data.len() != 6 {\n                anyhow::bail!(\"list len {}, not 6\", list.data.len());\n            }\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // By endpoint\n    for ep in &eps {\n        let all_attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        // Test Limit\n        let first_three: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/?limit=3\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(all_attempts.data.len(), 6);\n        assert_e<|fim_middle|>", "completion": "let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{}/attempt/endpoint/{}/?limit=10\", app.id, ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [958, 964]}
{"prompt": "<|fim_prefix|>KEY: &str = \"webhook-timestamp\";\nconst TOLERANCE_IN_SECONDS: i64 = 5 * 60;\nconst SIGNATURE_VERSION: &str = \"v1\";\n\nimpl Webhook {\n    pub fn new(secret: &str) -> Result<Self, WebhookError> {\n        let secret = secret.strip_prefix(PREFIX).unwrap_or(secret);\n        let key = base64::decode(secret)?;\n\n        Ok(Webhook { key })\n    }\n\n    pub fn from_bytes(secret: Vec<u8>) -> Result<Self, WebhookError> {\n        Ok(Webhook { key: secret })\n    }\n\n    pub fn verify<HM: HeaderMap>(&self, payload: &[u8], headers: &HM) -> Result<(), WebhookError> {\n        self.verify_inner(payload, headers, /* enforce_tolerance */ true)\n    }\n\n    pub fn verify_ignoring_timestamp<HM: HeaderMap>(\n        &self,\n        payload: &[u8],\n        headers: &HM,\n    ) -> Result<(), WebhookError> {\n        self.verify_inner(payload, headers, /* enforce_tolerance */ false)\n    }\n\n    fn verify_inner<HM: HeaderMap>(\n        &self,\n        payload: &[u8],\n        headers: &HM,\n        enforce_tolerance: bool,\n    ) -> Result<(), WebhookError> {\n        let msg_id = Self::get_header(headers, SVIX_MSG_ID_KEY, UNBRANDED_MSG_ID_KEY, \"id\")?;\n        let msg_signature = Self::get_header(\n            headers,\n            SVIX_MSG_SIGNATURE_KEY,\n            UNBRANDED_MSG_SIGNATURE_KEY,\n            \"signature\",\n        )?;\n        let msg_ts = Self::get_header(\n            headers,\n            SVIX_MSG_TIMESTAMP_KEY,\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            \"timestamp\",\n        )\n        .and_then(Self::parse_timestamp)?;\n\n        if enforce_tolerance {\n            Self::verify_timestamp(msg_ts)?;\n        }\n\n        let versioned_signature = self.sign(msg_id, msg_ts, payload)?;\n        let expected_signature = versioned_signature\n            .split_once(',')\n            .map(|x| x.1)\n            .ok_or(WebhookError::InvalidSignature)?;\n\n        msg_signature\n            .split(' ')\n            .filter_map(|x| x.split_once(','))\n            .filter(|x| x.0 == SIGNATURE_VERSION)\n            .any(|x| {\n                (x.1.len() == expected_signature.len())\n                    && (x\n                        .1\n                        .bytes()\n                        .zip(expected_signature.bytes())\n                        .fold(0, |acc, (a, b)| acc | (a ^ b))\n                        == 0)\n            })\n            .then_some(())\n            .ok_or(WebhookError::InvalidSignature)\n    }\n\n    pub fn sign(\n        &self,\n        msg_id: &str,\n        timestamp: i64,\n        payload: &[u8],\n    ) -> Result<String, WebhookError> {\n        let payload = std::str::from_utf8(payload).map_err(|_| WebhookError::InvalidPayload)?;\n        let to_sign = format!(\"{msg_id}.{timestamp}.{payload}\",);\n        let signed = hmac_sha256::HMAC::mac(to_sign.as_bytes(), &self.key);\n        let encoded = base64::encode(signed);\n\n        Ok(format!(\"{SIGNATURE_VERSION},{encoded}\"))\n    }\n\n    fn get_header<'a, HM: HeaderMap>(\n        headers: &'a HM,\n        svix_hdr: &'static str,\n        unbranded_hdr: &'static str,\n        err_name: &'static str,\n    ) -> Result<&'a str, WebhookError> {\n        use private::HeaderValueSealed as _;\n\n        headers\n            ._get(svix_hdr)\n            .or_else(|| headers._get(unbranded_hdr))\n            .ok_or(WebhookError::MissingHeader(err_name))?\n            ._to_str()\n            .ok_or(WebhookError::InvalidHeader(err_name))\n    }\n\n    fn parse_timestamp(hdr: &str) -> Result<i64, WebhookError> {\n        str::parse::<i64>(hdr).map_err(|_| WebhookError::InvalidTimestamp)\n    }\n\n    fn verify_timestamp(ts: i64) -> Result<(), WebhookError> {\n        let now = OffsetDateTime::now_utc().unix_timestamp();\n        if now - ts > TOLERANCE_IN_SECONDS {\n            Err(WebhookError::TimestampTooOldError)\n        } else if ts > now + TOLERANCE_IN_SECONDS {\n            Err(WebhookError::FutureTimestampError)\n        } else {\n            Ok(())\n        }\n    }\n}\n\n/// Trait to abstract over the `HeaderMap` types from both v0.2 and v1.0 of the\n/// `http` crate.\npub trait HeaderMap: private::HeaderMapSealed {}\n\nimpl HeaderMap for http02::HeaderMap {}\ni<|fim_suffix|>\nmod private {\n    pub trait HeaderMapSealed {\n        type HeaderValue: HeaderValueSealed;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue>;\n    }\n\n    impl HeaderMapSealed for http02::HeaderMap {\n        type HeaderValue = http02::HeaderValue;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue> {\n            self.get(name)\n        }\n    }\n    impl HeaderMapSealed for http1::HeaderMap {\n        type HeaderValue = http1::HeaderValue;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue> {\n            self.get(name)\n        }\n    }\n\n    pub trait HeaderValueSealed {\n        fn _to_str(&self) -> Option<&str>;\n    }\n\n    impl HeaderValueSealed for http02::HeaderValue {\n        fn _to_str(&self) -> Option<&str> {\n            self.to_str().ok()\n        }\n    }\n    impl HeaderValueSealed for http1::HeaderValue {\n        fn _to_str(&self) -> Option<&str> {\n            self.to_str().ok()\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use http02::HeaderMap;\n    use time::OffsetDateTime;\n\n    use super::{\n        Webhook, SVIX_MSG_ID_KEY, SVIX_MSG_SIGNATURE_KEY, SVIX_MSG_TIMESTAMP_KEY,\n        UNBRANDED_MSG_ID_KEY, UNBRANDED_MSG_SIGNATURE_KEY, UNBRANDED_MSG_TIMESTAMP_KEY,\n    };\n\n    fn get_svix_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(SVIX_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(SVIX_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            SVIX_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    fn get_unbranded_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(UNBRANDED_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    #[test]\n    fn test_sign() {\n        let wh = Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap();\n        assert_eq!(\n            \"v1,tZ1I4/hDygAJgO5TYxiSd6Sd0kDW6hPenDe+bTa3Kkw=\".to_owned(),\n            wh.sign(\n                \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\",\n                1649367553,\n                br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#\n            )\n            .unwrap()\n        );\n    }\n\n    #[test]\n    fn test_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            wh.verify(payload, &headers).unwrap();\n        }\n    }\n\n    #[test]\n    fn test_no_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = \"v1,R3PTzyfHASBKHH98a7yexTwaJ4yNIcGhFQc1yuN+BPU=\".to_owned();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_partial_signature() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5<|fim_middle|>", "completion": "impl HeaderMap for http1::HeaderMap {}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/webhooks.rs", "node_type": "impl_item", "line_range": [170, 170]}
{"prompt": "<|fim_prefix|>rs,\n) -> Vec<ValidationErrorItem> {\n    err.into_errors()\n        .into_iter()\n        .flat_map(|(k, v)| {\n            // Add the next field to the location\n            let mut loc = acc_path.clone();\n            loc.push(k.to_owned());\n\n            match v {\n                // If it's a [`validator::ValidationErrorsKind::Field`], then it will be a vector of\n                // errors to map to [`ValidationErrorItem`]s and insert to [`out`] before the next\n                // iteration\n                validator::ValidationErrorsKind::Field(vec) => vec\n                    .into_iter()\n                    .map(|err| ValidationErrorItem {\n                        loc: loc.clone(),\n                        msg: err\n                            .message\n                            .unwrap_or(Cow::Borrowed(\"Validation error\"))\n                            .to_string(),\n                        ty: \"value_error\".to_owned(),\n                    })\n                    .collect(),\n                // If it is a [`validator::ValidationErrorsKind::Struct`], then it will be another\n                // [`validator::ValidationErrors`] to search\n                validator::ValidationErrorsKind::Struct(errors) => validation_errors(loc, *errors),\n\n                // If it is a [`validator::ValidationErrorsKind::List`], then it will be an\n                // [`std::collections::BTreeMap`] of [`validator::ValidationErrors`] to search\n                validator::ValidationErrorsKind::List(map) => map\n                    .into_iter()\n                    .flat_map(|(k, v)| {\n                        // Add the list index to the location\n                        let mut loc = loc.clone();\n                        loc.push(format!(\"[{k}]\"));\n\n                        validation_errors(loc, *v)\n                    })\n                    .collect(),\n            }\n        })\n        .collect()\n}\n\n#[derive(Debug, Clone, Copy, Default, OperationIo)]\n#[aide(input_with = \"axum::extract::Json<T>\", json_schema)]\npub struct ValidatedJson<T>(pub T);\n\n#[async_trait]\nimpl<T, S> FromRequest<S> for ValidatedJson<T>\nwhere\n    T: DeserializeOwned + Validate,\n    S: Send + Sync,\n{\n    type Rejection = Error;\n\n    async fn from_request(req: Request, state: &S) -> Result<Self> {\n        let b = bytes::Bytes::from_request(req, state).await.map_err(|e| {\n            tracing::error!(\"Error reading body as bytes: {}\", e);\n\n            match e {\n                BytesRejection::FailedToBufferBody(FailedToBufferBody::LengthLimitError(_)) => {\n                    HttpError::too_large(None, None)\n                }\n\n                _ => HttpError::internal_server_error(\n                    None,\n                    Some(\"Failed to read request body\".to_owned()),\n                ),\n            }\n        })?;\n        let mut de = serde_json::Deserializer::from_slice(&b);\n\n        let value: T = serde_path_to_error::deserialize(&mut de).map_err(|e| {\n            let mut path = e\n                .path()\n                .to_string()\n                .split('.')\n                .map(ToOwned::to_owned)\n                .collect::<Vec<String>>();\n            let inner = e.inner();\n\n            let mut loc = vec![\"body\".to_owned()];\n            loc.append(&mut path);\n            HttpError::unprocessable_entity(vec![ValidationErrorItem {\n                loc,\n                msg: inner\n                    .source()\n                    .map(ToString::to_string)\n                    .unwrap_or_else(|| e.to_string()),\n                ty: \"value_error.jsondecode\".to_owned(),\n            }])\n        })?;\n\n        value.validate().map_err(|e| {\n            HttpError::unprocessable_entity(validation_errors(vec![\"body\".to_owned()], e))\n        })?;\n        Ok(ValidatedJson(value))\n    }\n}\n\n#[derive(Debug, Clone, Copy, Default)]\npub struct ValidatedQuery<T>(pub T);\n\n#[async_trait]\nimpl<T, S> FromRequestParts<S> for ValidatedQuery<T>\nwhere\n    T: DeserializeOwned + Validate,\n    S: Send + Sync,\n{\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, state: &S) -> Result<Self> {\n        l<|fim_suffix|>        value.validate().map_err(|e| {\n            HttpError::unprocessable_entity(validation_errors(vec![\"query\".to_owned()], e))\n        })?;\n        Ok(ValidatedQuery(value))\n    }\n}\n\nimpl<T> Deref for ValidatedQuery<T> {\n    type Target = T;\n\n    fn deref(&self) -> &Self::Target {\n        &self.0\n    }\n}\n\nimpl<T: JsonSchema> OperationInput for ValidatedQuery<T> {\n    fn operation_input(ctx: &mut aide::gen::GenContext, operation: &mut aide::openapi::Operation) {\n        axum::extract::Query::<T>::operation_input(ctx, operation)\n    }\n}\n\n// A special wrapper to handle query parameter lists. serde_qs and serde_urlencode can't\n// handle url query param arrays as flexibly as we need to support in our API\npub struct EventTypesQueryParams(pub Option<EventTypeNameSet>);\n\n#[async_trait]\nimpl<S> FromRequestParts<S> for EventTypesQueryParams\nwhere\n    S: Send + Sync,\n{\n    type Rejection = Error;\n\n    async fn from_request_parts(parts: &mut Parts, _state: &S) -> Result<Self> {\n        let pairs = form_urlencoded::parse(parts.uri.query().unwrap_or_default().as_bytes());\n\n        let event_types: HashSet<EventTypeName> = pairs\n            .filter(|(key, _)|\n                // want to handle both `?event_types=`, `?event_types[]=`, and `?event_types[1]=`\n                key == \"event_types\" || (key.starts_with(\"event_types[\") && key.ends_with(']')))\n            .flat_map(|(_, value)| {\n                value\n                    .split(',')\n                    .map(|x| EventTypeName(x.to_owned()))\n                    .collect::<Vec<_>>()\n            })\n            .collect();\n\n        if event_types.is_empty() {\n            Ok(Self(None))\n        } else {\n            let event_types = EventTypeNameSet(event_types);\n            event_types.validate().map_err(|e| {\n                HttpError::unprocessable_entity(validation_errors(vec![\"query\".to_owned()], e))\n            })?;\n            Ok(Self(Some(event_types)))\n        }\n    }\n}\n\nimpl OperationInput for EventTypesQueryParams {\n    fn operation_input(ctx: &mut aide::gen::GenContext, operation: &mut aide::openapi::Operation) {\n        // This struct must match what `EventTypesQuery` would be if we used a\n        // simple `#[derive(Deserialize)]` on it.\n        #[derive(JsonSchema)]\n        struct EventTypesQueryParams {\n            /// Filter response based on the event type\n            #[allow(unused)]\n            event_types: Option<EventTypeNameSet>,\n        }\n\n        Query::<EventTypesQueryParams>::operation_input(ctx, operation);\n    }\n}\n\npub async fn api_not_implemented() -> Result<()> {\n    Err(HttpError::not_implemented(None, None).into())\n}\n\npub fn validate_no_control_characters(str: &str) -> Result<(), ValidationError> {\n    let re = Regex::new(r\"[\\x00-\\x08]\").unwrap();\n    if re.is_match(str) {\n        return Err(validation_error(\n            Some(\"illegal_character\"),\n            Some(\"Control characters 0x00-0x08 not allowed.\"),\n        ));\n    }\n    Ok(())\n}\n\npub fn validate_no_control_characters_unrequired(\n    str: &UnrequiredField<String>,\n) -> Result<(), ValidationError> {\n    match str {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(str) => validate_no_control_characters(str),\n    }\n}\n\npub fn openapi_tag<T: AsRef<str>>(\n    tag: T,\n) -> impl Fn(TransformPathItem<'_>) -> TransformPathItem<'_> {\n    move |op| op.tag(tag.as_ref())\n}\n\npub fn openapi_desc<T: AsRef<str>>(\n    desc: T,\n) -> impl Fn(TransformOperation<'_>) -> TransformOperation<'_> {\n    move |op| op.description(desc.as_ref())\n}\n\npub fn get_unix_timestamp() -> u64 {\n    SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs()\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationPath {\n    pub app_id: ApplicationIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationEndpointPath {\n    pub app_id: ApplicationIdOrUid,\n    pub endpoint_id: EndpointIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema<|fim_middle|>", "completion": "let Query(value) = Query::<T>::from_request_parts(parts, state)\n            .await\n            .map_err(|err| HttpError::bad_request(None, Some(err.to_string())))?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "let_declaration", "line_range": [619, 621]}
{"prompt": "<|fim_prefix|>cryption, &cfg.default_signature_type)?;\n\n    let mut endp = endpoint::ActiveModel::new(app.id, key);\n    let metadata =\n        endpointmetadata::ActiveModel::new(endp.id.clone().unwrap(), mem::take(&mut data.metadata));\n    data.update_model(&mut endp);\n\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.insert(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointCreated(EndpointEvent::new(app.uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Create a new endpoint for the application.\n///\n/// When `secret` is `null` the secret is automatically generated (recommended)\n#[aide_annotate(op_id = \"v1.endpoint.create\")]\npub(super) async fn create_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        op_webhooks,\n        ..\n    }): State<AppState>,\n    _: Path<ApplicationPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointIn>,\n) -> Result<JsonStatus<201, EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let (endp, metadata) = create_endp_from_data(db, cfg, &op_webhooks, app, data)\n        .await\n        .trace()?;\n\n    Ok(JsonStatus((endp, metadata.data).into()))\n}\n\n/// Get an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.get\")]\npub(super) async fn get_endpoint(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointOut>> {\n    let (endp, metadata) = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .find_also_related(endpointmetadata::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let metadata = metadata.map(|m| m.data).unwrap_or_default();\n\n    Ok(Json((endp, metadata).into()))\n}\n\nasync fn update_endp_from_data(\n    db: &DatabaseConnection,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    endp: endpoint::ActiveModel,\n    metadata: endpointmetadata::ActiveModel,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.update(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    let app_uid = app.uid;\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointUpdated(EndpointEvent::new(app_uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Update an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.update\")]\npub(super) async fn update_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(mut data): ValidatedJson<EndpointUpdate>,\n) -> Result<JsonStatusUpsert<EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let models = endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n        .await\n        .trace()?;\n\n    if let Some((mut endp, mut metadata)) = models {\n        metadata.data = Set(mem::take(&mut data.metadata));\n        data.update_model(&mut endp);\n        <|fim_suffix|>\n        Ok(JsonStatusUpsert::Updated((endp, metadata.data).into()))\n    } else {\n        let data = data.into_in_with_default_key();\n        let (endp, metadata) = create_endp_from_data(db, cfg, op_webhooks, app, data)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Created((endp, metadata.data).into()))\n    }\n}\n\n/// Partially update an endpoint.\n#[aide_annotate]\npub(super) async fn patch_endpoint(\n    State(AppState {\n        ref db,\n        cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointPatch>,\n) -> Result<Json<EndpointOut>> {\n    if let UnrequiredNullableField::Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    if let UnrequiredField::Some(url) = &data.url {\n        validate_endpoint_url(url, cfg.endpoint_https_only)?;\n    }\n\n    let (mut endp, mut metadata) =\n        endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut patch_data = data; // need to alias so we can use data for `patch_field_non_nullable!`\n\n    let data = mem::take(&mut patch_data.metadata);\n    patch_field_non_nullable!(metadata, data);\n    patch_data.update_model(&mut endp);\n    let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n        .await\n        .trace()?;\n\n    Ok(Json((endp, metadata.data).into()))\n}\n\n/// Delete an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.delete\")]\npub(super) async fn delete_endpoint(\n    State(AppState {\n        ref db,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<NoContent> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    // Cloning the ID/UID out of endp before it's consumed below\n    let endpoint_id = endp.id.clone();\n    let endpoint_uid = endp.uid.clone();\n\n    let mut endp: endpoint::ActiveModel = endp.into();\n    endp.deleted = Set(true);\n    endp.uid = Set(None); // We don't want deleted UIDs to clash\n    endp.update(db).await?;\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointDeleted(EndpointEvent {\n                app_id: app.id,\n                app_uid: app.uid,\n                endpoint_id,\n                endpoint_uid,\n            }),\n        )\n        .await?;\n\n    Ok(NoContent)\n}\n\n/// This module is here so that our Result override doesn't conflict\nmod hack {\n    use sea_orm::FromQueryResult;\n\n    use crate::core::types::EventTypeName;\n\n    #[derive(Debug, FromQueryResult)]\n    pub struct EventTypeNameResult {\n        pub name: EventTypeName,\n    }\n}\n\nasync fn validate_event_types(\n    db: &DatabaseConnection,\n    event_types_ids: &EventTypeNameSet,\n    org_id: &OrganizationId,\n) -> Result<()> {\n    let event_types: Vec<EventTypeNameResult> = eventtype::Entity::secure_find(org_id.clone())\n        .filter(eventtype::Column::Deleted.eq(false))\n        .select_only()\n        .column(eventtype::Column::Name)\n        .into_model::<EventTypeNameResult>()\n        .all(db)\n        .await?;\n    let event_types: HashSet<EventTypeName> =\n        HashSet::from_iter(event_types.into_iter().map(|x| x.name));\n    let missing: Vec<&EventTypeName> = event_types_ids\n        .0\n        .iter()\n        .filter(|x| !event_types.contains(x))\n        .collect();\n\n    if missing.is_empty() {\n        Ok(())\n    } else {\n        let missing = missing\n            .into_iter()\n            .map(|x| &(x.0[..]))\n            .collect::<Vec<&str>>()\n            .join(\", \");\n        Err(HttpError::unp<|fim_middle|>", "completion": "let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n            .await\n            .trace()?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/crud.rs", "node_type": "let_declaration", "line_range": [200, 202]}
{"prompt": "<|fim_prefix|>eiver.data_recv.recv())\n        .await\n        .unwrap()\n        .unwrap();\n\n    assert_eq!(received_payload, msg_payload);\n}\n\n// This doesn't actually handle requests successfully, but it does allow us\n// to see which hostnames are requested of it.\nstruct MockProxyServer {\n    matched_hosts: Arc<Mutex<HashSet<String>>>,\n    addr: String,\n    variant: MockProxyVariant,\n}\n\nenum MockProxyVariant {\n    Http,\n    Socks5,\n}\n\nimpl MockProxyServer {\n    pub fn new(variant: MockProxyVariant) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n        let addr = match variant {\n            MockProxyVariant::Http => {\n                format!(\"http://{}\", listener.local_addr().unwrap())\n            }\n            MockProxyVariant::Socks5 => {\n                format!(\"socks5://{}\", listener.local_addr().unwrap())\n            }\n        };\n        let matched_hosts = Arc::new(Mutex::new(HashSet::new()));\n\n        match variant {\n            MockProxyVariant::Http => {\n                tokio::spawn(Self::http_listener(listener, matched_hosts.clone()))\n            }\n            MockProxyVariant::Socks5 => {\n                tokio::spawn(Self::socks5_listener(listener, matched_hosts.clone()))\n            }\n        };\n\n        Self {\n            matched_hosts,\n            addr,\n            variant,\n        }\n    }\n\n    pub async fn http_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        loop {\n            let (mut stream, _addr) = listener.accept().await.unwrap();\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let mut buffer = [0; 512];\n\n                if let Ok(size) = stream.read(&mut buffer).await {\n                    if size == 0 {\n                        return;\n                    }\n                    let request = String::from_utf8_lossy(&buffer[..size]);\n                    if let Some(host) = request\n                        .strip_prefix(\"CONNECT \")\n                        .and_then(|s| s.split(' ').next())\n                        .and_then(|s| s.strip_suffix(\":443\"))\n                    {\n                        let mut guard = matched_hosts.lock().unwrap();\n                        guard.insert(host.to_string());\n                    }\n                }\n            });\n        }\n    }\n\n    pub async fn socks5_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        use socks5_proto::{\n            handshake::{\n                Method as HandshakeMethod, Request as HandshakeRequest,\n                Response as HandshakeResponse,\n            },\n            Address, Reply, Request as SocksRequest, Response as SocksResponse,\n        };\n        loop {\n            let (mut stream, _) = match listener.accept().await {\n                Ok(v) => v,\n                Err(_) => continue,\n            };\n\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let hs_req = match HandshakeRequest::read_from(&mut stream).await {\n                    Ok(req) => req,\n                    Err(_) => {\n                        return;\n                    }\n                };\n\n                if hs_req.methods.contains(&HandshakeMethod::NONE) {\n                    if HandshakeResponse::new(HandshakeMethod::NONE)\n                        .write_to(&mut stream)\n                        .await\n                        .is_err()\n                    {\n                        return;\n                    }\n                } else {\n                    let _ = HandshakeResponse::new(HandshakeMethod::UNACCEPTABLE)\n                        .write_to(&mut stream)\n                        .await;\n                    return;\n                }\n\n                let Ok(socks_req) = SocksRequest::read_from(&mut stream).await else {\n                    return;\n                };\n\n                let host = match &socks_req.address {\n                    Address::SocketAddress(socket_addr) => socket_addr.ip().to_string(),\n                    Address::DomainAddress(domain_bytes, _port) => {\n                        String::from_utf8_lossy(domain_bytes).to_string()\n                    }\n                };\n                if !host.is_empty() {\n                    let mut guard = matched_hosts.lock().unwrap();\n                    guard.insert(host);\n                }\n\n                <|fim_suffix|>\n                let _ = abort_resp.write_to(&mut stream).await;\n                let _ = stream.shutdown().await;\n            });\n        }\n    }\n\n    pub fn matches(&self) -> HashSet<String> {\n        let guard = self.matched_hosts.lock().unwrap();\n        println!(\"************ MATCHES {guard:?}\");\n        guard.clone()\n    }\n}\n\n#[tokio::test]\nasync fn test_http_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Http);\n    test_proxy_exceptions(listener).await\n}\n\n#[tokio::test]\nasync fn test_socks5_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Socks5);\n    test_proxy_exceptions(listener).await\n}\n\nasync fn test_proxy_exceptions(listener: MockProxyServer) {\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(ProxyConfig {\n        addr: ProxyAddr::new(listener.addr.clone()).unwrap(),\n        noproxy: Some(ProxyBypassCfg(\"10.0.0.0/8, 8.8.8.8, 0ec2:1652:6021:693b:f928:565d:5a0e:de9f, www.svix.com, .google.com\".to_owned())),\n    });\n    cfg.retry_schedule = vec![];\n\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"proxyTest\").await.unwrap().id;\n\n    // Note: Real hostnames here are not ideal, but difficult to test SOCKS without valid DNS entries:\n    create_test_endpoint(&client, &app_id, \"https://www.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://play.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://www.google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://8.8.8.8\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://8.8.4.4\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://10.0.0.1\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(\n        &client,\n        &app_id,\n        \"https://[0ec2:1652:6021:693b:f928:565d:5a0e:de9f]\",\n    )\n    .await\n    .unwrap();\n\n    client\n        .post::<MessageIn, serde_json::Value>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({ \"test\": \"value\" })).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    tokio::time::sleep(std::time::Duration::from_secs(10)).await;\n\n    assert!(listener.matches().contains(\"8.8.4.4\"));\n    assert!(!listener.matches().contains(\"8.8.8.8\"));\n    assert!(!listener.matches().contains(\"10.0.0.1\"));\n    assert!(!listener\n        .matches()\n        .contains(\"[0ec2:1652:6021:693b:f928:565d:5a0e:de9f]\"));\n    match listener.variant {\n        MockProxyVariant::Http => {\n            assert!(listener.matches().contains(\"play.svix.com\"));\n\n            assert!(!listener.matches().contains(\"www.svix.com\"));\n            assert!(!listener.matches().contains(\"www.google.com\"));\n            assert!(!listener.matches().contains(\"google.com\"));\n\n            assert_eq!(listener.matches().len(), 2);\n        }\n        MockProxyVariant::Socks5 => {\n            // We can't assert hostnames here b/c DNS is resolved before\n            // calling the proxy.\n\n            // This is a very weak assertion, but the \"insta-retries\" that our\n            // client does mean that occasionally the same site resolve to\n            // different IPs that populate the match list separately:\n            assert!(listener.matches().len() >= 2);\n        }\n    }\n}\n<|fim_middle|>", "completion": "let abort_resp =\n                    SocksResponse::new(Reply::ConnectionNotAllowed, Address::unspecified());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_proxy.rs", "node_type": "let_declaration", "line_range": [217, 218]}
{"prompt": "<|fim_prefix|>use deno_core::JsRuntime;\nuse serde_json::json;\nuse svix_bridge_types::{TransformerInput, TransformerOutput};\n\nuse super::{run_script_inner, validate_script};\n\nfn get_test_rt() -> JsRuntime {\n    JsRuntime::new(Default::default())\n}\n\n// Really just trying to figure out if the deno runtime is working the way I hope.\n#[test]\nfn test_happy_fn() {\n    <|fim_suffix|>\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({ \"y\": 456 }).into(), src).unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n            assert_eq!(v[\"y\"].as_i64(), Some(456));\n        }\n        TransformerOutput::Invalid => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\nfn test_invalid_output_bool() {\n    let src = r#\"\n    function handler(input) {\n        return false;\n    }\n    \"#\n    .to_string();\n\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\n// FIXME: serde decodes arrays with keys like \"0\", \"1\"... in this situation, failing the test.\n#[ignore]\nfn test_invalid_output_array() {\n    let src = r#\"\n    function handler(input) {\n        return [1, 2];\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => {\n            panic!(\"got unexpected return value\");\n        }\n    }\n}\n\n/// Receives a string input, parses as JSON in js, then returns the result back to rust.\n#[test]\nfn test_string_input() {\n    let src = r#\"\n    function handler(input) {\n        return JSON.parse(input);\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(r#\"{\"x\": 123}\"#)),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n/// Take the string input and just add it to a field in the returned object.\n/// The string should make it through, back to rust, as-is.\n#[test]\nfn test_string_input2() {\n    let src = r#\"\n    function handler(input) {\n        return { \"payload\": input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(\"Hello World\")),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"payload\"].as_str(), Some(\"Hello World\"));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n#[test]\nfn test_validate_script_bad_syntax_is_err() {\n    assert!(validate_script(\"let 123 = ';\").is_err());\n}\n\n#[test]\nfn test_validate_script_empty_handler_is_ok() {\n    assert!(validate_script(\"function handler() { }\").is_ok());\n}\n\n#[test]\nfn test_validate_script_arrow_fn_is_ok() {\n    assert!(validate_script(\"const handler = () => ({ a: 123 })\").is_ok());\n}\n\n/// Technically, this should be legal though the utility is questionable.\n#[test]\nfn test_validate_script_empty_is_ok() {\n    assert!(validate_script(\"\").is_ok());\n    assert!(validate_script(\"    \").is_ok());\n}\n<|fim_middle|>", "completion": "let src = r#\"\n    function handler(input) {\n        return { \"x\": 123, ...input };\n    }\n    \"#\n    .to_string();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/tests.rs", "node_type": "let_declaration", "line_range": [14, 19]}
{"prompt": "<|fim_prefix|>duration_ms: Set(0), // Default to 0, will be updated after the request\n        attempt_number: Set(db_attempt_number),\n        ..Default::default()\n    };\n\n    match client.execute(req).await {\n        Ok(res) => {\n            // Calculate the duration in milliseconds\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            let status_code = res.status().as_u16() as i16;\n            let status = if res.status().is_success() {\n                MessageStatus::Success\n            } else {\n                MessageStatus::Fail\n            };\n\n            let http_error = if !res.status().is_success() {\n                Some(WebhookClientError::FailureStatus(res.status()))\n            } else {\n                None\n            };\n\n            let body = match res.into_body().collect().await {\n                Ok(collected) => {\n                    let bytes = collected.to_bytes();\n                    if bytes.len() > RESPONSE_MAX_SIZE {\n                        bytes_to_string(bytes.slice(..RESPONSE_MAX_SIZE))\n                    } else {\n                        bytes_to_string(bytes)\n                    }\n                }\n                Err(err) => format!(\"Error reading response body: {err}\"),\n            };\n\n            let attempt = messageattempt::ActiveModel {\n                response_status_code: Set(status_code),\n                response: Set(body),\n                status: Set(status),\n                response_duration_ms: Set(duration_ms),\n                ..attempt\n            };\n\n            match http_error {\n                Some(err) => Ok(CompletedDispatch::Failed(FailedDispatch(\n                    attempt,\n                    Error::generic(err),\n                ))),\n                None => Ok(CompletedDispatch::Successful(SuccessfulDispatch(attempt))),\n            }\n        }\n        Err(err) => {\n            // For errors, we still calculate the duration\n            let duration_ms = (Utc::now() - created_at).num_milliseconds();\n\n            Ok(CompletedDispatch::Failed(FailedDispatch(\n                messageattempt::ActiveModel {\n                    response_status_code: Set(0),\n                    response: Set(err.to_string()),\n                    status: Set(MessageStatus::Fail),\n                    response_duration_ms: Set(duration_ms),\n                    ..attempt\n                },\n                err.into(),\n            )))\n        }\n    }\n}\n\n#[tracing::instrument(skip_all, fields(response_code))]\nasync fn handle_successful_dispatch(\n    WorkerContext {\n        cache,\n        db,\n        op_webhook_sender,\n        ..\n    }: &WorkerContext<'_>,\n    DispatchContext {\n        org_id,\n        endp,\n        app_id,\n        app_uid,\n        msg_task,\n        msg_uid,\n        ..\n    }: DispatchContext<'_>,\n    SuccessfulDispatch(mut attempt): SuccessfulDispatch,\n) -> Result<()> {\n    attempt.ended_at = Set(Some(Utc::now().into()));\n    attempt.next_attempt = Set(None);\n    let attempt = attempt.insert(*db).await?;\n\n    process_endpoint_success(cache, app_id, org_id, endp).await?;\n\n    tracing::Span::current().record(\"response_code\", attempt.response_status_code);\n    tracing::info!(\"Webhook success.\");\n\n    if msg_task.attempt_count as usize >= OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER {\n        if let Err(e) = op_webhook_sender\n            .send_operational_webhook(\n                org_id,\n                OperationalWebhook::MessageAttemptRecovered(MessageAttemptEvent {\n                    app_id: app_id.clone(),\n                    app_uid: app_uid.cloned(),\n                    endpoint_id: msg_task.endpoint_id.clone(),\n                    msg_id: msg_task.msg_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await\n        {\n            tracing::error!(\n                \"Failed sending MessageAttemptRecovered Operational Webhook: {}\",\n                e\n            );\n        }\n    }\n\n    Ok(())\n}\n\nfn calculate_retry_delay(duration: Duration, err: &Error) -> Duration {\n    l<|fim_suffix|>    // Apply jitter with a maximum variation of JITTER_DELTA\n    rand::thread_rng()\n        .gen_range(duration.mul_f32(1.0 - JITTER_DELTA)..=duration.mul_f32(1.0 + JITTER_DELTA))\n}\n\n#[tracing::instrument(skip_all, fields(response_code))]\nasync fn handle_failed_dispatch(\n    WorkerContext {\n        db,\n        cache,\n        op_webhook_sender,\n        cfg,\n        queue_tx,\n        ..\n    }: &WorkerContext<'_>,\n    DispatchContext {\n        org_id,\n        app_id,\n        app_uid,\n        msg_uid,\n        endp,\n        msg_task,\n        ..\n    }: DispatchContext<'_>,\n    FailedDispatch(mut attempt, err): FailedDispatch,\n) -> Result<()> {\n    attempt.ended_at = Set(Some(Utc::now().into()));\n\n    tracing::Span::current().record(\"response_code\", attempt.response_status_code.try_as_ref());\n    tracing::info!(\"Webhook failure.\");\n\n    let retry_schedule = &cfg.retry_schedule;\n\n    let attempt_count = msg_task.attempt_count as usize;\n    if msg_task.trigger_type == MessageAttemptTriggerType::Manual {\n        tracing::debug!(\"Manual retry failed\");\n        attempt.next_attempt = Set(None);\n        attempt.insert(*db).await?;\n        Ok(())\n    } else if attempt_count < retry_schedule.len() {\n        let retry_delay = calculate_retry_delay(retry_schedule[attempt_count], &err);\n        let next_attempt_time =\n            Utc::now() + chrono::Duration::from_std(retry_delay).expect(\"Error parsing duration\");\n\n        attempt.next_attempt = Set(Some(next_attempt_time.into()));\n        let attempt = attempt.insert(*db).await?;\n\n        tracing::debug!(\n            retry_delay = retry_delay.as_secs(),\n            \"Worker failure retrying for attempt {}: {} {} {}\",\n            attempt_count,\n            err,\n            &attempt.id,\n            &endp.id\n        );\n\n        if attempt_count == (OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER - 1) {\n            if let Err(e) = op_webhook_sender\n                .send_operational_webhook(\n                    org_id,\n                    OperationalWebhook::MessageAttemptFailing(MessageAttemptEvent {\n                        app_id: app_id.clone(),\n                        app_uid: app_uid.cloned(),\n                        endpoint_id: msg_task.endpoint_id.clone(),\n                        msg_id: msg_task.msg_id.clone(),\n                        msg_event_id: msg_uid.cloned(),\n                        last_attempt: attempt.into(),\n                    }),\n                )\n                .await\n            {\n                tracing::error!(\n                    \"Failed sending MessageAttemptFailing Operational Webhook: {}\",\n                    e\n                );\n            }\n        }\n        queue_tx\n            .send(\n                &QueueTask::MessageV1(MessageTask {\n                    attempt_count: msg_task.attempt_count + 1,\n                    ..msg_task.clone()\n                }),\n                Some(retry_delay),\n            )\n            .await?;\n\n        Ok(())\n    } else {\n        attempt.next_attempt = Set(None);\n        let attempt = attempt.insert(*db).await?;\n\n        tracing::debug!(\n            \"Worker failure attempts exhausted: {} {} {}\",\n            err,\n            &attempt.id,\n            &endp.id\n        );\n\n        // Send common operational webhook\n        op_webhook_sender\n            .send_operational_webhook(\n                org_id,\n                OperationalWebhook::MessageAttemptExhausted(MessageAttemptEvent {\n                    app_id: app_id.clone(),\n                    app_uid: app_uid.cloned(),\n                    endpoint_id: msg_task.endpoint_id.clone(),\n                    msg_id: msg_task.msg_id.clone(),\n                    msg_event_id: msg_uid.cloned(),\n                    last_attempt: attempt.into(),\n                }),\n            )\n            .await?;\n\n        match process_endpoint_failure(\n            cache,\n            app_id,\n            org_id,\n            endp,\n            cfg.endpoint_failure_disable_after,\n        )\n        .await?\n        {\n            None => Ok(()),\n\n            Some(EndpointDisableInfo { first_failure_at }) => {\n             <|fim_middle|>", "completion": "let duration = if matches!(err.typ, ErrorType::Timeout(_))\n        || matches!(err.typ, ErrorType::Http(HttpError { status, .. }) if status == StatusCode::TOO_MANY_REQUESTS)\n    {\n        std::cmp::max(duration, Duration::from_secs(OVERLOAD_PENALTY_SECS))\n    } else {\n        duration\n    };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "let_declaration", "line_range": [485, 491]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct StreamingSinkListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n}\n\nimpl From<StreamingSinkListOptions> for svix::api::StreamingSinkListOptions {\n    fn from(value: StreamingSinkListOptions) -> Self {\n        let StreamingSinkListOptions {\n            limit,\n            iterator,\n            order,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            order,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct StreamingSinkCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<StreamingSinkCreateOptions> for svix::api::StreamingSinkCreateOptions {\n    fn from(value: StreamingSinkCreateOptions) -> Self {\n        <|fim_suffix|>\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct StreamingSinkRotateSecretOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<StreamingSinkRotateSecretOptions> for svix::api::StreamingSinkRotateSecretOptions {\n    fn from(value: StreamingSinkRotateSecretOptions) -> Self {\n        let StreamingSinkRotateSecretOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct StreamingSinkArgs {\n    #[command(subcommand)]\n    pub command: StreamingSinkCommands,\n}\n\n#[derive(Subcommand)]\npub enum StreamingSinkCommands {\n    /// List of all the stream's sinks.\n    List {\n        stream_id: String,\n        #[clap(flatten)]\n        options: StreamingSinkListOptions,\n    },\n    /// Creates a new sink.\n    Create {\n        stream_id: String,\n        stream_sink_in: Option<crate::json::JsonOf<StreamSinkIn>>,\n        #[clap(flatten)]\n        options: StreamingSinkCreateOptions,\n    },\n    /// Get a sink by id or uid.\n    Get { stream_id: String, sink_id: String },\n    /// Update a sink.\n    Update {\n        stream_id: String,\n        sink_id: String,\n        stream_sink_in: Option<crate::json::JsonOf<StreamSinkIn>>,\n    },\n    /// Delete a sink.\n    Delete { stream_id: String, sink_id: String },\n    /// Partially update a sink.\n    Patch {\n        stream_id: String,\n        sink_id: String,\n        stream_sink_patch: Option<crate::json::JsonOf<StreamSinkPatch>>,\n    },\n    /// Get the sink's signing secret (only supported for http sinks)\n    ///\n    /// This is used to verify the authenticity of the delivery.\n    ///\n    /// For more information please refer to [the consuming webhooks docs](https://docs.svix.com/consuming-webhooks/).\n    GetSecret { stream_id: String, sink_id: String },\n    /// Rotates the signing secret (only supported for http sinks).\n    RotateSecret {\n        stream_id: String,\n        sink_id: String,\n        endpoint_secret_rotate_in: Option<crate::json::JsonOf<EndpointSecretRotateIn>>,\n        #[clap(flatten)]\n        options: StreamingSinkRotateSecretOptions,\n    },\n    /// Set or unset the transformation code associated with this sink.\n    TransformationPartialUpdate {\n        stream_id: String,\n        sink_id: String,\n        sink_transform_in: Option<crate::json::JsonOf<SinkTransformIn>>,\n    },\n}\n\nimpl StreamingSinkCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { stream_id, options } => {\n                let resp = client\n                    .streaming()\n                    .sink()\n                    .list(stream_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                stream_id,\n                stream_sink_in,\n                options,\n            } => {\n                let resp = client\n                    .streaming()\n                    .sink()\n                    .create(\n                        stream_id,\n                        stream_sink_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { stream_id, sink_id } => {\n                let resp = client.streaming().sink().get(stream_id, sink_id).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                stream_id,\n                sink_id,\n                stream_sink_in,\n            } => {\n                let resp = client\n                    .streaming()\n                    .sink()\n                    .update(\n                        stream_id,\n                        sink_id,\n                        stream_sink_in.unwrap_or_default().into_inner(),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete { stream_id, sink_id } => {\n                client.streaming().sink().delete(stream_id, sink_id).await?;\n            }\n            Self::Patch {\n                stream_id,\n                sink_id,\n                stream_sink_patch,\n            } => {\n                let resp = client\n                    .streaming()\n                    .sink()\n                    .patch(\n                        stream_id,\n                        sink_id,\n                        stream_sink_patch.unwrap_or_default().into_inner(),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::GetSecret { stream_id, sink_id } => {\n                let resp = client\n                    .streaming()\n                    .sink()\n                    .get_secret(stream_id, sink_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::RotateSecret {\n                stream_id,\n                sink_id,\n                endpoint_secret_rotate_in,\n                options,\n            } => {\n                let resp = client\n                    .streaming()\n                    .sink()\n                    .rotate_secret(\n                        stream_id,\n                        sink_id,\n                        endpoint_secret_rotate_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::TransformationPartialUpdate {\n                stream_id,\n                sink_id,\n                sink_transform_in,\n            } => {\n                let resp = client\n                    .streaming()\n                    .sink()\n                    .transformation_partial_update(\n                        stream_id,\n                        sink_id,\n                        sink_transform_in.unwrap_or_default().into_inner(),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let StreamingSinkCreateOptions { idempotency_key } = value;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/streaming_sink.rs", "node_type": "let_declaration", "line_range": [41, 41]}
{"prompt": "<|fim_prefix|>xfe80\n}\n\nfn is_unique_local(addr: Ipv6Addr) -> bool {\n    (addr.segments()[0] & 0xfe00) == 0xfc00\n}\n\nfn is_documentation_v6(addr: Ipv6Addr) -> bool {\n    (addr.segments()[0] == 0x2001) && (addr.segments()[1] == 0xdb8)\n}\n\n#[cfg(test)]\nmod tests {\n    use std::{\n        net::{IpAddr, TcpListener},\n        path::PathBuf,\n        str::FromStr,\n        sync::Arc,\n    };\n\n    use axum::{routing, Router};\n    use axum_server::tls_openssl::{OpenSSLAcceptor, OpenSSLConfig};\n    use http::{HeaderValue, Method, Version};\n    use ipnet::IpNet;\n\n    use super::{is_allowed, CaseSensitiveHeaderMap, RequestBuilder, WebhookClient};\n\n    #[test]\n    fn is_allowed_test() {\n        // Copied shamelessly from the standard library `is_global` docs\n        assert!(!is_allowed(IpAddr::from([10, 254, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([192, 168, 10, 65])));\n        assert!(!is_allowed(IpAddr::from([172, 16, 10, 65])));\n        assert!(!is_allowed(IpAddr::from([0, 1, 2, 3])));\n        assert!(!is_allowed(IpAddr::from([0, 0, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([127, 0, 0, 1])));\n        assert!(!is_allowed(IpAddr::from([169, 254, 45, 1])));\n        assert!(!is_allowed(IpAddr::from([255, 255, 255, 255])));\n        assert!(!is_allowed(IpAddr::from([192, 0, 2, 255])));\n        assert!(!is_allowed(IpAddr::from([198, 51, 100, 65])));\n        assert!(!is_allowed(IpAddr::from([203, 0, 113, 6])));\n        assert!(!is_allowed(IpAddr::from([100, 100, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([192, 0, 0, 0])));\n        assert!(!is_allowed(IpAddr::from([192, 0, 0, 255])));\n        assert!(!is_allowed(IpAddr::from([250, 10, 20, 30])));\n        assert!(!is_allowed(IpAddr::from([198, 18, 0, 0])));\n\n        assert!(is_allowed(IpAddr::from([1, 1, 1, 1])));\n\n        assert!(!is_allowed(IpAddr::from([0, 0, 0, 0, 0, 0, 0, 0x1])));\n\n        assert!(is_allowed(IpAddr::from([0, 0, 0, 0xffff, 0, 0, 0, 0x1])));\n        assert!(is_allowed(\n            \"2001:4860:4860::8888\".parse::<IpAddr>().unwrap()\n        ));\n        assert!(is_allowed(\"::ffff:8.8.8.8\".parse::<IpAddr>().unwrap()));\n        assert!(!is_allowed(\"::ffff:127.0.0.1\".parse::<IpAddr>().unwrap()));\n        assert!(!is_allowed(\"::ffff:0.0.0.1\".parse::<IpAddr>().unwrap()));\n    }\n\n    #[test]\n    fn test_builder() {\n        match RequestBuilder::new().build() {\n            Err(e) => assert_eq!(\"Build failed: uri missing; version missing\", e.to_string()),\n            Ok(_) => panic!(),\n        }\n\n        assert!(RequestBuilder::new()\n            .version(Version::HTTP_11)\n            .build()\n            .is_err());\n\n        assert!(RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .is_ok());\n    }\n\n    #[test]\n    fn test_header_casings() {\n        let hdrs = CaseSensitiveHeaderMap::from([(\n            \"tEsT-header-1\".to_owned(),\n            HeaderValue::from_static(\"value\"),\n        )]);\n\n        let req = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .headers(hdrs)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req.header_names\n                .unwrap()\n                .get(\"test-header-1\".parse().unwrap())\n                .unwrap(),\n            \"tEsT-header-1\".as_bytes()\n        );\n        assert_eq!(\n            req.headers.get(\"test-header-1\").unwrap(),\n            HeaderValue::from_static(\"value\")\n        );\n    }\n\n    #[test]\n    fn test_url_basic_auth() {\n        let req = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://test:123@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req.headers.get(\"authorization\").unwrap(),\n            \"Basic dGVzdDoxMjM=\".as_bytes()\n        );\n\n        let req_user_only = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://test:@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_user_only.headers.get(\"authorization\").unwrap(),\n            \"Basic dGVzdDo=\".as_bytes()\n        );\n\n        let req_pass_only = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://:123@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_pass_only.headers.get(\"authorization\").unwrap(),\n            \"Basic OjEyMw==\".as_bytes()\n        );\n\n        let req_no_basic_auth = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert!(req_no_basic_auth.headers.get(\"authorization\").is_none());\n\n        let req_special_chars = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://test==:123==@127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_special_chars.headers.get(\"authorization\").unwrap(),\n            \"Basic dGVzdD09OjEyMz09\".as_bytes()\n        );\n    }\n\n    #[test]\n    fn test_host_header() {\n        <|fim_suffix|>\n\n        assert_eq!(req.headers.get(\"host\").unwrap(), \"127.0.0.1\".as_bytes());\n\n        let req_with_port = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1:8000/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        assert_eq!(\n            req_with_port.headers.get(\"host\").unwrap(),\n            \"127.0.0.1:8000\".as_bytes()\n        );\n    }\n\n    #[tokio::test]\n    async fn test_tls_verification_disable() {\n        // Self-signed certificates are expected to be found in `server/svix-server/tests/static` from\n        // the repository root.\n        //\n        // Some have been pre-generated into that directory via the following command:\n        //\n        // ```\n        // openssl req -x509 -newkey rsa:4096 -keyout ex_key.pem -out ex_cert.pem -sha256 \\\n        // -days 36500 -nodes\n        // ```\n        //\n        // Then, via the interactive prompt, a `.` was entered for all fields but the common name,\n        // which was set to `localhost`.\n        //\n        // NOTE: It doesn't really matter the contents of these files as long as they are a valid key\n        // and certificate that is self-signed, expired, or otherwise unable to pass verification.\n        let dir: PathBuf = [env!(\"CARGO_MANIFEST_DIR\"), \"tests\", \"static\"]\n            .iter()\n            .collect();\n        let config =\n            OpenSSLConfig::from_pem_file(dir.join(\"ex_cert.pem\"), dir.join(\"ex_key.pem\")).unwrap();\n        let acceptor = OpenSSLAcceptor::new(config);\n\n        let tcp = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        let url = format!(\"https://{}/\", tcp.local_addr().unwrap());\n\n        let app = Router::new().route(\"/\", routing::any(|| async { \"Hello\" }));\n\n        let _jh = tokio::spawn(async {\n            axum_server::from_tcp(tcp)\n                .acceptor(acceptor)\n                .serve(app.into_make_service())\n                .await\n                .unwrap();\n        });\n\n        let request = RequestBuilder::new()\n            .method(Method::GET)\n            .uri_str(&url)\n            .unwrap()\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();\n\n        let whitelist = Arc::new(vec![IpNet::new(\"127.0.0.1\".parse().unwrap(), 0).unwrap()]);\n\n        // Assert that a [`WebhookClient`] without the disabled flag will err on making to a request\n        // to this server with the self-signed certificate\n        let whc_with_validation = WebhookClient::new(Some(whitelist.clone()), None, false, None);\n        assert!(whc_with_validation.execute(request.clone()).await.is_err());\n\n        // And assert that when the flag is enabled, that it will succeed\n        let whc_without_validation = WebhookClient::new(Some(whitelist), None, true, None);\n        assert!(whc_without_validation.execute(request).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "let req = RequestBuilder::new()\n            .uri(url::Url::from_str(\"http://127.0.0.1/\").unwrap())\n            .version(Version::HTTP_11)\n            .build()\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [890, 894]}
{"prompt": "<|fim_prefix|>)]\npub struct EndpointSecretInternal {\n    marker: EndpointSecretMarker,\n\n    key: Vec<u8>,\n}\n\nimpl EndpointSecretInternal {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MINUS_ONE: usize = Self::KEY_SIZE - 1;\n\n    fn new(\n        encryption: &Encryption,\n        type_: EndpointSecretType,\n        key: &[u8],\n    ) -> crate::error::Result<Self> {\n        Ok(Self {\n            marker: EndpointSecretMarker {\n                type_,\n                encrypted: encryption.enabled(),\n            },\n            key: encryption.encrypt(key)?,\n        })\n    }\n\n    pub fn generate_symmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let buf: [u8; Self::KEY_SIZE] = rand::thread_rng().gen();\n        Self::new(encryption, EndpointSecretType::Hmac256, &buf)\n    }\n\n    pub fn generate_asymmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let key = AsymmetricKey::generate();\n        Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())\n    }\n\n    fn into_vec(mut self) -> Vec<u8> {\n        let marker: u8 = self.marker.to_u8();\n\n        let mut vec = vec![marker];\n        vec.append(&mut self.key);\n        vec\n    }\n\n    fn from_vec(v: Vec<u8>) -> crate::error::Result<Self> {\n        // Legacy had exact size\n        match v.len() {\n            0..=Self::KEY_SIZE_MINUS_ONE => Err(crate::error::Error::generic(\"Value too small\")),\n            Self::KEY_SIZE => Ok(Self {\n                marker: EndpointSecretMarker {\n                    type_: EndpointSecretType::Hmac256,\n                    encrypted: false,\n                },\n                key: v,\n            }),\n            _ => {\n                let marker = EndpointSecretMarker::from_u8(v[0])?;\n                Ok(Self {\n                    marker,\n                    key: v[1..].to_vec(),\n                })\n            }\n        }\n    }\n\n    pub fn into_endpoint_secret(\n        self,\n        encryption: &Encryption,\n    ) -> crate::error::Result<EndpointSecret> {\n        let key = self.key(encryption)?;\n        Ok(match self.type_() {\n            EndpointSecretType::Hmac256 => EndpointSecret::Symmetric(key),\n            EndpointSecretType::Ed25519 => {\n                EndpointSecret::Asymmetric(AsymmetricKey::from_slice(&key[..])?)\n            }\n        })\n    }\n\n    pub fn from_endpoint_secret(\n        endpoint_secret: EndpointSecret,\n        encryption: &Encryption,\n    ) -> crate::error::Result<Self> {\n        Ok(match endpoint_secret {\n            EndpointSecret::Symmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Hmac256, &key)?\n            }\n            EndpointSecret::Asymmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())?\n            }\n        })\n    }\n\n    pub fn sign(&self, encryption: &Encryption, bytes: &[u8]) -> Vec<u8> {\n        let key = self.key(encryption).unwrap();\n        // FIXME: remove unwrap\n        match self.marker.type_() {\n            EndpointSecretType::Hmac256 => hmac_sha256::HMAC::mac(bytes, key).to_vec(),\n            EndpointSecretType::Ed25519 => AsymmetricKey::from_slice(&key[..])\n                .unwrap()\n                .0\n                .sk\n                .sign(bytes, None)\n                .to_vec(),\n        }\n    }\n\n    fn key(&self, encryption: &Encryption) -> crate::error::Result<Vec<u8>> {\n        Ok(if self.marker.encrypted {\n            if encryption.enabled() {\n                encryption.decrypt(&self.key)?\n            } else {\n                return Err(crate::error::Error::generic(\n                    \"main_secret unset, can't decrypt key\",\n                ));\n            }\n        } else {\n            self.key.to_vec()\n        })\n    }\n\n    pub fn type_(&self) -> &EndpointSecretType {\n        self.marker.type_()\n    }\n}\n\nimpl Serialize for EndpointSecretInternal {\n    f<|fim_suffix|>}\n\nimpl<'de> Deserialize<'de> for EndpointSecretInternal {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n\n        String::deserialize(deserializer).and_then(|string| {\n            // For backwards compat when loading from ExpiringSigningKeys. Going forward we just b64 it\n            if string.starts_with(EndpointSecretType::Hmac256.secret_prefix()) {\n                Ok(Self {\n                    marker: EndpointSecretMarker {\n                        type_: EndpointSecretType::Hmac256,\n                        encrypted: false,\n                    },\n                    key: string\n                        .get(EndpointSecretType::Hmac256.secret_prefix().len()..)\n                        .ok_or_else(|| Error::custom(\"invalid prefix\".to_string()))\n                        .and_then(|string| {\n                            STANDARD\n                                .decode(string)\n                                .map_err(|err| Error::custom(err.to_string()))\n                        })?,\n                })\n            } else {\n                let buf = STANDARD\n                    .decode(string)\n                    .map_err(|err| Error::custom(err.to_string()))?;\n                Self::from_vec(buf).map_err(|err| Error::custom(err.to_string()))\n            }\n        })\n    }\n}\n\nimpl From<EndpointSecretInternal> for sea_orm::Value {\n    fn from(v: EndpointSecretInternal) -> Self {\n        Self::Bytes(Some(Box::new(v.into_vec())))\n    }\n}\n\nimpl sea_orm::TryGetable for EndpointSecretInternal {\n    fn try_get_by<I: sea_orm::ColIdx>(\n        res: &sea_orm::QueryResult,\n        index: I,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get_by(res, index) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }\n\n    fn try_get(\n        res: &sea_orm::QueryResult,\n        pre: &str,\n        col: &str,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get(res, pre, col) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }\n}\n\nimpl sea_orm::sea_query::Nullable for EndpointSecretInternal {\n    fn null() -> sea_orm::Value {\n        sea_orm::Value::Bytes(None)\n    }\n}\n\nimpl sea_orm::sea_query::ValueType for EndpointSecretInternal {\n    fn try_from(v: sea_orm::Value) -> Result<Self, sea_orm::sea_query::ValueTypeErr> {\n        match v {\n            sea_orm::Value::Bytes(Some(x)) => {\n                EndpointSecretInternal::from_vec(*x).map_err(|_| sea_orm::sea_query::ValueTypeErr)\n            }\n            _ => Err(sea_orm::sea_query::ValueTypeErr),\n        }\n    }\n\n    fn type_name() -> String {\n        stringify!(EndpointSecretInternal).to_owned()\n    }\n\n    fn column_type() -> sea_orm::sea_query::ColumnType {\n        sea_orm::sea_query::ColumnType::Binary(\n            Self::KEY_SIZE\n                .try_into()\n                .expect(\"Key size is not more than u32::MAX\"),\n        )\n    }\n\n    fn array_type() -> sea_orm::sea_query::ArrayType {\n        sea_orm::sea_query::ArrayType::Bytes\n    }\n}\n\n/// The external representation of the endpoint secret.\n/// This one is used for serializing to and from customers.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum EndpointSecret {\n    Symmetric(Vec<u8>),\n    Asymmetric(AsymmetricKey),\n}\n\nimpl EndpointSecret {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MAX: usize = 75;\n\n    pub fn serialize_secret_key(&self) -> String {\n        match self {\n            Self::Symmetric(key) => {\n                format!(\n                    \"{}{}\",\n     <|fim_middle|>", "completion": "fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        serializer.serialize_str(&STANDARD.encode(self.clone().into_vec()))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "function_item", "line_range": [834, 839]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct SegmentConfigOut {}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl SegmentConfigOut {\n    pub fn new() -> Self {\n        Self {}\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/segment_config_out.rs", "node_type": "impl_item", "line_range": [7, 11]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse aide::axum::{\n    routing::{get, get_with},\n    ApiRouter,\n};\nuse axum::{extract::State, http::StatusCode, Json};\nuse sea_orm::{query::Statement, ConnectionTrait, DatabaseBackend};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::aide_annotate;\n\nuse crate::{\n    core::cache::{kv_def, CacheBehavior, CacheKey, CacheValue},\n    queue::QueueTask,\n    v1::utils::{openapi_tag, NoContent},\n    AppState,\n};\n\nasync fn ping() -> NoContent {\n    NoContent\n}\n\n#[derive(Debug, Deserialize, Serialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum HealthStatusVariant {\n    Ok,\n    Error,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthStatus {\n    status: HealthStatusVariant,\n    // TODO: information field\n}\n\nimpl HealthStatus {\n    pub fn new_ok() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Ok,\n        }\n    }\n\n    pub fn new_error() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Error,\n        }\n    }\n\n    pub fn is_ok(&self) -> bool {\n        matches!(\n            self,\n            HealthStatus {\n                status: HealthStatusVariant::Ok,\n                ..\n            }\n        )\n    }\n}\nimpl<O, E> From<Result<O, E>> for HealthStatus {\n    fn from(res: Result<O, E>) -> Self {\n        match res {\n            Ok(_) => HealthStatus::new_ok(),\n            Err(_) => HealthStatus::new_error(),\n        }\n    }\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthReport {\n    database: HealthStatus,\n\n    queue: HealthStatus,\n    cache: HealthStatus,\n}\n\n#[derive(Deserialize, Serialize, Debug, PartialEq)]\nstruct HealthCheckCacheValue(());\nkv_def!(HealthCheckCacheKey, HealthCheckCacheValue);\n\n/// Verify the API server is up and running.\n#[aide_annotate(op_id = \"v1.health.get\")]\nasync fn health(\n    State(AppState {\n        ref db,\n        queue_tx,\n        cache,\n        ..\n    }): State<AppState>,\n) -> (StatusCode, Json<HealthReport>) {\n    // SELECT 1 FROM any table\n    l<|fim_suffix|>\n    // Send a [`HealthCheck`] through the queue\n    let queue: HealthStatus = queue_tx.send(&QueueTask::HealthCheck, None).await.into();\n\n    // Set a cache value with an expiration to ensure it works\n    let cache: HealthStatus = cache\n        .set(\n            &HealthCheckCacheKey(\"health_check_value\".to_owned()),\n            &HealthCheckCacheValue(()),\n            // Expires after this time, so it won't pollute the DB\n            Duration::from_millis(100),\n        )\n        .await\n        .into();\n\n    let status = if database.is_ok() && queue.is_ok() && cache.is_ok() {\n        StatusCode::OK\n    } else {\n        StatusCode::INTERNAL_SERVER_ERROR\n    };\n\n    (\n        status,\n        Json(HealthReport {\n            database,\n            queue,\n            cache,\n        }),\n    )\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Health\");\n\n    ApiRouter::new()\n        .api_route(\"/health/ping\", get(ping).head(ping))\n        .api_route_with(\n            \"/health\",\n            get_with(health, |op| op.response::<204, ()>().with(health_operation))\n                .head_with(health, health_operation),\n            tag,\n        )\n}\n<|fim_middle|>", "completion": "let database: HealthStatus = db\n        .execute(Statement::from_string(\n            DatabaseBackend::Postgres,\n            \"SELECT 1\".to_owned(),\n        ))\n        .await\n        .into();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/health.rs", "node_type": "let_declaration", "line_range": [94, 100]}
{"prompt": "<|fim_prefix|>n_address: SocketAddr,\n    #[serde(default = \"default_transformation_worker_count\")]\n    pub transformation_worker_count: NonZeroUsize,\n}\n\nimpl Config {\n    /// Build a Config from yaml source.\n    /// Optionally accepts a map to perform variable substitution with.\n    pub fn from_src(\n        raw_src: &str,\n        vars: Option<&HashMap<String, String>>,\n    ) -> std::io::Result<Self> {\n        let src = if let Some(vars) = vars {\n            let context = |key: &str| -> Result<Option<Cow<'_, str>>, LookupError<Infallible>> {\n                Ok(vars.get(key).map(Cow::from))\n            };\n            shellexpand::env_with_context(raw_src, context).map_err(|e: LookupError<_>| {\n                Error::other(format!(\"Variable substitution failed: {e}\"))\n            })?\n        } else {\n            Cow::Borrowed(raw_src)\n        };\n        let cfg: Self = serde_yaml::from_str(&src)\n            .map_err(|e| Error::other(format!(\"Failed to parse config: {e}\")))?;\n\n        for sc in &cfg.senders {\n            if let Some(tc) = sc.transformation() {\n                crate::runtime::validate_script(tc.source().as_str()).map_err(|e| {\n                    Error::other(format!(\n                        \"failed to parse transformation for sender `{}`: {e:?}\",\n                        &sc.name(),\n                    ))\n                })?;\n            }\n        }\n\n        for (name, tc) in cfg.receivers.iter().filter_map(|either| match either {\n            EitherReceiver::Webhook(receiver) => receiver\n                .transformation\n                .as_ref()\n                .map(|tc| (&receiver.name, tc)),\n            EitherReceiver::Poller(receiver) => receiver\n                .transformation\n                .as_ref()\n                .map(|tc| (&receiver.name, tc)),\n        }) {\n            crate::runtime::validate_script(tc.source().as_str()).map_err(|e| {\n                Error::other(format!(\n                    \"failed to parse transformation for receiver `{name}`: {e:?}\"\n                ))\n            })?;\n        }\n\n        Ok(cfg)\n    }\n}\n\nfn default_http_listen_address() -> SocketAddr {\n    \"0.0.0.0:5000\".parse().expect(\"default http listen address\")\n}\n\nfn default_transformation_worker_count() -> NonZeroUsize {\n    NonZeroUsize::new(4).expect(\"4 is greater than 0\")\n}\n\n#[derive(Deserialize)]\npub struct OtelExporterConfig {\n    /// The OpenTelemetry service name to use\n    pub service_name: Option<String>,\n    /// The OpenTelemetry address to send events to if given.\n    pub address: String,\n    /// The ratio at which to sample spans when sending to OpenTelemetry. When not given it defaults\n    /// to always sending. If the OpenTelemetry address is not set, this will do nothing.\n    pub sample_ratio: Option<f64>,\n}\n\n#[derive(Clone, Debug, Default, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogLevel {\n    #[default]\n    Info,\n    Debug,\n    Trace,\n}\n\nimpl fmt::Display for LogLevel {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Info => Level::INFO,\n            Self::Debug => Level::DEBUG,\n            Self::Trace => Level::TRACE,\n        }\n        .fmt(f)\n    }\n}\n\n#[derive(Clone, Debug, Default, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogFormat {\n    #[default]\n    Default,\n    Json,\n}\n\n/// Config for reading messages from plugins and forwarding to Svix.\n#[derive(Deserialize)]\npub struct WebhookSenderConfig {\n    pub name: String,\n    pub input: SenderInputOpts,\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: SenderOutputOpts,\n}\n\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum SenderInputOpts {\n    #[cfg(feature = \"kafka\")]\n    Kafka(KafkaInputOpts),\n    Queue(QueueInputOpts),\n}\n\nimpl WebhookSenderConfig {\n    pub fn into_sender_input(self) -> anyhow::Result<Box<dyn SenderInput>> {\n        Ok(match self.input {\n            #[cfg(feature = \"kafka\")]\n            SenderInputOpts::Kafka(input_opts) => svix_bridge_plugin_kafka::into_sender_input(\n                self.name,\n                input_opts,\n                self.transformation,\n                self.output,\n            )?,\n            SenderInputOpts::Queue(input_opts) => svix_bridge_plugin_queue::into_sender_input(\n                self.name,\n                input_opts,\n                self.transformation,\n                self.output,\n            )\n            .map_err(|e| anyhow!(\"{e}\"))?,\n        })\n    }\n}\n\nimpl WebhookSenderConfig {\n    pub fn name(&self) -> &str {\n        &self.name\n    }\n\n    pub fn transformation(&self) -> Option<&TransformationConfig> {\n        self.transformation.as_ref()\n    }\n}\n\n<|fim_suffix|>\n\n/// Config for receiving webhooks and forwarding them to plugins.\n#[derive(Deserialize)]\npub struct WebhookReceiverConfig {\n    pub name: String,\n    pub input: ReceiverInputOpts,\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: ReceiverOutputOpts,\n}\n\n#[derive(Deserialize)]\n#[allow(clippy::large_enum_variant)] // we're talking a couple hundred bytes only\n#[serde(untagged)]\npub enum ReceiverOutputOpts {\n    Http(HttpOutputOpts),\n    #[cfg(feature = \"kafka\")]\n    Kafka(KafkaOutputOpts),\n    Queue(QueueOutputOpts),\n}\n\nimpl WebhookReceiverConfig {\n    pub async fn into_receiver_output(self) -> anyhow::Result<Box<dyn ReceiverOutput>> {\n        match self.output {\n            ReceiverOutputOpts::Http(opts) => opts.into_receiver_output(self.name),\n            #[cfg(feature = \"kafka\")]\n            ReceiverOutputOpts::Kafka(opts) => {\n                svix_bridge_plugin_kafka::into_receiver_output(self.name, opts).map_err(Into::into)\n            }\n            ReceiverOutputOpts::Queue(x) => svix_bridge_plugin_queue::into_receiver_output(\n                self.name.clone(),\n                x,\n                self.transformation.as_ref(),\n            )\n            .await\n            .map_err(Into::into),\n        }\n    }\n}\n\n#[derive(Clone, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"kebab-case\")]\npub enum PollerInputOpts {\n    SvixMessagePoller {\n        /// Identifies this client, allowing the server to track progress during iteration.\n        /// Processes should not share a consumer id. Only exclusive access is permitted.\n        consumer_id: String,\n        /// The app portion of the polling endpoint, e.g. `app_XXXX`\n        app_id: String,\n        /// The poller portion of the polling endpoint, e.g. `poll_XXXX`\n        sink_id: String,\n        /// The auth token for the polling endpoint\n        token: String,\n        #[serde(default)]\n        svix_options: Option<SvixOptions>,\n    },\n}\n\nimpl PollerInputOpts {\n    pub fn svix_client(&self) -> Option<Svix> {\n        match self {\n            PollerInputOpts::SvixMessagePoller {\n                token,\n                svix_options,\n                ..\n            } => Some(Svix::new(\n                token.clone(),\n                svix_options.clone().map(Into::into),\n            )),\n        }\n    }\n}\n\n/// Config for fetching from HTTP endpoints and forwarding them to plugins.\n#[derive(Deserialize)]\npub struct PollerReceiverConfig {\n    pub name: String,\n    pub input: PollerInputOpts,\n    // FIXME: add a configurable polling schedule or interval\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: ReceiverOutputOpts,\n}\n\nimpl PollerReceiverConfig {\n    // FIXME: duplicate from WebhookReceiverConfig. Extract/refactor as TryFrom ReceiverOutputOpts?\n    pub async fn into_receiver_output(self) -> anyhow::Result<Box<dyn ReceiverOutput>> {\n        match self.output {\n            ReceiverOutputOpts::Http(opts) => opts.into_receiver_output(self.name),\n            #[cfg(feature = \"kafka\")]\n            ReceiverOutputOpts::Kafka(opts) => {\n                svix_bridge_plugin_kafka::into_receiver_output(self.name, opts).map_err(Into::into)\n            }\n            ReceiverOutputOpts::Queue(x) => svix_bridge_plugin_queue::into_receiver_output(\n                self.name.clone(),\n                x,\n                self.transformation.as_ref(),\n            )\n            .await\n            .map_err(Into::into),\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests;\n<|fim_middle|>", "completion": "impl TryFrom<WebhookSenderConfig> for Box<dyn SenderInput> {\n    type Error = anyhow::Error;\n\n    fn try_from(value: WebhookSenderConfig) -> Result<Self, Self::Error> {\n        value.into_sender_input()\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/config/mod.rs", "node_type": "impl_item", "line_range": [199, 205]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::fmt::Debug;\n\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chacha20poly1305::{\n    aead::{Aead, KeyInit},\n    Key, XChaCha20Poly1305, XNonce,\n};\nu<|fim_suffix|>use rand::Rng;\n\nuse crate::error::Result;\n\n// Asymmetric Signature keys\n#[derive(Clone, Eq)]\npub struct AsymmetricKey(pub KeyPair);\n\nimpl AsymmetricKey {\n    pub fn generate() -> AsymmetricKey {\n        AsymmetricKey(KeyPair::from_seed(Seed::generate()))\n    }\n\n    pub fn from_slice(bytes: &[u8]) -> Result<Self> {\n        Ok(AsymmetricKey(KeyPair::from_slice(bytes).map_err(|_| {\n            crate::error::Error::generic(\"Failed parsing key.\")\n        })?))\n    }\n\n    pub fn from_base64(b64: &str) -> Result<Self> {\n        let bytes = STANDARD\n            .decode(b64)\n            .map_err(|_| crate::error::Error::generic(\"Failed parsing base64\"))?;\n\n        Self::from_slice(bytes.as_slice())\n    }\n\n    pub fn pubkey(&self) -> &[u8] {\n        &self.0.pk[..]\n    }\n}\n\nimpl Debug for AsymmetricKey {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"<AsymmetricKey sk=*** pk={}>\",\n            STANDARD.encode(self.0.pk.as_slice())\n        )\n    }\n}\n\nimpl PartialEq for AsymmetricKey {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.as_slice() == other.0.as_slice()\n    }\n}\n\n#[derive(Clone, Debug)]\npub struct Encryption(Option<Key>);\n\nimpl Encryption {\n    const NONCE_SIZE: usize = 24;\n\n    pub fn new_noop() -> Self {\n        Self(None)\n    }\n\n    pub fn new(key: [u8; 32]) -> Self {\n        Self(Some(Key::from_slice(&key).to_owned()))\n    }\n\n    pub fn encrypt(&self, data: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce: [u8; Self::NONCE_SIZE] = rand::thread_rng().gen();\n            let nonce = XNonce::from_slice(&nonce);\n            let mut ciphertext = cipher\n                .encrypt(nonce, data)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))?;\n            let mut ret = nonce.to_vec();\n            ret.append(&mut ciphertext);\n            Ok(ret)\n        } else {\n            Ok(data.to_vec())\n        }\n    }\n\n    pub fn decrypt(&self, ciphertext: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce = &ciphertext[..Self::NONCE_SIZE];\n            let ciphertext = &ciphertext[Self::NONCE_SIZE..];\n            cipher\n                .decrypt(XNonce::from_slice(nonce), ciphertext)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))\n        } else {\n            Ok(ciphertext.to_vec())\n        }\n    }\n\n    pub fn enabled(&self) -> bool {\n        self.0.is_some()\n    }\n}\n\nimpl Default for Encryption {\n    fn default() -> Self {\n        Self::new_noop()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::Encryption;\n\n    #[test]\n    fn test_encryption() {\n        let clear = b\"hello world\";\n        let encryption = Encryption::new([1; 32]);\n        let ciphertext = encryption.encrypt(clear).unwrap();\n        let clear2 = encryption.decrypt(&ciphertext).unwrap();\n        assert_eq!(&clear[..], &clear2[..]);\n    }\n}\n<|fim_middle|>", "completion": "use ed25519_compact::*;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cryptography.rs", "node_type": "use_declaration", "line_range": [11, 11]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{collections::HashMap, sync::Arc};\n\nuse axum::async_trait;\nuse tokio::{\n    sync::RwLock,\n    task,\n    time::{sleep, Duration, Instant},\n};\n\nuse super::{Cache, CacheBehavior, CacheKey, Result};\n\n#[derive(Debug)]\nstruct ValueWrapper {\n    value: Vec<u8>,\n    ttl: Duration,\n    timer: Instant,\n}\n\nimpl ValueWrapper {\n    fn new(value: Vec<u8>, ttl: Duration) -> ValueWrapper {\n        ValueWrapper {\n            value,\n            ttl,\n            timer: Instant::now(),\n        }\n    }\n}\n\ntype State = HashMap<Vec<u8>, ValueWrapper>;\ntype SharedState = Arc<RwLock<State>>;\n\npub fn new() -> Cache {\n    let shared_state = Arc::new(RwLock::new(State::new()));\n\n    let shared_state_clone = shared_state.clone();\n    task::spawn(async move {\n        loop {\n            sleep(Duration::from_secs(60 * 5)).await;\n            shared_state_clone\n                .write()\n                .await\n                .retain(|_, v| check_is_expired(v))\n        }\n    });\n\n    MemoryCache { map: shared_state }.into()\n}\n\n#[derive(Clone)]\npub struct MemoryCache {\n    map: SharedState,\n}\n\n#[async_trait]\nimpl CacheBehavior for MemoryCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(self\n            .map\n            .read()\n            .await\n            .get(key)\n            .filter(|wrapper| check_is_expired(wrapper))\n            .map(|wrapper| wrapper.value.clone()))\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()> {\n        self.map\n            .write()\n            .await\n            .insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n        Ok(())\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool> {\n        let mut lock = self.map.write().await;\n\n        // TODO: use HashMap::try_insert when stable\n        // https://github.com/rust-lang/rust/issues/82766\n        if !lock.contains_key(key) {\n            lock.insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n            return Ok(true);\n        }\n\n        Ok(false)\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()> {\n        self.map.write().await.remove(key.as_ref().as_bytes());\n\n        Ok(())\n    }\n}\n\nfn check_is_expired(vw: &ValueWrapper) -> bool {\n    vw.timer.elapsed().as_millis() <= vw.ttl.as_millis()\n}\n\n#[cfg(test)]\nmod tests {\n    use serde::{Deserialize, Serialize};\n\n    use super::{\n        super::{kv_def, CacheValue},\n        *,\n    };\n    use crate::core::cache::string_kv_def;\n\n    // Test structures\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValA(usize);\n    kv_def!(TestKeyA, TestValA);\n    i<|fim_suffix|>\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValB(String);\n    kv_def!(TestKeyB, TestValB);\n    impl TestKeyB {\n        fn new(id: String) -> TestKeyB {\n            TestKeyB(format!(\"SVIX_TEST_KEY_B_{id}\"))\n        }\n    }\n\n    string_kv_def!(StringTestKey);\n    impl StringTestKey {\n        fn new(id: String) -> StringTestKey {\n            StringTestKey(format!(\"SVIX_TEST_KEY_STRING_{id}\"))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_cache_crud_no_ttl() {\n        let cache = new();\n\n        let (first_key, first_val_a, first_val_b) =\n            (TestKeyA::new(\"1\".to_owned()), TestValA(1), TestValA(2));\n        let (second_key, second_val_a, second_val_b) = (\n            TestKeyB::new(\"1\".to_owned()),\n            TestValB(\"1\".to_owned()),\n            TestValB(\"2\".to_owned()),\n        );\n        let (third_key, third_val_a, third_val_b) = (\n            StringTestKey::new(\"1\".to_owned()),\n            \"1\".to_owned(),\n            \"2\".to_owned(),\n        );\n\n        // Create\n        assert!(cache\n            .set(&first_key, &first_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Read\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_a));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_a));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_a)\n        );\n\n        // Update (overwrite)\n        assert!(cache\n            .set(&first_key, &first_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Confirm update\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_b));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_b));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_b)\n        );\n\n        // Delete\n        assert!(cache.delete(&first_key).await.is_ok());\n        assert!(cache.delete(&second_key).await.is_ok());\n        assert!(cache.delete(&third_key).await.is_ok());\n\n        // Confirm deletion\n        assert_eq!(cache.get::<TestValA>(&first_key).await.unwrap(), None);\n        assert_eq!(cache.get::<TestValB>(&second_key).await.unwrap(), None);\n        assert_eq!(cache.get_string(&third_key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_ttl() {\n        let cache = new();\n        let key = TestKeyA::new(\"key\".to_owned());\n\n        assert!(cache\n            .set(&key, &TestValA(1), Duration::from_secs(1),)\n            .await\n            .is_ok());\n        tokio::time::sleep(std::time::Duration::from_millis(1200)).await;\n        assert_eq!(cache.get::<TestValA>(&key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_nx_status() {\n        let cache = new();\n        let key = TestKeyA::new(\"nx_status_test_key\".to_owned());\n\n        assert!(cache\n            .set_if_not_exists(&key, &TestValA(1), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(!cache\n            .set_if_not_exists(&key, &TestValA(2), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(cache.delete(&key).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "impl TestKeyA {\n        fn new(id: String) -> TestKeyA {\n            TestKeyA(format!(\"SVIX_TEST_KEY_A_{id}\"))\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/memory.rs", "node_type": "impl_item", "line_range": [120, 124]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct EnvironmentExportOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EnvironmentExportOptions> for svix::api::EnvironmentExportOptions {\n    fn from(value: EnvironmentExportOptions) -> Self {\n        <|fim_suffix|>\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct EnvironmentImportOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EnvironmentImportOptions> for svix::api::EnvironmentImportOptions {\n    fn from(value: EnvironmentImportOptions) -> Self {\n        let EnvironmentImportOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct EnvironmentArgs {\n    #[command(subcommand)]\n    pub command: EnvironmentCommands,\n}\n\n#[derive(Subcommand)]\npub enum EnvironmentCommands {\n    /// Download a JSON file containing all org-settings and event types.\n    ///\n    /// Note that the schema for [`EnvironmentOut`] is subject to change. The fields\n    /// herein are provided for convenience but should be treated as JSON blobs.\n    Export {\n        #[clap(flatten)]\n        options: EnvironmentExportOptions,\n    },\n    /// Import a configuration into the active organization.\n    ///\n    /// It doesn't delete anything, only adds / updates what was passed to it.\n    ///\n    /// Note that the schema for [`EnvironmentIn`] is subject to change. The fields\n    /// herein are provided for convenience but should be treated as JSON blobs.\n    Import {\n        environment_in: Option<crate::json::JsonOf<EnvironmentIn>>,\n        #[clap(flatten)]\n        options: EnvironmentImportOptions,\n    },\n}\n\nimpl EnvironmentCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Export { options } => {\n                let resp = client.environment().export(Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Import {\n                environment_in,\n                options,\n            } => {\n                client\n                    .environment()\n                    .import(\n                        environment_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let EnvironmentExportOptions { idempotency_key } = value;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/environment.rs", "node_type": "let_declaration", "line_range": [13, 13]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse sea_orm::{\n    ColumnTrait, DatabaseConnection, DbBackend, DeleteResult, EntityTrait, QueryFilter,\n    SqlxPostgresConnector,\n};\nuse sqlx::postgres::PgPoolOptions;\n\nuse crate::{cfg::Configuration, core::types::OrganizationId};\n\npub mod models;\nu<|fim_suffix|>\nstatic MIGRATIONS: sqlx::migrate::Migrator = sqlx::migrate!();\n\nasync fn connect(dsn: &str, max_pool_size: u16) -> sqlx::Pool<sqlx::Postgres> {\n    if DbBackend::Postgres.is_prefix_of(dsn) {\n        PgPoolOptions::new()\n            .max_connections(max_pool_size.into())\n            .connect(dsn)\n            .await\n            .expect(\"Error connecting to Postgres\")\n    } else {\n        panic!(\"db_dsn format not recognized. {dsn}\")\n    }\n}\n\npub async fn init_db(cfg: &Configuration) -> DatabaseConnection {\n    SqlxPostgresConnector::from_sqlx_postgres_pool(connect(&cfg.db_dsn, cfg.db_pool_max_size).await)\n}\n\npub async fn run_migrations(cfg: &Configuration) {\n    let db = connect(&cfg.db_dsn, cfg.db_pool_max_size).await;\n    MIGRATIONS.run(&db).await.unwrap();\n}\n\n/// Wipe an organization from existence in a way that ensures the operation can be tried again on\n/// failure.\npub async fn wipe_org(cfg: &Configuration, org_id: OrganizationId) {\n    let db = init_db(cfg).await;\n\n    let applications: Vec<application::Model> = application::Entity::secure_find(org_id.clone())\n        .all(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error fetching applications associated with org ID {org_id}\"));\n\n    for application in applications {\n        let endpoints: Vec<endpoint::Model> = endpoint::Entity::secure_find(application.id.clone())\n            .all(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error fetching endpoints associated with application ID {}\",\n                    application.id\n                )\n            });\n\n        for endpoint in endpoints {\n            // First [`messageattempt`]s, then [`messagedestination`]s\n            let _: DeleteResult = messageattempt::Entity::delete_many()\n                .filter(messageattempt::Column::EndpId.eq(endpoint.id.clone()))\n                .exec(&db)\n                .await\n                .unwrap_or_else(|_| {\n                    panic!(\n                        \"Error deleting messageattempts associated with endpoint ID {}\",\n                        endpoint.id\n                    )\n                });\n        }\n\n        // Then [`message`]s, then [`endpoint`]s\n        let _: DeleteResult = message::Entity::delete_many()\n            .filter(message::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting messages associated with application ID {}\",\n                    application.id\n                )\n            });\n\n        let _: DeleteResult = endpoint::Entity::delete_many()\n            .filter(endpoint::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting endpoints associated with application ID {}\",\n                    application.id\n                )\n            });\n    }\n\n    // Then [`application`]s, then [`eventtype`]s\n    let _: DeleteResult = application::Entity::delete_many()\n        .filter(application::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting applications associated with org ID {org_id}\"));\n\n    let _: DeleteResult = eventtype::Entity::delete_many()\n        .filter(eventtype::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting event types associated with org ID {org_id}\"));\n}\n<|fim_middle|>", "completion": "use models::{application, endpoint, eventtype, message, messageattempt};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/mod.rs", "node_type": "use_declaration", "line_range": [13, 13]}
{"prompt": "<|fim_prefix|>use std::time::Duration;\n\npub use async_trait::async_trait;\nuse serde::{Deserialize, Serialize};\npub use svix;\nuse svix::api::{MessageIn, SvixOptions as _SvixOptions};\nuse tokio::sync::{mpsc, oneshot};\n\n#[derive(Deserialize, Default, Eq, PartialEq, Copy, Clone)]\n#[serde(rename_all = \"lowercase\")]\npub enum TransformerInputFormat {\n    String,\n    #[default]\n    Json,\n}\n\n#[derive(Deserialize, Clone)]\n#[serde(untagged)]\npub enum TransformationConfig {\n    /// If the config has a string value, we assume it expects the input parsed as json\n    /// ```yaml\n    /// transformation: function handler(x) {return { payload: x.foobar }; }\n    /// ```\n    ImplicitJson(String),\n    /// When the config has format/src fields, then you can optionally set the format to `string`,\n    /// in which case you have to parse it yourself inside the transformation.\n    /// ```yaml\n    /// transformation:\n    ///   format: string\n    ///   src: function handler(x) { return { payload: JSON.parse(x).foobar }; }\n    /// ```\n    Explicit {\n        format: TransformerInputFormat,\n        src: String,\n    },\n}\n\nimpl TransformationConfig {\n    pub fn source(&self) -> &String {\n        match self {\n            TransformationConfig::ImplicitJson(src) => src,\n            TransformationConfig::Explicit { src, .. } => src,\n        }\n    }\n\n    pub fn format(&self) -> TransformerInputFormat {\n        match self {\n            TransformationConfig::ImplicitJson(_) => TransformerInputFormat::Json,\n            TransformationConfig::Explicit { format, .. } => *format,\n        }\n    }\n}\n\n<|fim_suffix|>\n\n#[derive(Serialize)]\n#[serde(untagged)]\npub enum TransformerInput {\n    /// Transformations accept arbitrary json here, not restricted to an Object type.\n    /// The thing receiving the value will error if it can't marshall into a type it needs.\n    Json(serde_json::Value),\n    /// Aka \"raw\", we take the input as a utf-8 string and the transformation does whatever it\n    /// wants with it.\n    String(String),\n}\n\nimpl From<serde_json::Value> for TransformerInput {\n    fn from(value: serde_json::Value) -> Self {\n        Self::Json(value)\n    }\n}\n\nimpl From<String> for TransformerInput {\n    fn from(value: String) -> Self {\n        Self::String(value)\n    }\n}\n\n/// Plain old JSON objects are what the transformations expect to receive and produce.\npub type JsObject = serde_json::Map<String, serde_json::Value>;\n/// A channel for plugins to send payloads/scripts to for execution.\npub type TransformerTx = mpsc::UnboundedSender<TransformerJob>;\n/// The receiver side for transformations. The JS executor reads from this.\npub type TransformerRx = mpsc::UnboundedReceiver<TransformerJob>;\n/// A oneshot channel for the JS executor to \"publish\" return values to once complete.\n// FIXME: better error type?\npub type TransformerCallbackTx = oneshot::Sender<Result<TransformerOutput, ()>>;\n/// Used by the caller of the transformer to await the execution's output.\n// FIXME: better error type?\npub type TransformerCallbackRx = oneshot::Receiver<Result<TransformerOutput, ()>>;\n\n/// A transformation job sent to the JS executor.\n/// Once the script has been run on the payload, the transformed payload is sent back through the\n/// callback channel.\npub struct TransformerJob {\n    pub callback_tx: TransformerCallbackTx,\n    pub input: TransformerInput,\n    pub script: String,\n}\n\n#[derive(Debug)]\npub enum TransformerOutput {\n    /// A successfully transformed payload.\n    // Both senders and receivers require a map type (Object) but have different requirements which\n    // are best validated after the fact. For now, we validate only that we get a map type back.\n    Object(JsObject),\n    /// For cases where the JS script executes successfully but produces an unexpected output.\n    Invalid,\n}\n\nimpl TransformerJob {\n    pub fn new(script: String, input: TransformerInput) -> (Self, TransformerCallbackRx) {\n        let (callback_tx, callback_rx) = oneshot::channel();\n        (\n            Self {\n                input,\n                script,\n                callback_tx,\n            },\n            callback_rx,\n        )\n    }\n}\n\n/// Effectively a black box to the supervisor.\n///\n/// Plugins should run until they are done, and likely they should not be \"done\" until the program\n/// exits.\n#[async_trait]\npub trait SenderInput: Send {\n    fn name(&self) -> &str;\n    /// For plugins that want to run JS transformations on payloads.\n    /// Giving them a sender lets them pass messages to the JS executor.\n    fn set_transformer(&mut self, _tx: Option<TransformerTx>) {}\n    async fn run(&self);\n}\n\n#[async_trait]\npub trait PollerInput: Send {\n    fn name(&self) -> &str;\n    fn set_transformer(&mut self, _tx: Option<TransformerTx>) {}\n    async fn run(&self);\n}\n\npub type BoxError = Box<dyn std::error::Error + Send + Sync>;\n\n/// Represents something we can hand a webhook payload to.\n/// Aka a \"forwarder.\"\n///\n/// To start, we're only using this in conjunction with an HTTP server \"owned\" by the bridge binary.\n#[async_trait]\npub trait ReceiverOutput: Send + Sync {\n    fn name(&self) -> &str;\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError>;\n}\n\n#[derive(Deserialize, Debug, Clone, Default)]\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum WebhookVerifier {\n    Svix {\n        endpoint_secret: String,\n    },\n    #[default]\n    None,\n}\n\n#[derive(Debug, Clone, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"kebab-case\")]\npub enum ReceiverInputOpts {\n    Webhook {\n        path_id: String,\n        #[serde(default)]\n        verification: WebhookVerifier,\n    },\n    SvixWebhook {\n        path_id: String,\n        endpoint_secret: String,\n    },\n}\n\nimpl ReceiverInputOpts {\n    pub fn path_id(&self) -> &str {\n        match self {\n            ReceiverInputOpts::Webhook { path_id, .. }\n            | ReceiverInputOpts::SvixWebhook { path_id, .. } => path_id,\n        }\n    }\n}\n\n// N.b. the codegen types we get from openapi don't impl Deserialize so we need our own version.\n#[derive(Clone, Debug, Default, Deserialize)]\npub struct SvixOptions {\n    #[serde(default)]\n    pub debug: bool,\n    pub server_url: Option<String>,\n    pub timeout_secs: Option<u64>,\n    pub num_retries: Option<u32>,\n    pub retry_schedule_ms: Option<Vec<u64>>,\n    pub proxy_address: Option<String>,\n}\n\nimpl From<SvixOptions> for _SvixOptions {\n    fn from(\n        SvixOptions {\n            debug,\n            server_url,\n            timeout_secs,\n            num_retries,\n            retry_schedule_ms,\n            proxy_address,\n        }: SvixOptions,\n    ) -> Self {\n        _SvixOptions {\n            debug,\n            server_url,\n            timeout: timeout_secs.map(Duration::from_secs),\n            num_retries,\n            retry_schedule: retry_schedule_ms\n                .map(|sched| sched.into_iter().map(Duration::from_millis).collect()),\n            proxy_address,\n        }\n    }\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum SenderOutputOpts {\n    Svix(SvixSenderOutputOpts),\n}\n\n#[derive(Debug, Deserialize)]\npub struct SvixSenderOutputOpts {\n    /// Svix API token for the client.\n    pub token: String,\n    /// Options for the Svix client.\n    #[serde(default)]\n    pub options: Option<SvixOptions>,\n}\n\n/// Senders convert messages into Create Message API calls so the JSON pulled out of message queues\n/// or produced by transformations need to conform to this shape.\n#[derive(Clone, Deserialize, Serialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct CreateMessageRequest {\n    pub app_id: String,\n    pub message: MessageIn,\n}\n\n/// Receivers convert HTTP bodies into messages forwarded to (currently only) message queues, etc.\n///\n/// The `payload` field represents the message body given to the producer, and other fields may be\n/// added in the future allowing transformations to dynamically customize the producer behavior.\n#[derive(Clone, Deserialize, Serialize)]\npub struct ForwardRequest {\n    /// This is the payload that will be fed into a Receiver Output\n    // XXX: right now I think any arbitrary json value can work, but individual outputs may have\n    // more strict requirements.\n    // The fact this is repre<|fim_middle|>", "completion": "impl<S> From<S> for TransformationConfig\nwhere\n    S: Into<String>,\n{\n    fn from(value: S) -> Self {\n        Self::ImplicitJson(value.into())\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-types/src/lib.rs", "node_type": "impl_item", "line_range": [54, 61]}
{"prompt": "<|fim_prefix|>s_disable_tls_verification,\n        cfg.proxy_config.as_ref(),\n    );\n\n    tokio::spawn(\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_millis(500));\n            loop {\n                interval.tick().await;\n                let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                if num_workers > 0 {\n                    tracing::info!(\"{} active workers\", num_workers);\n                }\n            }\n        }\n        .instrument(tracing::error_span!(\n            \"worker_monitor\",\n            instance_id = tracing::field::Empty\n        )),\n    );\n\n    loop {\n        if task_limit > 0 {\n            let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n            if num_workers > task_limit.into() {\n                tokio::time::sleep(Duration::from_millis(100)).await;\n                continue;\n            }\n        }\n\n        if crate::is_shutting_down() {\n            tokio::join!(async move {\n                let mut interval = tokio::time::interval(Duration::from_millis(500));\n                loop {\n                    interval.tick().await;\n                    let num_workers = NUM_WORKERS.load(Ordering::Relaxed);\n                    if num_workers > 0 {\n                        tracing::info!(\n                            \"{} active workers, waiting to shut down worker.\",\n                            num_workers\n                        );\n                    } else {\n                        tracing::info!(\"No active workers, shutting down worker.\");\n                        break;\n                    }\n                }\n            });\n            break;\n        }\n\n        match queue_rx.receive_all(recv_deadline).await {\n            Ok(batch) => {\n                for delivery in batch {\n                    let cfg = cfg.clone();\n                    let cache = cache.clone();\n                    let db = db.clone();\n                    let queue_tx = queue_tx.clone();\n                    let queue_task = delivery.task.clone();\n                    let op_webhook_sender = op_webhook_sender.clone();\n                    let webhook_client = webhook_client.clone();\n\n                    tokio::spawn(async move {\n                        NUM_WORKERS.fetch_add(1, Ordering::Relaxed);\n                        let worker_context = WorkerContext {\n                            cfg: &cfg,\n                            db: &db,\n                            cache: &cache,\n                            op_webhook_sender: &op_webhook_sender,\n                            queue_tx: &queue_tx,\n                            webhook_client: &webhook_client,\n                        };\n\n                        let queue_task =\n                            Arc::try_unwrap(queue_task).unwrap_or_else(|arc| (*arc).clone());\n                        if process_queue_task(worker_context, queue_task)\n                            .await\n                            .is_err()\n                        {\n                            if let Err(err) = delivery.nack().await {\n                                tracing::error!(\n                                    \"Error sending 'nack' to Redis after task execution error: {}\",\n                                    err\n                                );\n                            }\n                        } else if let Err(err) = delivery.ack().await {\n                            tracing::error!(\n                                \"Error sending 'ack' to Redis after successful task execution: {}\",\n                                err\n                            );\n                        }\n\n                        NUM_WORKERS.fetch_sub(1, Ordering::Relaxed);\n                    });\n                }\n            }\n            Err(err) => {\n                tracing::error!(\"Error receiving task: {:?}\", err);\n                sleep(tokio::time::Duration::from_millis(10)).await;\n            }\n        }\n\n        update_last_poll_time().await;\n    }\n\n    Ok(())\n}\n\n#[cfg(test)]\nmod tests {\n    use std::collections::HashMap;\n\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    u<|fim_suffix|>    use ed25519_compact::Signature;\n\n    use super::{bytes_to_string, generate_msg_headers, sign_msg, CaseSensitiveHeaderMap};\n    use crate::core::{\n        cryptography::{AsymmetricKey, Encryption},\n        types::{BaseId, EndpointHeaders, EndpointSecret, EndpointSecretInternal, MessageId},\n    };\n\n    // [`generate_msg_headers`] tests\n    const TIMESTAMP: i64 = 1;\n    const WHITELABEL_HEADERS: bool = false;\n    const BODY: &str = \"{\\\"test\\\": \\\"body\\\"}\";\n    const ENDPOINT_SIGNING_KEYS: &[&EndpointSecretInternal] = &[];\n    const ENDPOINT_URL: &str = \"http://localhost:8071\";\n\n    /// Utility function that returns the default set of headers before configurable header are\n    /// accounted for\n    fn mock_headers() -> (CaseSensitiveHeaderMap, MessageId) {\n        let id = MessageId::new(None, None);\n\n        let signatures = sign_msg(\n            &Encryption::new_noop(),\n            TIMESTAMP,\n            BODY,\n            &id,\n            ENDPOINT_SIGNING_KEYS,\n        );\n\n        (\n            generate_msg_headers(\n                TIMESTAMP,\n                &id,\n                signatures,\n                WHITELABEL_HEADERS,\n                None,\n                ENDPOINT_URL,\n            )\n            .unwrap(),\n            id,\n        )\n    }\n\n    #[test]\n    fn test_generate_msg_headers() {\n        // The headers to be given to [`generate_msg_headers`]\n        let mut headers = HashMap::new();\n        headers.insert(\"test_key\".to_owned(), \"value\".to_owned());\n\n        // The invalid key should be skipped over so it is not included in the expected\n        let (mut expected, id) = mock_headers();\n        let _ = expected.insert(\"test_key\".to_owned(), \"value\".parse().unwrap());\n\n        let signatures = sign_msg(\n            &Encryption::new_noop(),\n            TIMESTAMP,\n            BODY,\n            &id,\n            ENDPOINT_SIGNING_KEYS,\n        );\n\n        let actual = generate_msg_headers(\n            TIMESTAMP,\n            &id,\n            signatures,\n            WHITELABEL_HEADERS,\n            Some(&EndpointHeaders(headers)),\n            ENDPOINT_URL,\n        )\n        .unwrap();\n\n        assert_eq!(expected, actual);\n    }\n\n    // Tests endpoint signing keys -- expected values are fetched from the Svix documentation for a\n    // direct comparison to the current implementation.\n    #[test]\n    fn test_generate_msg_headers_with_signing_key() {\n        let test_timestamp = 1614265330;\n        let test_body = \"{\\\"test\\\": 2432232314}\";\n        let test_key = EndpointSecretInternal::from_endpoint_secret(\n            EndpointSecret::Symmetric(STANDARD.decode(\"MfKQ9r8GKYqrTwjUPD8ILPZIo2LaLaSw\").unwrap()),\n            &Encryption::new_noop(),\n        )\n        .unwrap();\n        let test_message_id = MessageId(\"msg_p5jXN8AQM9LWM0D4loKWxJek\".to_owned());\n\n        let expected_signature_str = \"v1,g0hM9SsE+OTPJTGt/tmIKtSyZlE3uFJELVlNIOLJ1OE=\";\n\n        let signatures = sign_msg(\n            &Encryption::new_noop(),\n            test_timestamp,\n            test_body,\n            &test_message_id,\n            &[&test_key],\n        );\n\n        let actual = generate_msg_headers(\n            test_timestamp,\n            &test_message_id,\n            signatures,\n            WHITELABEL_HEADERS,\n            None,\n            ENDPOINT_URL,\n        )\n        .unwrap();\n\n        assert_eq!(\n            actual.get(\"svix-signature\").unwrap(),\n            expected_signature_str\n        );\n    }\n\n    // Tests asymmetric signing keys\n    #[test]\n    fn test_asymmetric_key_signing() {\n        let timestamp = 1614265330;\n        let body = \"{\\\"test\\\": 2432232314}\";\n        let asym_key = AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap();\n        let test_key = EndpointSecretInternal::from_endpoint_secret(\n            EndpointSecret::Asymmetric(asym_key.clone()),\n            &Encryption::new_noop(),\n        )\n        .unwrap();\n        let msg_id = MessageId(\"msg_p5jXN8AQM9LWM0D4loKWxJek\".to_owned());\n\n        let signatures = sign_msg(\n            &Encryption::new_no<|fim_middle|>", "completion": "use bytes::Bytes;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "use_declaration", "line_range": [1065, 1065]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct MessagePollerPollOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filters messages sent with this event type (optional).\n    #[arg(long)]\n    pub event_type: Option<String>,\n    /// Filters messages sent with this channel (optional).\n    #[arg(long)]\n    pub channel: Option<String>,\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n}\n\nimpl From<MessagePollerPollOptions> for svix::api::MessagePollerPollOptions {\n    fn from(value: MessagePollerPollOptions) -> Self {\n        let MessagePollerPollOptions {\n            limit,\n            iterator,\n            event_type,\n            channel,\n            after,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            event_type,\n            channel,\n            after: after.map(|dt| dt.to_rfc3339()),\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessagePollerConsumerPollOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n}\n\nimpl From<MessagePollerConsumerPollOptions> for svix::api::MessagePollerConsumerPollOptions {\n    fn from(value: MessagePollerConsumerPollOptions) -> Self {\n        let MessagePollerConsumerPollOptions { limit, iterator } = value;\n        Self { limit, iterator }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessagePollerConsumerSeekOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\n<|fim_suffix|>\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct MessagePollerArgs {\n    #[command(subcommand)]\n    pub command: MessagePollerCommands,\n}\n\n#[derive(Subcommand)]\npub enum MessagePollerCommands {\n    /// Reads the stream of created messages for an application, filtered on the Sink's event types and Channels.\n    Poll {\n        app_id: String,\n        sink_id: String,\n        #[clap(flatten)]\n        options: MessagePollerPollOptions,\n    },\n    /// Reads the stream of created messages for an application, filtered on the Sink's event types and\n    /// Channels, using server-managed iterator tracking.\n    ConsumerPoll {\n        app_id: String,\n        sink_id: String,\n        consumer_id: String,\n        #[clap(flatten)]\n        options: MessagePollerConsumerPollOptions,\n    },\n    /// Sets the starting offset for the consumer of a polling endpoint.\n    ConsumerSeek {\n        app_id: String,\n        sink_id: String,\n        consumer_id: String,\n        polling_endpoint_consumer_seek_in: crate::json::JsonOf<PollingEndpointConsumerSeekIn>,\n        #[clap(flatten)]\n        options: MessagePollerConsumerSeekOptions,\n    },\n}\n\nimpl MessagePollerCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Poll {\n                app_id,\n                sink_id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .poll(app_id, sink_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ConsumerPoll {\n                app_id,\n                sink_id,\n                consumer_id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .consumer_poll(app_id, sink_id, consumer_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ConsumerSeek {\n                app_id,\n                sink_id,\n                consumer_id,\n                polling_endpoint_consumer_seek_in,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .poller()\n                    .consumer_seek(\n                        app_id,\n                        sink_id,\n                        consumer_id,\n                        polling_endpoint_consumer_seek_in.into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "impl From<MessagePollerConsumerSeekOptions> for svix::api::MessagePollerConsumerSeekOptions {\n    fn from(value: MessagePollerConsumerSeekOptions) -> Self {\n        let MessagePollerConsumerSeekOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/message_poller.rs", "node_type": "impl_item", "line_range": [65, 70]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct IntegrationIn {\n    /// The set of feature flags the integration will have access to.\n    #[serde(rename = \"featureFlags\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub feature_flags: Option<Vec<String>>,\n\n    pub name: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl IntegrationIn {\n    pub fn new(name: String) -> Self {\n        Self {\n            feature_flags: None,\n            name,\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/integration_in.rs", "node_type": "impl_item", "line_range": [14, 21]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    borrow::Cow,\n    collections::HashSet,\n    error::Error as StdError,\n    ops::Deref,\n    sync::LazyLock,\n    time::{SystemTime, UNIX_EPOCH},\n};\n\nu<|fim_suffix|>use axum::{\n    async_trait,\n    extract::{\n        rejection::{BytesRejection, FailedToBufferBody},\n        FromRequest, FromRequestParts, Query, Request,\n    },\n    response::IntoResponse,\n};\nuse chrono::{DateTime, Utc};\nuse http::{request::Parts, StatusCode};\nuse regex::Regex;\nuse schemars::JsonSchema;\nuse sea_orm::{ColumnTrait, QueryFilter, QueryOrder, QuerySelect};\nuse serde::{de::DeserializeOwned, Deserialize, Serialize};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::types::{\n        ApplicationIdOrUid, BaseId, EndpointIdOrUid, EventTypeName, EventTypeNameSet,\n        MessageAttemptId, MessageIdOrUid,\n    },\n    error::{Error, HttpError, Result, ValidationErrorItem},\n};\n\npub mod patch;\nuse patch::UnrequiredField;\n\nconst fn default_limit() -> PaginationLimit {\n    PaginationLimit(50)\n}\n\nconst PAGINATION_LIMIT_CAP_HARD: bool = true;\nconst PAGINATION_LIMIT_CAP_LIMIT: u64 = 250;\nstatic PAGINATION_LIMIT_ERROR: LazyLock<String> =\n    LazyLock::new(|| format!(\"Given limit must not exceed {PAGINATION_LIMIT_CAP_LIMIT}\"));\n\nstatic FUTURE_QUERY_LIMIT: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::hours(1));\nstatic LIMITED_QUERY_DURATION: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::days(90));\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct PaginationDescending<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct Pagination<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Clone, Debug, JsonSchema)]\n#[schemars(transparent)]\npub struct PaginationLimit(pub u64);\n\nimpl<'de> Deserialize<'de> for PaginationLimit {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let limit = u64::deserialize(deserializer)?;\n\n        // Want hard limits to stay the same so they can be validated\n        if !PAGINATION_LIMIT_CAP_HARD && limit > PAGINATION_LIMIT_CAP_LIMIT {\n            Ok(PaginationLimit(PAGINATION_LIMIT_CAP_LIMIT))\n        } else {\n            Ok(PaginationLimit(limit))\n        }\n    }\n}\n\nimpl Validate for PaginationLimit {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        let mut errs = validator::ValidationErrors::new();\n\n        if self.0 > PAGINATION_LIMIT_CAP_LIMIT {\n            errs.add(\n                \"limit\",\n                validation_error(Some(\"pagination\"), Some(&PAGINATION_LIMIT_ERROR)),\n            );\n        }\n\n        if errs.is_empty() {\n            Ok(())\n        } else {\n            Err(errs)\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum ReversibleIterator<T: Validate> {\n    /// Regular iteration - backwards in time.\n    Normal(T),\n    /// Reversed iteration - forwards in time.\n    Prev(T),\n}\n\nimpl<T: Validate> ReversibleIterator<T> {\n    pub(crate) fn direction(&self) -> IteratorDirection {\n        match self {\n            Self::Normal(_) => IteratorDirection::Normal,\n            Self::Prev(_) => IteratorDirection::Prev,\n        }\n    }\n}\n\nimpl<'de, T: 'static + Deserialize<'de> + Validate + From<String>> Deserialize<'de>\n    for ReversibleIterator<T>\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).map(|s| {\n            if let Some(s) = s.strip_prefix('-') {\n                ReversibleIterator::Prev(T::from(s.to_owned()))\n            } else {\n                ReversibleIterator::Normal(T::from(s))\n            }\n        })\n    }\n}\n\nimpl<T: Validate> Validate for ReversibleIterator<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            ReversibleIterator::Normal(val) => val.validate(),\n            ReversibleIterator::Prev(val) => val.validate(),\n        }\n    }\n}\n\nimpl<T: Validate + JsonSchema> JsonSchema for ReversibleIterator<T> {\n    fn schema_name() -> String {\n        format!(\"ReversibleIterator_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        T::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\n/// Applies sorting and filtration to a query from its iterator, sort column, and limit\n/// queries based on time\n/// Our rules for limiting queries are as follows\n///\n/// If `before` is passed:\n/// * lower limit on query is `before - LIMITED_QUERY_DURATION`\n/// * upper limit is `before`\n///\n/// If `after` is passed:\n/// * lower limit is `after`\n/// * upper limit is `now + FUTURE_QUERY_LIMIT`\n///\n/// If prev-iterator is passed:\n/// * lower limit is `prev-iterator`\n/// * upper limit is `prev-iterator + LIMITED_QUERY_DURATION`\n///\n/// If (normal) iterator is passed:\n/// * lower limit is `iterator - LIMITED_QUERY_DURATION`\n/// * upper limit is `iterator`\n///\n/// If no iterator is passed:\n/// * lower limit is `now() - LIMITED_QUERY_DURATION` if\n///   neither `before` nor `after` were passed\npub(crate) fn filter_and_paginate_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }\n\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket limit on future\n        .filter(sort_column.lt(I::start_id(future_limit)));\n\n    (query, iter_direction)\n<|fim_middle|>", "completion": "use aide::{\n    transform::{TransformOperation, TransformPathItem},\n    OperationInput, OperationIo, OperationOutput,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "use_declaration", "line_range": [13, 16]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct AuthenticationAppPortalAccessOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationAppPortalAccessOptions>\n    for svix::api::AuthenticationAppPortalAccessOptions\n{\n    fn from(value: AuthenticationAppPortalAccessOptions) -> Self {\n        let AuthenticationAppPortalAccessOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationExpireAllOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationExpireAllOptions> for svix::api::AuthenticationExpireAllOptions {\n    fn from(value: AuthenticationExpireAllOptions) -> Self {\n        let AuthenticationExpireAllOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationLogoutOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationLogoutOptions> for svix::api::AuthenticationLogoutOptions {\n    <|fim_suffix|>\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationStreamPortalAccessOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationStreamPortalAccessOptions>\n    for svix::api::AuthenticationStreamPortalAccessOptions\n{\n    fn from(value: AuthenticationStreamPortalAccessOptions) -> Self {\n        let AuthenticationStreamPortalAccessOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationRotateStreamPollerTokenOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationRotateStreamPollerTokenOptions>\n    for svix::api::AuthenticationRotateStreamPollerTokenOptions\n{\n    fn from(value: AuthenticationRotateStreamPollerTokenOptions) -> Self {\n        let AuthenticationRotateStreamPollerTokenOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct AuthenticationArgs {\n    #[command(subcommand)]\n    pub command: AuthenticationCommands,\n}\n\n#[derive(Subcommand)]\npub enum AuthenticationCommands {\n    /// Use this function to get magic links (and authentication codes) for connecting your users to the Consumer Application Portal.\n    AppPortalAccess {\n        app_id: String,\n        app_portal_access_in: Option<crate::json::JsonOf<AppPortalAccessIn>>,\n        #[clap(flatten)]\n        options: AuthenticationAppPortalAccessOptions,\n    },\n    /// Expire all of the tokens associated with a specific application.\n    ExpireAll {\n        app_id: String,\n        application_token_expire_in: Option<crate::json::JsonOf<ApplicationTokenExpireIn>>,\n        #[clap(flatten)]\n        options: AuthenticationExpireAllOptions,\n    },\n    /// Logout an app token.\n    ///\n    /// Trying to log out other tokens will fail.\n    Logout {\n        #[clap(flatten)]\n        options: AuthenticationLogoutOptions,\n    },\n    /// Use this function to get magic links (and authentication codes) for connecting your users to the Stream Consumer Portal.\n    StreamPortalAccess {\n        stream_id: String,\n        stream_portal_access_in: Option<crate::json::JsonOf<StreamPortalAccessIn>>,\n        #[clap(flatten)]\n        options: AuthenticationStreamPortalAccessOptions,\n    },\n    /// Get the current auth token for the stream poller.\n    GetStreamPollerToken { stream_id: String, sink_id: String },\n    /// Create a new auth token for the stream poller API.\n    RotateStreamPollerToken {\n        stream_id: String,\n        sink_id: String,\n        rotate_poller_token_in: Option<crate::json::JsonOf<RotatePollerTokenIn>>,\n        #[clap(flatten)]\n        options: AuthenticationRotateStreamPollerTokenOptions,\n    },\n}\n\nimpl AuthenticationCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::AppPortalAccess {\n                app_id,\n                app_portal_access_in,\n                options,\n            } => {\n                let resp = client\n                    .authentication()\n                    .app_portal_access(\n                        app_id,\n                        app_portal_access_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ExpireAll {\n                app_id,\n                application_token_expire_in,\n                options,\n            } => {\n                client\n                    .authentication()\n                    .expire_all(\n                        app_id,\n                        application_token_expire_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n            Self::Logout { options } => {\n                client.authentication().logout(Some(options.into())).await?;\n            }\n            Self::StreamPortalAccess {\n                stream_id,\n                stream_portal_access_in,\n                options,\n            } => {\n                let resp = client\n                    .authentication()\n                    .stream_portal_access(\n                        stream_id,\n                        stream_portal_access_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::GetStreamPollerToken { stream_id, sink_id } => {\n                let resp = client\n                    .authentication()\n                    .get_stream_poller_token(stream_id, sink_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::RotateStreamPollerToken {\n                stream_id,\n                sink_id,\n                rotate_poller_token_in,\n                options,\n            } => {\n                let resp = client\n                    .authentication()\n                    .rotate_stream_poller_token(\n                        stream_id,\n                        sink_id,\n                        rotate_poller_token_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "fn from(value: AuthenticationLogoutOptions) -> Self {\n        let AuthenticationLogoutOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/authentication.rs", "node_type": "function_item", "line_range": [40, 43]}
{"prompt": "<|fim_prefix|>use std::{convert::Infallible, net::SocketAddr, sync::Arc, time::Duration};\n\nuse axum::{\n    extract::{FromRequestParts, Path, State},\n    http::{self, request},\n    routing::{get, post},\n    Router,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::{\n        api::{MessagePollerConsumerPollOptions, PollingEndpointMessageOut, Svix},\n        error::Error,\n    },\n    ForwardRequest, PollerInput, ReceiverOutput, TransformationConfig, TransformerInput,\n    TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\nuse tracing::instrument;\nuse types::{IntegrationId, IntegrationState, InternalState, SerializableRequest, Unvalidated};\n\nuse crate::{\n    config::{PollerInputOpts, PollerReceiverConfig, WebhookReceiverConfig},\n    webhook_receiver::types::SerializablePayload,\n};\n\nmod config;\nmod types;\nmod verification;\n\nfn router() -> Router<InternalState> {\n    Router::new()\n        .route(\n            \"/webhook/:integration_id\",\n            post(route).put(route).get(route).patch(route),\n        )\n        .route(\n            \"/webhook/:integration_id/\",\n            post(route).put(route).get(route).patch(route),\n        )\n        .route(\"/health\", get(health_handler))\n}\nstatic START_TIME: once_cell::sync::Lazy<std::time::Instant> =\n    once_cell::sync::Lazy::new(std::time::Instant::now);\n\nfn get_uptime_seconds() -> u64 {\n    START_TIME.elapsed().as_secs()\n}\n#[derive(serde::Serialize)]\nstruct HealthResponse {\n    pub status: &'static str,\n    pub version: &'static str,\n    pub uptime: u64,\n}\nasync fn health_handler() -> impl axum::response::IntoResponse {\n    let health_response = HealthResponse {\n        status: \"OK\",\n        version: env!(\"CARGO_PKG_VERSION\"),\n        uptime: get_uptime_seconds(),\n    };\n    axum::Json(health_response)\n}\npub async fn run(\n    listen_addr: SocketAddr,\n    routes: Vec<WebhookReceiverConfig>,\n    transformer_tx: TransformerTx,\n) -> std::io::Result<()> {\n    once_cell::sync::Lazy::force(&START_TIME);\n    let state = InternalState::from_receiver_configs(routes, transformer_tx)\n        .await\n        .map_err(std::io::Error::other)?;\n\n    <|fim_suffix|>\n\n    tracing::info!(\"Listening on: {listen_addr}\");\n    let listener = tokio::net::TcpListener::bind(listen_addr).await.unwrap();\n    axum::serve(listener, router)\n        .await\n        .map_err(std::io::Error::other)\n}\n\nstruct WebhookIdHeader(Option<String>);\n\n#[async_trait]\nimpl<S> FromRequestParts<S> for WebhookIdHeader {\n    type Rejection = Infallible;\n\n    async fn from_request_parts(\n        parts: &mut request::Parts,\n        _: &S,\n    ) -> Result<Self, Self::Rejection> {\n        Ok(Self(\n            parts\n                .headers\n                .get(\"svix-id\")\n                .or_else(|| parts.headers.get(\"webhook-id\"))\n                .and_then(|val| Some(val.to_str().ok()?.to_owned())),\n        ))\n    }\n}\n\n#[instrument(\n    skip_all,\n    level = \"error\",\n    fields(\n        msg_id = _msg_id,\n        integration_id = integration_id.as_ref(),\n    )\n)]\nasync fn route(\n    Path(integration_id): Path<IntegrationId>,\n    WebhookIdHeader(_msg_id): WebhookIdHeader,\n    State(InternalState {\n        routes,\n        transformer_tx,\n    }): State<InternalState>,\n    req: SerializableRequest<Unvalidated>,\n) -> Result<http::StatusCode, http::StatusCode> {\n    let IntegrationState {\n        verifier,\n        output,\n        transformation,\n    } = routes\n        .get(&integration_id)\n        .ok_or(http::StatusCode::NOT_FOUND)?;\n\n    let req = req.validate(verifier).await.inspect_err(|code| {\n        tracing::warn!(\"validation failed: {code}\");\n    })?;\n\n    let payload = parse_payload(\n        req.payload(),\n        transformation.as_ref(),\n        transformer_tx.clone(),\n    )\n    .await?;\n\n    handle(payload, Arc::clone(output)).await\n}\n\n// FIXME: Really odd return type - artifact of being extracted from the HTTP server\nasync fn handle(\n    payload: ForwardRequest,\n    output: Arc<Box<dyn ReceiverOutput>>,\n) -> Result<http::StatusCode, http::StatusCode> {\n    tracing::debug!(\"forwarding request\");\n    Ok(match output.handle(payload).await {\n        Ok(_) => http::StatusCode::NO_CONTENT,\n        Err(e) => {\n            tracing::error!(\"Error forwarding request: {}\", e);\n            http::StatusCode::INTERNAL_SERVER_ERROR\n        }\n    })\n}\n\n/// Figures out how to build a JSON object from the payload, optionally running it through a\n/// transformation.\n///\n/// WRT \"raw\" payloads, the return value here is going to be a JSON object regardless of whether\n/// or not the queue producer wants \"raw\" data.\n///\n/// When there's no transformation defined we therefore attempt to parse the body as json.\n/// When a transformation is defined, we branch to see if it expects string or json input.\n///\n/// For either case, we expect the value produced to match the schema of a [`ForwardRequest`].\nasync fn parse_payload(\n    payload: &SerializablePayload,\n    transformation: Option<&TransformationConfig>,\n    transformer_tx: TransformerTx,\n) -> Result<ForwardRequest, http::StatusCode> {\n    match transformation {\n        Some(xform) => {\n            let input = match xform.format() {\n                TransformerInputFormat::String => {\n                    TransformerInput::String(payload.as_string().map_err(|_| {\n                        tracing::error!(\"Unable to parse request body as string\");\n                        http::StatusCode::BAD_REQUEST\n                    })?)\n                }\n                TransformerInputFormat::Json => {\n                    TransformerInput::Json(payload.as_json().map_err(|_| {\n                        tracing::error!(\"Unable to parse request body as json\");\n                        http::StatusCode::BAD_REQUEST\n                    })?)\n                }\n            };\n            transform(input, xform.source().clone(), transformer_tx).await\n        }\n        // Keep the original payload as-is if there's no transformation specified, but stuff the\n        // whole thing into the payload field.\n        // The as_json() only gets us to `Value`, so we also need a `from_value` call to marshal\n        // into a [`ForwardRequest`] type.\n        None => Ok(ForwardRequest {\n            payload: payload.as_json().map_err(|_| {\n                tracing::error!(\"Unable to parse request body as json\");\n                http::StatusCode::BAD_REQUEST\n            })?,\n        }),\n    }\n}\n\n/// Attempts to run the payload through a js transformation.\nasync fn transform(\n    input: TransformerInput,\n    script: String,\n    tx: TransformerTx,\n) -> Result<ForwardRequest, http::StatusCode> {\n    let (job, callback) = TransformerJob::new(script, input);\n    if let Err(e) = tx.send(job) {\n        tracing::error!(\"transformations are not available: {}\", e);\n        return Err(http::StatusCode::INTERNAL_SERVER_ERROR);\n    }\n\n    match callback.await {\n        // This is the only \"good\" outcome giving a RHS value for the assignment.\n        // All other match arms should bail with a non-2xx status.\n        Ok(Ok(TransformerOutput::Object(obj))) => Ok(serde_json::from_value(\n            serde_json::Value::Object(obj),\n        )\n        .map_err(|e| {\n            tracing::error!(\"transformation produced invalid payload: {}\", e);\n            http::StatusCode::INTERNAL_SERVER_ERROR\n        })?),\n        Ok(Ok(TransformerOutput::Invalid)) => {\n            tracing::error!(\"transformation produced invalid payload\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n        _ => {\n            tracing::error!(\"transformation failed\");\n            Err(http::StatusCode::INTERNAL_SERVER_ERROR)\n        }\n    }\n}\n\nstruct SvixEventsPoller {\n    name: String,\n    input_opts: PollerInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n    output: Arc<Box<dyn ReceiverOutput>>,\n}\n\n#[async_trait]\nimpl PollerInput for SvixEventsPoller {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        run_inner(self).await\n    }\n}\n\nimpl PollerReceiverConfig {\n    pub async fn into_<|fim_middle|>", "completion": "let router = router().with_state(state);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/mod.rs", "node_type": "let_declaration", "line_range": [72, 72]}
{"prompt": "<|fim_prefix|>use std::{\n    collections::HashSet,\n    net::TcpListener,\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse http::StatusCode;\nuse serde::de::IgnoredAny;\nuse svix_server::{\n    cfg::{ProxyAddr, ProxyBypassCfg, ProxyConfig},\n    v1::endpoints::message::MessageIn,\n};\nuse tokio::{\n    io::{AsyncReadExt, AsyncWriteExt},\n    time::timeout,\n};\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg, TestClient, TestReceiver,\n};\n\n#[ignore] // works with microsocks running at the specified address\n#[tokio::test]\nasync fn test_message_delivery_via_socks5() {\n    use crate::utils::start_svix_server_with_cfg;\n\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(socks_proxy_config());\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n    run_proxy_test(&client).await;\n}\n\nfn socks_proxy_config() -> ProxyConfig {\n    ProxyConfig {\n        addr: ProxyAddr::new(\"socks5://localhost:1080\").unwrap(),\n        noproxy: None,\n    }\n}\n\n#[ignore] // works with tinyproxy running at the specified address\n#[tokio::test]\nasync fn test_message_delivery_via_http_proxy() {\n    use crate::utils::start_svix_server_with_cfg;\n\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(http_proxy_config());\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n    run_proxy_test(&client).await;\n}\n\nfn http_proxy_config() -> ProxyConfig {\n    ProxyConfig {\n        addr: ProxyAddr::new(\"http://localhost:8888\").unwrap(),\n        noproxy: None,\n    }\n}\n\nasync fn run_proxy_test(client: &TestClient) {\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let app_id = create_test_app(client, \"proxyTest\").await.unwrap().id;\n    create_test_endpoint(client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let msg_payload = serde_json::json!({ \"test\": \"value\" });\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let received_payload = timeout(Duration::from_secs(2), receiver.data_recv.recv())\n        .await\n        .unwrap()\n        .unwrap();\n\n    assert_eq!(received_payload, msg_payload);\n}\n\n// This doesn't actually handle requests successfully, but it does allow us\n// to see which hostnames are requested of it.\nstruct MockProxyServer {\n    matched_hosts: Arc<Mutex<HashSet<String>>>,\n    addr: String,\n    variant: MockProxyVariant,\n}\n\nenum MockProxyVariant {\n    Http,\n    Socks5,\n}\n\nimpl MockProxyServer {\n    pub fn new(variant: MockProxyVariant) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n        let addr = match variant {\n            MockProxyVariant::Http => {\n                format!(\"http://{}\", listener.local_addr().unwrap())\n            }\n            MockProxyVariant::Socks5 => {\n                format!(\"socks5://{}\", listener.local_addr().unwrap())\n            }\n        };\n        let matched_hosts = Arc::new(Mutex::new(HashSet::new()));\n\n        match variant {\n            MockProxyVariant::Http => {\n                tokio::spawn(Self::http_listener(listener, matched_hosts.clone()))\n            }\n            MockProxyVariant::Socks5 => {\n                tokio::spawn(Self::socks5_listener(listener, matched_hosts.clone()))\n            }\n        };\n\n        Self {\n            matched_hosts,\n            addr,\n            variant,\n        }\n    }\n\n    pub async fn http_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        loop {\n            let (mut stream, _addr) = listener.accept().await.unwrap();\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                <|fim_suffix|>\n\n                if let Ok(size) = stream.read(&mut buffer).await {\n                    if size == 0 {\n                        return;\n                    }\n                    let request = String::from_utf8_lossy(&buffer[..size]);\n                    if let Some(host) = request\n                        .strip_prefix(\"CONNECT \")\n                        .and_then(|s| s.split(' ').next())\n                        .and_then(|s| s.strip_suffix(\":443\"))\n                    {\n                        let mut guard = matched_hosts.lock().unwrap();\n                        guard.insert(host.to_string());\n                    }\n                }\n            });\n        }\n    }\n\n    pub async fn socks5_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        use socks5_proto::{\n            handshake::{\n                Method as HandshakeMethod, Request as HandshakeRequest,\n                Response as HandshakeResponse,\n            },\n            Address, Reply, Request as SocksRequest, Response as SocksResponse,\n        };\n        loop {\n            let (mut stream, _) = match listener.accept().await {\n                Ok(v) => v,\n                Err(_) => continue,\n            };\n\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let hs_req = match HandshakeRequest::read_from(&mut stream).await {\n                    Ok(req) => req,\n                    Err(_) => {\n                        return;\n                    }\n                };\n\n                if hs_req.methods.contains(&HandshakeMethod::NONE) {\n                    if HandshakeResponse::new(HandshakeMethod::NONE)\n                        .write_to(&mut stream)\n                        .await\n                        .is_err()\n                    {\n                        return;\n                    }\n                } else {\n                    let _ = HandshakeResponse::new(HandshakeMethod::UNACCEPTABLE)\n                        .write_to(&mut stream)\n                        .await;\n                    return;\n                }\n\n                let Ok(socks_req) = SocksRequest::read_from(&mut stream).await else {\n                    return;\n                };\n\n                let host = match &socks_req.address {\n                    Address::SocketAddress(socket_addr) => socket_addr.ip().to_string(),\n                    Address::DomainAddress(domain_bytes, _port) => {\n                        String::from_utf8_lossy(domain_bytes).to_string()\n                    }\n                };\n                if !host.is_empty() {\n                    let mut guard = matched_hosts.lock().unwrap();\n                    guard.insert(host);\n                }\n\n                let abort_resp =\n                    SocksResponse::new(Reply::ConnectionNotAllowed, Address::unspecified());\n                let _ = abort_resp.write_to(&mut stream).await;\n                let _ = stream.shutdown().await;\n            });\n        }\n    }\n\n    pub fn matches(&self) -> HashSet<String> {\n        let guard = self.matched_hosts.lock().unwrap();\n        println!(\"************ MATCHES {guard:?}\");\n        guard.clone()\n    }\n}\n\n#[tokio::test]\nasync fn test_http_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Http);\n    test_proxy_exceptions(listener).await\n}\n\n#[tokio::test]\nasync fn test_socks5_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Socks5);\n    test_proxy_exceptions(listener).await\n}\n\nasync fn test_proxy_exceptions(listener: MockProxyServer) {\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(ProxyConfig {\n        addr: ProxyAddr::new(listener.addr.clone()).unwrap(),\n        noproxy: Some(ProxyBypassCfg(\"10.0.0.0/8, 8.8.8.8, 0ec2:1652:6021:693b:f928:565d:5a0e:de9f, www.svix.com, .google.com\".to_owned())),\n    });\n    cfg.retry_schedule = vec![];\n\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"proxyTest\").await.unwrap().id;\n\n    // Note: Real hostnames here are not ideal, but difficult to test SOCKS without valid <|fim_middle|>", "completion": "let mut buffer = [0; 512];", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_proxy.rs", "node_type": "let_declaration", "line_range": [140, 140]}
{"prompt": "<|fim_prefix|>//! Use the `testing-docker-compose.yml` in the repo root to run the dependencies for testing,\n//! including the gcloud pubsub emulator.\n//!\n//! Use `run-tests.sh` to use the requisite environment for testing.\n\nuse std::time::Duration;\n\nuse gcloud_googleapis::pubsub::v1::{DeadLetterPolicy, PubsubMessage};\nuse gcloud_pubsub::{\n    client::{Client, ClientConfig},\n    subscription::{Subscription, SubscriptionConfig},\n    topic::Topic,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_queue::{\n    config::{GcpPubSubInputOpts, QueueInputOpts},\n    sender_input::QueueSender,\n};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nconst DEFAULT_PUBSUB_EMULATOR_HOST: &str = \"localhost:8085\";\n\nfn get_test_plugin(\n    svix_url: String,\n    subscription_id: String,\n    use_transformation: Option<TransformerInputFormat>,\n) -> QueueSender {\n    QueueSender::new(\n        \"test\".into(),\n        QueueInputOpts::GcpPubSub(GcpPubSubInputOpts {\n            subscription_id,\n            credentials_file: None,\n        }),\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n}\n\n<|fim_suffix|>\n\nfn random_chars() -> impl Iterator<Item = char> {\n    std::iter::repeat_with(fastrand::alphanumeric)\n}\n\nasync fn create_test_queue(client: &Client) -> (Topic, Subscription) {\n    let topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();\n    // Need to define a dead letter topic to avoid the \"bad\" test cases from pulling the nacked\n    // messages again and again.\n    let dead_letter_topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();\n    let subscription_name: String = \"subscription-\"\n        .chars()\n        .chain(random_chars().take(8))\n        .collect();\n\n    let topic = client.create_topic(&topic_name, None, None).await.unwrap();\n    let dead_letter_topic = client\n        .create_topic(&dead_letter_topic_name, None, None)\n        .await\n        .unwrap();\n    let subscription = client\n        .create_subscription(\n            &subscription_name,\n            &topic_name,\n            SubscriptionConfig {\n                // Messages published to the topic need to supply a unique ID to make use of this\n                enable_exactly_once_delivery: true,\n                dead_letter_policy: Some(DeadLetterPolicy {\n                    dead_letter_topic: dead_letter_topic.fully_qualified_name().into(),\n                    max_delivery_attempts: MAX_DELIVERY_ATTEMPTS,\n                }),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();\n\n    (topic, subscription)\n}\n\nasync fn publish(topic: &Topic, payload: &str) {\n    let publisher = topic.new_publisher(None);\n    let awaiter = publisher\n        .publish(PubsubMessage {\n            data: payload.to_owned().into_bytes(),\n            message_id: random_chars().take(6).collect(),\n            ..Default::default()\n        })\n        .await;\n    awaiter.get().await.unwrap();\n}\n\n/// General \"pause while we wait for messages to travel\" beat. If you're seeing flakes, bump this up.\nconst WAIT_MS: u64 = 100;\n/// Controls how many times a message can be nack'd before it lands on the dead letter topic.\nconst MAX_DELIVERY_ATTEMPTS: i32 = 5;\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), subscription.id(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&topic, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        // The transformed bit of the payload\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        subscription.id(),\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&topic, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::<|fim_middle|>", "completion": "async fn mq_connection() -> Client {\n    // The `Default` impl for `ClientConfig` looks for this env var. When set it branches for\n    // local-mode use using the addr in the env var and a hardcoded project id of `local-project`.\n    if std::env::var(\"PUBSUB_EMULATOR_HOST\").is_err() {\n        std::env::set_var(\"PUBSUB_EMULATOR_HOST\", DEFAULT_PUBSUB_EMULATOR_HOST);\n    }\n    Client::new(ClientConfig::default()).await.unwrap()\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/gcp_pubsub_consumer.rs", "node_type": "function_item", "line_range": [56, 63]}
{"prompt": "<|fim_prefix|>cription,\n            deleted,\n            deprecated,\n            schemas,\n            feature_flag,\n        } = self;\n\n        model.description = Set(description);\n        model.deleted = Set(deleted);\n        model.deprecated = Set(deprecated);\n        model.schemas = Set(schemas);\n        model.feature_flag = Set(feature_flag);\n    }\n}\n\n#[derive(Deserialize, ModelIn, Serialize, Validate, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\nstruct EventTypePatch {\n    #[serde(default, skip_serializing_if = \"UnrequiredField::is_absent\")]\n    #[validate(custom = \"validate_no_control_characters_unrequired\")]\n    description: UnrequiredField<String>,\n\n    #[serde(\n        default,\n        rename = \"archived\",\n        skip_serializing_if = \"UnrequiredField::is_absent\"\n    )]\n    deleted: UnrequiredField<bool>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredField::is_absent\")]\n    deprecated: UnrequiredField<bool>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredNullableField::is_absent\")]\n    schemas: UnrequiredNullableField<eventtype::Schema>,\n\n    #[serde(default, skip_serializing_if = \"UnrequiredNullableField::is_absent\")]\n    feature_flag: UnrequiredNullableField<FeatureFlag>,\n}\n\nimpl ModelIn for EventTypePatch {\n    type ActiveModel = eventtype::ActiveModel;\n\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EventTypePatch {\n            description,\n            deleted,\n            deprecated,\n            schemas,\n            feature_flag,\n        } = self;\n\n        patch_field_non_nullable!(model, description);\n        patch_field_non_nullable!(model, deleted);\n        patch_field_non_nullable!(model, deprecated);\n        patch_field_nullable!(model, schemas);\n        patch_field_nullable!(model, feature_flag);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EventTypeOut {\n    pub name: EventTypeName,\n    #[schemars(example = \"event_type_description_example\")]\n    pub description: String,\n    #[serde(rename = \"archived\")]\n    #[schemars(example = \"example_event_archived\", default = \"example_event_archived\")]\n    pub deleted: bool,\n    pub deprecated: bool,\n    /// The schema for the event type for a specific version as a JSON schema.\n    #[schemars(example = \"event_type_versioned_schemas_example\")]\n    pub schemas: Option<eventtype::Schema>,\n\n    pub created_at: DateTime<Utc>,\n    pub updated_at: DateTime<Utc>,\n    pub feature_flag: Option<FeatureFlag>,\n}\n\nimpl EventTypeOut {\n    fn without_payload(model: eventtype::Model) -> Self {\n        Self {\n            schemas: None,\n            ..model.into()\n        }\n    }\n}\n\n// FIXME: This can and should be a derive macro\nimpl From<eventtype::Model> for EventTypeOut {\n    fn from(model: eventtype::Model) -> Self {\n        Self {\n            name: model.name,\n            description: model.description,\n            deleted: model.deleted,\n            deprecated: model.deprecated,\n            schemas: model.schemas,\n            feature_flag: model.feature_flag,\n\n            created_at: model.created_at.into(),\n            updated_at: model.updated_at.into(),\n        }\n    }\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct ListFetchQueryParams {\n    /// When `true` archived (deleted but not expunged) items are included in the response\n    #[serde(default)]\n    pub include_archived: bool,\n    /// When `true` the full item (including the schema) is included in the response\n    #[serde(default)]\n    pub with_content: bool,\n}\n\n/// Return the list of event types.\n#[aide_annotate(op_id = \"v1.event-type.list\")]\nasync fn list_event_types(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<Pagination<ReversibleIterator<EventTypeName>>>,\n    fetch_options: ValidatedQuery<ListFetchQueryParams>,\n    permissions::ReadAll {\n        org_id,\n        feature_flags,\n        ..\n    }: permissions::ReadAll,\n) -> Result<Json<ListResponse<EventTypeOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    l<|fim_suffix|>    let iter_direction = iterator\n        .as_ref()\n        .map_or(IteratorDirection::Normal, |iter| iter.direction());\n\n    let mut query = eventtype::Entity::secure_find(org_id);\n\n    if !fetch_options.include_archived {\n        query = query.filter(eventtype::Column::Deleted.eq(false));\n    }\n\n    if let permissions::AllowedFeatureFlags::Some(flags) = feature_flags {\n        query = eventtype::Entity::filter_feature_flags(query, flags);\n    }\n\n    let query = apply_pagination(\n        query,\n        eventtype::Column::Name,\n        limit,\n        iterator,\n        Ordering::Ascending,\n    );\n\n    Ok(Json(EventTypeOut::list_response(\n        query\n            .all(db)\n            .await?\n            .into_iter()\n            .map(|x| {\n                if !fetch_options.with_content {\n                    EventTypeOut::without_payload(x)\n                } else {\n                    x.into()\n                }\n            })\n            .collect(),\n        limit as usize,\n        iter_direction,\n    )))\n}\n\n/// Create new or unarchive existing event type.\n///\n/// Unarchiving an event type will allow endpoints to filter on it and messages to be sent with it.\n/// Endpoints filtering on the event type before archival will continue to filter on it.\n/// This operation does not preserve the description and schemas.\n#[aide_annotate(op_id = \"v1.event-type.create\")]\nasync fn create_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<EventTypeIn>,\n) -> Result<JsonStatus<201, EventTypeOut>> {\n    let evtype = eventtype::Entity::secure_find_by_name(org_id.clone(), data.name.to_owned())\n        .one(db)\n        .await?;\n    let ret = match evtype {\n        Some(evtype) => {\n            if evtype.deleted {\n                let mut evtype: eventtype::ActiveModel = evtype.into();\n                evtype.deleted = Set(false);\n                data.update_model(&mut evtype);\n                evtype.update(db).await?\n            } else {\n                return Err(HttpError::conflict(\n                    Some(\"event_type_exists\".to_owned()),\n                    Some(\"An event_type with this name already exists\".to_owned()),\n                )\n                .into());\n            }\n        }\n        None => {\n            let evtype = eventtype::ActiveModel {\n                org_id: Set(org_id),\n                ..data.into()\n            };\n            evtype.insert(db).await.map_err(http_error_on_conflict)?\n        }\n    };\n    Ok(JsonStatus(ret.into()))\n}\n\n/// Get an event type.\n#[aide_annotate(op_id = \"v1.event-type.get\")]\nasync fn get_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(EventTypeNamePath { event_type_name }): Path<EventTypeNamePath>,\n    permissions::ReadAll {\n        org_id,\n        feature_flags,\n        ..\n    }: permissions::ReadAll,\n) -> Result<Json<EventTypeOut>> {\n    let mut query = eventtype::Entity::secure_find_by_name(org_id, event_type_name);\n    if let permissions::AllowedFeatureFlags::Some(flags) = feature_flags {\n        query = eventtype::Entity::filter_feature_flags(query, flags);\n    }\n    let evtype = query\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    Ok(Json(evtype.into()))\n}\n\n/// Update an event type.\n#[aide_annotate(op_id = \"v1.event-type.update\")]\nasync fn update_event_type(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(EventTypeNamePath { event_type_name }): Path<EventTypeNamePath>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<EventTypeUpdate>,\n) -> Result<JsonStatusUpsert<EventTypeOut>> {\n    let evtype = eventtype::Entity::secure_find_by_name(org_id.clone(), event_type_name.clone())\n        .one(db)\n        .await?;\n\n    match evtype {\n        Some(evtype) => {\n            let mut evtype: eventtype::ActiveModel = evtype.into();\n            data.update_model(&mut evtype);\n            let ret = evtype.update(db).await.map_err(http_error_<|fim_middle|>", "completion": "let iterator = pagination.iterator;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/event_type.rs", "node_type": "let_declaration", "line_range": [246, 246]}
{"prompt": "<|fim_prefix|>100);\n\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test\".to_owned()),\n            app_id: ApplicationId(\"test\".to_owned()),\n            endpoint_id: EndpointId(\"test\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n        p.send(&mt, None).await.unwrap();\n\n        let recv = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv.task, mt);\n        recv.nack().await.unwrap();\n\n        let recv = timeout(\n            Duration::from_millis(500) + delay,\n            c.receive_all(TEST_RECV_DEADLINE),\n        )\n        .await\n        .expect(\"Expected QueueTask\");\n        assert_eq!(*recv.unwrap().pop().unwrap().task, mt);\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_delay() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        let main_queue = \"{test}_delay\";\n        let delayed = \"{test}_delay_delayed\";\n        let lock = \"{test}_delay_delayed_lock\";\n        let dlq = \"{test}_delay_delayed_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(500);\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt1 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test1\".to_owned()),\n            app_id: ApplicationId(\"test1\".to_owned()),\n            endpoint_id: EndpointId(\"test1\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Scheduled,\n            attempt_count: 0,\n        });\n        let mt2 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n\n        p.send(&mt1, Some(Duration::from_millis(2000)))\n            .await\n            .unwrap();\n        p.send(&mt2, None).await.unwrap();\n\n        let recv2 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv2.task, mt2);\n        recv2.ack().await.unwrap();\n\n        let recv1 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv1.task, mt1);\n        recv1.ack().await.unwrap();\n    }\n\n    fn to_redis_key(id: &str, task: &QueueTask) -> String {\n        format!(\"{id}|{}\", serde_json::to_string(task).unwrap())\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_migrations() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        // Test queue name constants\n        let v1_main = \"{test}_migrations_main_v1\";\n        let v2_main = \"{test}_migrations_main_v2\";\n        let v3_main = \"{test}_migrations_main_v3\";\n\n        let v1_processing = \"{test}_migrations_processing_v1\";\n        let v2_processing = \"{test}_migrations_processing_v2\";\n        // v3_processing is the stream pending queue for v3_main\n\n        let v1_delayed = \"{test}_migrations_delayed_v1\";\n        let v2_delayed = \"{test}_migrations_delayed_v2\";\n        let v2_delayed_lock = \"{test}_migrations_delayed_lock_v2\";\n        // v3_delayed doesn not yet exist\n\n        {\n            let mut conn = pool.get().await.unwrap();\n\n            // Clear test keys\n            let _: () = conn\n                .del(&[\n                    v1_main,\n                    v2_main,\n                    v3_main,\n                    v1_processing,\n                    v2_processing,\n                    v1_delayed,\n                    v2_delayed,\n                ])\n                .await\n                .unwrap();\n\n            // Add v3 consumer group\n            <|fim_suffix|>\n\n            // Add v1 data\n            for num in 1..=10 {\n                let _: () = conn\n                    .rpush(\n                        v1_main,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            for num in 11..=15 {\n                let _: () = conn\n                    .zadd(\n                        v1_delayed,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                        Utc::now().timestamp() + 2,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // Move the first five of v1_main to v1_processing\n            for _ in 0..5 {\n                let _: () = conn\n                    .blmove(\n                        v1_main,\n                        v1_processing,\n                        Direction::Left,\n                        Direction::Right,\n                        0.0,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // v1 to v2\n            migrate_list(&mut conn, v1_main, v2_main).await.unwrap();\n            migrate_list(&mut conn, v1_processing, v2_processing)\n                .await\n                .unwrap();\n            migrate_sset(&mut conn, v1_delayed, v2_delayed)\n                .await\n                .unwrap();\n\n            // v2 to v3\n            migrate_list_to_stream(&mut conn, v2_main, v3_main)\n                .await\n                .unwrap();\n            migrate_list_to_stream(&mut conn, v2_processing, v3_main)\n                .await\n                .unwrap();\n        }\n\n        // Read\n        let (_p, mut c) = new_pair_inner(\n            &cfg,\n            Duration::from_secs(5),\n            \"\",\n            v3_main,\n            v2_delayed,\n            v2_delayed_lock,\n            \"dlq-bruh\",\n        )\n        .await;\n\n        // 2 second delay on the delayed and pending queue is inserted after main queue, so first\n        // the 6-10 should appear, then 1-5, then 11-15\n\n        let mut items = c.receive_all(TEST_RECV_DEADLINE).await.unwrap();\n        while items.len() < 15 {\n            let more_tasks = c.receive_all(TEST_RECV_DEADLINE).await.unwrap();\n            assert!(!more_tasks.is_empty(), \"failed to receive all the tasks\");\n            items.extend(more_tasks);\n        }\n\n        let mut items = items.into_iter();\n        for num in 6..=10 {\n            let recv = items.next().unwrap();\n            assert_eq!(\n                &*recv.task,\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                    app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                })\n            );\n            recv.ack().await.unwrap();\n        }\n        <|fim_middle|>", "completion": "let _: () = conn\n                .xgroup_create_mkstream(v3_main, super::WORKERS_GROUP, 0i8)\n                .await\n                .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [749, 752]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct EnvironmentExportOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EnvironmentExportOptions> for svix::api::EnvironmentExportOptions {\n    fn from(value: EnvironmentExportOptions) -> Self {\n        let EnvironmentExportOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct EnvironmentImportOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<EnvironmentImportOptions> for svix::api::EnvironmentImportOptions {\n    <|fim_suffix|>\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct EnvironmentArgs {\n    #[command(subcommand)]\n    pub command: EnvironmentCommands,\n}\n\n#[derive(Subcommand)]\npub enum EnvironmentCommands {\n    /// Download a JSON file containing all org-settings and event types.\n    ///\n    /// Note that the schema for [`EnvironmentOut`] is subject to change. The fields\n    /// herein are provided for convenience but should be treated as JSON blobs.\n    Export {\n        #[clap(flatten)]\n        options: EnvironmentExportOptions,\n    },\n    /// Import a configuration into the active organization.\n    ///\n    /// It doesn't delete anything, only adds / updates what was passed to it.\n    ///\n    /// Note that the schema for [`EnvironmentIn`] is subject to change. The fields\n    /// herein are provided for convenience but should be treated as JSON blobs.\n    Import {\n        environment_in: Option<crate::json::JsonOf<EnvironmentIn>>,\n        #[clap(flatten)]\n        options: EnvironmentImportOptions,\n    },\n}\n\nimpl EnvironmentCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Export { options } => {\n                let resp = client.environment().export(Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Import {\n                environment_in,\n                options,\n            } => {\n                client\n                    .environment()\n                    .import(\n                        environment_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "fn from(value: EnvironmentImportOptions) -> Self {\n        let EnvironmentImportOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/environment.rs", "node_type": "function_item", "line_range": [25, 28]}
{"prompt": "<|fim_prefix|>();\n    let msg_id = msg.id.clone();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap();\n    assert_eq!(content.unwrap().id, msg_id.clone());\n\n    let res = messagecontent::Entity::update_many()\n        .col_expr(\n            messagecontent::Column::Expiration,\n            Expr::value(Utc::now() - Duration::days(1)),\n        )\n        .filter(messagecontent::Column::Id.eq(msg_id.clone()))\n        .exec(&pool)\n        .await\n        .unwrap();\n    assert_eq!(1, res.rows_affected);\n\n    expired_message_cleaner::clean_expired_messages(&pool, 5000, false)\n        .await\n        .unwrap();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id)\n        .one(&pool)\n        .await\n        .unwrap();\n    assert!(content.is_none());\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period_messagecontent() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    let app_id = create_test_app(&client, \"test-content-expiration-period\")\n        .await\n        .unwrap()\n        .id;\n\n    let custom_retention_period = 5;\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value\" },\n                \"payloadRetentionPeriod\": custom_retention_period\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n    let msg_id = msg.id.clone();\n\n    let content: messagecontent::Model = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap()\n        .unwrap();\n\n    let expected = Utc::now() + Duration::days(custom_retention_period) + Duration::hours(1);\n    let actual: chrono::DateTime<Utc> = content.expiration.into();\n\n    assert!(actual < expected);\n}\n\n#[tokio::test]\nasync fn test_expunge_message_payload() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testApp\").await.unwrap().id;\n\n    let payload = json!({ \"sensitive\": \"data\" });\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{}/content/\", msg.id),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(msg.payload.0.get(), r#\"{\"expired\":true}\"#);\n}\n\n#[tokio::test]\nasync fn test_message_conflict() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg_in = json!({\n        \"eventType\": \"user.signup\",\n        \"payload\": { \"test\": \"value\" },\n        \"payloadRetentionPeriod\": 5,\n        \"eventId\": \"test1\",\n    });\n\n    let _: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            msg_in.clone(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    l<|fim_suffix|>}\n\n#[tokio::test]\nasync fn test_message_validation() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testApp\").await.unwrap().id;\n    let payload = json!({ \"large_payload\": \"payload-\".repeat(1_000_000) });\n\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, payload).unwrap(),\n            StatusCode::PAYLOAD_TOO_LARGE,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_raw_payload() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testRawPayload\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(axum::http::StatusCode::OK);\n\n    create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let msg_payload = json!({ \"test\": \"value1\" });\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            json!({\n                \"eventType\": \"payload.raw\",\n                \"payload\": {},\n                \"transformationsParams\": {\n                    \"rawPayload\": msg_payload.to_string(),\n                },\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let rec_body = receiver.data_recv.recv().await;\n    assert_eq!(msg_payload.to_string(), rec_body.unwrap().to_string());\n}\n\n#[tokio::test]\nasync fn test_create_message_with_application() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_uid = format!(\"app-created-in-cmg-{}\", rand_str(15));\n\n    // cmg without the application field fails\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_uid}/msg/\"),\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value\" }\n            }),\n            StatusCode::NOT_FOUND,\n        )\n        .await\n        .unwrap();\n\n    // cmg with application\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_uid}/msg/\"),\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value1\" },\n                \"application\": {\n                    \"name\": \"Test App Created With Message\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    // app was created\n    let app: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(app[\"uid\"], app_uid);\n    assert_eq!(app[\"name\"], \"Test App Created With Message\");\n\n    // Create another message to the now-existing app with the application field\n    // The application field should be ignored since the app already exists\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_uid}/msg/\"),\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value2\" },\n                \"application\": {\n                    \"name\": \"Updated name will be ignored\",\n                    \"uid\": app_uid,\n                }\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    // Verify the app name didn't change\n    let app_after: serde_json::Value = client\n        .get(&format!(\"api/v1/app/{app_uid}/\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(app_after[\"name\"], \"Test App Created With Message\");\n\n    // UID in path must match UID in body\n    let _: IgnoredAny = client\n        .post(\n            \"api/v1/app/different-uid/msg/\",\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value\" },\n                \"payloadRetentionPeriod\": 5,\n                \"application\": {\n                    \"name\": \"Test App\",\n                    \"uid\": app_uid,  // This doesn't match the path\n                }\n            }),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .awai<|fim_middle|>", "completion": "let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            msg_in,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_message.rs", "node_type": "let_declaration", "line_range": [552, 559]}
{"prompt": "<|fim_prefix|>>::try_into(status_code.as_u16()).unwrap()\n            );\n            assert_eq!(i.endpoint_id, endp_id);\n        }\n        receiver.jh.abort();\n    }\n\n    // non-HTTP-related failures:\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    // stop receiver before beginning tests:\n    receiver.jh.abort();\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        &cfg.retry_schedule.len() + 1,\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, MessageStatus::Fail);\n        assert_eq!(i.response_status_code, 0);\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n}\n\n#[tokio::test]\nasync fn test_message_attempts_empty_retry_schedule() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![];\n\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let (status_code, msg_status, attempt_count) =\n        (StatusCode::INTERNAL_SERVER_ERROR, MessageStatus::Fail, None);\n    let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(status_code);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data\"}))\n        .await\n        .unwrap();\n\n    let list = get_msg_attempt_list_and_assert_count(\n        &client,\n        &app_id,\n        &msg.id,\n        attempt_count.unwrap_or(&cfg.retry_schedule.len() + 1),\n    )\n    .await\n    .unwrap();\n\n    for i in list.data.iter() {\n        assert_eq!(i.status, msg_status);\n        println!(\"{} {status_code}\", i.response_status_code);\n        assert_eq!(\n            i.response_status_code,\n            TryInto::<i16>::try_into(status_code.as_u16()).unwrap()\n        );\n        assert_eq!(i.endpoint_id, endp_id);\n    }\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_combined_before_after_filtering() {\n    let (client, _) = start_svix_server().await;\n\n    let app = create_test_app(&client, \"test_app\").await.unwrap();\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ep = create_test_endpoint(&client, &app.id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    // Send a first message\n    create_test_message(\n        &client,\n        &app.id,\n        serde_json::json!({\n            \"test\": 1,\n        }),\n    )\n    .await\n    .unwrap();\n\n    // Wait until attempt was made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let ts1 = chrono::Utc::now();\n\n    // Send another two messages\n    for i in 1..=2 {\n        create_test_message(\n            &client,\n            &app.id,\n            serde_json::json!({\n                \"test\": i + 1,\n            }),\n        )\n        .await\n        .unwrap();\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 3 {\n            anyhow::bail!(\"list len {}, not 3\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n    let ts2 = chrono::Utc::now();\n\n    // Send another three messages\n    f<|fim_suffix|>\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        let list: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/endpoint/{}/\", app.id, ep.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if list.data.len() != 6 {\n            anyhow::bail!(\"list len {}, not 6\", list.data.len());\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // No timestamp-based filtering should yield all 6 messages\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\"api/v1/app/{}/attempt/endpoint/{}/?limit=10\", app.id, ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 6);\n\n    // Limiting the time to the second batch should only yield those two messages\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/\\\n                 ?limit=10&before={ts2}&after={ts1}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // We got all the data there is for the filters we supplied..\n    assert!(out.done);\n    assert_eq!(out.data.len(), 2);\n\n    // .. but we can still iterate from here when loosening filters.\n    let prev_iter = out.prev_iterator.unwrap();\n    let iter = out.iterator.unwrap();\n\n    // Can get the older three messages via pagination\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/?iterator={prev_iter}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 3);\n\n    // Can get the earlier message via pagination\n    let out: ListResponse<MessageAttemptOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{}/attempt/endpoint/{}/?iterator={iter}\",\n                app.id, ep.id\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(out.done);\n    assert_eq!(out.data.len(), 1);\n}\n\n#[tokio::test]\nasync fn test_pagination_by_endpoint() {\n    let (client, _jh) = start_svix_server().await;\n\n    // Setup six endpoints and six messages so there's a sufficient number to test pagination\n    let app = create_test_app(&client, \"app1\").await.unwrap();\n\n    let mut receivers = Vec::new();\n    for _ in 0..6 {\n        receivers.push(TestReceiver::start(StatusCode::OK));\n    }\n\n    let mut eps = Vec::new();\n    for receiver in &receivers {\n        eps.push(\n            create_test_endpoint(&client, &app.id, &receiver.endpoint)\n                .await\n                .unwrap(),\n        );\n    }\n\n    let mut messages = Vec::new();\n    for i in 1..=6usize {\n        messages.push(\n            async {\n                // the requests that depend on time (ie, `before` and `after`) can flake if too many\n                // messages are created too close together.\n                // This short sleep aims to separate them a little so we can get clean counts.\n                tokio::time::sleep(Duration::from_millis(10)).await;\n                create_test_message(\n                    &client,\n                    &app.id,\n                    serde_json::json!({\n                        \"test\": i,\n                    }),\n                )\n                .await\n                .unwrap()\n            }\n            .await,\n        );\n    }\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        for endp_id in eps.iter().map(|ep| &ep.id) {\n            let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{}/attempt/endpoint/{endp_id}/\", app.id),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if list.data.len() != 6 {\n                <|fim_middle|>", "completion": "for i in 1..=3 {\n        create_test_message(\n            &client,\n            &app.id,\n            serde_json::json!({\n                \"test\": i + 3,\n            }),\n        )\n        .await\n        .unwrap();\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "for_expression", "line_range": [926, 936]}
{"prompt": "<|fim_prefix|>cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n        p.send(&mt, None).await.unwrap();\n\n        let recv = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv.task, mt);\n        recv.ack().await.unwrap();\n\n        if let Ok(recv) = timeout(delay, c.receive_all(TEST_RECV_DEADLINE)).await {\n            panic!(\"Received unexpected QueueTask {:?}\", recv.unwrap()[0].task);\n        }\n\n        let mut conn = pool\n            .get()\n            .await\n            .expect(\"Error retrieving connection from Redis pool\");\n        // And assert that the task has been deleted\n        assert!(conn\n            .xread::<_, _, StreamReadReply>(&[main_queue], &[0])\n            .await\n            .unwrap()\n            .keys\n            .is_empty());\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_nack() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        let main_queue = \"{test}_nack\";\n        let delayed = \"{test}_nack_delayed\";\n        let lock = \"{test}_nack_delayed_lock\";\n        let dlq = \"{test}_nack_delayed_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(100);\n\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test\".to_owned()),\n            app_id: ApplicationId(\"test\".to_owned()),\n            endpoint_id: EndpointId(\"test\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n        p.send(&mt, None).await.unwrap();\n\n        let recv = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv.task, mt);\n        recv.nack().await.unwrap();\n\n        let recv = timeout(\n            Duration::from_millis(500) + delay,\n            c.receive_all(TEST_RECV_DEADLINE),\n        )\n        .await\n        .expect(\"Expected QueueTask\");\n        assert_eq!(*recv.unwrap().pop().unwrap().task, mt);\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_delay() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        let main_queue = \"{test}_delay\";\n        let delayed = \"{test}_delay_delayed\";\n        let lock = \"{test}_delay_delayed_lock\";\n        let dlq = \"{test}_delay_delayed_dlq\";\n\n        cleanup(&pool, main_queue, delayed, lock).await;\n\n        let delay = Duration::from_millis(500);\n        let (p, mut c) = new_pair_inner(&cfg, delay, \"\", main_queue, delayed, lock, dlq).await;\n\n        let mt1 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test1\".to_owned()),\n            app_id: ApplicationId(\"test1\".to_owned()),\n            endpoint_id: EndpointId(\"test1\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Scheduled,\n            attempt_count: 0,\n        });\n        let mt2 = QueueTask::MessageV1(MessageTask {\n            msg_id: MessageId(\"test2\".to_owned()),\n            app_id: ApplicationId(\"test2\".to_owned()),\n            endpoint_id: EndpointId(\"test2\".to_owned()),\n            trigger_type: MessageAttemptTriggerType::Manual,\n            attempt_count: 0,\n        });\n\n        p.send(&mt1, Some(Duration::from_millis(2000)))\n            .await\n            .unwrap();\n        p.send(&mt2, None).await.unwrap();\n\n        let recv2 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();\n        assert_eq!(*recv2.task, mt2);\n        recv2.ack().await.unwrap();\n\n        <|fim_suffix|>\n        assert_eq!(*recv1.task, mt1);\n        recv1.ack().await.unwrap();\n    }\n\n    fn to_redis_key(id: &str, task: &QueueTask) -> String {\n        format!(\"{id}|{}\", serde_json::to_string(task).unwrap())\n    }\n\n    #[tokio::test]\n    #[ignore]\n    async fn test_migrations() {\n        let cfg = crate::cfg::load().unwrap();\n        let pool = get_pool(&cfg).await;\n\n        // Test queue name constants\n        let v1_main = \"{test}_migrations_main_v1\";\n        let v2_main = \"{test}_migrations_main_v2\";\n        let v3_main = \"{test}_migrations_main_v3\";\n\n        let v1_processing = \"{test}_migrations_processing_v1\";\n        let v2_processing = \"{test}_migrations_processing_v2\";\n        // v3_processing is the stream pending queue for v3_main\n\n        let v1_delayed = \"{test}_migrations_delayed_v1\";\n        let v2_delayed = \"{test}_migrations_delayed_v2\";\n        let v2_delayed_lock = \"{test}_migrations_delayed_lock_v2\";\n        // v3_delayed doesn not yet exist\n\n        {\n            let mut conn = pool.get().await.unwrap();\n\n            // Clear test keys\n            let _: () = conn\n                .del(&[\n                    v1_main,\n                    v2_main,\n                    v3_main,\n                    v1_processing,\n                    v2_processing,\n                    v1_delayed,\n                    v2_delayed,\n                ])\n                .await\n                .unwrap();\n\n            // Add v3 consumer group\n            let _: () = conn\n                .xgroup_create_mkstream(v3_main, super::WORKERS_GROUP, 0i8)\n                .await\n                .unwrap();\n\n            // Add v1 data\n            for num in 1..=10 {\n                let _: () = conn\n                    .rpush(\n                        v1_main,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            for num in 11..=15 {\n                let _: () = conn\n                    .zadd(\n                        v1_delayed,\n                        to_redis_key(\n                            &num.to_string(),\n                            &QueueTask::MessageV1(MessageTask {\n                                msg_id: MessageId(format!(\"TestMessageID{num}\")),\n                                app_id: ApplicationId(\"TestApplicationID\".to_owned()),\n                                endpoint_id: EndpointId(\"TestEndpointID\".to_owned()),\n                                trigger_type: MessageAttemptTriggerType::Manual,\n                                attempt_count: 0,\n                            }),\n                        ),\n                        Utc::now().timestamp() + 2,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // Move the first five of v1_main to v1_processing\n            for _ in 0..5 {\n                let _: () = conn\n                    .blmove(\n                        v1_main,\n                        v1_processing,\n                        Direction::Left,\n                        Direction::Right,\n                        0.0,\n                    )\n                    .await\n                    .unwrap();\n            }\n\n            // v1 to v2\n            migrate_list(&mut conn, v1_main, v2_main).await.unwrap();\n            migrate_list(&mut conn, v1_processing, v2_processing)\n                .await\n                .unwrap();\n            migrate_sset(&mut conn, v1_delayed, v2_delayed)\n                .await\n                .unwrap();\n\n            // v2 to v3\n<|fim_middle|>", "completion": "let recv1 = c\n            .receive_all(TEST_RECV_DEADLINE)\n            .await\n            .unwrap()\n            .pop()\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [697, 702]}
{"prompt": "<|fim_prefix|>sage_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let received_payload = timeout(Duration::from_secs(2), receiver.data_recv.recv())\n        .await\n        .unwrap()\n        .unwrap();\n\n    assert_eq!(received_payload, msg_payload);\n}\n\n// This doesn't actually handle requests successfully, but it does allow us\n// to see which hostnames are requested of it.\nstruct MockProxyServer {\n    matched_hosts: Arc<Mutex<HashSet<String>>>,\n    addr: String,\n    variant: MockProxyVariant,\n}\n\nenum MockProxyVariant {\n    Http,\n    Socks5,\n}\n\nimpl MockProxyServer {\n    pub fn new(variant: MockProxyVariant) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n        let addr = match variant {\n            MockProxyVariant::Http => {\n                format!(\"http://{}\", listener.local_addr().unwrap())\n            }\n            MockProxyVariant::Socks5 => {\n                format!(\"socks5://{}\", listener.local_addr().unwrap())\n            }\n        };\n        let matched_hosts = Arc::new(Mutex::new(HashSet::new()));\n\n        match variant {\n            MockProxyVariant::Http => {\n                tokio::spawn(Self::http_listener(listener, matched_hosts.clone()))\n            }\n            MockProxyVariant::Socks5 => {\n                tokio::spawn(Self::socks5_listener(listener, matched_hosts.clone()))\n            }\n        };\n\n        Self {\n            matched_hosts,\n            addr,\n            variant,\n        }\n    }\n\n    pub async fn http_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        loop {\n            let (mut stream, _addr) = listener.accept().await.unwrap();\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let mut buffer = [0; 512];\n\n                if let Ok(size) = stream.read(&mut buffer).await {\n                    if size == 0 {\n                        return;\n                    }\n                    let request = String::from_utf8_lossy(&buffer[..size]);\n                    if let Some(host) = request\n                        .strip_prefix(\"CONNECT \")\n                        .and_then(|s| s.split(' ').next())\n                        .and_then(|s| s.strip_suffix(\":443\"))\n                    {\n                        let mut guard = matched_hosts.lock().unwrap();\n                        guard.insert(host.to_string());\n                    }\n                }\n            });\n        }\n    }\n\n    pub async fn socks5_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        use socks5_proto::{\n            handshake::{\n                Method as HandshakeMethod, Request as HandshakeRequest,\n                Response as HandshakeResponse,\n            },\n            Address, Reply, Request as SocksRequest, Response as SocksResponse,\n        };\n        loop {\n            let (mut stream, _) = match listener.accept().await {\n                Ok(v) => v,\n                Err(_) => continue,\n            };\n\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let hs_req = match HandshakeRequest::read_from(&mut stream).await {\n                    Ok(req) => req,\n                    Err(_) => {\n                        return;\n                    }\n                };\n\n                if hs_req.methods.contains(&HandshakeMethod::NONE) {\n                    if HandshakeResponse::new(HandshakeMethod::NONE)\n                        .write_to(&mut stream)\n                        .await\n                        .is_err()\n                    {\n                        return;\n                    }\n                } else {\n                    let _ = HandshakeResponse::new(HandshakeMethod::UNACCEPTABLE)\n                        .write_to(&mut stream)\n                        .await;\n                    return;\n                }\n\n                let Ok(socks_req) = SocksRequest::read_from(&mut stream).await else {\n                    return;\n                };\n\n                let host = <|fim_suffix|>;\n                if !host.is_empty() {\n                    let mut guard = matched_hosts.lock().unwrap();\n                    guard.insert(host);\n                }\n\n                let abort_resp =\n                    SocksResponse::new(Reply::ConnectionNotAllowed, Address::unspecified());\n                let _ = abort_resp.write_to(&mut stream).await;\n                let _ = stream.shutdown().await;\n            });\n        }\n    }\n\n    pub fn matches(&self) -> HashSet<String> {\n        let guard = self.matched_hosts.lock().unwrap();\n        println!(\"************ MATCHES {guard:?}\");\n        guard.clone()\n    }\n}\n\n#[tokio::test]\nasync fn test_http_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Http);\n    test_proxy_exceptions(listener).await\n}\n\n#[tokio::test]\nasync fn test_socks5_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Socks5);\n    test_proxy_exceptions(listener).await\n}\n\nasync fn test_proxy_exceptions(listener: MockProxyServer) {\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(ProxyConfig {\n        addr: ProxyAddr::new(listener.addr.clone()).unwrap(),\n        noproxy: Some(ProxyBypassCfg(\"10.0.0.0/8, 8.8.8.8, 0ec2:1652:6021:693b:f928:565d:5a0e:de9f, www.svix.com, .google.com\".to_owned())),\n    });\n    cfg.retry_schedule = vec![];\n\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"proxyTest\").await.unwrap().id;\n\n    // Note: Real hostnames here are not ideal, but difficult to test SOCKS without valid DNS entries:\n    create_test_endpoint(&client, &app_id, \"https://www.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://play.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://www.google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://8.8.8.8\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://8.8.4.4\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://10.0.0.1\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(\n        &client,\n        &app_id,\n        \"https://[0ec2:1652:6021:693b:f928:565d:5a0e:de9f]\",\n    )\n    .await\n    .unwrap();\n\n    client\n        .post::<MessageIn, serde_json::Value>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({ \"test\": \"value\" })).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    tokio::time::sleep(std::time::Duration::from_secs(10)).await;\n\n    assert!(listener.matches().contains(\"8.8.4.4\"));\n    assert!(!listener.matches().contains(\"8.8.8.8\"));\n    assert!(!listener.matches().contains(\"10.0.0.1\"));\n    assert!(!listener\n        .matches()\n        .contains(\"[0ec2:1652:6021:693b:f928:565d:5a0e:de9f]\"));\n    match listener.variant {\n        MockProxyVariant::Http => {\n            assert!(listener.matches().contains(\"play.svix.com\"));\n\n            assert!(!listener.matches().contains(\"www.svix.com\"));\n            assert!(!listener.matches().contains(\"www.google.com\"));\n            assert!(!listener.matches().contains(\"google.com\"));\n\n            assert_eq!(listener.matches().len(), 2);\n        }\n        MockProxyVariant::Socks5 => {\n            // We can't assert hostnames here b/c DNS is resolved before\n            // calling the proxy.\n\n            // This is a very weak assertion, but the \"insta-retries\" that our\n            // client does mean that occasionally the same site resolve to\n            // different IPs that populate the match list separately:\n            assert!(listener.matches().len() >= 2);\n        }\n    }\n}\n<|fim_middle|>", "completion": "match &socks_req.address {\n                    Address::SocketAddress(socket_addr) => socket_addr.ip().to_string(),\n                    Address::DomainAddress(domain_bytes, _port) => {\n                        String::from_utf8_lossy(domain_bytes).to_string()\n                    }\n                }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_proxy.rs", "node_type": "match_expression", "line_range": [206, 211]}
{"prompt": "<|fim_prefix|>ured backend information for the queue. May panic is the configuration has\n    /// not been validated\n    pub fn queue_backend(&self) -> QueueBackend<'_> {\n        let err = \"Called [`queue_backend`] before validating configuration\";\n\n        match self.queue_type {\n            QueueType::Memory => QueueBackend::Memory,\n            QueueType::Redis => QueueBackend::Redis(self.queue_dsn().expect(err)),\n            QueueType::RedisCluster => QueueBackend::RedisCluster(self.queue_dsn().expect(err)),\n            QueueType::RedisSentinel => QueueBackend::RedisSentinel(\n                self.queue_dsn().expect(err),\n                self.redis_sentinel_cfg.as_ref().expect(err),\n            ),\n            QueueType::RabbitMQ => QueueBackend::RabbitMq(self.rabbit_dsn.as_ref().expect(err)),\n        }\n    }\n\n    /// Fetches the configured backend information for the cache, or `None` if the [`CacheType`] is\n    ///  `None`. May panic is the configuration has not been validated\n    pub fn cache_backend(&self) -> CacheBackend<'_> {\n        let err = \"Called [`cache_backend`] before validating configuration\";\n\n        match self.cache_type {\n            CacheType::None => CacheBackend::None,\n            CacheType::Memory => CacheBackend::Memory,\n            CacheType::Redis => CacheBackend::Redis(self.cache_dsn().expect(err)),\n            CacheType::RedisCluster => CacheBackend::RedisCluster(self.cache_dsn().expect(err)),\n            CacheType::RedisSentinel => CacheBackend::RedisSentinel(\n                self.cache_dsn().expect(err),\n                self.redis_sentinel_cfg.as_ref().expect(err),\n            ),\n        }\n    }\n}\n\n#[derive(Clone, Debug, Deserialize)]\npub struct InternalConfig {\n    /// The region to use in the Svix URL given in th dashboard access endpoint\n    #[serde(default = \"default_region\")]\n    pub region: String,\n\n    /// The base url to use for the app portal\n    #[serde(default = \"default_app_portal_url\")]\n    pub app_portal_url: String,\n}\n\nfn default_region() -> String {\n    \"self_hosted\".to_owned()\n}\n\nfn default_app_portal_url() -> String {\n    \"https://app.svix.com\".to_owned()\n}\n\n#[derive(Debug, Eq, PartialEq)]\npub enum QueueBackend<'a> {\n    Memory,\n    Redis(&'a str),\n    RedisCluster(&'a str),\n    RedisSentinel(&'a str, &'a SentinelConfig),\n    RabbitMq(&'a str),\n}\n\n#[derive(Debug, Eq, PartialEq)]\npub enum CacheBackend<'a> {\n    None,\n    Memory,\n    Redis(&'a str),\n    RedisCluster(&'a str),\n    RedisSentinel(&'a str, &'a SentinelConfig),\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogLevel {\n    Info,\n    Debug,\n    Trace,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogFormat {\n    Default,\n    Json,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum QueueType {\n    Memory,\n    Redis,\n    RedisCluster,\n    RedisSentinel,\n    RabbitMQ,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum CacheType {\n    Memory,\n    Redis,\n    RedisCluster,\n    RedisSentinel,\n    None,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum DefaultSignatureType {\n    Hmac256,\n    Ed25519,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum Environment {\n    Dev,\n    Staging,\n    Prod,\n}\n\nimpl std::fmt::Display for Environment {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"{}\",\n            match self {\n                Environment::Dev => \"dev\",\n                Environment::Staging => \"staging\",\n                Environment::Prod => \"prod\",\n            }\n        )\n    }\n}\n\nimpl fmt::Display for LogLevel {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Info => Level::INFO,\n            Self::Debug => Level::DEBUG,\n            Self::Trace => Level::TRACE,\n        }\n        .fmt(f)\n    }\n}\n\n#[derive(Clone, Debug, Deserialize, Eq, PartialEq)]\npub struct SentinelConfig {\n    #[serde(rename = \"sentinel_service_name\")]\n    pub service_name: String,\n    #[serde(default)]\n    pub redis_tls_mode_secure: bool,\n    pub redis_db: Option<i64>,\n    pub redis_username: Option<String>,\n    pub redis_password: Option<String>,\n    #[serde(default)]\n    pub redis_use_resp3: bool,\n}\n\nimpl From<SentinelConfig> for omniqueue::backends::redis::SentinelConfig {\n    fn from(val: SentinelConfig) -> Self {\n        let SentinelConfig {\n            service_name,\n            redis_tls_mode_secure,\n            redis_db,\n            redis_username,\n            redis_password,\n            redis_use_resp3,\n        } = val;\n        omniqueue::backends::redis::SentinelConfig {\n            service_name,\n            redis_tls_mode_secure,\n            redis_db,\n            redis_username,\n            redis_password,\n            redis_use_resp3,\n        }\n    }\n}\n\n/// Try to extract a [`ConfigurationInner`] from the provided [`Figment`]. Any error message should\n/// indicate the missing required field(s).\nfn try_extract(figment: Figment) -> anyhow::Result<ConfigurationInner> {\n    // Explicitly override error if `jwt_secret` is not set, as the default error does not mention\n    // the field name due it coming from an inlined field `ConfigurationInner::jwt_signing_config`\n    // See: <https://github.com/SergioBenitez/Figment/issues/80>\n    if !figment.contains(\"jwt_secret\") {\n        bail!(\"missing field `jwt_secret`\");\n    }\n\n    Ok(figment.extract()?)\n}\n\npub fn load() -> anyhow::Result<Arc<ConfigurationInner>> {\n    if let Ok(db_url) = std::env::var(\"DATABASE_URL\") {\n        // If we have DATABASE_URL set, we should potentially use it.\n        const DB_DSN: &str = \"SVIX_DB_DSN\";\n        if std::env::var_os(DB_DSN).is_none() {\n            std::env::set_var(DB_DSN, db_url);\n        }\n    }\n\n    let merged = Figment::new()\n        .merge(Toml::string(DEFAULTS))\n        .merge(Toml::file(\"config.toml\"))\n        .merge(Env::prefixed(\"SVIX_\"));\n\n    let config = try_extract(merged).context(\"failed to extract configuration\")?;\n\n    config\n        .validate()\n        .context(\"failed to validate configuration\")?;\n    Ok(Arc::from(config))\n}\n\n#[cfg(test)]\nmod tests {\n    use std::sync::Arc;\n\n    use figment::{\n        providers::{Format as _, Toml},\n        Figment,\n    };\n\n    use super::{load, try_extract, CacheBackend, CacheType, QueueBackend, QueueType};\n    use crate::core::security::{JWTAlgorithm, JwtSigningConfig};\n\n    #[test]\n    fn test_cache_or_queue_dsn_priority() {\n        let mut cfg = load().unwrap();\n        let cfg = Arc::make_mut(&mut cfg);\n\n        // Override all relevant values\n        cfg.queue_type = QueueType::Redis;\n        cfg.cache_type = CacheType::Redis;\n        cfg.queue_dsn = Some(\"test_a\".to_owned());\n        cfg.cache_dsn = Some(\"test_b\".to_owned());\n        cfg.redis_dsn = Some(\"this_value_should_be_overridden\".to_owned());\n\n        // Assert that the queue_dsn and cache_dsn overwrite the `redis_dsn`\n        assert_eq!(cfg.queue_backend(), QueueBackend::Redis(\"test_a\"));\n        assert_eq!(cfg.cache_backend(), CacheBackend::Redis(\"test_b\"));\n    }\n\n    #[test]\n    fn test_try_extract_missing_jwt_secret() {\n        let defaults = Figment::new();\n\n        let actual = try_extract(defaults);\n\n        l<|fim_suffix|>        assert_eq!(err.to_string(), \"missing field `jwt_secret`\");\n    }\n\n    #[test]\n    fn test_jwt_signing_fallback() {\n        let raw_config = r#\"\njwt_secret = \"not_actually_a_secret\"\n        \"#;\n\n        let actual: JwtSigningConfig = Figment::new()\n            .merge(Toml::string(raw_config))\n            .extract()\n            .unwrap();\n\n        assert!(matches!(actual, JwtSigningConfig::Default { .. }));\n    }\n\n    #[test]\n    fn test_jwt_select_algorithm() {\n        let raw_config = r#\"\njwt_secret = \"not_actually_a_secret\"\njwt_algorithm = \"HS512\"\n        \"#;\n\n        let actual: JwtSigningConfig = Figment::new()\n            .merge(Toml::string(raw_config))\n            .extract()\n            .unwrap();\n\n        assert!(matches!(\n            actual,\n            JwtSigningConfig::Advanced(JWTAlgorithm::HS512(_))\n        ));\n    }\n}\n<|fim_middle|>", "completion": "let err = actual.unwrap_err();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/cfg.rs", "node_type": "let_declaration", "line_range": [651, 651]}
{"prompt": "<|fim_prefix|>does\n        // not impact functionality, so continue as usual.\n        if let Err(e) = consumer_group_resp {\n            if !e.to_string().contains(\"BUSYGROUP\") {\n                panic!(\n                    \"error creating consumer group or stream: {:?}, {:?}, {:?}, {:?}, {e:?}\",\n                    e.kind(),\n                    e.detail(),\n                    e.code(),\n                    e.category()\n                )\n            };\n        }\n    }\n\n    // Redis durations are given in integer numbers of milliseconds, so the pending_duration (the\n    // time in which a task is allowed to be processing before being restarted) must be converted to\n    // one.\n    let pending_duration: i64 = pending_duration\n        .as_millis()\n        .try_into()\n        .expect(\"Pending duration out of bounds\");\n\n    // Migrate v1 queues to v2 and v2 queues to v3 on a loop with exponential backoff.\n    tokio::spawn({\n        let pool = pool.clone();\n\n        async move {\n            let delays = [\n                // 11.25 min\n                Duration::from_secs(60 * 11 + 15),\n                // 22.5 min\n                Duration::from_secs(60 * 22 + 30),\n                // 45 min\n                Duration::from_secs(60 * 45),\n                // 1.5 hours\n                Duration::from_secs(60 * 30 * 3),\n                // 3 hours\n                Duration::from_secs(60 * 60 * 3),\n                // 6 hours\n                Duration::from_secs(60 * 60 * 6),\n                // 12 hours\n                Duration::from_secs(60 * 60 * 12),\n                // 24 hours\n                Duration::from_secs(60 * 60 * 24),\n            ];\n\n            run_migration_schedule(&delays, pool).await;\n        }\n    });\n\n    // Metrics task\n    tokio::spawn({\n        let pool = pool.clone();\n        let main_queue_name = main_queue_name.clone();\n        let delayed_queue_name = delayed_queue_name.clone();\n        let deadletter_queue_name = dlq_name.clone();\n\n        async move {\n            let mut interval = tokio::time::interval(Duration::from_secs(1));\n            let main_queue = RedisQueueType::Stream(&main_queue_name);\n            let pending = RedisQueueType::StreamPending {\n                stream: &main_queue_name,\n                group: WORKERS_GROUP,\n            };\n            let delayed_queue = RedisQueueType::SortedSet(&delayed_queue_name);\n            let deadletter_queue = RedisQueueType::List(&deadletter_queue_name);\n            let metrics =\n                crate::metrics::RedisQueueMetrics::new(&opentelemetry::global::meter(\"svix.com\"));\n            loop {\n                interval.tick().await;\n                metrics\n                    .record(\n                        &pool,\n                        &main_queue,\n                        &pending,\n                        &delayed_queue,\n                        &deadletter_queue,\n                    )\n                    .await;\n            }\n        }\n    });\n\n    let config = RedisConfig {\n        dsn: dsn.to_owned(),\n        max_connections: cfg.redis_pool_max_size,\n        reinsert_on_nack: false, // TODO\n        queue_key: main_queue_name,\n        delayed_queue_key: delayed_queue_name,\n        delayed_lock_key: delayed_lock_name,\n        consumer_group: WORKERS_GROUP.to_owned(),\n        consumer_name: WORKER_CONSUMER.to_owned(),\n        payload_key: QUEUE_KV_KEY.to_owned(),\n        ack_deadline_ms: pending_duration,\n        dlq_config: Some(DeadLetterQueueConfig {\n            queue_key: dlq_name,\n            max_receives: 3,\n        }),\n        sentinel_config: cfg.redis_sentinel_cfg.clone().map(|c| c.into()),\n    };\n\n    match &cfg.queue_type {\n        QueueType::RedisCluster => {\n            let (producer, consumer) = RedisBackend::cluster_builder(config)\n                .build_pair()\n                .await\n                .expect(\"Error initializing redis-cluster queue\");\n\n            let producer = TaskQueueProducer::new(producer);\n            let consumer = TaskQueueConsumer::new(consumer);\n            (producer, consumer)\n        }\n        QueueType::RedisSentinel => {\n            <|fim_suffix|>\n\n            let producer = TaskQueueProducer::new(producer);\n            let consumer = TaskQueueConsumer::new(consumer);\n            (producer, consumer)\n        }\n        QueueType::Redis => {\n            let (producer, consumer) = RedisBackend::builder(config)\n                .build_pair()\n                .await\n                .expect(\"Error initializing redis queue\");\n\n            let producer = TaskQueueProducer::new(producer);\n            let consumer = TaskQueueConsumer::new(consumer);\n            (producer, consumer)\n        }\n        _ => panic!(\"Unsupported backend!\"),\n    }\n}\n\nfn task_from_redis_key(key: &str) -> serde_json::Result<Arc<QueueTask>> {\n    // Get the first delimiter -> it has to have the |\n    let pos = key\n        .find('|')\n        .ok_or_else(|| serde::de::Error::custom(\"key must contain '|'\"))?;\n    serde_json::from_str(&key[pos + 1..])\n}\n\nasync fn migrate_v2_to_v3_queues(conn: &mut RedisConnection<'_>) -> Result<()> {\n    migrate_list_to_stream(conn, LEGACY_V2_MAIN, MAIN).await?;\n    migrate_list_to_stream(conn, LEGACY_V2_PROCESSING, MAIN).await?;\n\n    Ok(())\n}\n\nasync fn migrate_list_to_stream(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    let batch_size = 1000;\n    loop {\n        let legacy_keys: Vec<String> = conn\n            .lpop(legacy_queue, NonZeroUsize::new(batch_size))\n            .await?;\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n\n        let mut pipe = redis::pipe();\n        for key in legacy_keys {\n            let task = match task_from_redis_key(&key) {\n                Ok(t) => t,\n                Err(e) => {\n                    tracing::error!(error = &e as &dyn std::error::Error, \"Invalid legacy key\");\n                    continue;\n                }\n            };\n            let _ = pipe.xadd(\n                queue,\n                GENERATE_STREAM_ID,\n                &[(QUEUE_KV_KEY, serde_json::to_string(&task).unwrap())],\n            );\n        }\n\n        let _: () = pipe.query_async(conn).await?;\n    }\n}\n\nasync fn migrate_v1_to_v2_queues(conn: &mut RedisConnection<'_>) -> Result<()> {\n    migrate_list(conn, LEGACY_V1_MAIN, LEGACY_V2_MAIN).await?;\n    migrate_list(conn, LEGACY_V1_PROCESSING, LEGACY_V2_PROCESSING).await?;\n    migrate_sset(conn, LEGACY_V1_DELAYED, DELAYED).await?;\n\n    Ok(())\n}\n\nasync fn migrate_list(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    let batch_size = 1000;\n    loop {\n        // Checking for old messages from queue\n        let legacy_keys: Vec<String> = conn\n            .lpop(legacy_queue, NonZeroUsize::new(batch_size))\n            .await?;\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n        let _: () = conn.rpush(queue, legacy_keys).await?;\n    }\n}\n\nasync fn migrate_sset(\n    conn: &mut RedisConnection<'_>,\n    legacy_queue: &str,\n    queue: &str,\n) -> Result<()> {\n    let batch_size = 1000;\n    loop {\n        // Checking for old messages from LEGACY_DELAYED\n        let legacy_keys: Vec<(String, f64)> = conn.zpopmin(legacy_queue, batch_size).await?;\n\n        if legacy_keys.is_empty() {\n            break Ok(());\n        }\n        tracing::info!(\n            \"Migrating {} keys from queue {}\",\n            legacy_keys.len(),\n            legacy_queue\n        );\n        let legacy_keys: Vec<(f64, String)> =\n            legacy_keys.into_iter().map(|(x, y)| (y, x)).collect();\n\n        let _: () = conn.zadd_multiple(queue, &legacy_keys).await?;\n    }\n}\n\n#[cfg(test)]\npub mod tests {\n    use std::time::Duration;\n\n    use chrono::Utc;\n    use redis::{streams::StreamReadReply, AsyncCommands as _, Direction};\n    use tokio::time::timeout;\n\n    use super::{migrate_list, migrate_list_to_stream, migrate_sset, new_pair_inner};\n   <|fim_middle|>", "completion": "let (producer, consumer) = RedisBackend::sentinel_builder(config)\n                .build_pair()\n                .await\n                .expect(\"Error initializing redis-cluster queue\");", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/redis.rs", "node_type": "let_declaration", "line_range": [272, 275]}
{"prompt": "<|fim_prefix|>wait.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let before_msg = Utc::now();\n\n    let msg = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt)\n        .await\n        .unwrap();\n\n    tokio::time::sleep(Duration::from_millis(10)).await;\n    let after_msg = Utc::now();\n\n    // recovery time after msg -- should be no additional attempts\n    recover_webhooks(\n        &client,\n        after_msg,\n        &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/recover/\"),\n    )\n    .await;\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt)\n        .await\n        .unwrap();\n\n    // recovery time before msg -- should be 1 additional attempt\n    recover_webhooks(\n        &client,\n        before_msg,\n        &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/recover/\"),\n    )\n    .await;\n\n    get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, base_attempt_cnt + 1)\n        .await\n        .unwrap();\n\n    receiver.jh.abort();\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_max() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp_id = create_test_endpoint(&client, &app_id, \"http://www.example.com\")\n        .await\n        .unwrap()\n        .id;\n\n    for _ in 0..ExpiringSigningKeys::MAX_OLD_KEYS {\n        client\n            .post_without_response(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/secret/rotate/\"),\n                json!({ \"key\": null }),\n                StatusCode::NO_CONTENT,\n            )\n            .await\n            .unwrap();\n    }\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/secret/rotate/\"),\n            json!({ \"key\": null }),\n            StatusCode::BAD_REQUEST,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_signing_e2e() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let endp = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let secret1: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret2: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_ne!(secret1.key, secret2.key);\n\n    let secret3_key = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret3_key }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret3: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(secret3_key, secret3.key);\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    l<|fim_suffix|>    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret1, secret2, secret3] {\n        if let EndpointSecret::Symmetric(key) = &sec.key {\n            let sec = STANDARD.encode(key);\n            let wh = Webhook::new(&sec).unwrap();\n            wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_signing_symmetric_and_asymmetric() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Asymmetric key\n    let secret_2 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    // Long key\n    let secret_3 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n\n    let ep_in = EndpointIn {\n        url: Url::parse(&receiver.endpoint).unwrap(),\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\" }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Rotate back to symmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret_3.serialize_public_key() }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret_1, secret_2, secret_3] {\n        match sec {\n            EndpointSecret::Symmetric(key) => {\n                let sec = STANDARD.encode(key);\n                let wh = Webhook::new(&sec).unwrap();\n                wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n            }\n            EndpointSecret::Asymmetric(key) => {\n                let msg_id = last_headers.get(\"svix-id\").unwrap().to_str().unwrap();\n                let timestamp = last_headers\n                    .get(\"svix-timestamp\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let signatures = last_headers\n                    .get(\"svix-signature\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let to_sign = format!(\"{msg_id}.{timestamp}.{}\", &last_body);\n                let found =\n                    signatures\n                        .split(' ')\n                        .filter(|x| x.starts_with(\"v1a,\"))\n                        .any(|signature| {\n                            let sig: Signature = Signature::from_slice(\n                                STANDARD\n                                    .decode(&signature[\"v1a,\".len()..])\n                                    .unwrap()\n                                    .as_slice(),\n                            )\n                            .unwrap();\n                            key.0.pk.verify(to_sign.as_bytes(), &sig).is_ok()\n                        });\n                assert!(found);\n            }\n        }\n    }<|fim_middle|>", "completion": "let last_headers = receiver.header_recv.recv().await.unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1093, 1093]}
{"prompt": "<|fim_prefix|>     anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n\n    pub async fn delete(&self, endpoint: &str, expected_code: StatusCode) -> Result<()> {\n        let mut req = self.client.delete(self.build_uri(endpoint));\n        req = self.add_headers(req);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        if expected_code == StatusCode::NO_CONTENT {\n            let res_body = resp.text().await.context(\"error receiving response\")?;\n            anyhow::ensure!(res_body.is_empty());\n        }\n\n        Ok(())\n    }\n\n    pub async fn patch<I: Serialize, O: DeserializeOwned>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<O> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        resp.json()\n            .await\n            .context(\"error receiving/parsing response\")\n    }\n\n    pub async fn patch_without_response<I: Serialize>(\n        &self,\n        endpoint: &str,\n        input: I,\n        expected_code: StatusCode,\n    ) -> Result<()> {\n        let mut req = self.client.patch(self.build_uri(endpoint));\n        req = self.add_headers(req).json(&input);\n\n        let resp = req.send().await.context(\"error sending request\")?;\n\n        if resp.status() != expected_code {\n            anyhow::bail!(\n                \"assertion failed: expected status {}, actual status {}\",\n                expected_code,\n                resp.status()\n            );\n        }\n\n        let res_body = resp.text().await.context(\"error receiving response\")?;\n        anyhow::ensure!(res_body.is_empty());\n\n        Ok(())\n    }\n}\n\npub fn get_default_test_config() -> ConfigurationInner {\n    let _ = dotenvy::dotenv();\n    let cfg = svix_server::cfg::load().unwrap();\n\n    cfg.as_ref().clone()\n}\n\npub async fn start_svix_server() -> (TestClient, tokio::task::JoinHandle<()>) {\n    start_svix_server_with_cfg(&get_default_test_config()).await\n}\n\npub async fn start_svix_server_with_cfg(\n    cfg: &ConfigurationInner,\n) -> (TestClient, tokio::task::JoinHandle<()>) {\n    start_svix_server_with_cfg_and_org_id(cfg, OrganizationId::new(None, None)).await\n}\n\npub async fn start_svix_server_with_cfg_and_org_id(\n    cfg: &ConfigurationInner,\n    org_id: OrganizationId,\n) -> (TestClient, tokio::task::JoinHandle<()>) {\n    let prefix = svix_ksuid::Ksuid::new(None, None).to_string();\n    start_svix_server_with_cfg_and_org_id_and_prefix(cfg, org_id, prefix).await\n}\n\npub async fn start_svix_server_with_cfg_and_org_id_and_prefix(\n    cfg: &ConfigurationInner,\n    org_id: OrganizationId,\n    prefix: String,\n) -> (TestClient, tokio::task::JoinHandle<()>) {\n    let (tracing_subscriber, _guard) = setup_tracing(cfg, /* for_test = */ true);\n\n    let cfg = Arc::new(cfg.clone());\n\n    let token = generate_org_token(&cfg.jwt_signing_config, org_id).unwrap();\n    let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n    let base_uri = format!(\"http://{}\", listener.local_addr().unwrap());\n\n    // Could update this fn to take a tokio TcpListener instead, but that's a pretty large diff\n    // for very little benefit (since this is just test code anyways).\n    listener.set_nonblocking(true).unwrap();\n    let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n    let jh = tokio::spawn(\n        svix_server::run_with_prefix(Some(prefix), cfg, Some(listener))\n            .with_subscriber(tracing_subscriber),\n    );\n\n    (TestClient::new(base_uri, &token), jh)\n}\n\n#[derive(Debug)]\npub struct TestReceiver {\n    pub endpoint: String,\n    pub jh: tokio::task::JoinHandle<()>,\n    pub data_recv: mpsc::Receiver<serde_json::Value>,\n    pub header_recv: mpsc::Receiver<HeaderMap>,\n    pub response_status_code: Arc<Mutex<ResponseStatusCode>>,\n}\n\n#[derive(Clone)]\npub struct TestAppState<T: IntoResponse + Clone> {\n    tx: mpsc::Sender<serde_json::Value>,\n    header_tx: mpsc::Sender<HeaderMap>,\n    response_status_code: Arc<Mutex<ResponseStatusCode>>,\n    response_body: T,\n}\n\n#[derive(Debug, Clone)]\npub struct ResponseStatusCode {\n    pub status_code: axum::http::StatusCode,\n}\n\nimpl TestReceiver {\n    pub fn start(resp_with: axum::http::StatusCode) -> Self {\n        Self::start_with_body(resp_with, ())\n    }\n\n    pub fn start_with_body<T>(resp_with: axum::http::StatusCode, body: T) -> Self\n    where\n        T: IntoResponse + Clone + Send + Sync + 'static,\n    {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n        let endpoint = format!(\"http://{}/\", listener.local_addr().unwrap());\n\n        let (tx, data_recv) = mpsc::channel(32);\n        let (header_tx, header_recv) = mpsc::channel(32);\n\n        let response_status_code = Arc::new(Mutex::new(ResponseStatusCode {\n            status_code: resp_with,\n        }));\n\n        let routes = axum::Router::new()\n            .route(\n                \"/\",\n                axum::routing::post(test_receiver_route).get(test_receiver_route),\n            )\n            .with_state(TestAppState {\n                tx,\n                header_tx,\n                response_status_code: response_status_code.clone(),\n                response_body: body,\n            })\n            .into_make_service();\n\n        let jh = tokio::spawn(async move {\n            axum::serve(listener, routes).await.unwrap();\n        });\n\n        TestReceiver {\n            endpoint,\n            jh,\n            data_recv,\n            header_recv,\n            response_status_code,\n        }\n    }\n\n    pub(crate) fn try_recv_body_value(\n        &mut self,\n    ) -> Result<serde_json::Value, mpsc::error::TryRecvError> {\n        let payload = self.data_recv.try_recv()?;\n        Ok(serde_json::from_value(payload).unwrap())\n    }\n\n    pub(crate) async fn recv_body(&mut self) -> Option<serde_json::Value> {\n        self.data_recv.recv().await\n    }\n\n    pub(crate) async fn recv_body_value(&mut self) -> Option<serde_json::Value> {\n        let payload = tokio::time::timeout(Duration::from_secs(30), self.data_recv.recv())\n            .await\n            .expect(\"timed out\")?;\n        Some(serde_json::from_value(payload).unwrap())\n    }\n\n    pub fn set_response_status_code(&self, resp_with: axum::http::StatusCode) {\n        self.response_status_code.lock().unwrap().status_code = resp_with;\n    }\n}\n\nasync fn test_receiver_route<T: IntoResponse + Clone>(\n    axum::extract::State(TestAppState {\n        tx,\n        header_tx,\n        response_status_code,\n        response_body,\n    }): axum::extract::State<TestAppState<T>>,\n    headers: HeaderMap,\n    axum::Json(json): axum::Json<serde_json::Value>,\n) -> (axum::http::StatusCode, impl IntoResponse) {\n    tx.send(json).await.unwrap();\n    header_tx.send(headers).await.unwrap();\n    (\n        response_status_code.lock().unwrap().status_code,\n        response_body,\n    )\n}\n\npub async fn run_with_retries<O, F, C>(f: C) -> Result<O>\nwhere\n    F: Future<Output = Result<O>>,\n    C: Fn() -> F,\n{\n    for attempt in 0..50 {\n        <|fim_suffix|>\n        if out.is_ok() {\n            return out;\n        } else if let Err(err) = out {\n            println!(\"Attempt {attempt}: {err}\");\n        }\n\n        tokio::time::sleep(std::time::Duration::from_millis(50)).await;\n    }\n\n    anyhow::bail!(\"All attempts failed\");\n}\n<|fim_middle|>", "completion": "let out = f().await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/mod.rs", "node_type": "let_declaration", "line_range": [482, 482]}
{"prompt": "<|fim_prefix|>mismatched\n/// types and out of bounds values. There may be any number of these per 422 UNPROCESSABLE ENTITY\n/// error.\npub struct ValidationErrorItem {\n    /// The location as a [`Vec`] of [`String`]s -- often in the form `[\"body\", \"field_name\"]`,\n    /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, detail: String) -> Self {\n        Self {\n            status,\n            body: HttpErrorBody::Standard(StandardHttpError { code, detail }),\n        }\n    }\n\n    pub fn bad_request(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::BAD_REQUEST,\n            code.unwrap_or_else(|| \"generic_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Generic error\".to_owned()),\n        )\n    }\n\n    pub fn not_found(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_FOUND,\n            code.unwrap_or_else(|| \"not_found\".to_owned()),\n            detail.unwrap_or_else(|| \"Entity not found\".to_owned()),\n        )\n    }\n\n    pub fn unauthorized(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::UNAUTHORIZED,\n            code.unwrap_or_else(|| \"authentication_failed\".to_owned()),\n            detail.unwrap_or_else(|| \"Incorrect authentication credentials.\".to_owned()),\n        )\n    }\n\n    pub fn permission_denied(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::FORBIDDEN,\n            code.unwrap_or_else(|| \"insufficient access\".to_owned()),\n            detail.unwrap_or_else(|| \"Insufficient access for the given operation.\".to_owned()),\n        )\n    }\n\n    pub fn conflict(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::CONFLICT,\n            code.unwrap_or_else(|| \"conflict\".to_owned()),\n            detail.unwrap_or_else(|| \"A conflict has occurred\".to_owned()),\n        )\n    }\n\n    pub fn unprocessable_entity(detail: Vec<ValidationErrorItem>) -> Self {\n        Self {\n            status: StatusCode::UNPROCESSABLE_ENTITY,\n            body: HttpErrorBody::Validation(ValidationHttpError { detail }),\n        }\n    }\n\n    pub fn internal_server_error(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::INTERNAL_SERVER_ERROR,\n            code.unwrap_or_else(|| \"server_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Internal Server Error\".to_owned()),\n        )\n    }\n\n    pub fn not_implemented(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_IMPLEMENTED,\n            code.unwrap_or_else(|| \"not_implemented\".to_owned()),\n            detail.unwrap_or_else(|| \"This API endpoint is not yet implemented.\".to_owned()),\n        )\n    }\n\n    pub fn too_large(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::PAYLOAD_TOO_LARGE,\n            code.unwrap_or_else(|| \"payload_too_large\".to_owned()),\n            detail.unwrap_or_else(|| \"Request payload is too large.\".to_owned()),\n        )\n    }\n}\n\nimpl From<HttpError> for Error {\n    fn from(err: HttpError) -> Error {\n        Error::http(err)\n    }\n}\n\nimpl fmt::Display for HttpError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match &self.body {\n            HttpErrorBody::Standard(StandardHttpError { code, detail }) => write!(\n                f,\n                \"status={} code=\\\"{code}\\\" detail=\\\"{detail}\\\"\",\n                self.status\n            ),\n\n            HttpErrorBody::Validation(ValidationHttpError { detail }) => {\n                write!(\n                    f,\n                    \"status={} detail={}\",\n                    self.status,\n                    serde_json::to_string(&detail)\n                        .unwrap_or_else(|e| format!(\"\\\"unserializable error for {e}\\\"\"))\n                )\n            }\n        }\n    }\n}\n\nimpl IntoResponse for HttpError {\n    fn into_response(self) -> Response {\n        (self.status, Json(self.body)).into_response()\n    }\n}\n\nimpl From<ErrorType> for Error {\n    fn from(typ: ErrorType) -> Self {\n        Self { trace: vec![], typ }\n    }\n}\n\n// FIXME - delete\ni<|fim_suffix|>\n/// Utility function for Converting a [`DbErr`] into an [`Error`].\n///\n/// The error \"duplicate key value violates unique constraint\" is converted to\n/// an HTTP \"conflict\" error. This is to be used in `map_err` calls on\n/// creation/update of records.\npub fn http_error_on_conflict(db_err: DbErr) -> Error {\n    if is_conflict_err(&db_err) {\n        HttpError::conflict(None, None).into()\n    } else {\n        Error::database(db_err)\n    }\n}\n\npub fn is_conflict_err(db_err: &DbErr) -> bool {\n    use DbErr as E;\n    let rt_err = match db_err {\n        E::Exec(e) | E::Query(e) | E::Conn(e) => e,\n        // If sqlx ever extends this enum, I want a compile time error so we're forced to update this function.\n        // Hence we list out all the enumerations, rather than using a default match statement\n        E::TryIntoErr { .. }\n        | E::ConvertFromU64(_)\n        | E::UnpackInsertId\n        | E::UpdateGetPrimaryKey\n        | E::RecordNotFound(_)\n        | E::AttrNotSet(_)\n        | E::Custom(_)\n        | E::Type(_)\n        | E::Json(_)\n        | E::Migration(_)\n        | E::RecordNotInserted\n        | E::RecordNotUpdated\n        | E::ConnectionAcquire(_) => return false,\n    };\n\n    let sqlx_err = match rt_err {\n        RuntimeErr::SqlxError(e) => e,\n        RuntimeErr::Internal(_) => return false,\n    };\n\n    sqlx_err\n        .as_database_error()\n        .and_then(|e| e.code())\n        .filter(|code| code == \"23505\")\n        .is_some()\n}\n\npub fn is_timeout_error(db_err: &DbErr) -> bool {\n    let runtime_err = match &db_err {\n        DbErr::Conn(e) | DbErr::Exec(e) | DbErr::Query(e) => e,\n        _ => return false,\n    };\n\n    let sqlx_err = match runtime_err {\n        RuntimeErr::SqlxError(e) => e,\n        RuntimeErr::Internal(_) => return false,\n    };\n\n    match sqlx_err.as_database_error() {\n        // STUPID - no other good way to ID statement timeouts\n        Some(e) => e\n            .message()\n            .contains(\"canceling statement due to statement timeout\"),\n        None => false,\n    }\n}\n\n/// Returns true if the DbErr results from weirdness with a slow/long connection.\n/// This is distinct from [is_timeout_error], which reports whether the underlying\n/// query actually timed out on the pg side.\n///\n/// [is_connection_timeout_error] reports whether the connection to pg itself was slow\n/// for some reason.\npub fn is_connection_timeout_error(db_err: &DbErr) -> bool {\n    use DbErr as E;\n    let rt_err = match db_err {\n        E::ConnectionAcquire(_) | E::Conn(_) => return true,\n        E::Exec(e) | E::Query(e) => e.to_string(),\n\n        // If sqlx ever extends this enum, I want a compile time error so we're forced to update this function.\n        // Hence we list out all the enumerations, rather than using a default match statement\n        E::TryIntoErr { .. }\n        | E::ConvertFromU64(_)\n        | E::UnpackInsertId\n        | E::UpdateGetPrimaryKey\n        | E::RecordNotFound(_)\n        | E::AttrNotSet(_)\n        | E::Custom(_)\n        | E::Type(_)\n        | E::Json(_)\n        | E::Migration(_)\n        | E::RecordNotInserted\n        | E::RecordNotUpdated => return false,\n    };\n\n    const ERRORS: [&str; 3] = [\n        \"Connection pool timed out\",\n        \"Connection reset by peer\",\n        \"unexpected end of file\",\n    ];\n    for e in ERRORS {\n        if rt_err.contains(e) {\n            return true;\n        }\n    }\n\n    false\n}\n<|fim_middle|>", "completion": "impl From<crate::core::webhook_http_client::Error> for Error {\n    fn from(err: webhook_http_client::Error) -> Error {\n        match err {\n            webhook_http_client::Error::TimedOut => Self::timeout(err),\n            _ => Error::generic(err),\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "impl_item", "line_range": [433, 440]}
{"prompt": "<|fim_prefix|><|fim_suffix|>\n\nuse tracing::warn;\n\npub async fn run_with_retries<\n    T,\n    E: std::error::Error,\n    F: Future<Output = Result<T, E>>,\n    FN: FnMut() -> F,\n>(\n    mut fun: FN,\n    should_retry: impl Fn(&E) -> bool,\n    retry_schedule: &[Duration],\n) -> Result<T, E> {\n    let mut retry = Retry::new(should_retry, retry_schedule);\n    loop {\n        if let Some(result) = retry.run(&mut fun).await {\n            return result;\n        }\n    }\n}\n\n/// A state machine for retrying an asynchronous operation.\n///\n/// Unfortunately needed to get around Rust's lack of `AttachedFn*` traits.\n/// For usage, check the implementation of `run_with_retries`.`\npub struct Retry<'a, Re> {\n    retry_schedule: &'a [Duration],\n    should_retry: Re,\n}\n\nimpl<'a, Re> Retry<'a, Re> {\n    pub fn new(should_retry: Re, retry_schedule: &'a [Duration]) -> Self {\n        Self {\n            retry_schedule,\n            should_retry,\n        }\n    }\n\n    pub async fn run<T, E, F, Fut>(&mut self, f: F) -> Option<Result<T, E>>\n    where\n        E: std::error::Error,\n        F: FnOnce() -> Fut,\n        Fut: Future<Output = Result<T, E>>,\n        Re: Fn(&E) -> bool,\n    {\n        match f().await {\n            // If the function succeeded, we're done\n            Ok(t) => Some(Ok(t)),\n            Err(e) => {\n                let should_retry = &self.should_retry;\n                if self.retry_schedule.is_empty() || !should_retry(&e) {\n                    // If we already used up all the retries or should_retry returns false,\n                    // return the latest error and stop retrying.\n                    self.retry_schedule = &[];\n                    Some(Err(e))\n                } else {\n                    // Otherwise, wait and let the caller call retry.run() again.\n                    warn!(\"Retrying after error: {e}\");\n                    tokio::time::sleep(self.retry_schedule[0]).await;\n                    self.retry_schedule = &self.retry_schedule[1..];\n                    None\n                }\n            }\n        }\n    }\n}\n<|fim_middle|>", "completion": "use std::{future::Future, time::Duration};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/retry.rs", "node_type": "use_declaration", "line_range": [1, 1]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n#![warn(clippy::all)]\n#![forbid(unsafe_code)]\n\nuse anyhow::bail;\nuse clap::{Parser, Subcommand};\nuse dotenvy::dotenv;\nuse svix_server::{\n    cfg,\n    core::{\n        security::{default_org_id, generate_org_token},\n        types::{EndpointSecretInternal, OrganizationId},\n    },\n    db,\n    db::wipe_org,\n    run, setup_tracing,\n};\nuse tracing_subscriber::util::SubscriberInitExt;\nuse validator::Validate;\n\n#[cfg(all(target_env = \"msvc\", feature = \"jemalloc\"))]\ncompile_error!(\"jemalloc cannot be enabled on msvc\");\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\n#[global_allocator]\nstatic GLOBAL: tikv_jemallocator::Jemalloc = tikv_jemallocator::Jemalloc;\n\nmod wait_for;\nu<|fim_suffix|>\n// The names and default ports of services to wait-for\nconst POSTGRES_NAME: &str = \"PostgreSQL\";\nconst POSTGRES_PORT: u16 = 5432;\n\nconst REDIS_NAME: &str = \"Redis\";\nconst REDIS_PORT: u16 = 6379;\n\n#[derive(Parser)]\n#[clap(author, version, about = env!(\"CARGO_PKG_DESCRIPTION\"), long_about = None)]\nstruct Args {\n    #[clap(subcommand)]\n    command: Option<Commands>,\n\n    /// Run database migrations before starting\n    #[clap(long, value_parser)]\n    run_migrations: bool,\n\n    /// The time to wait for a successful connection to the database before timing out in seconds.\n    #[clap(long, value_parser)]\n    wait_for: Option<u64>,\n}\n\n#[derive(Subcommand)]\nenum Commands {\n    /// JWT utilities\n    #[clap()]\n    Jwt {\n        #[clap(subcommand)]\n        command: JwtCommands,\n    },\n    /// Asymmetric Key utilities\n    #[clap()]\n    AsymmetricKey {\n        #[clap(subcommand)]\n        command: AsymmetricKeyCommands,\n    },\n    /// Run database migrations and exit\n    #[clap()]\n    Migrate,\n\n    #[clap()]\n    Wipe {\n        #[clap(value_parser = org_id_parser)]\n        org_id: OrganizationId,\n\n        #[clap(long)]\n        yes_i_know_what_im_doing: bool,\n    },\n\n    /// Generate OpenAPI JSON specification and exit\n    #[clap()]\n    GenerateOpenapi,\n\n    /// Health check command\n    #[clap()]\n    Healthcheck {\n        /// The server URL, for example http://localhost:8071\n        server_url: String,\n    },\n}\n\n#[derive(Subcommand)]\nenum JwtCommands {\n    /// Generate a new JWT\n    #[clap()]\n    Generate {\n        #[clap(value_parser = org_id_parser)]\n        /// Optional org_id to use when generating token (otherwise, default is used).\n        org_id: Option<OrganizationId>,\n    },\n}\n\n#[derive(Subcommand)]\nenum AsymmetricKeyCommands {\n    /// Generate a new asymmetric key\n    #[clap()]\n    Generate,\n}\n\nfn org_id_parser(s: &str) -> Result<OrganizationId, String> {\n    let ret = OrganizationId(s.to_owned());\n    ret.validate().map_err(|x| x.to_string())?;\n    Ok(ret)\n}\n\n#[tokio::main]\nasync fn main() -> anyhow::Result<()> {\n    dotenv().ok();\n\n    let args = Args::parse();\n\n    // Handle commands that don't need configuration first\n    if let Some(Commands::Healthcheck { server_url }) = args.command {\n        let client = reqwest::Client::new();\n        let response = client\n            .head(format!(\"{server_url}/api/v1/health\"))\n            .send()\n            .await?;\n\n        if response.status().is_success() {\n            return Ok(());\n        } else {\n            return Err(anyhow::anyhow!(\n                \"healthcheck failed ({})\",\n                response.status()\n            ));\n        }\n    }\n\n    let cfg = cfg::load()?;\n\n    let (tracing_subscriber, _guard) = setup_tracing(&cfg, /* for_test = */ false);\n    tracing_subscriber.init();\n\n    if let Some(wait_for_seconds) = args.wait_for {\n        let mut wait_for = Vec::with_capacity(2);\n        wait_for.push(wait_for_dsn(\n            &cfg.db_dsn,\n            POSTGRES_NAME,\n            POSTGRES_PORT,\n            wait_for_seconds,\n        ));\n\n        if let Some(redis_dsn) = &cfg.redis_dsn {\n            wait_for.push(wait_for_dsn(\n                redis_dsn,\n                REDIS_NAME,\n                REDIS_PORT,\n                wait_for_seconds,\n            ));\n        }\n\n        if let Err(e) = futures::future::try_join_all(wait_for).await {\n            bail!(e);\n        }\n    }\n\n    if args.run_migrations {\n        tracing::debug!(\"Migrations: Running\");\n        db::run_migrations(&cfg).await;\n        tracing::debug!(\"Migrations: Success\");\n    }\n\n    match args.command {\n        Some(Commands::Migrate) => {\n            db::run_migrations(&cfg).await;\n            println!(\"Migrations: success\");\n        }\n        Some(Commands::Jwt {\n            command: JwtCommands::Generate { org_id },\n        }) => {\n            let org_id = org_id.unwrap_or_else(default_org_id);\n            match generate_org_token(&cfg.jwt_signing_config, org_id) {\n                Ok(token) => println!(\"Token (Bearer): {token}\"),\n                Err(e) => tracing::error!(\"Error generating token: {e}\"),\n            }\n        }\n        Some(Commands::AsymmetricKey { command }) => match command {\n            AsymmetricKeyCommands::Generate => {\n                let secret = EndpointSecretInternal::generate_asymmetric(&cfg.encryption)\n                    .unwrap()\n                    .into_endpoint_secret(&cfg.encryption)\n                    .unwrap();\n                println!(\"Secret key: {}\", secret.serialize_secret_key());\n                println!(\"Public key: {}\", secret.serialize_public_key());\n            }\n        },\n        Some(Commands::Wipe {\n            org_id,\n            yes_i_know_what_im_doing,\n        }) => {\n            if yes_i_know_what_im_doing {\n                wipe_org(&cfg, org_id).await;\n            } else {\n                println!(\"Please confirm you wish to wipe this organization with the `--yes-i-know-what-im-doing` flag\");\n            }\n        }\n        Some(Commands::GenerateOpenapi) => {\n            let mut openapi = svix_server::openapi::initialize_openapi();\n\n            let router = svix_server::v1::router();\n            _ = aide::axum::ApiRouter::new()\n                .nest(\"/api/v1\", router)\n                .finish_api_with(&mut openapi, svix_server::openapi::add_security_scheme);\n\n            svix_server::openapi::postprocess_spec(&mut openapi);\n            println!(\n                \"{}\",\n                serde_json::to_string_pretty(&openapi).expect(\"Failed to serialize JSON spec\")\n            );\n        }\n\n        Some(Commands::Healthcheck { .. }) => {\n            unreachable!(\"Healthcheck command should be handled before config loading\")\n        }\n        None => {\n            run(cfg).await;\n        }\n    };\n\n    opentelemetry::global::shutdown_tracer_provider();\n    Ok(())\n}\n<|fim_middle|>", "completion": "use wait_for::wait_for_dsn;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/main.rs", "node_type": "use_declaration", "line_range": [31, 31]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse std::fmt;\n\nuse serde::{Deserialize, Serialize};\n\n/// Defines the ordering in a listing of results.\n#[derive(\n    Clone, Copy, Debug, Default, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize,\n)]\npub enum Ordering {\n    #[default]\n    #[serde(rename = \"ascending\")]\n    Ascending,\n    #[serde(rename = \"descending\")]\n    Descending,\n}\n\nimpl fmt::Display for Ordering {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        let value = match self {\n            Self::Ascending => \"ascending\",\n            Self::Descending => \"descending\",\n        };\n        f.write_str(value)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/ordering.rs", "node_type": "function_item", "line_range": [19, 25]}
{"prompt": "<|fim_prefix|><|fim_suffix|>\n\nuse axum::{\n    extract::{Path, State},\n    Json,\n};\nuse sea_orm::{entity::prelude::*, ActiveValue::Set, QuerySelect, TransactionTrait};\nuse svix_server_derive::aide_annotate;\nuse url::Url;\n\nuse self::hack::EventTypeNameResult;\nuse super::{EndpointIn, EndpointOut, EndpointPatch, EndpointUpdate};\nuse crate::{\n    cfg::Configuration,\n    core::{\n        operational_webhooks::{EndpointEvent, OperationalWebhook, OperationalWebhookSender},\n        permissions,\n        types::{EndpointId, EventTypeName, EventTypeNameSet, OrganizationId},\n    },\n    db::models::{application, endpoint, endpointmetadata, eventtype},\n    error::{http_error_on_conflict, HttpError, Result, Traceable, ValidationErrorItem},\n    v1::utils::{\n        apply_pagination,\n        patch::{patch_field_non_nullable, UnrequiredField, UnrequiredNullableField},\n        ApplicationEndpointPath, ApplicationPath, IteratorDirection, JsonStatus, JsonStatusUpsert,\n        ListResponse, ModelIn, ModelOut, NoContent, Ordering, Pagination, PaginationLimit,\n        ReversibleIterator, ValidatedJson, ValidatedQuery,\n    },\n    AppState,\n};\n\n/// List the application's endpoints.\n#[aide_annotate(op_id = \"v1.endpoint.list\")]\npub(super) async fn list_endpoints(\n    State(AppState { ref db, .. }): State<AppState>,\n    _: Path<ApplicationPath>,\n    ValidatedQuery(pagination): ValidatedQuery<Pagination<ReversibleIterator<EndpointId>>>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<ListResponse<EndpointOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    let iterator = pagination.iterator;\n    let iter_direction = iterator\n        .as_ref()\n        .map_or(IteratorDirection::Normal, |iter| iter.direction());\n\n    let query = apply_pagination(\n        endpoint::Entity::secure_find(app.id),\n        endpoint::Column::Id,\n        limit,\n        iterator,\n        pagination.order.unwrap_or(Ordering::Descending),\n    );\n\n    let results = query\n        .find_also_related(endpointmetadata::Entity)\n        .all(db)\n        .await?\n        .into_iter()\n        .map(|(endp, metadata)| {\n            let metadata = metadata.map(|m| m.data).unwrap_or_default();\n            (endp, metadata).into()\n        })\n        .collect();\n\n    Ok(Json(EndpointOut::list_response(\n        results,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\nasync fn create_endp_from_data(\n    db: &DatabaseConnection,\n    cfg: &Configuration,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    mut data: EndpointIn,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let key = data.key_take_or_generate(&cfg.encryption, &cfg.default_signature_type)?;\n\n    let mut endp = endpoint::ActiveModel::new(app.id, key);\n    let metadata =\n        endpointmetadata::ActiveModel::new(endp.id.clone().unwrap(), mem::take(&mut data.metadata));\n    data.update_model(&mut endp);\n\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.insert(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointCreated(EndpointEvent::new(app.uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Create a new endpoint for the application.\n///\n/// When `secret` is `null` the secret is automatically generated (recommended)\n#[aide_annotate(op_id = \"v1.endpoint.create\")]\npub(super) async fn create_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        op_webhooks,\n        ..\n    }): State<AppState>,\n    _: Path<ApplicationPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointIn>,\n) -> Result<JsonStatus<201, EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let (endp, metadata) = create_endp_from_data(db, cfg, &op_webhooks, app, data)\n        .await\n        .trace()?;\n\n    Ok(JsonStatus((endp, metadata.data).into()))\n}\n\n/// Get an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.get\")]\npub(super) async fn get_endpoint(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointOut>> {\n    let (endp, metadata) = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .find_also_related(endpointmetadata::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let metadata = metadata.map(|m| m.data).unwrap_or_default();\n\n    Ok(Json((endp, metadata).into()))\n}\n\nasync fn update_endp_from_data(\n    db: &DatabaseConnection,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    endp: endpoint::ActiveModel,\n    metadata: endpointmetadata::ActiveModel,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.update(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    let app_uid = app.uid;\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointUpdated(EndpointEvent::new(app_uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Update an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.update\")]\npub(super) async fn update_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(mut data): ValidatedJson<EndpointUpdate>,\n) -> Result<JsonStatusUpsert<EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let models = endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n        .await\n        .trace()?;\n\n    if let Some((mut endp, mut metadata)) = models {\n        metadata.data = Set(mem::take(&mut data.metadata));\n        data.update_model(&mut endp);\n        let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Updated((endp, metadata.data).into()))\n    } else {\n        let data = data.into_in_with_default_key();\n        let (endp, metadata) = create_endp_from_data(db, cfg, op_webhooks, app, data)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Created((endp, metadata.data).into()))\n    }\n}\n\n/// Partially update an endpoint.\n#[aide_annotate]\npub(super) async fn patch_endpoint(\n    State(AppState {\n        ref db,\n        cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointPatch>,\n) -> Result<Json<EndpointOut>> {\n    if let UnrequiredNullableField::Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    if let UnrequiredField::Some(url) = &data.url {\n        validate_endpoint_url(url, cfg.endpoint_https_only)?;\n    }\n\n    let (mut endp, mut metadata) =\n        endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut patch_data = data; // need to alias so we can use <|fim_middle|>", "completion": "use std::{collections::HashSet, mem};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/crud.rs", "node_type": "use_declaration", "line_range": [1, 1]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{collections::HashMap, sync::Arc};\n\nuse axum::async_trait;\nuse tokio::{\n    sync::RwLock,\n    task,\n    time::{sleep, Duration, Instant},\n};\n\nuse super::{Cache, CacheBehavior, CacheKey, Result};\n\n#[derive(Debug)]\nstruct ValueWrapper {\n    value: Vec<u8>,\n    ttl: Duration,\n    timer: Instant,\n}\n\nimpl ValueWrapper {\n    fn new(value: Vec<u8>, ttl: Duration) -> ValueWrapper {\n        ValueWrapper {\n            value,\n            ttl,\n            timer: Instant::now(),\n        }\n    }\n}\n\ntype State = HashMap<Vec<u8>, ValueWrapper>;\ntype SharedState = Arc<RwLock<State>>;\n\npub fn new() -> Cache {\n    let shared_state = Arc::new(RwLock::new(State::new()));\n\n    let shared_state_clone = shared_state.clone();\n    task::spawn(async move {\n        loop {\n            sleep(Duration::from_secs(60 * 5)).await;\n            shared_state_clone\n                .write()\n                .await\n                .retain(|_, v| check_is_expired(v))\n        }\n    });\n\n    MemoryCache { map: shared_state }.into()\n}\n\n#[derive(Clone)]\npub struct MemoryCache {\n    map: SharedState,\n}\n\n#[async_trait]\nimpl CacheBehavior for MemoryCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(self\n            .map\n            .read()\n            .await\n            .get(key)\n            .filter(|wrapper| check_is_expired(wrapper))\n            .map(|wrapper| wrapper.value.clone()))\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()> {\n        self.map\n            .write()\n            .await\n            .insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n        Ok(())\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool> {\n        let mut lock = self.map.write().await;\n\n        // TODO: use HashMap::try_insert when stable\n        // https://github.com/rust-lang/rust/issues/82766\n        if !lock.contains_key(key) {\n            lock.insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n            return Ok(true);\n        }\n\n        Ok(false)\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()> {\n        self.map.write().await.remove(key.as_ref().as_bytes());\n\n        Ok(())\n    }\n}\n\nfn check_is_expired(vw: &ValueWrapper) -> bool {\n    vw.timer.elapsed().as_millis() <= vw.ttl.as_millis()\n}\n\n#[cfg(test)]\nmod tests {\n    use serde::{Deserialize, Serialize};\n\n    use super::{\n        super::{kv_def, CacheValue},\n        *,\n    };\n    u<|fim_suffix|>\n    // Test structures\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValA(usize);\n    kv_def!(TestKeyA, TestValA);\n    impl TestKeyA {\n        fn new(id: String) -> TestKeyA {\n            TestKeyA(format!(\"SVIX_TEST_KEY_A_{id}\"))\n        }\n    }\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValB(String);\n    kv_def!(TestKeyB, TestValB);\n    impl TestKeyB {\n        fn new(id: String) -> TestKeyB {\n            TestKeyB(format!(\"SVIX_TEST_KEY_B_{id}\"))\n        }\n    }\n\n    string_kv_def!(StringTestKey);\n    impl StringTestKey {\n        fn new(id: String) -> StringTestKey {\n            StringTestKey(format!(\"SVIX_TEST_KEY_STRING_{id}\"))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_cache_crud_no_ttl() {\n        let cache = new();\n\n        let (first_key, first_val_a, first_val_b) =\n            (TestKeyA::new(\"1\".to_owned()), TestValA(1), TestValA(2));\n        let (second_key, second_val_a, second_val_b) = (\n            TestKeyB::new(\"1\".to_owned()),\n            TestValB(\"1\".to_owned()),\n            TestValB(\"2\".to_owned()),\n        );\n        let (third_key, third_val_a, third_val_b) = (\n            StringTestKey::new(\"1\".to_owned()),\n            \"1\".to_owned(),\n            \"2\".to_owned(),\n        );\n\n        // Create\n        assert!(cache\n            .set(&first_key, &first_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Read\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_a));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_a));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_a)\n        );\n\n        // Update (overwrite)\n        assert!(cache\n            .set(&first_key, &first_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Confirm update\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_b));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_b));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_b)\n        );\n\n        // Delete\n        assert!(cache.delete(&first_key).await.is_ok());\n        assert!(cache.delete(&second_key).await.is_ok());\n        assert!(cache.delete(&third_key).await.is_ok());\n\n        // Confirm deletion\n        assert_eq!(cache.get::<TestValA>(&first_key).await.unwrap(), None);\n        assert_eq!(cache.get::<TestValB>(&second_key).await.unwrap(), None);\n        assert_eq!(cache.get_string(&third_key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_ttl() {\n        let cache = new();\n        let key = TestKeyA::new(\"key\".to_owned());\n\n        assert!(cache\n            .set(&key, &TestValA(1), Duration::from_secs(1),)\n            .await\n            .is_ok());\n        tokio::time::sleep(std::time::Duration::from_millis(1200)).await;\n        assert_eq!(cache.get::<TestValA>(&key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_nx_status() {\n        let cache = new();\n        let key = TestKeyA::new(\"nx_status_test_key\".to_owned());\n\n        assert!(cache\n            .set_if_not_exists(&key, &TestValA(1), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(!cache\n            .set_if_not_exists(&key, &TestValA(2), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(cache.delete(&key).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "use crate::core::cache::string_kv_def;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/memory.rs", "node_type": "use_declaration", "line_range": [113, 113]}
{"prompt": "<|fim_prefix|>_return_type: false,\n        }\n    }\n\n    pub fn with_body_param<T: serde::Serialize>(mut self, param: T) -> Self {\n        self.serialized_body = Some(serde_json::to_string(&param).unwrap());\n        self\n    }\n\n    pub fn with_optional_header_param(\n        mut self,\n        basename: &'static str,\n        param: Option<String>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.header_params.insert(basename, value);\n        }\n        self\n    }\n\n    pub fn with_query_param(mut self, basename: &'static str, param: impl QueryParamValue) -> Self {\n        self.query_params.insert(basename, param.encode());\n        self\n    }\n\n    pub fn with_optional_query_param<T: QueryParamValue>(\n        mut self,\n        basename: &'static str,\n        param: Option<T>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.query_params.insert(basename, value.encode());\n        }\n        self\n    }\n\n    pub fn with_path_param(mut self, basename: &'static str, param: String) -> Self {\n        self.path_params.insert(basename, param);\n        self\n    }\n\n    pub fn returns_nothing(mut self) -> Self {\n        self.no_return_type = true;\n        self\n    }\n\n    pub async fn execute<T: DeserializeOwned>(self, conf: &Configuration) -> Result<T, Error> {\n        match self.execute_with_backoff(conf).await? {\n            // This is a hack; if there's no_ret_type, T is (), but serde_json gives an\n            // error when deserializing \"\" into (), so deserialize 'null' into it\n            // instead.\n            // An alternate option would be to require T: Default, and then return\n            // T::default() here instead since () implements that, but then we'd\n            // need to impl default for all models.\n            None => Ok(serde_json::from_str(\"null\").expect(\"serde null value\")),\n            Some(bytes) => Ok(serde_json::from_slice(&bytes).map_err(Error::generic)?),\n        }\n    }\n\n    async fn execute_with_backoff(mut self, conf: &Configuration) -> Result<Option<Bytes>, Error> {\n        let no_return_type = self.no_return_type;\n        if self.method == http1::Method::POST && !self.header_params.contains_key(\"idempotency-key\")\n        {\n            self.header_params\n                .insert(\"idempotency-key\", format!(\"auto_{}\", uuid::Uuid::new_v4()));\n        }\n\n        const MAX_BACKOFF: Duration = Duration::from_secs(5);\n\n        let retry_schedule = match &conf.retry_schedule {\n            Some(schedule) => schedule,\n            None => &std::iter::successors(Some(Duration::from_millis(20)), |last_backoff| {\n                Some(MAX_BACKOFF.min(*last_backoff * 2))\n            })\n            .take(conf.num_retries as usize)\n            .collect(),\n        };\n        let mut retries = retry_schedule.iter();\n\n        let mut request = self.build_request(conf)?;\n        request\n            .headers_mut()\n            .insert(\"svix-req-id\", rand::rng().random::<u32>().into());\n\n        let mut retry_count = 0;\n\n        let execute_request = async |request| {\n            let response = conf.client.request(request).await.map_err(Error::generic)?;\n\n            let status = response.status();\n            if !status.is_success() {\n                Err(Error::from_response(status, response.into_body()).await)\n            } else if no_return_type {\n                Ok(None)\n            } else {\n                let bytes = response\n                    .into_body()\n                    .collect()\n                    .await\n                    .map_err(Error::generic)?\n                    .to_bytes();\n                Ok(Some(bytes))\n            }\n        };\n\n        loop {\n            let request_fut = execute_request(request.clone());\n            let res = if let Some(duration) = conf.timeout {\n                tokio::time::timeout(duration, request_fut)\n                    .await\n                    .map_err(Error::generic)?\n            } else {\n                request_fut.await\n            };\n\n            let next_backoff = retries.next().copied();\n\n            match res {\n                Ok(result) => return Ok(result),\n                e @ Err(Error::Validation(_)) => return e,\n                Err(Error::Http(err)) if err.status.as_u16() < 500 => return Err(Error::Http(err)),\n                e @ Err(_) => {\n                    if next_backoff.is_none() {\n                        return e;\n                    }\n                }\n            }\n\n            tokio::time::sleep(next_backoff.expect(\"next_backoff is always Some\")).await;\n            retry_count += 1;\n\n            request\n                .headers_mut()\n                .insert(\"svix-retry-count\", retry_count.into());\n        }\n    }\n\n    fn build_request(self, conf: &Configuration) -> Result<http1::Request<Full<Bytes>>, Error> {\n        const FRAGMENT: &AsciiSet = &CONTROLS.add(b' ').add(b'\"').add(b'<').add(b'>').add(b'`');\n        const PATH: &AsciiSet = &FRAGMENT.add(b'#').add(b'?').add(b'{').add(b'}');\n        const PATH_SEGMENT: &AsciiSet = &PATH.add(b'/').add(b'%');\n\n        let mut path = self.path.to_owned();\n        for (k, v) in self.path_params {\n            // replace {id} with the value of the id path param\n            let percent_encoded_path_param_value =\n                utf8_percent_encode(&v, PATH_SEGMENT).to_string();\n            path = path.replace(&format!(\"{{{k}}}\"), &percent_encoded_path_param_value);\n        }\n\n        let mut uri = format!(\"{}{}\", conf.base_path, path);\n\n        <|fim_suffix|>\n        for (key, val) in self.query_params {\n            query_string.append_pair(key, &val);\n        }\n\n        let query_string_str = query_string.finish();\n        if !query_string_str.is_empty() {\n            uri += \"?\";\n            uri += &query_string_str;\n        }\n\n        let uri = http1::Uri::try_from(uri).map_err(Error::generic)?;\n        let mut req_builder = http1::Request::builder().uri(uri).method(self.method);\n\n        let mut request = if let Some(body) = self.serialized_body {\n            let req_headers = req_builder.headers_mut().unwrap();\n            req_headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n            req_headers.insert(CONTENT_LENGTH, body.len().into());\n            req_builder.body(Full::from(body)).map_err(Error::generic)?\n        } else {\n            req_builder.body(Full::default()).map_err(Error::generic)?\n        };\n\n        let request_headers = request.headers_mut();\n\n        // Detect the authorization type if it hasn't been set.\n        let auth = if conf.bearer_access_token.is_some() {\n            Auth::Bearer\n        } else {\n            Auth::None\n        };\n        match auth {\n            Auth::Bearer => {\n                if let Some(token) = &conf.bearer_access_token {\n                    let value = format!(\"Bearer {token}\")\n                        .try_into()\n                        .map_err(Error::generic)?;\n                    request_headers.insert(AUTHORIZATION, value);\n                }\n            }\n            Auth::None => {}\n        }\n\n        if let Some(user_agent) = &conf.user_agent {\n            let value = user_agent.try_into().map_err(Error::generic)?;\n            request_headers.insert(USER_AGENT, value);\n        }\n\n        for (k, v) in self.header_params {\n            let v = v.try_into().map_err(Error::generic)?;\n            request_headers.insert(k, v);\n        }\n\n        Ok(request)\n    }\n}\n\npub(crate) trait QueryParamValue {\n    fn encode(&self) -> String;\n}\n\nmacro_rules! impl_query_param_value {\n    ($ty:ty) => {\n        impl QueryParamValue for $ty {\n            fn encode(&self) -> String {\n                self.to_string()\n            }\n        }\n    };\n}\n\nimpl_query_param_value!(bool);\nimpl_query_param_value!(i32);\nimpl_query_param_value!(String);\nimpl_query_param_value!(models::BackgroundTaskStatus);\nimpl_query_param_value!(models::BackgroundTaskType);\nimpl_query_param_value!(models::ConnectorProduct);\nimpl_query_param_value!(models::MessageStatus);\nimpl_query_param_value!(models::Ordering);\nimpl_query_param_value!(models::StatusCodeClass);\n\nimpl QueryParamValue for Vec<String> {\n    fn encode(&self) -> String {\n        self.iter().format(\",\").to_string()\n    }\n}\n<|fim_middle|>", "completion": "let mut query_string = url::form_urlencoded::Serializer::new(\"\".to_owned());", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/request.rs", "node_type": "let_declaration", "line_range": [198, 198]}
{"prompt": "<|fim_prefix|>use std::{io::Read, str::FromStr};\n\nuse anyhow::{Context, Error, Result};\nuse colored_json::{Color, ColorMode, ToColoredJson};\nuse serde::{de::DeserializeOwned, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq)]\npub struct JsonOf<T>(T);\n\nimpl<T: DeserializeOwned> FromStr for JsonOf<T> {\n    type Err = Error;\n\n    fn from_str(s: &str) -> Result<Self, Self::Err> {\n        <|fim_suffix|>\n    }\n}\n\nimpl<T> JsonOf<T> {\n    pub fn into_inner(self) -> T {\n        self.0\n    }\n}\n\npub fn print_json_output<T>(val: &T, color_mode: ColorMode) -> Result<()>\nwhere\n    T: Serialize,\n{\n    let styler = colored_json::Styler {\n        integer_value: Color::Green.foreground(),\n        float_value: Color::Green.foreground(),\n        bool_value: Color::Yellow.foreground(),\n        nil_value: Color::Magenta.foreground(),\n        string_include_quotation: true,\n        ..Default::default()\n    };\n    let s = serde_json::to_string_pretty(val)?.to_colored_json_with_styler(color_mode, styler)?;\n\n    println!(\"{s}\");\n    Ok(())\n}\n<|fim_middle|>", "completion": "if s == \"-\" {\n            let mut stdin = std::io::stdin().lock();\n            let mut input = String::new();\n            stdin\n                .read_to_string(&mut input)\n                .context(\"Error reading stdin for '-' argument\")?;\n            Ok(JsonOf(serde_json::from_str(&input)?))\n        } else {\n            Ok(JsonOf(serde_json::from_str(s)?))\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/json.rs", "node_type": "if_expression", "line_range": [14, 23]}
{"prompt": "<|fim_prefix|>use axum::{\n    extract::{Path, State},\n    Json,\n};\nuse sea_orm::ActiveModelTrait;\nuse svix_server_derive::aide_annotate;\n\nuse super::{EndpointHeadersIn, EndpointHeadersOut, EndpointHeadersPatchIn};\n<|fim_suffix|>\n\n/// Get the additional headers to be sent with the webhook\n#[aide_annotate(op_id = \"v1.endpoint.get-headers\")]\npub(super) async fn get_endpoint_headers(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointHeadersOut>> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    match endp.headers {\n        Some(h) => Ok(Json(h.into())),\n        None => Ok(Json(EndpointHeadersOut::default())),\n    }\n}\n\n/// Set the additional headers to be sent with the webhook\n#[aide_annotate(op_id = \"v1.endpoint.update-headers\")]\npub(super) async fn update_endpoint_headers(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointHeadersIn>,\n) -> Result<NoContent> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut endp: endpoint::ActiveModel = endp.into();\n    data.update_model(&mut endp);\n    endp.update(db).await?;\n\n    Ok(NoContent)\n}\n\n/// Partially set the additional headers to be sent with the webhook\n#[aide_annotate(op_id = \"v1.endpoint.patch-headers\")]\npub(super) async fn patch_endpoint_headers(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointHeadersPatchIn>,\n) -> Result<NoContent> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut endp: endpoint::ActiveModel = endp.into();\n    data.update_model(&mut endp);\n    endp.update(db).await?;\n\n    Ok(NoContent)\n}\n\n#[cfg(test)]\nmod tests {\n\n    use std::collections::{HashMap, HashSet};\n\n    use sea_orm::ActiveValue::Set;\n\n    use crate::{\n        core::types::{EndpointHeaders, EndpointHeadersPatch},\n        db::models::endpoint,\n        v1::{\n            endpoints::endpoint::{EndpointHeadersOut, EndpointHeadersPatchIn},\n            utils::ModelIn,\n        },\n    };\n\n    #[test]\n    fn test_into_endpoint_headers_out() {\n        let ep = EndpointHeaders(HashMap::from([\n            (\"x-non-sensitive\".to_owned(), \"all-clear\".to_owned()),\n            (\"authorization\".to_owned(), \"should-be-omitted\".to_owned()),\n        ]));\n\n        let epo: EndpointHeadersOut = ep.into();\n        assert_eq!(\n            HashMap::from([(\"x-non-sensitive\".to_owned(), \"all-clear\".to_owned())]),\n            epo.headers\n        );\n        assert_eq!(HashSet::from([\"authorization\".to_owned()]), epo.sensitive);\n    }\n\n    #[test]\n    fn test_patch_endpoint_model_update() {\n        let existing_hdrs = EndpointHeaders(HashMap::from([\n            (\"x-1\".to_owned(), \"123\".to_owned()),\n            (\"x-2\".to_owned(), \"456\".to_owned()),\n        ]));\n        let patched_hdrs = EndpointHeadersPatchIn {\n            headers: EndpointHeadersPatch(HashMap::from([\n                (\"x-1\".to_owned(), Some(\"789\".to_owned())),\n                (\"x-3\".to_owned(), Some(\"123\".to_owned())),\n            ])),\n        };\n        let updated_hdrs = EndpointHeaders(HashMap::from([\n            (\"x-1\".to_owned(), \"789\".to_owned()),\n            (\"x-2\".to_owned(), \"456\".to_owned()),\n            (\"x-3\".to_owned(), \"123\".to_owned()),\n        ]));\n        let mut model = endpoint::ActiveModel {\n            headers: Set(Some(existing_hdrs)),\n            ..Default::default()\n        };\n\n        patched_hdrs.update_model(&mut model);\n        assert_eq!(model.headers, Set(Some(updated_hdrs)));\n    }\n}\n<|fim_middle|>", "completion": "use crate::{\n    core::permissions,\n    db::models::endpoint,\n    error::{HttpError, Result},\n    v1::utils::{ApplicationEndpointPath, ModelIn, NoContent, ValidatedJson},\n    AppState,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/headers.rs", "node_type": "use_declaration", "line_range": [9, 15]}
{"prompt": "<|fim_prefix|>ome(secret_throwaway.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Set the raw value to the database (like legacy)\n    db.execute(Statement::from_sql_and_values(\n        DatabaseBackend::Postgres,\n        \"UPDATE endpoint SET key = $1 WHERE id = $2\",\n        vec![raw_key.clone().into(), endp_1.id.clone().into()],\n    ))\n    .await\n    .unwrap();\n\n    let endp_1 = get_endpoint(&client, &app_id, &endp_1.id).await.unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let (secret, ep) = (secret_1, endp_1);\n    assert_eq!(\n        secret.serialize_public_key(),\n        client\n            .get::<EndpointSecretOutTest>(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n                StatusCode::OK\n            )\n            .await\n            .unwrap()\n            .key\n    );\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_encryption_in_database() {\n    let mut cfg = get_default_test_config();\n    cfg.encryption = Encryption::new([1; 32]);\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let db = Arc::new(cfg);\n    let db = svix_server::db::init_db(&db).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret_encrypted: Option<QueryResult> = db\n        .query_one(Statement::from_sql_and_values(\n            DatabaseBackend::Postgres,\n            \"SELECT key FROM endpoint WHERE id = $1\",\n            vec![ep.id.clone().into()],\n        ))\n        .await\n        .unwrap();\n    let secret_encrypted: Vec<u8> = secret_encrypted.unwrap().try_get(\"\", \"key\").unwrap();\n\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    let secret_clear: Option<QueryResult> = db\n        .query_one(Statement::from_sql_and_values(\n            DatabaseBackend::Postgres,\n            \"SELECT key FROM endpoint WHERE id = $1\",\n            vec![ep.id.clone().into()],\n        ))\n        .await\n        .unwrap();\n    let secret_clear: Vec<u8> = secret_clear.unwrap().try_get(\"\", \"key\").unwrap();\n\n    // Ensure that the length of the encrypted is much longer than the clear\n    assert!(secret_encrypted.len() > secret_clear.len() + 10);\n}\n\n#[tokio::test]\nasync fn test_endpoint_filter_events() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_empty_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"filterTypes\": [],\n    });\n\n    let ep_with_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"filterTypes\": [\"et1\"],\n    });\n\n    let ep_no_events: serde_json::Value = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n    });\n\n    let expected_et = EventTypeNameSet(HashSet::from([EventTypeName(\"et1\".to_owned())]));\n\n    let _ep_with_empty_events: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_empty_events,\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _ep_with_nonexistent_event: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"et1\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    l<|fim_suffix|>\n    assert_eq!(ep_with_valid_event.ep.event_types_ids.unwrap(), expected_et);\n\n    let ep_removed_events: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_valid_event.id),\n            ep_no_events.to_owned(),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(ep_removed_events.ep.event_types_ids.is_none());\n\n    let ep_removed_events = get_endpoint(&client, &app_id, &ep_removed_events.id)\n        .await\n        .unwrap();\n\n    assert!(ep_removed_events.ep.event_types_ids.is_none());\n\n    let ep_updated_events: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_valid_event.id),\n            ep_with_events.to_owned(),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_updated_events.ep.event_types_ids.unwrap(), expected_et);\n\n    let ep_updated_events: EndpointOut = get_endpoint(&client, &app_id, &ep_with_valid_event.id)\n        .await\n        .unwrap();\n\n    assert_eq!(ep_updated_events.ep.event_types_ids.unwrap(), expected_et);\n}\n\n#[tokio::test]\nasync fn test_endpoint_filter_channels() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    // Channels must not be empty:\n    let ep_empty_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"channels\": [],\n    });\n\n    let ep_with_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"channels\": [\"tag1\"],\n    });\n\n    let ep_without_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n    });\n\n    let expected_ec = EventChannelSet(HashSet::from([EventChannel(\"tag1\".to_owned())]));\n\n    let _ep_w_empty_channel: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_empty_channels,\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let ep_with_channel: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_channels.to_owned(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_with_channel.ep.channels.unwrap(), expected_ec);\n\n    let ep_with_deleted_channel: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_channel.id),\n            ep_without_channels,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(ep_with_deleted_channel.ep.channels.is_none());\n\n    // GET / assert channels empty\n    let ep_with_deleted_channel: EndpointOut = get_endpoint(&client, &app_id, &ep_with_channel.id)\n        .await\n        .unwrap();\n\n    assert!(ep_with_deleted_channel.ep.channels.is_none());\n\n    // Update with channels:\n    let updated_ep_with_channel: EndpointOut = client\n        .put(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{}/\",\n                ep_with_deleted_channel.id\n            ),\n            ep_with_channels,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(updated_ep_with_channel.ep.channels.unwrap(), expected_ec);\n\n    // GET / assert channels match\n    let updated_ep_with_channel: EndpointOut =\n        get_endpoint(&client, &app_id, &updated_ep_with_channel.id)\n            .await\n            .unwrap();\n\n    assert_eq!(updated_ep_with_channel.ep.channels.unwrap(), expected_ec);\n}\n\n#[tokio::test]\nasync fn test_rate_limit() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = EndpointIn {\n        rate_limit: Some(100),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    assert_eq!(endp.ep.rate_limit.unwrap(), 100);\n\n    let endp = put_endpoint(\n        &client,\n        &app_id,\n        &endp.id,\n        EndpointIn {\n         <|fim_middle|>", "completion": "let ep_with_valid_event: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1740, 1747]}
{"prompt": "<|fim_prefix|>)\n            .then_some(())\n            .ok_or(WebhookError::InvalidSignature)\n    }\n\n    pub fn sign(\n        &self,\n        msg_id: &str,\n        timestamp: i64,\n        payload: &[u8],\n    ) -> Result<String, WebhookError> {\n        let payload = std::str::from_utf8(payload).map_err(|_| WebhookError::InvalidPayload)?;\n        let to_sign = format!(\"{msg_id}.{timestamp}.{payload}\",);\n        let signed = hmac_sha256::HMAC::mac(to_sign.as_bytes(), &self.key);\n        let encoded = base64::encode(signed);\n\n        Ok(format!(\"{SIGNATURE_VERSION},{encoded}\"))\n    }\n\n    fn get_header<'a, HM: HeaderMap>(\n        headers: &'a HM,\n        svix_hdr: &'static str,\n        unbranded_hdr: &'static str,\n        err_name: &'static str,\n    ) -> Result<&'a str, WebhookError> {\n        use private::HeaderValueSealed as _;\n\n        headers\n            ._get(svix_hdr)\n            .or_else(|| headers._get(unbranded_hdr))\n            .ok_or(WebhookError::MissingHeader(err_name))?\n            ._to_str()\n            .ok_or(WebhookError::InvalidHeader(err_name))\n    }\n\n    fn parse_timestamp(hdr: &str) -> Result<i64, WebhookError> {\n        str::parse::<i64>(hdr).map_err(|_| WebhookError::InvalidTimestamp)\n    }\n\n    fn verify_timestamp(ts: i64) -> Result<(), WebhookError> {\n        let now = OffsetDateTime::now_utc().unix_timestamp();\n        if now - ts > TOLERANCE_IN_SECONDS {\n            Err(WebhookError::TimestampTooOldError)\n        } else if ts > now + TOLERANCE_IN_SECONDS {\n            Err(WebhookError::FutureTimestampError)\n        } else {\n            Ok(())\n        }\n    }\n}\n\n/// Trait to abstract over the `HeaderMap` types from both v0.2 and v1.0 of the\n/// `http` crate.\npub trait HeaderMap: private::HeaderMapSealed {}\n\nimpl HeaderMap for http02::HeaderMap {}\nimpl HeaderMap for http1::HeaderMap {}\n\nmod private {\n    pub trait HeaderMapSealed {\n        type HeaderValue: HeaderValueSealed;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue>;\n    }\n\n    impl HeaderMapSealed for http02::HeaderMap {\n        type HeaderValue = http02::HeaderValue;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue> {\n            self.get(name)\n        }\n    }\n    impl HeaderMapSealed for http1::HeaderMap {\n        type HeaderValue = http1::HeaderValue;\n        fn _get(&self, name: &str) -> Option<&Self::HeaderValue> {\n            self.get(name)\n        }\n    }\n\n    pub trait HeaderValueSealed {\n        fn _to_str(&self) -> Option<&str>;\n    }\n\n    impl HeaderValueSealed for http02::HeaderValue {\n        fn _to_str(&self) -> Option<&str> {\n            self.to_str().ok()\n        }\n    }\n    impl HeaderValueSealed for http1::HeaderValue {\n        fn _to_str(&self) -> Option<&str> {\n            self.to_str().ok()\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use http02::HeaderMap;\n    use time::OffsetDateTime;\n\n    use super::{\n        Webhook, SVIX_MSG_ID_KEY, SVIX_MSG_SIGNATURE_KEY, SVIX_MSG_TIMESTAMP_KEY,\n        UNBRANDED_MSG_ID_KEY, UNBRANDED_MSG_SIGNATURE_KEY, UNBRANDED_MSG_TIMESTAMP_KEY,\n    };\n\n    fn get_svix_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(SVIX_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(SVIX_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            SVIX_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    fn get_unbranded_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(UNBRANDED_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    #[test]\n    f<|fim_suffix|>\n    #[test]\n    fn test_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            wh.verify(payload, &headers).unwrap();\n        }\n    }\n\n    #[test]\n    fn test_no_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = \"v1,R3PTzyfHASBKHH98a7yexTwaJ4yNIcGhFQc1yuN+BPU=\".to_owned();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_partial_signature() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        // Just `v1,`\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = format!(\n                \"{},\",\n                signature.split(',').collect::<Vec<&str>>().first().unwrap()\n            );\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n\n        // Non-empty but still partial signature (first few bytes)\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = &signature[0..8];\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_incorrect_timestamp() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        // Checks that timestamps that are in the future or too old are rejected by\n        // `verify` but okay for `verify_ignoring_timestamp`.\n        for ts in [\n            OffsetDateTime::now_utc().unix_timestamp() - (super::TOLERANCE_IN_SECONDS + 1),\n            OffsetDateTime::now_utc().unix_timestamp() + (super::TOLERANCE_IN_SECONDS + 1),\n        ] {\n            let signature = wh.sign(msg_id, ts, payload).unwrap();\n            let mut headers = get_svix_headers(msg_id, &signature);\n            headers.insert(\n                super::SVIX_MSG_TIMESTAMP_KEY,\n                ts.to_string().parse().unwrap(),\n            );\n\n            assert!(wh.verify(payload, &headers,).is_err());\n            // Timestamp tolerance is not considered in this case.\n            assert!(wh.verify_ignoring_timestamp(payload, &headers,).is_ok());\n        }\n\n        let ts = OffsetDateTime::now_utc().unix_timestamp();\n        let signature = wh.sign(msg_id, ts, payload).unwrap();\n        let mut headers = get_svix_headers(msg_id, &signature);\n        <|fim_middle|>", "completion": "fn test_sign() {\n        let wh = Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap();\n        assert_eq!(\n            \"v1,tZ1I4/hDygAJgO5TYxiSd6Sd0kDW6hPenDe+bTa3Kkw=\".to_owned(),\n            wh.sign(\n                \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\",\n                1649367553,\n                br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#\n            )\n            .unwrap()\n        );\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/webhooks.rs", "node_type": "function_item", "line_range": [248, 259]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining an interface for sending webhook events about the service.\n\nuse std::sync::Arc;\n\nuse chrono::{DateTime, Utc};\nuse schemars::JsonSchema;\nuse serde::Serialize;\nuse svix::api::{MessageIn, Svix, SvixOptions};\n\nuse super::{\n    security::generate_management_token,\n    types::{\n        ApplicationId, ApplicationUid, EndpointId, EndpointUid, MessageAttemptId, MessageId,\n        MessageUid, OrganizationId,\n    },\n};\nuse crate::{\n    core::security::JwtSigningConfig,\n    db::models::{endpoint, messageattempt},\n    error::{Error, HttpError, Result},\n};\n\n/// Sent when an endpoint has been automatically disabled after continuous failures.\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointDisabledEventData {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub endpoint_id: EndpointId,\n    pub endpoint_uid: Option<EndpointUid>,\n    pub fail_since: DateTime<Utc>,\n}\n\n/// Sent when an endpoint is created, updated, or deleted\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointEvent {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub endpoint_id: EndpointId,\n    pub endpoint_uid: Option<EndpointUid>,\n}\n\nimpl EndpointEvent {\n    pub fn new(app_uid: Option<&ApplicationUid>, endp: &endpoint::Model) -> Self {\n        Self {\n            app_id: endp.app_id.clone(),\n            app_uid: app_uid.cloned(),\n            endpoint_id: endp.id.clone(),\n            endpoint_uid: endp.uid.clone(),\n        }\n    }\n}\n\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageAttempetLast {\n    pub id: MessageAttemptId,\n    pub response_status_code: i16,\n    pub timestamp: DateTime<Utc>,\n}\n\nimpl From<messageattempt::Model> for MessageAttempetLast {\n    fn from(attempt: messageattempt::Model) -> Self {\n        Self {\n            id: attempt.id,\n            response_status_code: attempt.response_status_code,\n            timestamp: attempt.created_at.into(),\n        }\n    }\n}\n\n/// Sent when a message delivery has failed (all of the retry attempts have been exhausted) as a\n/// \"message.attempt.exhausted\" type or after it's failed four times as a \"message.attempt.failing\"\n/// event.\n#[derive(Debug, Serialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageAttemptEvent {\n    pub app_id: ApplicationId,\n    pub app_uid: Option<ApplicationUid>,\n    pub msg_id: MessageId,\n    pub msg_event_id: Option<MessageUid>,\n    pub endpoint_id: EndpointId,\n    pub last_attempt: MessageAttempetLast,\n}\n\n#[derive(Debug, Serialize)]\n#[serde(tag = \"type\", content = \"data\")]\npub enum OperationalWebhook {\n    #[serde(rename = \"endpoint.disabled\")]\n    EndpointDisabled(EndpointDisabledEventData),\n    #[serde(rename = \"endpoint.created\")]\n    EndpointCreated(EndpointEvent),\n    #[serde(rename = \"endpoint.updated\")]\n    EndpointUpdated(EndpointEvent),\n    #[serde(rename = \"endpoint.deleted\")]\n    EndpointDeleted(EndpointEvent),\n    #[serde(rename = \"message.attempt.exhausted\")]\n    MessageAttemptExhausted(MessageAttemptEvent),\n    #[serde(rename = \"message.attempt.failing\")]\n    MessageAttemptFailing(MessageAttemptEvent),\n    #[serde(rename = \"message.attempt.recovered\")]\n    MessageAttemptRecovered(MessageAttemptEvent),\n}\n\npub type OperationalWebhookSender = Arc<OperationalWebhookSenderInner>;\n\npub struct OperationalWebhookSenderInner {\n    signing_config: Arc<JwtSigningConfig>,\n    url: Option<String>,\n}\n\nimpl OperationalWebhookSenderInner {\n    pub fn new(keys: Arc<JwtSigningConfig>, mut url: Option<String>) -> Arc<Self> {\n        // Sanitize the URL if present\n        if let Some(url) = &mut url {\n            // Remove trailing slashes\n            while url.ends_with('/') {\n                url.pop();\n            }\n        }\n\n        Arc::new(Self {\n            signing_config: keys,\n            url,\n        })\n    }\n\n    pub async fn send_operational_webhook(\n        &self,\n        recipient_org_id: &OrganizationId,\n        payload: OperationalWebhook,\n    ) -> Result<()> {\n        let Some(url) = &self.url else { return Ok(()) };\n\n        let op_webhook_token =\n            generate_management_token(&self.signing_config).map_err(Error::generic)?;\n        let svix_api = Svix::new(\n            op_webhook_token,\n            Some(SvixOptions {\n                server_url: Some(url.to_string()),\n                ..Default::default()\n            }),\n        );\n\n        let payload = serde_json::to_value(payload)\n            .map_err(|_| HttpError::internal_server_error(None, None))?;\n\n        // Get the event type from the type field\n        l<|fim_suffix|>\n        let recipient_org_id = recipient_org_id.to_string();\n\n        tokio::spawn(async move {\n            // This sends a webhook under the Svix management organization. This organization contains\n            // applications which are each a regular organization. The recipient's OrganizationId is the\n            // app UID to use.\n            let resp = svix_api\n                .message()\n                .create(\n                    recipient_org_id.clone(),\n                    MessageIn {\n                        event_type,\n                        payload,\n                        ..MessageIn::default()\n                    },\n                    None,\n                )\n                .await;\n\n            match resp {\n                Ok(_) => {}\n                // Ignore 404s because not every org will have an associated application\n                Err(svix::error::Error::Http(svix::error::HttpErrorContent {\n                    status: http02::StatusCode::NOT_FOUND,\n                    ..\n                })) => {\n                    tracing::warn!(\n                        \"Operational webhooks are enabled, but no listener found for organization {}\",\n                        recipient_org_id,\n                    );\n                }\n                Err(e) => {\n                    tracing::error!(\n                        \"Failed sending operational webhook for {} {}\",\n                        recipient_org_id,\n                        e.to_string()\n                    );\n                }\n            }\n        });\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let event_type: String = payload\n            .get(\"type\")\n            .ok_or_else(|| HttpError::internal_server_error(None, None))?\n            .as_str()\n            .ok_or_else(|| HttpError::internal_server_error(None, None))?\n            .to_string();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/operational_webhooks.rs", "node_type": "let_declaration", "line_range": [153, 158]}
{"prompt": "<|fim_prefix|>e()),\n        org_id: Set(app.org_id),\n        ..data.into()\n    };\n\n    let (msg, msg_content) = db\n        .transaction(|txn| {\n            async move {\n                let msg = msg.insert(txn).await.map_err(http_error_on_conflict)?;\n                let msg_content =\n                    messagecontent::ActiveModel::new(msg.id.clone(), payload, msg.expiration);\n                let msg_content = msg_content.insert(txn).await?;\n                Ok((msg, msg_content))\n            }\n            .boxed()\n        })\n        .await?;\n\n    let trigger_type = MessageAttemptTriggerType::Scheduled;\n    if !create_message_app\n        .filtered_endpoints(trigger_type, &msg.event_type, msg.channels.as_ref())\n        .is_empty()\n    {\n        queue_tx\n            .send(\n                &MessageTaskBatch::new_task(\n                    msg.id.clone(),\n                    app.id.clone(),\n                    force_endpoint,\n                    MessageAttemptTriggerType::Scheduled,\n                ),\n                None,\n            )\n            .await?;\n    }\n\n    let msg_out = MessageOut::from_msg_and_payload(msg, Some(msg_content.payload), with_content);\n\n    Ok(msg_out)\n}\n\npub fn validate_create_app_uid(\n    app_id_or_uid: &ApplicationIdOrUid,\n    data: &ApplicationIn,\n) -> Result<()> {\n    // If implicit app creation is requested then the UID must be set\n    // in the request body, and it must match the UID given in the path\n    if let Some(uid) = &data.uid {\n        if uid.0 != app_id_or_uid.0 {\n            return Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n                loc: vec![\n                    \"body\".to_string(),\n                    \"application\".to_string(),\n                    \"uid\".to_string(),\n                ],\n                msg: \"Application UID in the path and body must match\".to_string(),\n                ty: \"application_uid_mismatch\".to_string(),\n            }])\n            .into());\n        }\n    } else {\n        return Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n            loc: vec![\n                \"body\".to_string(),\n                \"application\".to_string(),\n                \"uid\".to_string(),\n            ],\n            msg: \"Application UID not set in body\".to_string(),\n            ty: \"application_uid_missing\".to_string(),\n        }])\n        .into());\n    }\n    Ok(())\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct GetMessageQueryParams {\n    /// When `true` message payloads are included in the response\n    #[serde(default = \"default_true\")]\n    with_content: bool,\n}\n\n/// Get a message by its ID or eventID.\n#[aide_annotate(op_id = \"v1.message.get\")]\nasync fn get_message(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgPath { msg_id, .. }): Path<ApplicationMsgPath>,\n    ValidatedQuery(GetMessageQueryParams { with_content }): ValidatedQuery<GetMessageQueryParams>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<MessageOut>> {\n    let (msg, msg_content) = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .find_also_related(messagecontent::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    let msg_out =\n        MessageOut::from_msg_and_payload(msg, msg_content.map(|c| c.payload), with_content);\n    Ok(Json(msg_out))\n}\n\n/// Delete the given message's payload. Useful in cases when a message was accidentally sent with sensitive content.\n///\n/// The message can't be replayed or resent once its payload has been deleted or expired.\n#[aide_annotate(op_id = \"v1.message.expunge-content\")]\nasync fn expunge_message_content(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgPath { msg_id, .. }): Path<ApplicationMsgPath>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n) -> Result<StatusCode> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    let msg_id = msg.id.clone();\n    let mut msg = msg.into_active_model();\n\n    msg.legacy_payload = Set(None);\n    msg.update(db).await?;\n\n    messagecontent::Entity::delete_by_id(msg_id)\n        .exec(db)\n        .await?;\n    Ok(StatusCode::NO_CONTENT)\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Message\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app/:app_id/msg\",\n            post_with(create_message, create_message_operation)\n                .get_with(list_messages, list_messages_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id\",\n            get_with(get_message, get_message_operation),\n            &tag,\n        )\n        .api_route_with(\n            \"/app/:app_id/msg/:msg_id/content\",\n            delete_with(expunge_message_content, expunge_message_content_operation),\n            tag,\n        )\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{\n        default_true, CreateMessageQueryParams, GetMessageQueryParams, ListMessagesQueryParams,\n        MessageIn,\n    };\n\n    const CHANNEL_INVALID: &str = \"$$invalid-channel\";\n    const CHANNEL_VALID: &str = \"valid-channel\";\n    const EVENT_TYPE_INVALID: &str = \"$$invalid-eventType\";\n    const EVENT_TYPE_VALID: &str = \"valid-eventType\";\n    const EVENT_ID_INVALID: &str = \"$$invalid-eventId\";\n    const EVENT_ID_VALID: &str = \"valid-eventId\";\n    const EVENT_CHANNELS_INVALID: &[&str] = &[\"valid-event-channel\", \"&&invalid-event-channel\"];\n    const EVENT_CHANNELS_VALID: &[&str] = &[\"valid-event-channel1\", \"valid-event-channel2\"];\n\n    #[test]\n    fn test_message_in_validation() {\n        let invalid_1: MessageIn = serde_json::from_value(json!({\n            \"eventId\": EVENT_ID_INVALID,\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": {}\n        }))\n        .unwrap();\n\n        let invalid_2: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_INVALID,\n            \"payload\": {}\n        }))\n        .unwrap();\n\n        let invalid_3: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": {},\n            \"channels\": EVENT_CHANNELS_INVALID\n        }))\n        .unwrap();\n\n        let invalid_4: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": \"this should be invalid\",\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n\n        let invalid_5: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": json!([ \"this should be invalid\" ]),\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n\n        l<|fim_suffix|>\n        for m in [\n            invalid_1, invalid_2, invalid_3, invalid_4, invalid_5, invalid_6,\n        ] {\n            assert!(m.validate().is_err());\n        }\n\n        let valid: MessageIn = serde_json::from_value(json!({\n            \"eventId\": EVENT_ID_VALID,\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": {},\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();\n        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_list_messages_query_params_validation() {\n        let invalid: ListMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": CHANNEL_INVALID })).unwrap();\n        assert!(invalid.validate().is_err());\n\n        let valid: ListMessagesQueryParams =\n            serde_json::from_value(json!({ \"channel\": CHANNEL_VALID })).unwrap();\n        valid.validate().unwrap();\n    }\n\n    #[test]\n    fn test_default_true() {\n        assert!(default_true());\n    }\n\n    #[test]\n    fn test_create_message_query_params_default() {\n        let q: CreateMessageQueryParams = serde_json::from_value(json!({})).unwrap();\n        assert!(q.with_content);\n    }\n\n    #[test]\n    fn test_get_message_query_params_default() {\n        let q: GetMessageQueryParams = serde_json::from_value(json!({})).unwrap();\n        assert!(q.with_content);\n    }\n}\n<|fim_middle|>", "completion": "let invalid_6: MessageIn = serde_json::from_value(json!({\n            \"eventType\": EVENT_TYPE_VALID,\n            \"payload\": json!([ { \"msg\": \"this should be invalid\" } ]),\n            \"channels\": EVENT_CHANNELS_VALID\n        }))\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/message.rs", "node_type": "let_declaration", "line_range": [623, 628]}
{"prompt": "<|fim_prefix|>tion and organization ID.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageApp {\n    pub id: ApplicationId,\n    pub uid: Option<ApplicationUid>,\n    pub org_id: OrganizationId,\n    pub rate_limit: Option<u16>,\n    endpoints: Vec<CreateMessageEndpoint>,\n    deleted: bool,\n}\n\nimpl CreateMessageApp {\n    /// Fetch all requisite information for creating a [`CreateMessageApp`] from the PostgreSQL\n    /// database\n    async fn fetch_from_pg_by_model(\n        db: &DatabaseTransaction,\n        app: application::Model,\n    ) -> Result<CreateMessageApp> {\n        let endpoints = endpoint::Entity::secure_find(app.id.clone())\n            .all(db)\n            .await?\n            .into_iter()\n            .map(TryInto::try_into)\n            .collect::<Result<Vec<_>>>()?;\n\n        Ok(CreateMessageApp {\n            id: app.id,\n            uid: app.uid,\n            org_id: app.org_id,\n            rate_limit: app\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Application rate limit out of bounds\"))?,\n            endpoints,\n            deleted: app.deleted,\n        })\n    }\n\n    /// Fetches all information for creating a [`CreateMessageApp`] from the Redis cache if it\n    /// exists or from PostgreSQL otherwise. If the RedisCache is Some, but does not contain the\n    /// requisite information, fetch it from PostgreSQL and insert the data into the cache.\n    pub async fn layered_fetch(\n        cache: &Cache,\n        pg: &DatabaseConnection,\n        app: Option<application::Model>,\n        org_id: OrganizationId,\n        app_id: ApplicationId,\n        ttl: Duration,\n    ) -> Result<Option<CreateMessageApp>> {\n        let cache_key = AppEndpointKey::new(&org_id, &app_id);\n\n        // First check Redis\n        if let Ok(Some(cma)) = cache.get::<CreateMessageApp>(&cache_key).await {\n            if cma.deleted {\n                return Ok(None);\n            } else {\n                return Ok(Some(cma));\n            }\n        }\n\n        // Then check PostgreSQL\n        let db = pg.begin().await?;\n        // Fetch the [`application::Model`] either given or from the ID\n        let app = if let Some(app) = app {\n            app\n        } else if let Some(app) = application::Entity::secure_find_by_id(org_id, app_id)\n            .one(&db)\n            .await?\n        {\n            app\n        } else {\n            return Ok(None);\n        };\n\n        // Fetch the actual [`CreateMessageApp`]\n        let out = Self::fetch_from_pg_by_model(&db, app).await?;\n\n        // Insert it into Redis\n        let _ = cache.set(&cache_key, &out, ttl).await;\n\n        if out.deleted {\n            return Ok(None);\n        }\n\n        Ok(Some(out))\n    }\n\n    pub fn filtered_endpoints(\n        &self,\n        trigger_type: MessageAttemptTriggerType,\n        event_type: &EventTypeName,\n        channels: Option<&EventChannelSet>,\n    ) -> Vec<CreateMessageEndpoint> {\n        self.endpoints\n            .iter()\n            .filter(|endpoint| {\n                // No disabled or deleted endpoints ever\n                !endpoint.disabled && !endpoint.deleted\n                    // Manual attempt types go through regardless\n                    && (trigger_type == MessageAttemptTriggerType::Manual\n                        || (\n                            // If an endpoint has event types and it matches ours, or has no event types\n                            endpoint\n                                .event_types_ids\n                                .as_ref()\n                                .map(|x| x.0.contains(event_type))\n                                .unwrap_or(true)\n                            // If an endpoint has no channels accept all messages, otherwise only if their channels overlap.\n                            // A message with no channels doesn't match an endpoint with channels.\n                            && endpoint\n                                .channels\n                                .as_ref()\n                                .map(|x| {\n                                    !x.0.is_disjoint(\n                                        channels.map(|x| &x.0).unwrap_or(&HashSet::new()),\n                                    )\n                                })\n                                .unwrap_or(true)\n                        ))\n            })\n            .cloned()\n            .collect()\n    }\n}\n\n/// The information for each individual endpoint cached with the creation of a message.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageEndpoint {\n    pub id: EndpointId,\n    pub url: String,\n    pub key: EndpointSecretInternal,\n    pub event_types_ids: Option<EventTypeNameSet>,\n    pub channels: Option<EventChannelSet>,\n    pub rate_limit: Option<u16>,\n    // Same type as the `DateTimeWithTimeZone from SeaORM used in the endpoint model\n    pub first_failure_at: Option<DateTime<FixedOffset>>,\n    pub headers: Option<EndpointHeaders>,\n    pub disabled: bool,\n    pub deleted: bool,\n    // outside of this module, valid_signing_keys should be used instead\n    old_signing_keys: Option<ExpiringSigningKeys>,\n}\n\nimpl CreateMessageEndpoint {\n    pub fn valid_signing_keys(&self) -> Vec<&EndpointSecretInternal> {\n        match self.old_signing_keys {\n            Some(ref old_keys) => std::iter::once(&self.key)\n                .chain(\n                    old_keys\n                        .0\n                        .iter()\n                        .filter(|x| x.expiration > Utc::now())\n                        .map(|x| &x.key),\n                )\n                .collect(),\n            None => vec![&self.key],\n        }\n    }\n}\n\nimpl TryFrom<endpoint::Model> for CreateMessageEndpoint {\n    type Error = Error;\n\n    fn try_from(m: endpoint::Model) -> Result<CreateMessageEndpoint> {\n        Ok(CreateMessageEndpoint {\n            id: m.id,\n            url: m.url,\n            key: m.key,\n            old_signing_keys: m.old_keys,\n            event_types_ids: m.event_types_ids,\n            channels: m.channels,\n            rate_limit: m\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Endpoint rate limit out of bounds\"))?,\n            first_failure_at: m.first_failure_at,\n            headers: m.headers,\n            disabled: m.disabled,\n            deleted: m.deleted,\n        })\n    }\n}\n\nkv_def!(AppEndpointKey, CreateMessageApp);\n<|fim_suffix|>\n\n#[cfg(test)]\nmod tests {\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use chrono::Utc;\n\n    use super::CreateMessageEndpoint;\n    use crate::core::{\n        cryptography::Encryption,\n        types::{\n            EndpointId, EndpointSecret, EndpointSecretInternal, ExpiringSigningKey,\n            ExpiringSigningKeys,\n        },\n    };\n\n    #[test]\n    fn test_valid_signing_keys() {\n        let key = EndpointSecretInternal::from_endpoint_secret(\n            EndpointSecret::Symmetric(STANDARD.decode(\"MfKQ9r8GKYqrTwjUPD8ILPZIo2LaLaSw\").unwrap()),\n            &Encryption::new_noop(),\n        )\n        .unwrap();\n\n        let unexpired_old_key = ExpiringSigningKey {\n            key: key.clone(),\n            expiration: Utc::now()\n                + chrono::Duration::hours(ExpiringSigningKeys::OLD_KEY_EXPIRY_HOURS),\n        };\n        let expired_old_key = ExpiringSigningKey {\n            key: key.clone(),\n            expiration: Utc::now()\n                - chrono::Duration::hours(ExpiringSigningKeys::OLD_KEY_EXPIRY_HOURS),\n        };\n        let old_signing_keys = Some(ExpiringSigningKeys(vec![\n            unexpired_old_key,\n            expired_old_key,\n        ]));\n\n        let cme = CreateMessageEndpoint {\n            id: EndpointId::from(\"Test\".to_string()),\n            url: \"\".to_string(),\n            key,\n            old_signing_keys,\n            event_types_ids: None,\n            channels: None,\n            rate_limit: None,\n            first_failure_at: None,\n            headers: None,\n            disabled: false,\n            deleted: false,\n        };\n\n        let keys = cme.valid_signing_keys();\n\n        assert_eq!(keys.len(), 2);\n    }\n}\n<|fim_middle|>", "completion": "impl AppEndpointKey {\n    // FIXME: Rewrite doc comment when AppEndpointValue members are known\n    /// Returns a key for fetching all cached endpoints for a given organization and application.\n    pub fn new(org: &OrganizationId, app: &ApplicationId) -> AppEndpointKey {\n        AppEndpointKey(format!(\"SVIX_CACHE_APP_v3_{org}_{app}\"))\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/message_app.rs", "node_type": "impl_item", "line_range": [208, 214]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::fmt::Debug;\n\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse chacha20poly1305::{\n    aead::{Aead, KeyInit},\n    Key, XChaCha20Poly1305, XNonce,\n};\nuse ed25519_compact::*;\nuse rand::Rng;\n\nuse crate::error::Result;\n\n// Asymmetric Signature keys\n#[derive(Clone, Eq)]\npub struct AsymmetricKey(pub KeyPair);\n\nimpl AsymmetricKey {\n    pub fn generate() -> AsymmetricKey {\n        AsymmetricKey(KeyPair::from_seed(Seed::generate()))\n    }\n\n    pub fn from_slice(bytes: &[u8]) -> Result<Self> {\n        Ok(AsymmetricKey(KeyPair::from_slice(bytes).map_err(|_| {\n            crate::error::Error::generic(\"Failed parsing key.\")\n        })?))\n    }\n\n    pub fn from_base64(b64: &str) -> Result<Self> {\n        let bytes = STANDARD\n            .decode(b64)\n            .map_err(|_| crate::error::Error::generic(\"Failed parsing base64\"))?;\n\n        Self::from_slice(bytes.as_slice())\n    }\n\n    pub fn pubkey(&self) -> &[u8] {\n        &self.0.pk[..]\n    }\n}\n\nimpl Debug for AsymmetricKey {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"<AsymmetricKey sk=*** pk={}>\",\n            STANDARD.encode(self.0.pk.as_slice())\n        )\n    }\n}\n\nimpl PartialEq for AsymmetricKey {\n    fn eq(&self, other: &Self) -> bool {\n        self.0.as_slice() == other.0.as_slice()\n    }\n}\n\n#[derive(Clone, Debug)]\npub struct Encryption(Option<Key>);\n\nimpl Encryption {\n    const NONCE_SIZE: usize = 24;\n\n    p<|fim_suffix|>\n    pub fn new(key: [u8; 32]) -> Self {\n        Self(Some(Key::from_slice(&key).to_owned()))\n    }\n\n    pub fn encrypt(&self, data: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce: [u8; Self::NONCE_SIZE] = rand::thread_rng().gen();\n            let nonce = XNonce::from_slice(&nonce);\n            let mut ciphertext = cipher\n                .encrypt(nonce, data)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))?;\n            let mut ret = nonce.to_vec();\n            ret.append(&mut ciphertext);\n            Ok(ret)\n        } else {\n            Ok(data.to_vec())\n        }\n    }\n\n    pub fn decrypt(&self, ciphertext: &[u8]) -> Result<Vec<u8>> {\n        if let Some(main_key) = self.0.as_ref() {\n            let cipher = XChaCha20Poly1305::new(main_key);\n            let nonce = &ciphertext[..Self::NONCE_SIZE];\n            let ciphertext = &ciphertext[Self::NONCE_SIZE..];\n            cipher\n                .decrypt(XNonce::from_slice(nonce), ciphertext)\n                .map_err(|_| crate::error::Error::generic(\"Encryption failed\"))\n        } else {\n            Ok(ciphertext.to_vec())\n        }\n    }\n\n    pub fn enabled(&self) -> bool {\n        self.0.is_some()\n    }\n}\n\nimpl Default for Encryption {\n    fn default() -> Self {\n        Self::new_noop()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use super::Encryption;\n\n    #[test]\n    fn test_encryption() {\n        let clear = b\"hello world\";\n        let encryption = Encryption::new([1; 32]);\n        let ciphertext = encryption.encrypt(clear).unwrap();\n        let clear2 = encryption.decrypt(&ciphertext).unwrap();\n        assert_eq!(&clear[..], &clear2[..]);\n    }\n}\n<|fim_middle|>", "completion": "pub fn new_noop() -> Self {\n        Self(None)\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cryptography.rs", "node_type": "function_item", "line_range": [66, 68]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{error, fmt, panic::Location};\n\nuse aide::OperationOutput;\nuse axum::{\n    extract::rejection::{ExtensionRejection, PathRejection},\n    response::{IntoResponse, Response},\n    Json,\n};\nuse hyper::StatusCode;\nuse schemars::JsonSchema;\nuse sea_orm::{DbErr, RuntimeErr, TransactionError};\nuse serde::Serialize;\nuse serde_json::json;\n\nuse crate::core::webhook_http_client;\n\n/// A short-hand version of a [`std::result::Result`] that defaults to Svix'es [Error].\npub type Result<T, E = Error> = std::result::Result<T, E>;\n\n/// The error type returned from the Svix API\n#[derive(Debug)]\npub struct Error {\n    // the file name and line number of the error. Used for debugging non Http errors\n    pub trace: Vec<&'static Location<'static>>,\n    pub typ: ErrorType,\n}\n\nimpl Error {\n    #[track_caller]\n    fn new(typ: ErrorType) -> Self {\n        let trace = vec![Location::caller()];\n        Self { trace, typ }\n    }\n\n    #[track_caller]\n    pub fn generic(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Generic(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn database(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Database(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn conflict(e: DbErr) -> Self {\n        Self::new(ErrorType::Conflict(e))\n    }\n\n    #[track_caller]\n    pub fn queue(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Queue(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn validation(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Validation(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn http(h: HttpError) -> Self {\n        Self {\n            trace: Vec::with_capacity(0), // no debugging necessary\n            typ: ErrorType::Http(h),\n        }\n    }\n\n    #[track_caller]\n    pub fn cache(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Cache(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::Timeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn db_timeout(s: impl fmt::Display) -> Self {\n        Self::new(ErrorType::DbTimeout(s.to_string()))\n    }\n\n    #[track_caller]\n    pub fn connection_timeout(e: DbErr) -> Self {\n        Self::new(ErrorType::ConnectionTimeout(e))\n    }\n\n    #[track_caller]\n    pub fn trace(mut self) -> Self {\n        self.trace.push(Location::caller());\n        self\n    }\n}\n\nimpl fmt::Display for Error {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        self.typ.fmt(f)\n    }\n}\n\nimpl error::Error for Error {\n    fn source(&self) -> Option<&(dyn error::Error + 'static)> {\n        None\n    }\n}\n\nimpl IntoResponse for Error {\n    fn into_response(self) -> Response {\n        let stringified: Vec<String> = self.trace.into_iter().map(ToString::to_string).collect();\n        match self.typ {\n            ErrorType::Http(s) => {\n                tracing::debug!(\"{:?}, location: {:?}\", &s, stringified);\n                s.into_response()\n            }\n            s => {\n                tracing::error!(\"type: {:?}, location: {:?}\", s, stringified);\n                (StatusCode::INTERNAL_SERVER_ERROR, Json(json!({}))).into_response()\n            }\n        }\n    }\n}\n\nimpl OperationOutput for Error {\n    type Inner = Self;\n}\n\npub trait Traceable<T> {\n    /// Pushes the current [`Location`] onto the error's trace stack\n    #[track_caller]\n    fn trace(self) -> Result<T>;\n}\n\nimpl<T> Traceable<T> for Result<T> {\n    fn trace(self) -> Result<T> {\n        // Using `map_err` would lose `#[track_caller]` information\n        match self {\n            Err(e) => Err(e.trace()),\n            ok => ok,\n        }\n    }\n}\n\nimpl From<DbErr> for Error {\n    #[track_caller]\n    fn from(err: DbErr) -> Self {\n        if is_timeout_error(&err) {\n            Error::db_timeout(err)\n        } else i<|fim_suffix|>    }\n}\n\nimpl From<redis::RedisError> for Error {\n    #[track_caller]\n    fn from(value: redis::RedisError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<omniqueue::QueueError> for Error {\n    #[track_caller]\n    fn from(value: omniqueue::QueueError) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl<E: error::Error + 'static> From<bb8::RunError<E>> for Error {\n    #[track_caller]\n    fn from(value: bb8::RunError<E>) -> Self {\n        Error::queue(value)\n    }\n}\n\nimpl From<ExtensionRejection> for Error {\n    #[track_caller]\n    fn from(value: ExtensionRejection) -> Self {\n        Error::generic(value)\n    }\n}\n\nimpl From<PathRejection> for Error {\n    #[track_caller]\n    fn from(value: PathRejection) -> Self {\n        Error::generic(value)\n    }\n}\n\nimpl From<crate::core::cache::Error> for Error {\n    #[track_caller]\n    fn from(value: crate::core::cache::Error) -> Self {\n        Error::cache(value)\n    }\n}\n\nimpl From<TransactionError<Error>> for Error {\n    #[track_caller]\n    fn from(value: TransactionError<Error>) -> Self {\n        match value {\n            TransactionError::Connection(db_err) => Error::database(db_err),\n            TransactionError::Transaction(crate_err) => crate_err, // preserve the trace that comes from within the transaction\n        }\n    }\n}\n\nimpl From<lapin::Error> for Error {\n    #[track_caller]\n    fn from(value: lapin::Error) -> Self {\n        Error::queue(format_args!(\"{value:?}\"))\n    }\n}\n\n#[derive(Debug)]\npub enum ErrorType {\n    /// A generic error\n    Generic(String),\n    /// Database error\n    Database(String),\n    /// Queue error\n    Queue(String),\n    /// Database error\n    Validation(String),\n    /// Any kind of HttpError\n    Http(HttpError),\n    /// Cache error\n    Cache(String),\n    /// Timeout error\n    Timeout(String),\n    /// Database timeout error\n    DbTimeout(String),\n    /// Connection timeout error\n    ConnectionTimeout(DbErr),\n    /// Conflict error\n    Conflict(DbErr),\n}\n\nimpl fmt::Display for ErrorType {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Generic(s) => s.fmt(f),\n            Self::Database(s) => s.fmt(f),\n            Self::Queue(s) => s.fmt(f),\n            Self::Validation(s) => s.fmt(f),\n            Self::Http(s) => s.fmt(f),\n            Self::Cache(s) => s.fmt(f),\n            Self::Timeout(s) => s.fmt(f),\n            Self::DbTimeout(s) => s.fmt(f),\n            Self::ConnectionTimeout(s) => s.fmt(f),\n            Self::Conflict(s) => s.fmt(f),\n        }\n    }\n}\n\nimpl From<HttpError> for ErrorType {\n    fn from(e: HttpError) -> Self {\n        Self::Http(e)\n    }\n}\n\n// Python generation relies on the title of this being `HttpError`\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HttpErrorOut\", title = \"HttpError\")]\npub struct StandardHttpError {\n    code: String,\n    detail: String,\n}\n\n#[derive(Debug, Clone, Serialize, JsonSchema)]\n#[schemars(rename = \"HTTPValidationError\")]\npub struct ValidationHttpError {\n    detail: Vec<ValidationErrorItem>,\n}\n\n#[derive(Debug, Clone, Serialize)]\n#[serde(untagged)]\npub enum HttpErrorBody {\n    Standard(StandardHttpError),\n    Validation(ValidationHttpError),\n}\n\n#[derive(Debug, Clone, Serialize, PartialEq, Eq, JsonSchema)]\n/// Validation errors have their own schema to provide context for invalid requests eg. mismatched\n/// types and out of bounds values. There may be any number of these per 422 UNPROCESSABLE ENTITY\n/// error.\npub struct ValidationErrorItem {\n    /// The location as a [`Vec`] of [`String`]s -- often in the form `[\"body\", \"field_name\"]`,\n    /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, detail: String) -> Self {\n        Self {\n            status,\n            body: HttpErrorBody::Standard(StandardHttpError { code, d<|fim_middle|>", "completion": "if is_conflict_err(&err) {\n            Error::conflict(err)\n        } else if is_connection_timeout_error(&err) {\n            Error::connection_timeout(err)\n        } else {\n            Error::database(err)\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "if_expression", "line_range": [151, 157]}
{"prompt": "<|fim_prefix|>use std::iter;\n\nuse axum::{\n    extract::{Path, State},\n    Json,\n};\nuse chrono::{Duration, Utc};\nuse sea_orm::{ActiveModelTrait, ActiveValue::Set};\nuse svix_server_derive::aide_annotate;\n\nuse super::{EndpointSecretOut, EndpointSecretRotateIn};\nuse crate::{\n    cfg::DefaultSignatureType,\n    core::{\n        cryptography::Encryption,\n        operational_webhooks::{EndpointEvent, OperationalWebhook},\n        permissions,\n        types::{EndpointSecretInternal, ExpiringSigningKey, ExpiringSigningKeys},\n    },\n    db::models::endpoint,\n    error::{HttpError, Result},\n    v1::utils::{ApplicationEndpointPath, NoContent, ValidatedJson},\n    AppState,\n};\n\npub(super) fn generate_secret(\n    encryption: &Encryption,\n    sig_type: &DefaultSignatureType,\n) -> Result<EndpointSecretInternal> {\n    match sig_type {\n        DefaultSignatureType::Hmac256 => EndpointSecretInternal::generate_symmetric(encryption),\n        DefaultSignatureType::Ed25519 => EndpointSecretInternal::generate_asymmetric(encryption),\n    }\n}\n\n/// Get the endpoint's signing secret.\n///\n/// This is used to verify the authenticity of the webhook.\n/// For more information please refer to [the consuming webhooks docs](https://docs.svix.com/consuming-webhooks/).\n#[aide_annotate(op_id = \"v1.endpoint.get-secret\")]\npub(super) async fn get_endpoint_secret(\n    State(AppState { ref db, cfg, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointSecretOut>> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    Ok(Json(EndpointSecretOut {\n        key: endp.key.into_endpoint_secret(&cfg.encryption)?,\n    }))\n}\n\n/// Rotates the endpoint's signing secret.  The previous secret will be valid for the next 24 hours.\n#[aide_annotate(op_id = \"v1.endpoint.rotate-secret\")]\npub(super) async fn rotate_endpoint_secret(\n    State(AppState {\n        ref db,\n        cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointSecretRotateIn>,\n) -> Result<NoContent> {\n    let mut endp = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let now = Utc::now();\n    let last_key = ExpiringSigningKey {\n        key: endp.key.clone(),\n        expiration: now + Duration::hours(ExpiringSigningKeys::OLD_KEY_EXPIRY_HOURS),\n    };\n\n    if let Some(ref old_keys) = endp.old_keys {\n        if old_keys.0.len() + 1 > ExpiringSigningKeys::MAX_OLD_KEYS {\n            return Err(HttpError::bad_request(\n                Some(\"limit_reached\".to_owned()),\n                Some(format!(\n                    \"You can only rotate a key {} times within the last {}.\",\n                    ExpiringSigningKeys::MAX_OLD_KEYS,\n                    ExpiringSigningKeys::OLD_KEY_EXPIRY_HOURS\n                )),\n            )\n            .into());\n        }\n    }\n\n    let old_keys = endp.old_keys.take();\n\n    let endp = endpoint::ActiveModel {\n        key: Set(<|fim_suffix|>),\n\n        old_keys: Set(Some(ExpiringSigningKeys(\n            iter::once(last_key)\n                .chain(\n                    old_keys\n                        .map(|x| x.0.into_iter())\n                        .unwrap_or_else(|| vec![].into_iter()),\n                )\n                .collect(),\n        ))),\n        ..endp.into()\n    };\n    let endp = endp.update(db).await?;\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointUpdated(EndpointEvent::new(app.uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok(NoContent)\n}\n<|fim_middle|>", "completion": "if let Some(key) = data.key {\n            EndpointSecretInternal::from_endpoint_secret(key, &cfg.encryption)?\n        } else {\n            generate_secret(&cfg.encryption, &cfg.default_signature_type)?\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/secrets.rs", "node_type": "if_expression", "line_range": [96, 100]}
{"prompt": "<|fim_prefix|>//! Test module for worker functionality that depends on external networking and test utilities.\n//! As such they are included with integration tests for organizational purposes.\nuse std::{net::TcpListener, sync::Arc, time::Duration};\n\nuse axum::extract::State;\nuse http::StatusCode;\nuse svix_server::v1::{\n    endpoints::{attempt::MessageAttemptOut, endpoint::EndpointOut},\n    utils::ListResponse,\n};\nuse tokio::sync::Mutex;\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, create_test_message},\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n};\n\n/// Runs a full Axum server with two endpoints. The first endpoint redirects to the second endpoint\n/// while the second endpoint records whether it has been visited. This is such that we can check\n/// that no redirection is taken by the Svix worker's `reqwest::Client`\nstruct RedirectionVisitReportingReceiver {\n    pub base_uri: String,\n    pub jh: tokio::task::JoinHandle<()>,\n    pub has_been_visited: Arc<Mutex<bool>>,\n}\n\n#[derive(Clone)]\nstruct RedirectionVisitReportingState {\n    has_been_visited: Arc<Mutex<bool>>,\n    resp_with: axum::http::StatusCode,\n}\n\nimpl RedirectionVisitReportingReceiver {\n    pub fn start(resp_with: axum::http::StatusCode) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n        let base_uri = format!(\"http://{}\", listener.local_addr().unwrap());\n\n        let has_been_visited = Arc::new(Mutex::new(false));\n\n        let routes = axum::Router::new()\n            .route(\n                \"/first/\",\n                axum::routing::post(redirecting_receiver_route).get(redirecting_receiver_route),\n            )\n            .route(\n                \"/second/\",\n                axum::routing::post(visit_reporting_receiver_route)\n                    .get(visit_reporting_receiver_route),\n            )\n            .with_state(RedirectionVisitReportingState {\n                has_been_visited: has_been_visited.clone(),\n                resp_with,\n            })\n            .into_make_service();\n\n        let jh = tokio::spawn(async move {\n            axum::serve(listener, routes).await.unwrap();\n        });\n\n        RedirectionVisitReportingReceiver {\n            base_uri,\n            jh,\n            has_been_visited,\n        }\n    }\n}\n\nasync fn redirecting_receiver_route() -> axum::response::Redirect {\n    axum::response::Redirect::permanent(\"/second/\")\n}\n\n<|fim_suffix|>\n\n// The worker has\n#[tokio::test]\nasync fn test_no_redirects_policy() {\n    let (client, _jh) = start_svix_server().await;\n    let receiver = RedirectionVisitReportingReceiver::start(StatusCode::OK);\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let _ep_id = create_test_endpoint(&client, &app_id, &format!(\"{}/first/\", receiver.base_uri))\n        .await\n        .unwrap()\n        .id;\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n        .await\n        .unwrap()\n        .id;\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{msg_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let attempt = attempts.data.first();\n\n        if let Some(attempt) = attempt {\n            assert_eq!(attempt.response_status_code, 308);\n            Ok(())\n        } else {\n            anyhow::bail!(\"No attempt found\");\n        }\n    })\n    .await\n    .unwrap();\n\n    // Assert that the second endpoint has not been visited\n    assert!(!*receiver.has_been_visited.lock().await);\n\n    receiver.jh.abort();\n}\n\n/// This tests that endpoints are successfully disabled after the retry schedule is exhausted\n/// multiple times without intermittent success over a period exceeding the grace period. So the\n/// tests don't take too long, these grace period and expiration period will be reconfigured to be\n/// on the order of seconds\n#[tokio::test]\nasync fn test_endpoint_disable_on_repeated_failure() {\n    let mut cfg = get_default_test_config();\n\n    if !matches!(cfg.cache_type, svix_server::cfg::CacheType::None) {\n        cfg.retry_schedule = vec![];\n        cfg.endpoint_failure_disable_after = Duration::from_secs(2);\n\n        let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n        let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n        let ep_id = create_test_endpoint(&client, &app_id, \"http://bad.url/\")\n            .await\n            .unwrap()\n            .id;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        tokio::time::sleep(Duration::from_millis(2_500)).await;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        run_with_retries(|| async {\n            let ep: EndpointOut = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if !ep.ep.disabled {\n                anyhow::bail!(\"Endpoint not disabled\")\n            } else {\n                Ok(())\n            }\n        })\n        .await\n        .unwrap();\n    }\n}\n\n/// This tests that if a consistently failing endpoint is only tried after the expiration period\n/// has been exceeded, that it will not be disabled.\n#[tokio::test]\nasync fn test_endpoint_disable_expiration_duration() {\n    let mut cfg = get_default_test_config();\n\n    if !matches!(cfg.cache_type, svix_server::cfg::CacheType::None) {\n        cfg.retry_schedule = vec![];\n        cfg.endpoint_failure_disable_after = Duration::from_millis(250);\n\n        let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n        let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n        let ep_id = create_test_endpoint(&client, &app_id, \"http://bad.url/\")\n            .await\n            .unwrap()\n            .id;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        tokio::time::sleep(Duration::from_millis(1200)).await;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        // Cannot run with retries as it's not disabled by default and we are checking that it remains\n        // not disabled. So another sleep is required here.\n        tokio::time::sleep(Duration::from_millis(500)).await;\n        let ep: EndpointOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert!(!ep.ep.disabled);\n    }\n}\n\n/// Because the endpoint disabling system requires failures, we need a test receiver that fails\n/// some of the time, but not all of the time, such as to be able to test that a successful response\n/// after a failure clears the cache for that endpoint.\nstruct SporadicallyFailingReceiver {\n    pub base_uri: String,\n    pub jh: tokio::task::JoinHandle<()>,\n}\n\n#[derive(Clone)]\nstruct SporadicallyFailingState {\n    count: Arc<Mutex<u8>>,\n    resp_with: (http::StatusCode, http::StatusCode),\n}\n\nimpl SporadicallyFailingReceiver {\n    pub fn start(resp_with: (http::StatusCode, http::StatusCode)) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n        let base_uri = format!(\"http://{}\", listener.local_addr().unwrap());\n\n        let count = Arc::new(Mutex::new(0u8));\n\n        let routes = axum::Router::new()\n            .route(\n                \"/\",\n                axum::routing::post(sporadically_failing_route).get(sporadically_failing_route),\n            )\n   <|fim_middle|>", "completion": "async fn visit_reporting_receiver_route(\n    State(RedirectionVisitReportingState {\n        has_been_visited: visited,\n        resp_with,\n    }): State<RedirectionVisitReportingState>,\n) -> StatusCode {\n    *visited.lock().await = true;\n    resp_with\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/worker.rs", "node_type": "function_item", "line_range": [75, 83]}
{"prompt": "<|fim_prefix|>use serde::Deserialize;\nuse svix_bridge_types::{\n    ReceiverOutput, SenderInput, SenderOutputOpts, TransformationConfig, TransformerInputFormat,\n};\n\nuse crate::sender_input::QueueSender;\npub use crate::{\n    gcp_pubsub::{GcpPubSubInputOpts, GcpPubSubOutputOpts},\n    rabbitmq::{RabbitMqInputOpts, RabbitMqOutputOpts},\n    receiver_output::QueueForwarder,\n    redis::{RedisInputOpts, RedisOutputOpts},\n    sqs::{SqsInputOpts, SqsOutputOpts},\n};\n\npub fn into_sender_input(\n    name: String,\n    input_opts: QueueInputOpts,\n    transformation: Option<TransformationConfig>,\n    output: SenderOutputOpts,\n) -> Result<Box<dyn SenderInput>, &'static str> {\n    // FIXME: see if this check is still needed. String transforms worked for the omniqueue redis receiver, I think?\n    if matches!(input_opts, QueueInputOpts::Redis(_))\n        && transformation\n            .as_ref()\n            .map(|t| t.format() != TransformerInputFormat::Json)\n            .unwrap_or_default()\n    {\n        return Err(\"redis only supports json formatted transformations\");\n    }\n\n    Ok(Box::new(QueueSender::new(\n        name,\n        input_opts,\n        transformation,\n        output,\n    )))\n}\n\npub async fn into_receiver_output(\n    name: String,\n    opts: QueueOutputOpts,\n    // Annoying to have to pass this, but certain backends (redis) only work with certain transformations (json).\n    transformation: Option<&TransformationConfig>,\n) -> Result<Box<dyn ReceiverOutput>, crate::Error> {\n    // FIXME: see if this check is still needed. String transforms worked for the omniqueue redis receiver, I think?\n    if matches!(opts, QueueOutputOpts::Redis(_))\n        && transformation\n            .as_ref()\n            .map(|t| t.format() != TransformerInputFormat::Json)\n            .unwrap_or_default()\n    {\n        return Err(crate::Error::Generic(\n            \"redis only supports json formatted transformations\".to_string(),\n        ));\n    }\n\n    let forwarder = QueueForwarder::from_receiver_output_opts(name, opts).await?;\n    Ok(Box::new(forwarder))\n}\n\n// TODO: feature flag the variants, thread the features down through to generic-queue\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum QueueInputOpts {\n    #[serde(rename = \"gcp-pubsub\")]\n    GcpPubSub(GcpPubSubInputOpts),\n    RabbitMQ(RabbitMqInputOpts),\n    Redis(RedisInputOpts),\n    Sqs(SqsInputOpts),\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[allow(clippy::large_enum_variant)] // Largest variant isn't _that_ big\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum QueueOutputOpts {\n    #[serde(rename = \"gcp-pubsub\")]\n    GcpPubSub(GcpPubSubOutputOpts),\n    RabbitMQ(RabbitMqOutputOpts),\n    Redis(RedisOutputOpts),\n    Sqs(SqsOutputOpts),\n}\n\n#[cfg(test)]\nmod tests {\n    use svix_bridge_types::{\n        SenderOutputOpts, SvixSenderOutputOpts, TransformationConfig, TransformerInputFormat,\n    };\n\n    use super::{into_receiver_output, into_sender_input};\n    use crate::{\n        config::{QueueInputOpts, QueueOutputOpts},\n        redis::{RedisInputOpts, RedisOutputOpts},\n    };\n\n    // FIXME: can't support raw payload access for redis because it requires JSON internally.\n    //   Revisit after `omniqueue` adoption.\n    #[test]\n    fn redis_sender_with_string_transformation_is_err() {\n        let input_opts = QueueInputOpts::Redis(RedisInputOpts {\n            dsn: \"\".to_string(),\n            max_connections: 0,\n            reinsert_on_nack: false,\n            queue_key: \"\".to_string(),\n            delayed_queue_key: None,\n            consumer_group: \"\".to_string(),\n            consumer_name: \"\".to_string(),\n            ack_deadline_ms: 2_000,\n        });\n\n        let err = into_sender_input(\n            \"redis-with-string-transformation\".to_owned(),\n            input_opts,\n            Some(TransformationConfig::Explicit {\n                format: TransformerInputFormat::String,\n                src: String::new(),\n            }),\n            SenderOutputOpts::Svix(SvixSenderOutputOpts {\n                token: \"\".to_string(),\n                options: None,\n            }),\n        )\n        .err()\n        .expect(\"invalid config didn't result in error\");\n\n        assert_eq!(err, \"redis only supports json formatted transformations\")\n    }\n\n    // FIXME: can't support raw payload access for redis because it requires JSON internally.\n    //   Revisit after `omniqueue` adoption.\n    #[tokio::test]\n    async fn test_redis_receiver_string_transform_is_err() {\n        <|fim_suffix|>\n\n        // Explicit String fails\n        let res = into_receiver_output(\n            \"\".to_string(),\n            redis_out,\n            Some(TransformationConfig::Explicit {\n                src: String::new(),\n                format: TransformerInputFormat::String,\n            })\n            .as_ref(),\n        )\n        .await;\n        assert!(matches!(\n            res.err()\n                .expect(\"invalid config didn't result in error\"),\n            crate::error::Error::Generic(msg) if msg == \"redis only supports json formatted transformations\"\n        ));\n    }\n}\n<|fim_middle|>", "completion": "let redis_out = QueueOutputOpts::Redis(RedisOutputOpts {\n            dsn: \"\".to_string(),\n            max_connections: 0,\n            queue_key: \"\".to_string(),\n            delayed_queue_key: None,\n            ack_deadline_ms: 2_000,\n        });", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/src/config.rs", "node_type": "let_declaration", "line_range": [132, 138]}
{"prompt": "<|fim_prefix|>\n            serialized_body: None,\n            no_return_type: false,\n        }\n    }\n\n    pub fn with_body_param<T: serde::Serialize>(mut self, param: T) -> Self {\n        self.serialized_body = Some(serde_json::to_string(&param).unwrap());\n        self\n    }\n\n    pub fn with_optional_header_param(\n        mut self,\n        basename: &'static str,\n        param: Option<String>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.header_params.insert(basename, value);\n        }\n        self\n    }\n\n    pub fn with_query_param(mut self, basename: &'static str, param: impl QueryParamValue) -> Self {\n        self.query_params.insert(basename, param.encode());\n        self\n    }\n\n    pub fn with_optional_query_param<T: QueryParamValue>(\n        mut self,\n        basename: &'static str,\n        param: Option<T>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.query_params.insert(basename, value.encode());\n        }\n        self\n    }\n\n    pub fn with_path_param(mut self, basename: &'static str, param: String) -> Self {\n        self.path_params.insert(basename, param);\n        self\n    }\n\n    pub fn returns_nothing(mut self) -> Self {\n        self.no_return_type = true;\n        self\n    }\n\n    pub async fn execute<T: DeserializeOwned>(self, conf: &Configuration) -> Result<T, Error> {\n        match self.execute_with_backoff(conf).await? {\n            // This is a hack; if there's no_ret_type, T is (), but serde_json gives an\n            // error when deserializing \"\" into (), so deserialize 'null' into it\n            // instead.\n            // An alternate option would be to require T: Default, and then return\n            // T::default() here instead since () implements that, but then we'd\n            // need to impl default for all models.\n            None => Ok(serde_json::from_str(\"null\").expect(\"serde null value\")),\n            Some(bytes) => Ok(serde_json::from_slice(&bytes).map_err(Error::generic)?),\n        }\n    }\n\n    async fn execute_with_backoff(mut self, conf: &Configuration) -> Result<Option<Bytes>, Error> {\n        let no_return_type = self.no_return_type;\n        if self.method == http1::Method::POST && !self.header_params.contains_key(\"idempotency-key\")\n        {\n            self.header_params\n                .insert(\"idempotency-key\", format!(\"auto_{}\", uuid::Uuid::new_v4()));\n        }\n\n        const MAX_BACKOFF: Duration = Duration::from_secs(5);\n\n        let retry_schedule = match &conf.retry_schedule {\n            Some(schedule) => schedule,\n            None => &std::iter::successors(Some(Duration::from_millis(20)), |last_backoff| {\n                Some(MAX_BACKOFF.min(*last_backoff * 2))\n            })\n            .take(conf.num_retries as usize)\n            .collect(),\n        };\n        let mut retries = retry_schedule.iter();\n\n        let mut request = self.build_request(conf)?;\n        request\n            .headers_mut()\n            .insert(\"svix-req-id\", rand::rng().random::<u32>().into());\n\n        let mut retry_count = 0;\n\n        let execute_request = async |request| {\n            let response = conf.client.request(request).await.map_err(Error::generic)?;\n\n            let status = response.status();\n            if !status.is_success() {\n                Err(Error::from_response(status, response.into_body()).await)\n            } else if no_return_type {\n                Ok(None)\n            } else {\n                let bytes = response\n                    .into_body()\n                    .collect()\n                    .await\n                    .map_err(Error::generic)?\n                    .to_bytes();\n                Ok(Some(bytes))\n            }\n        };\n\n        loop {\n            let request_fut = execute_request(request.clone());\n            let res = if let Some(duration) = conf.timeout {\n                tokio::time::timeout(duration, request_fut)\n                    .await\n                    .map_err(Error::generic)?\n            } else {\n                request_fut.await\n            };\n\n            let next_backoff = retries.next().copied();\n\n            match res {\n                Ok(result) => return Ok(result),\n                e @ Err(Error::Validation(_)) => return e,\n                Err(Error::Http(err)) if err.status.as_u16() < 500 => return Err(Error::Http(err)),\n                e @ Err(_) => {\n                    if next_backoff.is_none() {\n                        return e;\n                    }\n                }\n            }\n\n            tokio::time::sleep(next_backoff.expect(\"next_backoff is always Some\")).await;\n            retry_count += 1;\n\n            request\n                .headers_mut()\n                .insert(\"svix-retry-count\", retry_count.into());\n        }\n    }\n\n    fn build_request(self, conf: &Configuration) -> Result<http1::Request<Full<Bytes>>, Error> {\n        const FRAGMENT: &AsciiSet = &CONTROLS.add(b' ').add(b'\"').add(b'<').add(b'>').add(b'`');\n        const PATH: &AsciiSet = &FRAGMENT.add(b'#').add(b'?').add(b'{').add(b'}');\n        const PATH_SEGMENT: &AsciiSet = &PATH.add(b'/').add(b'%');\n\n        let mut path = self.path.to_owned();\n        for (k, v) in self.path_params {\n            // replace {id} with the value of the id path param\n            let percent_encoded_path_param_value =\n                utf8_percent_encode(&v, PATH_SEGMENT).to_string();\n            path = path.replace(&format!(\"{{{k}}}\"), &percent_encoded_path_param_value);\n        }\n\n        let mut uri = format!(\"{}{}\", conf.base_path, path);\n\n        let mut query_string = url::form_urlencoded::Serializer::new(\"\".to_owned());\n        for (key, val) in self.query_params {\n            query_string.append_pair(key, &val);\n        }\n\n        let query_string_str = query_string.finish();\n        if !query_string_str.is_empty() {\n            uri += \"?\";\n            uri += &query_string_str;\n        }\n\n        let uri = http1::Uri::try_from(uri).map_err(Error::generic)?;\n        let mut req_builder = http1::Request::builder().uri(uri).method(self.method);\n\n        let mut request = if let Some(body) = self.serialized_body {\n            let req_headers = req_builder.headers_mut().unwrap();\n            req_headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n            req_headers.insert(CONTENT_LENGTH, body.len().into());\n            req_builder.body(Full::from(body)).map_err(Error::generic)?\n        } else {\n            req_builder.body(Full::default()).map_err(Error::generic)?\n        };\n\n        let request_headers = request.headers_mut();\n\n        // Detect the authorization type if it hasn't been set.\n        <|fim_suffix|>\n        match auth {\n            Auth::Bearer => {\n                if let Some(token) = &conf.bearer_access_token {\n                    let value = format!(\"Bearer {token}\")\n                        .try_into()\n                        .map_err(Error::generic)?;\n                    request_headers.insert(AUTHORIZATION, value);\n                }\n            }\n            Auth::None => {}\n        }\n\n        if let Some(user_agent) = &conf.user_agent {\n            let value = user_agent.try_into().map_err(Error::generic)?;\n            request_headers.insert(USER_AGENT, value);\n        }\n\n        for (k, v) in self.header_params {\n            let v = v.try_into().map_err(Error::generic)?;\n            request_headers.insert(k, v);\n        }\n\n        Ok(request)\n    }\n}\n\npub(crate) trait QueryParamValue {\n    fn encode(&self) -> String;\n}\n\nmacro_rules! impl_query_param_value {\n    ($ty:ty) => {\n        impl QueryParamValue for $ty {\n            fn encode(&self) -> String {\n                self.to_string()\n            }\n        }\n    };\n}\n\nimpl_query_param_value!(bool);\nimpl_query_param_value!(i32);\nimpl_query_param_value!(String);\nimpl_query_param_value!(models::BackgroundTaskStatus);\nimpl_query_param_value!(models::BackgroundTaskType);\nimpl_query_param_value!(models::ConnectorProduct);\nimpl_query_param_value!(models::MessageStatus);\nimpl_query_param_value!(models::Ordering);\nimpl_query_param_value!(models::StatusCodeClass);\n\nimpl QueryParamValue for Vec<String> {\n    fn encode(&self) -> String {\n        self.iter().format(\",\").to_string()\n    }\n}\n<|fim_middle|>", "completion": "let auth = if conf.bearer_access_token.is_some() {\n            Auth::Bearer\n        } else {\n            Auth::None\n        };", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/request.rs", "node_type": "let_declaration", "line_range": [224, 228]}
{"prompt": "<|fim_prefix|>use std::sync::Arc;\n\nuse axum::{\n    body::Body,\n    http::{Request, StatusCode},\n};\nuse serde_json::json;\nuse svix_bridge_types::{\n    async_trait, svix::webhooks::Webhook, BoxError, ForwardRequest, ReceiverOutput,\n    TransformationConfig, TransformerInput, TransformerInputFormat, TransformerJob,\n    TransformerOutput,\n};\nuse tower::{Service, ServiceExt};\n\nuse super::router;\nuse crate::webhook_receiver::{\n    types::{IntegrationState, InternalState},\n    verification::{NoVerifier, SvixVerifier},\n};\n\nstruct FakeReceiverOutput {\n    tx: tokio::sync::mpsc::UnboundedSender<serde_json::Value>,\n}\n\nimpl FakeReceiverOutput {\n    pub fn new() -> (\n        Self,\n        tokio::sync::mpsc::UnboundedReceiver<serde_json::Value>,\n    ) {\n        let (tx, rx) = tokio::sync::mpsc::unbounded_channel();\n        (Self { tx }, rx)\n    }\n}\n\n#[async_trait]\nimpl ReceiverOutput for FakeReceiverOutput {\n    fn name(&self) -> &str {\n        \"fake output\"\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.tx.send(request.payload)?;\n        Ok(())\n    }\n}\n\n#[tokio::test]\nasync fn test_forwarding_no_verification() {\n    <|fim_suffix|>\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n}\n\n/// Registers 2 receivers and sends 1 request to each.\n#[tokio::test]\nasync fn test_forwarding_multiple_receivers() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"a\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: None,\n            },\n        ),\n        (\n            \"b\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/a\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n\n    let request = Request::builder()\n        .uri(\"/webhook/b\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n/// Registers 2 receivers, one with a transformation and one without. Sends 1 request to each.\n#[tokio::test]\nasync fn test_transformation_json() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let mut input = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            input.insert(\"__TRANSFORMED__\".into(), json!(true));\n            let out = json!({ \"payload\": input });\n\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"transformed\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: Some(\n                    \"handler = (x) => ({ payload: {__TRANSFORMED__: true, ...x }})\".into(),\n                ),\n            },\n        ),\n        (\n            \"as-is\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The `__TRANSFORMED__` key should have been added\n    assert_eq!(\n        json!(forwarded),\n        json!({\"a\": true, \"__TRANSFORMED__\": true})\n    );\n\n    let request = Request::builder()\n        .uri(\"/webhook/as-is\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    // The same payload should come through, without any transformation.\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_transformation_string() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let out = match x.input {\n                TransformerInput::String(input) => json!({\"payload\": { \"got\": input }})\n                    .as_object()\n                    .cloned()\n                    .unwrap(),\n                _ => unreachable!(),\n            };\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"transformed\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: Some(TransformationConfig::Explicit {\n                f<|fim_middle|>", "completion": "let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/tests.rs", "node_type": "let_declaration", "line_range": [49, 49]}
{"prompt": "<|fim_prefix|>mod cluster;\nmod sentinel;\n\nuse std::{sync::Arc, time::Duration};\n\nuse bb8::{Pool, RunError};\nuse bb8_redis::RedisConnectionManager;\nuse redis::{\n    aio::ConnectionManagerConfig, sentinel::SentinelNodeConnectionInfo, AsyncConnectionConfig,\n    ProtocolVersion, RedisConnectionInfo, RedisError, TlsMode,\n};\nuse sentinel::RedisSentinelConnectionManager;\nuse tokio::sync::Mutex;\n\npub use self::cluster::RedisClusterConnectionManager;\nuse crate::cfg::{CacheBackend, QueueBackend, SentinelConfig};\n\npub const REDIS_CONN_TIMEOUT: Duration = Duration::from_secs(2);\n\npub enum RedisVariant<'a> {\n    Clustered,\n    NonClustered,\n    Sentinel(&'a SentinelConfig),\n}\n\n#[derive(Clone)]\npub enum RedisManager {\n    Clustered(Pool<RedisClusterConnectionManager>),\n    NonClustered(Pool<RedisConnectionManager>),\n    Sentinel(Pool<crate::redis::sentinel::RedisSentinelConnectionManager>),\n    ClusteredUnpooled(redis::cluster_async::ClusterConnection),\n    NonClusteredUnpooled(redis::aio::ConnectionManager),\n    SentinelUnpooled(Arc<Mutex<redis::sentinel::SentinelClient>>),\n}\n\nimpl RedisManager {\n    async fn new_pooled(dsn: &str, variant: RedisVariant<'_>, max_conns: u16) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let mgr = RedisClusterConnectionManager::new(dsn)\n                    .expect(\"Error initializing redis cluster client\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis cluster connection pool\");\n                RedisManager::Clustered(pool)\n            }\n            RedisVariant::NonClustered => {\n                let mgr =\n                    RedisConnectionManager::new(dsn).expect(\"Error initializing redis client\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::NonClustered(pool)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                <|fim_suffix|>\n                let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };\n                let mgr = RedisSentinelConnectionManager::new(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                )\n                .expect(\"Error initializing RedisSentinelConnectionManager\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::Sentinel(pool)\n            }\n        }\n    }\n\n    async fn new_unpooled(dsn: &str, variant: RedisVariant<'_>) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let cli = redis::cluster::ClusterClient::builder(vec![dsn])\n                    .retries(1)\n                    .connection_timeout(REDIS_CONN_TIMEOUT)\n                    .build()\n                    .expect(\"Error initializing redis-unpooled cluster client\");\n                let con = cli\n                    .get_async_connection()\n                    .await\n                    .expect(\"Failed to get redis-cluster-unpooled connection\");\n                RedisManager::ClusteredUnpooled(con)\n            }\n            RedisVariant::NonClustered => {\n                let cli =\n                    redis::Client::open(dsn).expect(\"Error initializing redis unpooled client\");\n                let con = redis::aio::ConnectionManager::new_with_config(\n                    cli,\n                    ConnectionManagerConfig::new()\n                        .set_number_of_retries(1)\n                        .set_connection_timeout(REDIS_CONN_TIMEOUT),\n                )\n                .await\n                .expect(\"Failed to get redis-unpooled connection manager\");\n                RedisManager::NonClusteredUnpooled(con)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);\n                let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };\n                let cli = redis::sentinel::SentinelClient::build(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                    redis::sentinel::SentinelServerType::Master,\n                )\n                .expect(\"Failed to build sentinel client\");\n\n                RedisManager::SentinelUnpooled(Arc::new(Mutex::new(cli)))\n            }\n        }\n    }\n\n    pub async fn from_cache_backend(cache_backend: &CacheBackend<'_>) -> Self {\n        match cache_backend {\n            CacheBackend::Redis(dsn) => Self::new_unpooled(dsn, RedisVariant::NonClustered).await,\n            CacheBackend::RedisCluster(dsn) => {\n                Self::new_unpooled(dsn, RedisVariant::Clustered).await\n            }\n            CacheBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_unpooled(dsn, RedisVariant::Sentinel(cfg)).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn from_queue_backend(queue_backend: &QueueBackend<'_>, max_conns: u16) -> Self {\n        match queue_backend {\n            QueueBackend::Redis(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::NonClustered, max_conns).await\n            }\n            QueueBackend::RedisCluster(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::Clustered, max_conns).await\n            }\n            QueueBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_pooled(dsn, RedisVariant::Sentinel(cfg), max_conns).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn get(&self) -> Result<RedisConnection<'_>, RunError<RedisError>> {\n        match self {\n            Self::Clustered(pool) => Ok(RedisConnection::Clustered(pool.get().await?)),\n            Self::NonClustered(pool) => Ok(RedisConnection::NonClustered(pool.get().await?)),\n            Self::Sentinel(pool) => Ok(RedisConnection::SentinelPooled(pool.get().await?)),\n            Self::ClusteredUnpooled(conn) => Ok(RedisConnection::ClusteredUnpooled(conn.clone())),\n            Self::NonClusteredUnpooled(conn) => {\n                Ok(RedisConnection::NonClusteredUnpooled(conn.clone()))\n            }\n            Self::SentinelUnpooled(conn) => {\n                let mut conn = conn.lock().await;\n                let con = conn\n                    .get_async_connection_with_config(\n                        &AsyncConnectionConfig::new().set_response_timeout(REDIS_CONN_TIMEOUT),\n                    )\n                    .await?;\n                Ok(RedisConnection::SentinelUnpooled(con))\n            }\n        }\n    }\n}\n\npub enum RedisConnection<'a> {\n    Clustered(bb8::PooledConnection<'a, RedisClusterConnectionManager>),\n    NonClustered(bb8::PooledConnect<|fim_middle|>", "completion": "let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/redis/mod.rs", "node_type": "let_declaration", "line_range": [60, 60]}
{"prompt": "<|fim_prefix|>\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    l<|fim_suffix|>\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"2\"] {\n            let list_failed: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_failed.data.len() == 2);\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_3));\n            anyhow::ensure!(list_failed.data.iter().any(|x| x.msg == msg_4));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // No messages should still be listed as `sending`\n    let l: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status=3\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(l.data.len(), 0);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_sending() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"test\": \"data4\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"3\"] {\n            let list_sending: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_sending.data.len() == 2);\n            anyhow::ensure!(list_sending.data.iter().any(|x| x.msg == msg_3));\n            anyhow::ensure!(list_sending.data.iter().any(|x| x.msg == msg_4));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\nstruct FailFirstSucceedSecond {\n    first_done: Arc<Mutex<bool>>,\n}\n\nimpl FailFirstSucceedSecond {\n    fn new() -> Self {\n        Self {\n            f<|fim_middle|>", "completion": "let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [241, 243]}
{"prompt": "<|fim_prefix|>ap();\n\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret3_key }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret3: EndpointSecretOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", endp.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(secret3_key, secret3.key);\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret1, secret2, secret3] {\n        if let EndpointSecret::Symmetric(key) = &sec.key {\n            let sec = STANDARD.encode(key);\n            let wh = Webhook::new(&sec).unwrap();\n            wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n        } else {\n            panic!(\"Shouldn't get here\");\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_rotate_signing_symmetric_and_asymmetric() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Asymmetric key\n    let secret_2 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    // Long key\n    let secret_3 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n\n    let ep_in = EndpointIn {\n        url: Url::parse(&receiver.endpoint).unwrap(),\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\" }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    // Rotate back to symmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp.id),\n            json!({ \"key\": secret_3.serialize_public_key() }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let raw_payload = r#\"{\"test\":\"data1\"}\"#;\n    let payload = serde_json::from_str(raw_payload).unwrap();\n    let _msg = create_test_message(&client, &app_id, payload)\n        .await\n        .unwrap();\n\n    let last_headers = receiver.header_recv.recv().await.unwrap();\n    let last_body = receiver.data_recv.recv().await.unwrap().to_string();\n\n    for sec in [secret_1, secret_2, secret_3] {\n        match sec {\n            EndpointSecret::Symmetric(key) => {\n                let sec = STANDARD.encode(key);\n                let wh = Webhook::new(&sec).unwrap();\n                wh.verify(last_body.as_bytes(), &last_headers).unwrap();\n            }\n            EndpointSecret::Asymmetric(key) => {\n                let msg_id = last_headers.get(\"svix-id\").unwrap().to_str().unwrap();\n                let timestamp = last_headers\n                    .get(\"svix-timestamp\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                let signatures = last_headers\n                    .get(\"svix-signature\")\n                    .unwrap()\n                    .to_str()\n                    .unwrap();\n                l<|fim_suffix|>                let found =\n                    signatures\n                        .split(' ')\n                        .filter(|x| x.starts_with(\"v1a,\"))\n                        .any(|signature| {\n                            let sig: Signature = Signature::from_slice(\n                                STANDARD\n                                    .decode(&signature[\"v1a,\".len()..])\n                                    .unwrap()\n                                    .as_slice(),\n                            )\n                            .unwrap();\n                            key.0.pk.verify(to_sign.as_bytes(), &sig).is_ok()\n                        });\n                assert!(found);\n            }\n        }\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_secret_config() {\n    let mut cfg = get_default_test_config();\n    cfg.default_signature_type = DefaultSignatureType::Ed25519;\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = default_test_endpoint();\n\n    let ep = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let key1 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key1.starts_with(\"whpk_\"));\n\n    // Rotate to asmmetric\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": null }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let key2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    assert!(key2.starts_with(\"whpk_\"));\n}\n\n#[tokio::test]\nasync fn test_custom_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_1 = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    // Long key\n    let secret_2 = EndpointSecret::Symmetric(STANDARD.decode(\"TUdfVE5UMnZlci1TeWxOYXQtX1ZlTW1kLTRtMFdhYmEwanIxdHJvenRCbmlTQ2hFdzBnbHhFbWdFaTJLdzQwSA==\").unwrap());\n    // Asymmetric key\n    let secret_3 = EndpointSecret::Asymmetric(AsymmetricKey::from_base64(\"6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\").unwrap());\n    assert_eq!(\n        secret_3.serialize_public_key(),\n        \"whpk_1SiA4o9hyqTCpIqC5V9HUakiiaeACeqfZTInDBbOir4=\"\n    );\n\n    let mut ep_in = EndpointIn {\n        key: Some(secret_1.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    ep_in.key = Some(secret_2.clone());\n    let endp_2 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // We rotate the key after because it's easier than setting json! for everything\n    ep_in.key = Some(secret_2.clone());\n    let endp_3 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", endp_3.id),\n            json!({\n                \"key\": \"whsk_6Xb/dCcHpPea21PS1N9VY/NZW723CEc77N4rJCubMbfVKIDij2HKpMKkioLlX0dRqSKJp4AJ6p9lMicMFs6Kvg==\",\n            }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    for (secret, ep) in [(secret_1, endp_1), (secret_2, endp_2), (secret_3, endp_3)] {\n        assert_eq!(\n            secret.serialize_public_key(),\n            <|fim_middle|>", "completion": "let to_sign = format!(\"{msg_id}.{timestamp}.{}\", &last_body);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1182, 1182]}
{"prompt": "<|fim_prefix|>//! Allocator stats are only available when we're using jemalloc, and jemalloc doesn't work on windows.\n//!\n//! 2 impls for the helper functions are therefore provided. One set that does nothing (for windows)\n//! and another that works in the non-windows world.\n//!\n//! Care should be taken to keep the signatures aligned between these two so the callsites can be\n//! used consistently regardless of whether jemalloc is in use or not.\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\npub use supported::*;\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\npub use unsupported::*;\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\nmod supported {\n    use std::sync::Arc;\n\n    use tikv_jemalloc_ctl::{epoch, stats};\n\n    pub struct AllocatorStatMibs {\n        epoch: tikv_jemalloc_ctl::epoch_mib,\n        allocated: stats::allocated_mib,\n        resident: stats::resident_mib,\n    }\n\n    <|fim_suffix|>\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        let e = epoch::mib()?;\n        let allocated = stats::allocated::mib()?;\n        let resident = stats::resident::mib()?;\n\n        Ok(Arc::new(AllocatorStatMibs {\n            epoch: e,\n            allocated,\n            resident,\n        }))\n    }\n}\n\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\nmod unsupported {\n    use std::sync::Arc;\n\n    use anyhow::anyhow;\n\n    pub struct AllocatorStatMibs;\n\n    pub fn get_allocator_stats(\n        _bust_cache: bool,\n        _mibs: Arc<AllocatorStatMibs>,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        Ok(None)\n    }\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        Err(anyhow!(\"metric collection is not supported\"))\n    }\n}\n<|fim_middle|>", "completion": "pub fn get_allocator_stats(\n        bust_cache: bool,\n        mibs: &AllocatorStatMibs,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        if bust_cache {\n            // Stats are cached internally and advancing the epoch is a way to invalidate those caches.\n            mibs.epoch.advance()?;\n        }\n        let allocated = mibs.allocated.read()?;\n        let resident = mibs.resident.read()?;\n        Ok(Some((allocated, resident)))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/allocator.rs", "node_type": "function_item", "line_range": [26, 37]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::ingest_message_attempt_exhausted_event_data::IngestMessageAttemptExhaustedEventData;\n\n/// Sent when a message delivery has failed (all of the retry attempts have been\n/// exhausted).\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct IngestMessageAttemptExhaustedEvent {\n    pub data: IngestMessageAttemptExhaustedEventData,\n\n    pub r#type: String,\n}\n\nimpl IngestMessageAttemptExhaustedEvent {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(data: IngestMessageAttemptExhaustedEventData, r#type: String) -> Self {\n        Self { data, r#type }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/ingest_message_attempt_exhausted_event.rs", "node_type": "function_item", "line_range": [16, 18]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::{Duration, Instant};\n\nuse sea_orm::{\n    ConnectionTrait, DatabaseConnection, DbErr, ExecResult, QueryResult, Statement,\n    TransactionTrait, UpdateResult,\n};\n\nuse crate::error::{Error, Result};\n\ntype DbResult<T> = std::result::Result<T, DbErr>;\n\nasync fn exec_without_timeout(pool: &DatabaseConnection, stmt: Statement) -> DbResult<ExecResult> {\n    let increase_timeout = Statement::from_string(\n        pool.get_database_backend(),\n        \"SET LOCAL statement_timeout=0;\",\n    );\n    let tx = pool.begin().await?;\n    let _ = tx.execute(increase_timeout).await?;\n    let res = tx.execute(stmt).await?;\n    tx.commit().await?;\n    Ok(res)\n}\nasync fn query_one_without_timeout(\n    pool: &DatabaseConnection,\n    stmt: Statement,\n) -> DbResult<Option<QueryResult>> {\n    let increase_timeout = Statement::from_string(\n        pool.get_database_backend(),\n        \"SET LOCAL statement_timeout=0;\",\n    );\n    let tx = pool.begin().await?;\n    let _ = tx.execute(increase_timeout).await?;\n    let res = tx.query_one(stmt).await?;\n    tx.commit().await?;\n    Ok(res)\n}\n\n/// Nullifies the payload column for expired messages,\n/// `limit` sets how many rows to update at a time.\npub async fn clean_expired_messages(\n    pool: &DatabaseConnection,\n    limit: u32,\n    enable_legacy_message_cleaner: bool,\n) -> DbResult<UpdateResult> {\n    // See the docs for [`has_message_payloads_pending_expiry`] for background on the legacy cleaner.\n    let legacy_row_count = if enable_legacy_message_cleaner {\n        let legacy_res = {\n            let legacy_stmt = Statement::from_sql_and_values(\n                pool.get_database_backend(),\n                r#\"\n        UPDATE message SET payload = NULL WHERE id IN (\n            SELECT id FROM message\n            WHERE\n                expiration <= now()\n                AND payload IS NOT NULL\n            LIMIT $1\n            FOR UPDATE SKIP LOCKED\n        )\n    \"#,\n                [limit.into()],\n            );\n\n            exec_without_timeout(pool, legacy_stmt).await?\n        };\n        legacy_res.rows_affected()\n    } else {\n        0\n    };\n\n    let stmt = Statement::from_sql_and_values(\n        pool.get_database_backend(),\n        r#\"\n        DELETE FROM messagecontent WHERE id = any(\n            array(\n                SELECT id FROM messagecontent\n                WHERE\n                    expiration <= now()\n                LIMIT $1\n                FOR UPDATE SKIP LOCKED\n            )\n        )\n    \"#,\n        [limit.into()],\n    );\n    let res = pool.execute(stmt).await?;\n\n    Ok(UpdateResult {\n        rows_affected: legacy_row_count + res.rows_affected(),\n    })\n}\n\n/// Checks to see if the message table has any non-null payloads requiring expiry.\n///\n/// ## Background\n///\n/// Initially payloads were modeled as a field in `message`, but later migrated to a separate\n/// table (`messagecontent`). In cases where there are no longer any payloads to expire in `message` we\n/// can avoid the expense of running the cleaner on the `message` table since all new messages should now be using\n/// `messagecontent`.\nasync fn has_message_payloads_pending_expiry(pool: &DatabaseConnection) -> Result<bool> {\n    query_one_without_timeout(\n        pool,\n        Statement::from_string(\n            pool.get_database_backend(),\n            r#\"SELECT EXISTS (SELECT 1 FROM message WHERE payload IS NOT NULL LIMIT 1)\"#,\n        ),\n    )\n    .await?\n    .ok_or_else(|| Error::generic(\"failed to check for message payloads\"))?\n    .try_get_by_index(0)\n    .map_err(|e| Error::generic(format_args!(\"failed to check for message payloads: {e}\")))\n}\n\n/// Polls the database for expired messages to nullify payloads for.\n///\n/// Uses a variable polling schedule, based on affected row counts each iteration of the loop.\npub async fn expired_message_cleaner_loop(pool: &DatabaseConnection) -> Result<()> {\n    let message_table_needs_cleaning = has_message_payloads_pending_expiry(pool).await?;\n    if !message_table_needs_cleaning {\n        tracing::info!(\"No payloads pending expiry found in `message` table. Skipping the cleaner for this table.\");\n    }\n\n    // When fewer rows than the batch size have been updated, take a nap for this long.\n    const IDLE: Duration = Duration::from_secs(60 * 60 * 12);\n    const ON_ERROR: Duration = Duration::from_secs(10);\n    const BATCH_SIZE: u32 = 5_000;\n    let mut sleep_time = None;\n    while !crate::is_shutting_down() {\n        if let Some(duration) = sleep_time {\n            if crate::shutting_down_token()\n                .run_until_cancelled_owned(tokio::time::sleep(duration))\n                .await\n                .is_none()\n            {\n                return Ok(());\n            }\n        }\n\n        l<|fim_suffix|>        match clean_expired_messages(pool, BATCH_SIZE, message_table_needs_cleaning).await {\n            Err(err) => {\n                tracing::error!(\"{}\", err);\n                sleep_time = Some(ON_ERROR);\n            }\n            Ok(UpdateResult { rows_affected }) => {\n                if rows_affected > 0 {\n                    tracing::debug!(elapsed =? start.elapsed(), \"expired {} payloads\", rows_affected);\n                }\n\n                sleep_time = if rows_affected < (BATCH_SIZE as _) {\n                    Some(IDLE)\n                } else {\n                    // When we see full batches, don't sleep at all.\n                    None\n                };\n            }\n        }\n    }\n\n    Ok(())\n}\n<|fim_middle|>", "completion": "let start = Instant::now();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/expired_message_cleaner.rs", "node_type": "let_declaration", "line_range": [142, 142]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse std::fmt;\n\nuse serde::{Deserialize, Serialize};\n\n#[derive(\n    Clone, Copy, Debug, Default, Eq, PartialEq, Ord, PartialOrd, Hash, Serialize, Deserialize,\n)]\npub enum AppPortalCapability {\n    #[default]\n    #[serde(rename = \"ViewBase\")]\n    ViewBase,\n    #[serde(rename = \"ViewEndpointSecret\")]\n    ViewEndpointSecret,\n    #[serde(rename = \"ManageEndpointSecret\")]\n    ManageEndpointSecret,\n    #[serde(rename = \"ManageTransformations\")]\n    ManageTransformations,\n    #[serde(rename = \"CreateAttempts\")]\n    CreateAttempts,\n    #[serde(rename = \"ManageEndpoint\")]\n    ManageEndpoint,\n}\n\nimpl fmt::Display for AppPortalCapability {\n    fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result {\n        let value = <|fim_suffix|>;\n        f.write_str(value)\n    }\n}\n<|fim_middle|>", "completion": "match self {\n            Self::ViewBase => \"ViewBase\",\n            Self::ViewEndpointSecret => \"ViewEndpointSecret\",\n            Self::ManageEndpointSecret => \"ManageEndpointSecret\",\n            Self::ManageTransformations => \"ManageTransformations\",\n            Self::CreateAttempts => \"CreateAttempts\",\n            Self::ManageEndpoint => \"ManageEndpoint\",\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/app_portal_capability.rs", "node_type": "match_expression", "line_range": [27, 34]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{collections::HashMap, sync::Arc};\n\nuse axum::async_trait;\nuse tokio::{\n    sync::RwLock,\n    task,\n    time::{sleep, Duration, Instant},\n};\n\nuse super::{Cache, CacheBehavior, CacheKey, Result};\n\n#[derive(Debug)]\nstruct ValueWrapper {\n    value: Vec<u8>,\n    ttl: Duration,\n    timer: Instant,\n}\n\nimpl ValueWrapper {\n    fn new(value: Vec<u8>, ttl: Duration) -> ValueWrapper {\n        ValueWrapper {\n            value,\n            ttl,\n            timer: Instant::now(),\n        }\n    }\n}\n\ntype State = HashMap<Vec<u8>, ValueWrapper>;\ntype SharedState = Arc<RwLock<State>>;\n\npub fn new() -> Cache {\n    let shared_state = Arc::new(RwLock::new(State::new()));\n\n    let shared_state_clone = shared_state.clone();\n    task::spawn(async move {\n        loop {\n            sleep(Duration::from_secs(60 * 5)).await;\n            shared_state_clone\n                .write()\n                .await\n                .retain(|_, v| check_is_expired(v))\n        }\n    });\n\n    MemoryCache { map: shared_state }.into()\n}\n\n#[derive(Clone)]\npub struct MemoryCache {\n    map: SharedState,\n}\n\n#[async_trait]\nimpl CacheBehavior for MemoryCache {\n    fn should_retry(&self, _e: &super::Error) -> bool {\n        false\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>> {\n        Ok(self\n            .map\n            .read()\n            .await\n            .get(key)\n            .filter(|wrapper| check_is_expired(wrapper))\n            .map(|wrapper| wrapper.value.clone()))\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()> {\n        self.map\n            .write()\n            .await\n            .insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n        Ok(())\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool> {\n        let mut lock = self.map.write().await;\n\n        // TODO: use HashMap::try_insert when stable\n        // https://github.com/rust-lang/rust/issues/82766\n        if !lock.contains_key(key) {\n            lock.insert(key.to_owned(), ValueWrapper::new(value.to_owned(), ttl));\n            return Ok(true);\n        }\n\n        Ok(false)\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()> {\n        self.map.write().await.remove(key.as_ref().as_bytes());\n\n        Ok(())\n    }\n}\n\nfn check_is_expired(vw: &ValueWrapper) -> bool {\n    vw.timer.elapsed().as_millis() <= vw.ttl.as_millis()\n}\n\n#[cfg(test)]\nmod tests {\n    use serde::{Deserialize, Serialize};\n\n    use super::{\n        super::{kv_def, CacheValue},\n        *,\n    };\n    use crate::core::cache::string_kv_def;\n\n    // Test structures\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValA(usize);\n    kv_def!(TestKeyA, TestValA);\n    impl TestKeyA {\n        fn new(id: String) -> TestKeyA {\n            TestKeyA(format!(\"SVIX_TEST_KEY_A_{id}\"))\n        }\n    }\n\n    #[derive(Deserialize, Serialize, Debug, PartialEq)]\n    struct TestValB(String);\n    kv_def!(TestKeyB, TestValB);\n    impl TestKeyB {\n        fn new(id: String) -> TestKeyB {\n            TestKeyB(format!(\"SVIX_TEST_KEY_B_{id}\"))\n        }\n    }\n\n    string_kv_def!(StringTestKey);\n    impl StringTestKey {\n        fn new(id: String) -> StringTestKey {\n            StringTestKey(format!(\"SVIX_TEST_KEY_STRING_{id}\"))\n        }\n    }\n\n    #[tokio::test]\n    async fn test_cache_crud_no_ttl() {\n        let cache = new();\n\n        let (first_key, first_val_a, first_val_b) =\n            (TestKeyA::new(\"1\".to_owned()), TestValA(1), TestValA(2));\n        let (second_key, second_val_a, second_val_b) = (\n            TestKeyB::new(\"1\".to_owned()),\n            TestValB(\"1\".to_owned()),\n            TestValB(\"2\".to_owned()),\n        );\n        l<|fim_suffix|>\n        // Create\n        assert!(cache\n            .set(&first_key, &first_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_a, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Read\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_a));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_a));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_a)\n        );\n\n        // Update (overwrite)\n        assert!(cache\n            .set(&first_key, &first_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set(&second_key, &second_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n        assert!(cache\n            .set_string(&third_key, &third_val_b, Duration::from_secs(30),)\n            .await\n            .is_ok());\n\n        // Confirm update\n        assert_eq!(cache.get(&first_key).await.unwrap(), Some(first_val_b));\n        assert_eq!(cache.get(&second_key).await.unwrap(), Some(second_val_b));\n        assert_eq!(\n            cache.get_string(&third_key).await.unwrap(),\n            Some(third_val_b)\n        );\n\n        // Delete\n        assert!(cache.delete(&first_key).await.is_ok());\n        assert!(cache.delete(&second_key).await.is_ok());\n        assert!(cache.delete(&third_key).await.is_ok());\n\n        // Confirm deletion\n        assert_eq!(cache.get::<TestValA>(&first_key).await.unwrap(), None);\n        assert_eq!(cache.get::<TestValB>(&second_key).await.unwrap(), None);\n        assert_eq!(cache.get_string(&third_key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_ttl() {\n        let cache = new();\n        let key = TestKeyA::new(\"key\".to_owned());\n\n        assert!(cache\n            .set(&key, &TestValA(1), Duration::from_secs(1),)\n            .await\n            .is_ok());\n        tokio::time::sleep(std::time::Duration::from_millis(1200)).await;\n        assert_eq!(cache.get::<TestValA>(&key).await.unwrap(), None);\n    }\n\n    #[tokio::test]\n    async fn test_cache_nx_status() {\n        let cache = new();\n        let key = TestKeyA::new(\"nx_status_test_key\".to_owned());\n\n        assert!(cache\n            .set_if_not_exists(&key, &TestValA(1), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(!cache\n            .set_if_not_exists(&key, &TestValA(2), Duration::from_secs(30),)\n            .await\n            .unwrap());\n        assert_eq!(cache.get(&key).await.unwrap(), Some(TestValA(1)));\n\n        assert!(cache.delete(&key).await.is_ok());\n    }\n}\n<|fim_middle|>", "completion": "let (third_key, third_val_a, third_val_b) = (\n            StringTestKey::new(\"1\".to_owned()),\n            \"1\".to_owned(),\n            \"2\".to_owned(),\n        );", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/memory.rs", "node_type": "let_declaration", "line_range": [153, 157]}
{"prompt": "<|fim_prefix|>     .unwrap();\n\n    // LIST\n    let list_app_1: ListResponse<EndpointOut> = client\n        .get(&format!(\"api/v1/app/{app_1}/endpoint/\"), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list_app_1.data.len(), 3);\n    assert!(list_app_1.data.contains(&app_1_ep_1));\n    assert!(list_app_1.data.contains(&app_1_ep_2));\n    assert!(list_app_1.data.contains(&app_1_ep_3));\n\n    let list_app_2: ListResponse<EndpointOut> = client\n        .get(&format!(\"api/v1/app/{}/endpoint/\", &app_2), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list_app_2.data.len(), 2);\n    assert!(list_app_2.data.contains(&app_2_ep_1));\n    assert!(list_app_2.data.contains(&app_2_ep_2));\n\n    // DELETE\n    delete_endpoint(&client, &app_1, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_1, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_2, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_2, &app_2_ep_2.id)\n        .await\n        .unwrap();\n\n    // CONFIRM DELETION\n    get_endpoint_404(&client, &app_1, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_1, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_2_ep_2.id)\n        .await\n        .unwrap();\n\n    let mut ep_with_metadata = endpoint_in(\"https://somewhere.beyond.the.c\");\n    ep_with_metadata.metadata = metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#);\n    let ep = post_endpoint(&client, &app_1, ep_with_metadata)\n        .await\n        .unwrap();\n    assert_eq!(ep.metadata, metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#));\n\n    let ep_alias = get_endpoint(&client, &app_1, &ep.id).await.unwrap();\n    assert_eq!(\n        ep_alias.metadata,\n        metadata(r#\"{\"foo\": \"bar\", \"bizz\": \"baz\"}\"#)\n    );\n\n    // Test that metadata may be unset\n    let ep_alias2: EndpointOut = client\n        .patch(\n            &format!(\"api/v1/app/{app_1}/endpoint/{}/\", ep.id),\n            json!({\n                \"metadata\": {},\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(ep_alias2.metadata, metadata(r#\"{}\"#));\n}\n\n#[tokio::test]\nasync fn test_list() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"App1\").await.unwrap().id;\n    common_test_list::<EndpointOut, EndpointIn>(\n        &client,\n        &format!(\"api/v1/app/{app_id}/endpoint/\"),\n        |i| endpoint_in(&format!(\"https://localhost/{i}\")),\n        false,\n        true,\n    )\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_endpoint_list_ordering() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"App1\").await.unwrap().id;\n\n    for i in 0..5 {\n        create_test_endpoint(&client, &app_id, &format!(\"https://test.url/{i}\"))\n            .await\n            .unwrap();\n        // Sleep to account for ksuid 4ms resolution\n        tokio::time::sleep(Duration::from_millis(5)).await;\n    }\n\n    let first_list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/?limit=2\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // First iterate through in order\n    assert_eq!(\n        first_list.data.first().unwrap().ep.url,\n        \"https://test.url/4\"\n    );\n    assert_eq!(first_list.data.last().unwrap().ep.url, \"https://test.url/3\");\n    assert!(!first_list.done);\n\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?limit=2&iterator={}\",\n                first_list.iterator.unwrap()\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/2\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/1\");\n    assert!(!list.done);\n\n    // Iterate with previous iterator\n    l<|fim_suffix|>\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/4\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/3\");\n    assert!(list.done);\n\n    // Iterate in ascending order\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\"api/v1/app/{}/endpoint/?limit=3&order=ascending\", &app_id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/0\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/2\");\n    assert!(!list.done);\n\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?limit=3&order=ascending&iterator={}\",\n                list.iterator.unwrap(),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/3\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/4\");\n    assert!(list.done);\n\n    // Previous iterator on descending order\n    let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?limit=2&order=ascending&iterator={}\",\n                list.prev_iterator.unwrap(),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list.data.first().unwrap().ep.url, \"https://test.url/1\");\n    assert_eq!(list.data.last().unwrap().ep.url, \"https://test.url/2\");\n}\n\n/// Tests that there is at most one endpoint with a single UID for all endpoints associated with\n/// any application\n#[tokio::test]\nasync fn test_uid() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1: &str = \"v1EndpointUidTestApp1\";\n    const APP_NAME_2: &str = \"v1EndpointUidTestApp2\";\n\n    const EP_URI_APP_1_EP_1: &str = \"http://v1EndpointUidTestApp1Ep1.test\";\n    const EP_URI_APP_1_EP_2: &str = \"http://v1EndpointUidTestApp1Ep2.test\";\n    const EP_URI_APP_2: &str = \"http://v1EndpointUidTestApp2Ep1.test\";\n\n    const DUPLICATE_UID: &str = \"test_uid\";\n\n    // Same App\n\n    // Double Create -- on creation, it should return an error if identical UIDs are used for\n    // endpoints in the same app\n    let app_id = create_test_app(&client, APP_NAME_1).await.unwrap().id;\n    let uid = EndpointUid(DUPLICATE_UID.to_owned());\n\n    let mut ep_1 = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1.uid = Some(uid.clone());\n\n    let mut ep_2 = endpoint_in(EP_URI_APP_1_EP_2);\n    ep_2.uid = Some(uid.clone());\n\n    let ep_1 = post_endpoint(&client, &app_id, ep_1).await.unwrap();\n\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_2,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Update One to Existing -- on update it should return an error if attempting to change\n    // the UID to that of an existing endpoint associated with the same app\n    let ep_2 = create_test_endpoint(&client, &app_id, EP_URI_APP_1_EP_2)\n        .await\n        .unwrap();\n\n    let mut ep_2_with_duplicate_uid = endpoint_in(EP_URI_APP_1_EP_2);\n    ep_2_with_duplicate_uid.uid = Some(uid.clone());\n\n    client\n        .put::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_2.id),\n            ep_2_with_duplicate_uid,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n\n    // Update One to Identical -- however it should not return an error if updating the\n    // existing endpoint to one with the same UID\n    let mut ep_1_with_duplicate_id = endpoint_in(EP_URI_APP_1_EP_1);\n    ep_1_with_duplicate_id.uid = Some(uid.clone());\n\n    let ep_1_updated = client\n        .put::<_, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_1.id),\n            ep_1_with_duplicate_id,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(ep_1.id, ep_1_updated.id);\n    assert_eq!(ep_1.ep.uid, ep_1_updated.ep.uid);\n\n    // Delete One then Create O<|fim_middle|>", "completion": "let list: ListResponse<EndpointOut> = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/?iterator={}\",\n                list.prev_iterator.unwrap()\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [683, 692]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct AuthenticationAppPortalAccessOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationAppPortalAccessOptions>\n    for svix::api::AuthenticationAppPortalAccessOptions\n{\n    fn from(value: AuthenticationAppPortalAccessOptions) -> Self {\n        let AuthenticationAppPortalAccessOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationExpireAllOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationExpireAllOptions> for svix::api::AuthenticationExpireAllOptions {\n    fn from(value: AuthenticationExpireAllOptions) -> Self {\n        let AuthenticationExpireAllOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationLogoutOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationLogoutOptions> for svix::api::AuthenticationLogoutOptions {\n    fn from(value: AuthenticationLogoutOptions) -> Self {\n        let AuthenticationLogoutOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationStreamPortalAccessOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationStreamPortalAccessOptions>\n    for svix::api::AuthenticationStreamPortalAccessOptions\n{\n    fn from(value: AuthenticationStreamPortalAccessOptions) -> Self {\n        let AuthenticationStreamPortalAccessOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct AuthenticationRotateStreamPollerTokenOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<AuthenticationRotateStreamPollerTokenOptions>\n    for svix::api::AuthenticationRotateStreamPollerTokenOptions\n{\n    <|fim_suffix|>\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct AuthenticationArgs {\n    #[command(subcommand)]\n    pub command: AuthenticationCommands,\n}\n\n#[derive(Subcommand)]\npub enum AuthenticationCommands {\n    /// Use this function to get magic links (and authentication codes) for connecting your users to the Consumer Application Portal.\n    AppPortalAccess {\n        app_id: String,\n        app_portal_access_in: Option<crate::json::JsonOf<AppPortalAccessIn>>,\n        #[clap(flatten)]\n        options: AuthenticationAppPortalAccessOptions,\n    },\n    /// Expire all of the tokens associated with a specific application.\n    ExpireAll {\n        app_id: String,\n        application_token_expire_in: Option<crate::json::JsonOf<ApplicationTokenExpireIn>>,\n        #[clap(flatten)]\n        options: AuthenticationExpireAllOptions,\n    },\n    /// Logout an app token.\n    ///\n    /// Trying to log out other tokens will fail.\n    Logout {\n        #[clap(flatten)]\n        options: AuthenticationLogoutOptions,\n    },\n    /// Use this function to get magic links (and authentication codes) for connecting your users to the Stream Consumer Portal.\n    StreamPortalAccess {\n        stream_id: String,\n        stream_portal_access_in: Option<crate::json::JsonOf<StreamPortalAccessIn>>,\n        #[clap(flatten)]\n        options: AuthenticationStreamPortalAccessOptions,\n    },\n    /// Get the current auth token for the stream poller.\n    GetStreamPollerToken { stream_id: String, sink_id: String },\n    /// Create a new auth token for the stream poller API.\n    RotateStreamPollerToken {\n        stream_id: String,\n        sink_id: String,\n        rotate_poller_token_in: Option<crate::json::JsonOf<RotatePollerTokenIn>>,\n        #[clap(flatten)]\n        options: AuthenticationRotateStreamPollerTokenOptions,\n    },\n}\n\nimpl AuthenticationCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::AppPortalAccess {\n                app_id,\n                app_portal_access_in,\n                options,\n            } => {\n                let resp = client\n                    .authentication()\n                    .app_portal_access(\n                        app_id,\n                        app_portal_access_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ExpireAll {\n                app_id,\n                application_token_expire_in,\n                options,\n            } => {\n                client\n                    .authentication()\n                    .expire_all(\n                        app_id,\n                        application_token_expire_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n            }\n            Self::Logout { options } => {\n                client.authentication().logout(Some(options.into())).await?;\n            }\n            Self::StreamPortalAccess {\n                stream_id,\n                stream_portal_access_in,\n                options,\n            } => {\n                let resp = client\n                    .authentication()\n                    .stream_portal_access(\n                        stream_id,\n                        stream_portal_access_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::GetStreamPollerToken { stream_id, sink_id } => {\n                let resp = client\n                    .authentication()\n                    .get_stream_poller_token(stream_id, sink_id)\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::RotateStreamPollerToken {\n                stream_id,\n                sink_id,\n                rotate_poller_token_in,\n                options,\n            } => {\n                let resp = client\n                    .authentication()\n                    .rotate_stream_poller_token(\n                        stream_id,\n                        sink_id,\n                        rotate_poller_token_in.unwrap_or_default().into_inner(),\n                        Some(options.into()),\n                    )\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "fn from(value: AuthenticationRotateStreamPollerTokenOptions) -> Self {\n        let AuthenticationRotateStreamPollerTokenOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/authentication.rs", "node_type": "function_item", "line_range": [70, 73]}
{"prompt": "<|fim_prefix|>/*\n * Svix API\n *\n * No description provided (generated by Openapi Generator https://github.com/openapitools/openapi-generator)\n *\n * The version of the OpenAPI document: 1.1.1\n *\n * Generated by: https://openapi-generator.tech\n */\n\n#[allow(unused_imports)]\nuse crate::models;\n#[allow(unused_imports)]\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Default, Debug, PartialEq, Serialize, Deserialize)]\npub struct HttpErrorOut {\n    #[serde(rename = \"code\")]\n    pub code: String,\n    #[serde(rename = \"detail\")]\n    pub detail: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl HttpErrorOut {\n    pub fn new(code: String, detail: String) -> HttpErrorOut {\n        HttpErrorOut { code, detail }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/http_error_out.rs", "node_type": "impl_item", "line_range": [24, 28]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct HubspotConfigOut {}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl HubspotConfigOut {\n    pub fn new() -> Self {\n        Self {}\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/hubspot_config_out.rs", "node_type": "impl_item", "line_range": [7, 11]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Configuration-dependent queue tests. This will depend on the set environment variables as with\n//! the e2e tests such as to allow testing multiple queue backends via the test script.\n\nuse std::{str::FromStr, time::Duration};\n\nuse http::StatusCode;\nuse redis::AsyncCommands as _;\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::Configuration,\n    core::types::{\n        ApplicationId, BaseId, EndpointId, MessageAttemptTriggerType, MessageId, OrganizationId,\n    },\n    queue::{\n        new_pair, MessageTask, QueueTask, TaskQueueConsumer, TaskQueueDelivery, TaskQueueProducer,\n    },\n    redis::RedisManager,\n    v1::endpoints::message::MessageOut,\n};\nuse tokio::time::timeout;\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id_and_prefix,\n};\n\n// TODO: Don't copy this from the Redis queue test directly, place the fn somewhere both can access\nasync fn get_pool(cfg: &Configuration) -> RedisManager {\n    RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await\n}\n\nfn task_queue_delivery_to_u16(tqd: &TaskQueueDelivery) -> u16 {\n    match &*tqd.task {\n        QueueTask::HealthCheck => panic!(\"Health check in test\"),\n        QueueTask::MessageBatch(batch) => u16::from_str(batch.msg_id.as_str()).unwrap(),\n        QueueTask::MessageV1(task) => u16::from_str(task.msg_id.as_str()).unwrap(),\n    }\n}\n\nasync fn test_many_queue_consumers_inner(prefix: &str, delay: Option<Duration>) {\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n\n    // This test assumes an empty queue, so load Redis and delete the test key\n    {\n        let pool = get_pool(&cfg).await;\n        let mut conn = pool.get().await.unwrap();\n\n        l<|fim_suffix|>    }\n\n    // Make 20 producers and 20 consumers using the same configuration\n    let mut producers_and_consumers: Vec<(TaskQueueProducer, TaskQueueConsumer)> = Vec::new();\n    for _ in 0..20 {\n        producers_and_consumers.push(new_pair(&cfg, Some(prefix)).await);\n    }\n\n    // Add 200 test messagesÂ¹ with unique message IDs to each producer for a\n    // total of 4000 unique messages\n    //\n    // Â¹ it is important for this number to be no smaller than MAX_MESSAGES in\n    //   TaskQueueConsumer::receive_all\n    for (index, (p, _c)) in producers_and_consumers.iter().enumerate() {\n        for num in 0..200 {\n            p.send(\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"{}\", index * 200 + num)),\n                    app_id: ApplicationId(\"TestApplicationId\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointId\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                }),\n                delay,\n            )\n            .await\n            .unwrap();\n        }\n    }\n\n    let mut join_handles = Vec::new();\n    // Producers need to stay alive for the remainder of the test for in-memory queue which uses\n    // [`tokio::mpsc`]s, so add them to this [`Vec`]\n    let mut producers = Vec::new();\n\n    // Ensure that consumers run on separate OS threads and receive messages until 500ms has passed\n    // without any messages\n    for (p, mut c) in producers_and_consumers {\n        producers.push(p);\n        let handle = tokio::runtime::Handle::current();\n        join_handles.push(std::thread::spawn(move || {\n            handle.block_on(async move {\n                let mut out = Vec::new();\n                let mut read = 0;\n\n                while let Ok(recv) = timeout(\n                    Duration::from_secs(1),\n                    c.receive_all(Duration::from_secs(5)),\n                )\n                .await\n                {\n                    let recv = recv.unwrap();\n                    read += recv.len();\n                    for r in recv {\n                        out.push(task_queue_delivery_to_u16(&r));\n                        r.ack().await.unwrap();\n                    }\n                }\n\n                (out, read)\n            })\n        }));\n    }\n\n    // Create a Vec with all the threads' outputs\n    let mut out = Vec::new();\n    for jh in join_handles {\n        let (mut jh_out, read): (Vec<u16>, usize) = jh.join().unwrap();\n        out.append(&mut jh_out);\n\n        if read < 20 {\n            panic!(\"Consumer starved, only read {read} messages\");\n        }\n    }\n\n    // Sort it by the message ID\n    out.sort();\n\n    // Then assert that all the messages are there\n    assert_eq!(out.len(), 4000);\n    for (idx, &num) in out.iter().enumerate() {\n        assert_eq!(idx, num as usize);\n    }\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n// run with `cargo test -- --ignored redis` only when redis is up and configured\n#[ignore]\nasync fn test_many_queue_consumers() {\n    test_many_queue_consumers_inner(\"test_many_queue_consumers_\", None).await;\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n#[ignore]\nasync fn test_many_queue_consumers_delayed() {\n    test_many_queue_consumers_inner(\n        \"test_many_queue_consumers_delayed_\",\n        Some(Duration::from_millis(500)),\n    )\n    .await;\n}\n\n#[tokio::test]\n#[ignore]\nasync fn test_redis_streams_dlq() {\n    let mut cfg = get_default_test_config();\n    cfg.worker_enabled = false;\n    cfg.redis_pending_duration_secs = 1;\n\n    let cfg = std::sync::Arc::new(cfg);\n    let prefix = svix_ksuid::Ksuid::new(None, None).to_string();\n\n    let pool = get_pool(&cfg).await;\n    let mut conn = pool.get().await.unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n        .await\n        .unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_dlq\"))\n        .await\n        .unwrap();\n\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id_and_prefix(\n        &cfg,\n        OrganizationId::new(None, None),\n        prefix.clone(),\n    )\n    .await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let _message_1: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({\"test\": \"value\"})).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let (_p, mut c) = new_pair(&cfg, Some(&prefix)).await;\n\n    let wait_time = std::time::Duration::from_millis(1_500);\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n\n    tokio::time::sleep(wait_time).await;\n\n    // Redrive\n    client\n        .post_without_response(\n            \"/api/v1/admin/redrive-dlq\",\n            serde_json::Value::Null,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n}\n<|fim_middle|>", "completion": "let _: () = conn\n            .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n            .await\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/redis_queue.rs", "node_type": "let_declaration", "line_range": [52, 55]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse reqwest::StatusCode;\nuse serde_json::json;\nuse svix_server::{\n    core::types::{EndpointUid, MessageStatus},\n    v1::{\n        endpoints::{\n            attempt::{EndpointMessageOut, MessageAttemptOut},\n            endpoint::{EndpointIn, EndpointOut},\n        },\n        utils::ListResponse,\n    },\n};\nuse wiremock::{matchers, Mock, MockServer, Respond, ResponseTemplate};\n\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, create_test_msg_with,\n        endpoint_in, get_msg_attempt_list_and_assert_count,\n    },\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n    TestReceiver,\n};\n\n#[tokio::test]\nasync fn test_expunge_attempt_response_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let sensitive_response_json = serde_json::json!({\"sensitive\":\"data\"});\n    let mut receiver = TestReceiver::start_with_body(\n        axum::http::StatusCode::OK,\n        axum::Json(sensitive_response_json.clone()),\n    );\n\n    let endpoint_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap()\n        .id;\n\n    receiver.data_recv.recv().await;\n\n    let attempt = run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endpoint_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        if attempts.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", attempts.data.len());\n        }\n        Ok(attempts.data[0].clone())\n    })\n    .await\n    .unwrap();\n\n    let attempt_response: serde_json::Value = serde_json::from_str(&attempt.response).unwrap();\n    assert_eq!(sensitive_response_json, attempt_response);\n\n    let attempt_id = &attempt.id;\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let attempt: MessageAttemptOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\"EXPUNGED\", &attempt.response);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    l<|fim_suffix|>    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = crea<|fim_middle|>", "completion": "let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [101, 101]}
{"prompt": "<|fim_prefix|>use std::{\n    str,\n    time::{Duration, Instant},\n};\n\nuse rdkafka::{\n    consumer::{CommitMode, Consumer as _},\n    error::KafkaError,\n    Message as _,\n};\nuse svix_bridge_types::{\n    async_trait,\n    svix::api::{MessageCreateOptions, Svix},\n    CreateMessageRequest, JsObject, SenderInput, SenderOutputOpts, TransformationConfig,\n    TransformerInput, TransformerInputFormat, TransformerJob, TransformerOutput, TransformerTx,\n};\nuse tokio::task::spawn_blocking;\n\nuse crate::{config::KafkaInputOpts, Error, Result};\n\npub struct KafkaConsumer {\n    name: String,\n    opts: KafkaInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n}\n\nimpl KafkaConsumer {\n    pub fn new(\n        name: String,\n        opts: KafkaInputOpts,\n        transformation: Option<TransformationConfig>,\n        output: SenderOutputOpts,\n    ) -> Result<Self> {\n        Ok(Self {\n            name,\n            transformation,\n            transformer_tx: None,\n            opts,\n            svix_client: match output {\n                SenderOutputOpts::Svix(output) => {\n                    Svix::new(output.token, output.options.map(Into::into))\n                }\n            },\n        })\n    }\n\n    #[tracing::instrument(skip_all)]\n    async fn process(&self, msg: &rdkafka::message::BorrowedMessage<'_>) -> Result<()> {\n        let payload = msg.payload().ok_or_else(|| Error::MissingPayload)?;\n        let payload = if let Some(transformation) = &self.transformation {\n            let input = match transformation.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload =\n                        serde_json::from_slice(payload).map_err(Error::Deserialization)?;\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    let raw_payload = str::from_utf8(payload).map_err(Error::NonUtf8Payload)?;\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n\n            <|fim_suffix|>\n            let object = self.transform(script, input).await?;\n            serde_json::from_value(serde_json::Value::Object(object))\n                .map_err(Error::Deserialization)?\n        } else {\n            serde_json::from_slice(payload).map_err(Error::Deserialization)?\n        };\n\n        let CreateMessageRequest { app_id, message } = payload;\n\n        let KafkaInputOpts::Inner {\n            group_id, topic, ..\n        } = &self.opts;\n\n        let options = MessageCreateOptions {\n            with_content: None,\n            // If committing the message fails or the process crashes after posting the webhook but\n            // before committing, this makes sure that the next run of this fn with the same kafka\n            // message doesn't end up creating a duplicate webhook in svix.\n            idempotency_key: Some(format!(\n                \"svix_bridge_kafka_{group_id}_{topic}_{}\",\n                msg.offset()\n            )),\n        };\n\n        self.svix_client\n            .message()\n            .create(app_id, message, Some(options))\n            .await?;\n\n        Ok(())\n    }\n\n    async fn transform(&self, script: String, input: TransformerInput) -> Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx\n            .as_ref()\n            .ok_or_else(|| Error::transformation(\"transformations not configured\"))?\n            .send(job)\n            .map_err(|e| Error::transformation(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::transformation(\"transformation rx failed\"))\n            .and_then(|x| {\n                x.map_err(|_e| Error::transformation(\"transformation execution failed\"))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => Err(Error::transformation(\n                \"transformation produced unexpected value\",\n            )),\n        }\n    }\n\n    async fn run_inner(&self) -> Result<()> {\n        let opts = self.opts.clone();\n        // `ClientConfig::create` does blocking I/O.\n        // Same for subscribe, most likely.\n        let consumer = spawn_blocking(move || {\n            let KafkaInputOpts::Inner { topic, .. } = &opts;\n            let topic = topic.clone();\n\n            let consumer = opts.create_consumer()?;\n            tracing::debug!(\"Created StreamConsumer\");\n\n            consumer.subscribe(&[&topic])?;\n            tracing::debug!(topic, \"Subscribed\");\n\n            Ok::<_, KafkaError>(consumer)\n        })\n        .await\n        .expect(\"create_consumer task panicked\")?;\n\n        loop {\n            // It's fine to pull messages one-by-one without any buffering in our own code because\n            // rdkafka buffers messages internally through a background task / thread.\n            let msg = consumer.recv().await?;\n            tracing::debug!(\"Received a message\");\n\n            let mut process_error_count = 0;\n            while let Err(e) = self.process(&msg).await {\n                match e {\n                    // If the payload is invalid, log an error and continue.\n                    // It would fail the same way if retried.\n                    Error::MissingPayload\n                    | Error::Deserialization(_)\n                    | Error::NonUtf8Payload(_) => {\n                        tracing::error!(error = &e as &dyn std::error::Error, \"invalid payload\");\n                        break;\n                    }\n\n                    // If the error is (possibly) transient, retry a few times.\n                    // After that, bubble up the error so it's logged at error level.\n                    Error::Kafka(_) | Error::SvixClient(_) | Error::Transformation { .. } => {\n                        process_error_count += 1;\n                        if process_error_count >= 3 {\n                            return Err(e);\n                        }\n\n                        tracing::warn!(\n                            error = &e as &dyn std::error::Error,\n                            \"failed to process payload from kafka\"\n                        );\n\n                        // retry\n                    }\n                }\n            }\n\n            // FIXME(jplatte): Should we introduce logic to only commit every N messages to reduce\n            // back and forth on the Kafka connection / disk writes inside Kafka?\n            //\n            // Background: messages in Kafka are not committed individually, rather what this call\n            // does is update the stored stream position for the consumer group.\n            consumer.commit_message(&msg, CommitMode::Async)?;\n        }\n    }\n}\n\n#[async_trait]\nimpl SenderInput for KafkaConsumer {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        let mut fails: u64 = 0;\n        let mut last_fail = Instant::now();\n\n        let KafkaInputOpts::Inner { topic, .. } = &self.opts;\n        tracing::info!(topic, \"Starting to listen for messages\");\n\n        loop {\n            if let Err(e) = self.run_inner().await {\n                tracing::error!(\"{e}\");\n            }\n\n            if last_fail.elapsed() > Duration::from_secs(10) {\n                // reset the fail count if we didn't have a hiccup in the past short while.\n                tracing::trace!(\"been a while since last fail, resetting count\");\n                fails = 0;\n            } else {\n                fails += 1;\n            }\n\n            last_fail = Instant::now();\n            tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n        }\n    }\n}\n<|fim_middle|>", "completion": "let script = transformation.source().clone();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-kafka/src/input.rs", "node_type": "let_declaration", "line_range": [65, 65]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n<|fim_suffix|>\n\n/// Sent on a successful dispatch after an earlier failure op webhook has\n/// already been sent.\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct IngestMessageAttemptRecoveredEvent {\n    pub data: IngestMessageAttemptRecoveredEventData,\n\n    pub r#type: String,\n}\n\nimpl IngestMessageAttemptRecoveredEvent {\n    pub fn new(data: IngestMessageAttemptRecoveredEventData, r#type: String) -> Self {\n        Self { data, r#type }\n    }\n}\n<|fim_middle|>", "completion": "use super::ingest_message_attempt_recovered_event_data::IngestMessageAttemptRecoveredEventData;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/ingest_message_attempt_recovered_event.rs", "node_type": "use_declaration", "line_range": [4, 4]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n#![warn(clippy::all)]\n#![forbid(unsafe_code)]\n\nuse std::{borrow::Cow, sync::LazyLock, time::Duration};\n\nuse aide::axum::ApiRouter;\nuse cfg::ConfigurationInner;\nuse opentelemetry::trace::TracerProvider as _;\nuse opentelemetry_otlp::WithExportConfig;\nuse opentelemetry_sdk::{metrics::SdkMeterProvider, runtime::Tokio};\nuse queue::TaskQueueProducer;\nuse redis::RedisManager;\nuse sea_orm::DatabaseConnection;\nu<|fim_suffix|>use svix_ksuid::{KsuidLike, KsuidMs};\nuse tokio::net::TcpListener;\nuse tokio_util::sync::CancellationToken;\nuse tower::layer::layer_fn;\nuse tower_http::{\n    cors::{AllowHeaders, Any, CorsLayer},\n    normalize_path::NormalizePath,\n};\nuse tracing_subscriber::{layer::SubscriberExt as _, Layer as _};\n\nuse crate::{\n    cfg::{CacheBackend, Configuration},\n    core::{\n        cache,\n        cache::Cache,\n        idempotency::IdempotencyService,\n        operational_webhooks::{OperationalWebhookSender, OperationalWebhookSenderInner},\n    },\n    db::init_db,\n    expired_message_cleaner::expired_message_cleaner_loop,\n    worker::queue_handler,\n};\n\npub mod cfg;\npub mod core;\npub mod db;\npub mod error;\npub mod expired_message_cleaner;\npub mod metrics;\npub mod openapi;\npub mod queue;\npub mod redis;\npub mod v1;\npub mod worker;\n\nconst CRATE_NAME: &str = env!(\"CARGO_CRATE_NAME\");\n\nstatic SHUTTING_DOWN_TOKEN: LazyLock<CancellationToken> = LazyLock::new(CancellationToken::new);\n\n/// Has someone requested shutdown?\npub fn is_shutting_down() -> bool {\n    SHUTTING_DOWN_TOKEN.is_cancelled()\n}\n\n/// Request a CancellationToken for the application shut down\npub fn shutting_down_token() -> CancellationToken {\n    SHUTTING_DOWN_TOKEN.clone()\n}\n\n/// Shut down the application\npub fn start_shut_down() {\n    SHUTTING_DOWN_TOKEN.cancel();\n}\n\npub static INSTANCE_ID: LazyLock<String> =\n    LazyLock::new(|| hex::encode(KsuidMs::new(None, None).to_string()));\n\nasync fn graceful_shutdown_handler() {\n    let ctrl_c = async {\n        tokio::signal::ctrl_c()\n            .await\n            .expect(\"Failed to install Ctrl+C handler\");\n    };\n\n    #[cfg(unix)]\n    let sigterm = async {\n        tokio::signal::unix::signal(tokio::signal::unix::SignalKind::terminate())\n            .expect(\"Failed to install SIGTERM handler\")\n            .recv()\n            .await;\n    };\n\n    #[cfg(not(unix))]\n    let sigterm = std::future::pending::<()>();\n\n    tokio::select! {\n        _ = ctrl_c => {},\n        _ = sigterm => {},\n    }\n\n    tracing::info!(\"Received shutdown signal. Shutting down gracefully...\");\n    start_shut_down();\n}\n\npub async fn run(cfg: Configuration) {\n    let _metrics = setup_metrics(&cfg);\n    run_with_prefix(cfg.queue_prefix.clone(), cfg, None).await\n}\n\n#[derive(Clone)]\npub struct AppState {\n    db: DatabaseConnection,\n    queue_tx: TaskQueueProducer,\n    cfg: Configuration,\n    cache: Cache,\n    op_webhooks: OperationalWebhookSender,\n}\n\n// Made public for the purpose of E2E testing in which a queue prefix is necessary to avoid tests\n// consuming from each others' queues\npub async fn run_with_prefix(\n    prefix: Option<String>,\n    cfg: Configuration,\n    listener: Option<TcpListener>,\n) {\n    tracing::debug!(\"DB: Initializing pool\");\n    let pool = init_db(&cfg).await;\n    tracing::debug!(\"DB: Started\");\n\n    tracing::debug!(\"Cache: Initializing {:?}\", cfg.cache_type);\n    let cache_backend = cfg.cache_backend();\n    let cache = match &cache_backend {\n        CacheBackend::None => cache::none::new(),\n        CacheBackend::Memory => cache::memory::new(),\n        CacheBackend::Redis(_)\n        | CacheBackend::RedisCluster(_)\n        | CacheBackend::RedisSentinel(_, _) => {\n            let mgr = RedisManager::from_cache_backend(&cache_backend).await;\n            cache::redis::new(mgr)\n        }\n    };\n    tracing::debug!(\"Cache: Started\");\n\n    tracing::debug!(\"Queue: Initializing {:?}\", cfg.queue_type);\n    let (queue_tx, queue_rx) = queue::new_pair(&cfg, prefix.as_deref()).await;\n    tracing::debug!(\"Queue: Started\");\n\n    let op_webhook_sender = OperationalWebhookSenderInner::new(\n        cfg.jwt_signing_config.clone(),\n        cfg.operational_webhook_address.clone(),\n    );\n\n    // OpenAPI/aide must be initialized before any routers are constructed\n    // because its initialization sets generation-global settings which are\n    // needed at router-construction time.\n    let mut openapi = openapi::initialize_openapi();\n\n    let svc_cache = cache.clone();\n    // build our application with a route\n    let app_state = AppState {\n        db: pool.clone(),\n        queue_tx: queue_tx.clone(),\n        cfg: cfg.clone(),\n        cache: cache.clone(),\n        op_webhooks: op_webhook_sender.clone(),\n    };\n    let v1_router = v1::router().with_state::<()>(app_state);\n\n    // Initialize all routes which need to be part of OpenAPI first.\n    let app = ApiRouter::new()\n        .nest_api_service(\"/api/v1\", v1_router)\n        .finish_api(&mut openapi);\n\n    openapi::postprocess_spec(&mut openapi);\n    let docs_router = docs::router(openapi);\n    let app = app.merge(docs_router).layer((\n        layer_fn(move |service| IdempotencyService {\n            cache: svc_cache.clone(),\n            service,\n        }),\n        CorsLayer::new()\n            .allow_origin(Any)\n            .allow_methods(Any)\n            .allow_headers(AllowHeaders::mirror_request())\n            .max_age(Duration::from_secs(600)),\n    ));\n    let svc = tower::make::Shared::new(\n        // It is important that this service wraps the router instead of being\n        // applied via `Router::layer`, as it would run after routing then.\n        NormalizePath::trim_trailing_slash(app),\n    );\n\n    let with_api = cfg.api_enabled;\n    let with_worker = cfg.worker_enabled;\n    let listen_address = cfg.listen_address;\n\n    let ((), worker_loop, expired_message_cleaner_loop) = tokio::join!(\n        async {\n            if with_api {\n                let listener = match listener {\n                    Some(l) => l,\n                    None => TcpListener::bind(listen_address)\n                        .await\n                        .expect(\"Error binding to listen_address\"),\n                };\n                tracing::debug!(\"API: Listening on {}\", listener.local_addr().unwrap());\n\n                axum::serve(listener, svc)\n                    .with_graceful_shutdown(graceful_shutdown_handler())\n                    .await\n                    .unwrap();\n            } else {\n                tracing::debug!(\"API: off\");\n                graceful_shutdown_handler().await;\n            }\n        },\n        async {\n            if with_worker {\n                tracing::debug!(\"Worker: Started\");\n                queue_handler(\n                    &cfg,\n                    cache.clone(),\n                    pool.clone(),\n                    queue_tx,\n                    queue_rx,\n                    op_webhook_sender,\n                )\n                .await\n            } else {\n                tracing::debug!(\"Worker: off\");\n                Ok(())\n            }\n        },\n        async {\n            if with_worker {\n                tracing::debug!(\"Expired message cleaner: Started\");\n                expired_message_cleaner_loop(&pool).await\n            } else {\n                tracing::debug!(\"Expired message cleaner: off\");\n                Ok(())\n            }\n        }\n    );\n\n    worker_loop.expect(\"Error initializing worker\");\n    expired_message_cleaner_loop.expect(\"Error initializing expired message cleaner\")\n}\n\npub fn setup_tracing(\n    cfg: &ConfigurationInner,\n    for_test: bool,\n) -> (tracing::Dispatch, sentry::ClientInitGuard) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        let level = cfg.log_level.to_string();\n        let mut var = vec![\n            format!(\"{CRATE_NAME}={level}\"),\n            format!(\"tower_http={level}\"),\n        ];\n\n        if cfg.db_tracing {\n            var.push(format!(\"sqlx={level}\"));\n        }\n\n        var.join(\",\")\n    });\n\n    let otel_laye<|fim_middle|>", "completion": "use sentry::integrations::tracing::EventFilter;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/lib.rs", "node_type": "use_declaration", "line_range": [17, 17]}
{"prompt": "<|fim_prefix|>       anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_failed_message_gets_requeued() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver_1 = TestReceiver::start(axum::http::StatusCode::INTERNAL_SERVER_ERROR);\n    let _endp_id = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value\" });\n\n    let msg_res: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    receiver_1.data_recv.recv().await;\n\n    receiver_1.set_response_status_code(axum::http::StatusCode::OK);\n\n    let last_body = receiver_1.data_recv.recv().await.unwrap();\n\n    assert_eq!(msg_payload.to_string(), last_body.to_string());\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 200) {\n            anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, json!({ \"test\": \"value\" })).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n    let msg_id = msg.id.clone();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap();\n    assert_eq!(content.unwrap().id, msg_id.clone());\n\n    let res = messagecontent::Entity::update_many()\n        .col_expr(\n            messagecontent::Column::Expiration,\n            Expr::value(Utc::now() - Duration::days(1)),\n        )\n        .filter(messagecontent::Column::Id.eq(msg_id.clone()))\n        .exec(&pool)\n        .await\n        .unwrap();\n    assert_eq!(1, res.rows_affected);\n\n    expired_message_cleaner::clean_expired_messages(&pool, 5000, false)\n        .await\n        .unwrap();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id)\n        .one(&pool)\n        .await\n        .unwrap();\n    assert!(content.is_none());\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period_messagecontent() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    let app_id = create_test_app(&client, \"test-content-expiration-period\")\n        .await\n        .unwrap()\n        .id;\n\n    let custom_retention_period = 5;\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value\" },\n                \"payloadRetentionPeriod\": custom_retention_period\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n    let msg_id = msg.id.clone();\n\n    let content: messagecontent::Model = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap()\n        .unwrap();\n\n    l<|fim_suffix|>    let actual: chrono::DateTime<Utc> = content.expiration.into();\n\n    assert!(actual < expected);\n}\n\n#[tokio::test]\nasync fn test_expunge_message_payload() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testApp\").await.unwrap().id;\n\n    let payload = json!({ \"sensitive\": \"data\" });\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{}/content/\", msg.id),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(msg.payload.0.get(), r#\"{\"expired\":true}\"#);\n}\n\n#[tokio::test]\nasync fn test_message_conflict() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg_in = json!({\n        \"eventType\": \"user.signup\",\n        \"payload\": { \"test\": \"value\" },\n        \"payloadRetentionPeriod\": 5,\n        \"eventId\": \"test1\",\n    });\n\n    let _: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            msg_in.clone(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            msg_in,\n            StatusCode::CONFLICT,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_message_validation() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testApp\").await.unwrap().id;\n    let payload = json!({ \"large_payload\": \"payload-\".repeat(1_000_000) });\n\n    client\n        .post::<_, IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, payload).unwrap(),\n            StatusCode::PAYLOAD_TOO_LARGE,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_raw_payload() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testRawPayload\").await.unwrap().id;\n\n    let mut receiver = TestReceiver::start(axum::http::StatusCode::OK);\n\n    create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let msg_payload = json!({ \"test\": \"value1\" });\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            json!({\n                \"eventType\": \"payload.raw\",\n                \"payload\": {},\n                \"transformationsParams\": {\n                    \"rawPayload\": msg_payload.to_string(),\n                },\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let rec_body = receiver.data_recv.recv().await;\n    assert_eq!(msg_payload.to_string(), rec_body.unwrap().to_string());\n}\n\n#[tokio::test]\nasync fn test_create_message_with_application() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_uid = format!(\"app-created-in-cmg-{}\", rand_str(15));\n\n    // cmg without the application field fails\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_uid}/msg/\"),\n            json!({\n                \"event<|fim_middle|>", "completion": "let expected = Utc::now() + Duration::days(custom_retention_period) + Duration::hours(1);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_message.rs", "node_type": "let_declaration", "line_range": [464, 464]}
{"prompt": "<|fim_prefix|>use std::collections::HashMap;\n\nuse schemars::JsonSchema;\nuse serde::{Deserialize, Serialize};\n\nuse crate::json_wrapper;\n\npub const MAX_METADATA_SIZE: usize = 4096;\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Default, JsonSchema)]\n#[schemars(transparent)]\npub struct Metadata(HashMap<String, String>);\n\njson_wrapper!(Metadata);\n\nimpl Metadata {\n    pub fn is_empty(&self) -> bool {\n        self.0.is_empty()\n    }\n}\n\nimpl<'de> Deserialize<'de> for Metadata {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        <|fim_suffix|>\n        let metadata = inner.unwrap_or_default(); // coerce `null` to `{}`\n\n        let size = serde_json::to_string(&metadata)\n            .map(|blob| blob.len())\n            .map_err(|_| serde::de::Error::custom(\"metadata is not valid json\"))?;\n\n        if size > MAX_METADATA_SIZE {\n            return Err(serde::de::Error::custom(format!(\n                \"metadata must be less than or equal to {MAX_METADATA_SIZE} bytes\"\n            )));\n        }\n\n        Ok(Self(metadata))\n    }\n}\n<|fim_middle|>", "completion": "let inner: Option<HashMap<String, String>> = Deserialize::deserialize(deserializer)?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/metadata.rs", "node_type": "let_declaration", "line_range": [27, 27]}
{"prompt": "<|fim_prefix|>Ok(resolve_and_cache_response(\n                    &cache,\n                    &key,\n                    service,\n                    Request::from_parts(parts, body),\n                )\n                .await)\n            })\n        } else {\n            Box::pin(async move { Ok(service.call(req).await.into_response()) })\n        }\n    }\n}\n\n/// Retrieves an [`IdempotencyKey`] from the [`Parts`] of a [`Request`] returning None in the event\n/// that not all erquisite parts are there.\nfn get_key(parts: &Parts) -> Option<IdempotencyKey> {\n    let key = if let Some(Ok(key)) = parts.headers.get(\"idempotency-key\").map(|v| v.to_str()) {\n        key\n    } else {\n        // No idempotency-key -- pass off to service and do not cache\n        return None;\n    };\n\n    let auth = if let Some(Ok(auth)) = parts.headers.get(\"Authorization\").map(|v| v.to_str()) {\n        auth\n    } else {\n        // No auth token -- pass off to service and do not cache\n        return None;\n    };\n\n    let uri = parts.uri.to_string();\n\n    Some(IdempotencyKey::new(auth, key, &uri))\n}\n\n/// If the lock could not be set, then another request with that key has been completed or is being\n/// completed, so loop until it has been completed or times out\nasync fn lock_loop(\n    cache: &Cache,\n    key: &IdempotencyKey,\n) -> Result<Option<SerializedResponse>, Error> {\n    let mut total_delay_duration = std::time::Duration::from_millis(0);\n\n    loop {\n        total_delay_duration += wait_duration();\n        tokio::time::sleep(wait_duration()).await;\n\n        match cache.get::<SerializedResponse>(key).await {\n            // Value has been retrieved from cache, so return it\n            Ok(Some(resp @ SerializedResponse::Finished { .. })) => return Ok(Some(resp)),\n\n            // Request setting the lock has not been resolved yet, so wait a little and loop again\n            Ok(Some(SerializedResponse::Start)) => {\n                if total_delay_duration > expiry_starting() {\n                    return Ok(None);\n                }\n            }\n\n            // Start value has expired\n            Ok(None) => return Ok(None),\n\n            Err(e) => return Err(Error::database(format_args!(\"{e:?}\"))),\n        }\n    }\n}\n\n/// Resolve the service and cache the result assuming the response is successful\nasync fn resolve_and_cache_response<S>(\n    cache: &Cache,\n    key: &IdempotencyKey,\n    service: S,\n    request: Request,\n) -> Response\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    let (parts, body) = resolve_service(service, request).await.into_parts();\n\n    // If a 2xx response, cache the actual response\n    if parts.status.is_success() {\n        // TODO: Don't skip over Err value\n        let bytes = body.collect().await.ok().map(|c| c.to_bytes());\n\n        let resp = SerializedResponse::Finished {\n            code: parts.status.into(),\n            headers: Some(\n                parts\n                    .headers\n                    .iter()\n                    .map(|(k, v)| (k.as_str().to_owned(), v.as_bytes().to_owned()))\n                    .collect(),\n            ),\n            body: bytes.clone().map(|b| b.to_vec()),\n        };\n\n        if cache.set(key, &resp, expiry_default()).await.is_err() {\n            return StatusCode::INTERNAL_SERVER_ERROR.into_response();\n        }\n\n        // Assumes None to be an empty byte array\n        let bytes = bytes.unwrap_or_default();\n        Response::from_parts(parts, Body::from(bytes)).into_response()\n    }\n    // If any other status, unset the start lock and return the response\n    else {\n        if cache.delete(key).await.is_err() {\n            return StatusCode::INTERNAL_SERVER_ERROR.into_response();\n        }\n\n        Response::from_parts(parts, body).into_response()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use std::sync::Arc;\n\n    use axum::{extract::State, routing::post, serve, Router};\n    use http::StatusCode;\n    use reqwest::Client;\n    use tokio::{net::TcpListener, sync::Mutex, task::JoinHandle};\n    use tower::ServiceBuilder;\n\n    u<|fim_suffix|>    use crate::core::{\n        cache,\n        security::generate_org_token,\n        types::{BaseId, OrganizationId},\n    };\n\n    #[derive(Clone)]\n    struct TestAppState {\n        count: Arc<Mutex<u16>>,\n        wait: Option<std::time::Duration>,\n    }\n\n    /// Starts a basic Axum server with one endpoint which counts the number of times the endpoint\n    /// has been polled from. This will be nested in the [`IdempotencyService`] such that, providing\n    /// a key may result in the count not increasing and a prior result being displayed.\n    ///\n    /// This function takes a variable length of time to complete with the delay input used for\n    /// testing the start lock.\n    ///\n    /// This function will return a join handle to that server, its URL and an [`Arc<Mutex<usize>>`]\n    /// that points to the count of the server such that its internal state may be monitored.\n    async fn start_service(\n        wait: Option<std::time::Duration>,\n    ) -> (JoinHandle<()>, String, Arc<Mutex<u16>>) {\n        dotenvy::dotenv().ok();\n\n        let cache = cache::memory::new();\n\n        let count = Arc::new(Mutex::new(0));\n\n        let listener = TcpListener::bind(\"127.0.0.1:0\").await.unwrap();\n        let endpoint = format!(\"http://{}/\", listener.local_addr().unwrap());\n\n        let jh = tokio::spawn({\n            let count = count.clone();\n            async move {\n                let svc = Router::new()\n                    .route(\"/\", post(service_endpoint).get(service_endpoint))\n                    .layer(\n                        ServiceBuilder::new().layer_fn(move |service| IdempotencyService {\n                            cache: cache.clone(),\n                            service,\n                        }),\n                    )\n                    .with_state(TestAppState { count, wait })\n                    .into_make_service();\n                serve(listener, svc).await.unwrap();\n            }\n        });\n\n        (jh, endpoint, count)\n    }\n\n    /// Only to be used via [`start_service`] -- this is the actual endpoint implementation\n    async fn service_endpoint(State(TestAppState { wait, count }): State<TestAppState>) -> String {\n        let mut count = count.lock().await;\n        *count += 1;\n\n        if let Some(wait) = wait {\n            tokio::time::sleep(wait).await;\n        }\n\n        format!(\"{count}\")\n    }\n\n    #[tokio::test]\n    async fn test_basic_idempotency() {\n        let (_jh, endpoint, count) = start_service(None).await;\n        let client = Client::new();\n\n        // Generate a new token so that keys are unique\n        dotenvy::dotenv().ok();\n        let cfg = crate::cfg::load().unwrap();\n        let token = generate_org_token(&cfg.jwt_signing_config, OrganizationId::new(None, None))\n            .unwrap()\n            .to_string();\n\n        // Sanity check on test service\n        assert_eq!(*count.lock().await, 0);\n        let _ = client.post(&endpoint).send().await;\n        assert_eq!(*count.lock().await, 1);\n\n        // Idempotency key not yet used -- should increment\n        let resp_1 = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"1\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await\n            .unwrap();\n        assert_eq!(*count.lock().await, 2);\n\n        // Now used the count should not increment\n        let resp_2 = client\n            .post(&endpoint)\n            .header(\"idempotency-key\", \"1\")\n            .header(\"Authorization\", &token)\n            .send()\n            .await\n            .unwrap();\n        assert_eq!(*count.lock().await, 2);\n\n        // And the responses should be equivalent\n        assert_eq!(resp_1.status(), resp_2.status());\n        //assert_eq!(resp_1.headers(), resp_2.headers());\n        assert_eq!(resp_1.text().await.unwrap(), resp_2.text().await.unwrap());\n\n        // No key -- should increment\n        let _ = client.post(&endpoint).send().await;\n        assert_eq!(*count.lock().await, 3);\n\n        // Same key -- should not increment\n        let _ = client\n            .post(&endpoint)\n            .header(\"idempote<|fim_middle|>", "completion": "use super::IdempotencyService;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/idempotency.rs", "node_type": "use_declaration", "line_range": [363, 363]}
{"prompt": "<|fim_prefix|>)\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_multiple_endpoints() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let mut receiver_2 = TestReceiver::start(axum::http::StatusCode::INTERNAL_SERVER_ERROR);\n    let mut receiver_3 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let _endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n    let _endp_id_2 = create_test_endpoint(&client, &app_id, &receiver_2.endpoint)\n        .await\n        .unwrap()\n        .id;\n    let _endp_id_3 = create_test_endpoint(&client, &app_id, &receiver_3.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value1\" });\n\n    let msg_res: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let (rec_body_1, rec_body_2, rec_body_3) = tokio::join!(\n        receiver_1.data_recv.recv(),\n        receiver_2.data_recv.recv(),\n        receiver_3.data_recv.recv()\n    );\n\n    assert_eq!(msg_payload.to_string(), rec_body_1.unwrap().to_string());\n    assert_eq!(msg_payload.to_string(), rec_body_2.unwrap().to_string());\n    assert_eq!(msg_payload.to_string(), rec_body_3.unwrap().to_string());\n\n    receiver_2.set_response_status_code(axum::http::StatusCode::OK);\n\n    let rec_body_2 = receiver_2.data_recv.recv().await.unwrap();\n\n    assert_eq!(msg_payload.to_string(), rec_body_2.to_string());\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 200) {\n            anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_failed_message_gets_requeued() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let mut receiver_1 = TestReceiver::start(axum::http::StatusCode::INTERNAL_SERVER_ERROR);\n    let _endp_id = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_payload = json!({ \"test\": \"value\" });\n\n    let msg_res: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    receiver_1.data_recv.recv().await;\n\n    receiver_1.set_response_status_code(axum::http::StatusCode::OK);\n\n    let last_body = receiver_1.data_recv.recv().await.unwrap();\n\n    assert_eq!(msg_payload.to_string(), last_body.to_string());\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{}/\", msg_res.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        if !attempts.data.iter().any(|x| x.response_status_code == 200) {\n            anyhow::bail!(\"could not find successful attempt\");\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    l<|fim_suffix|>    let msg_id = msg.id.clone();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap();\n    assert_eq!(content.unwrap().id, msg_id.clone());\n\n    let res = messagecontent::Entity::update_many()\n        .col_expr(\n            messagecontent::Column::Expiration,\n            Expr::value(Utc::now() - Duration::days(1)),\n        )\n        .filter(messagecontent::Column::Id.eq(msg_id.clone()))\n        .exec(&pool)\n        .await\n        .unwrap();\n    assert_eq!(1, res.rows_affected);\n\n    expired_message_cleaner::clean_expired_messages(&pool, 5000, false)\n        .await\n        .unwrap();\n\n    let content: Option<messagecontent::Model> = messagecontent::Entity::find_by_id(msg_id)\n        .one(&pool)\n        .await\n        .unwrap();\n    assert!(content.is_none());\n}\n\n#[tokio::test]\nasync fn test_payload_retention_period_messagecontent() {\n    let (client, _jh) = start_svix_server().await;\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n    let pool = svix_server::db::init_db(&cfg).await;\n\n    let app_id = create_test_app(&client, \"test-content-expiration-period\")\n        .await\n        .unwrap()\n        .id;\n\n    let custom_retention_period = 5;\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            json!({\n                \"eventType\": \"test.event\",\n                \"payload\": { \"test\": \"value\" },\n                \"payloadRetentionPeriod\": custom_retention_period\n            }),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n    let msg_id = msg.id.clone();\n\n    let content: messagecontent::Model = messagecontent::Entity::find_by_id(msg_id.clone())\n        .one(&pool)\n        .await\n        .unwrap()\n        .unwrap();\n\n    let expected = Utc::now() + Duration::days(custom_retention_period) + Duration::hours(1);\n    let actual: chrono::DateTime<Utc> = content.expiration.into();\n\n    assert!(actual < expected);\n}\n\n#[tokio::test]\nasync fn test_expunge_message_payload() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"testApp\").await.unwrap().id;\n\n    let payload = json!({ \"sensitive\": \"data\" });\n    let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(\n        msg.payload.0.get(),\n        serde_json::to_string(&payload).unwrap()\n    );\n\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{}/content/\", msg.id),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let msg = client\n        .get::<MessageOut>(\n            &format!(\"api/v1/app/{app_id}/msg/{}/\", msg.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(msg.payload.0.get(), r#\"{\"expired\":true}\"#);\n}\n\n#[tokio::test]\nasync fn test_message_conflict() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let msg_in = json!({\n        \"eventType\": \"user.signup\",\n        \"payload\": { \"test\": \"value\" },\n        \"payloadRetentionPeriod\": 5,\n        \"eventId\": \"test1\",\n    });\n\n    let _: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            msg_in.clone(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let _<|fim_middle|>", "completion": "let msg: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, json!({ \"test\": \"value\" })).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_message.rs", "node_type": "let_declaration", "line_range": [393, 400]}
{"prompt": "<|fim_prefix|>use omniqueue::DynConsumer;\nuse svix_bridge_types::{\n    async_trait, svix::api::Svix, SenderInput, SenderOutputOpts, TransformationConfig,\n    TransformerTx,\n};\n\nuse crate::{config::QueueInputOpts, gcp_pubsub, rabbitmq, run_inner, sqs, Consumer};\n\npub struct QueueSender {\n    name: String,\n    source: String,\n    system: String,\n    input_opts: QueueInputOpts,\n    transformation: Option<TransformationConfig>,\n    transformer_tx: Option<TransformerTx>,\n    svix_client: Svix,\n}\n\nimpl std::fmt::Debug for QueueSender {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        f.debug_struct(\"SenderInput\").finish()\n    }\n}\n\nfn system_name(opts: &QueueInputOpts) -> &'static str {\n    match opts {\n        QueueInputOpts::GcpPubSub(_) => \"gcp-pubsub\",\n        QueueInputOpts::RabbitMQ(_) => \"rabbitmq\",\n        QueueInputOpts::Redis(_) => \"redis\",\n        QueueInputOpts::Sqs(_) => \"sqs\",\n    }\n}\n\nfn source_name(opts: &QueueInputOpts) -> &str {\n    match opts {\n        QueueInputOpts::GcpPubSub(opts) => &opts.subscription_id,\n        QueueInputOpts::RabbitMQ(opts) => &opts.queue_name,\n        QueueInputOpts::Redis(opts) => &opts.queue_key,\n        QueueInputOpts::Sqs(opts) => &opts.queue_dsn,\n    }\n}\n\nimpl QueueSender {\n    pub fn new(\n        name: String,\n        input: QueueInputOpts,\n        transformation: Option<TransformationConfig>,\n        output: SenderOutputOpts,\n    ) -> Self {\n        Self {\n            name,\n            source: source_name(&input).into(),\n            system: system_name(&input).into(),\n            input_opts: input,\n            transformation,\n            transformer_tx: None,\n            svix_client: match output {\n                SenderOutputOpts::Svix(output) => {\n                    Svix::new(output.token, output.options.map(Into::into))\n                }\n            },\n        }\n    }\n}\n\n#[async_trait]\nimpl Consumer for QueueSender {\n    fn source(&self) -> &str {\n        &self.source\n    }\n\n    <|fim_suffix|>\n\n    fn transformer_tx(&self) -> Option<&TransformerTx> {\n        self.transformer_tx.as_ref()\n    }\n\n    fn transformation(&self) -> Option<&TransformationConfig> {\n        self.transformation.as_ref()\n    }\n\n    fn svix_client(&self) -> &Svix {\n        &self.svix_client\n    }\n\n    async fn consumer(&self) -> std::io::Result<DynConsumer> {\n        Ok(match &self.input_opts {\n            QueueInputOpts::GcpPubSub(cfg) => gcp_pubsub::consumer(cfg).await?,\n            QueueInputOpts::RabbitMQ(cfg) => rabbitmq::consumer(cfg).await?,\n            QueueInputOpts::Redis(cfg) => crate::redis::consumer(cfg).await?,\n            QueueInputOpts::Sqs(cfg) => sqs::consumer(cfg).await?,\n        })\n    }\n}\n\n#[async_trait]\nimpl SenderInput for QueueSender {\n    fn name(&self) -> &str {\n        &self.name\n    }\n\n    fn set_transformer(&mut self, tx: Option<TransformerTx>) {\n        self.transformer_tx = tx;\n    }\n\n    async fn run(&self) {\n        run_inner(self).await\n    }\n}\n<|fim_middle|>", "completion": "fn system(&self) -> &str {\n        &self.system\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/src/sender_input/mod.rs", "node_type": "function_item", "line_range": [72, 74]}
{"prompt": "<|fim_prefix|>use std::{\n    collections::HashMap,\n    future::Future,\n    net::{IpAddr, Ipv4Addr, Ipv6Addr, SocketAddr},\n    pin::Pin,\n    str::FromStr,\n    sync::Arc,\n    task::{ready, Poll},\n    time::{Duration, Instant},\n};\n\nuse axum::{body::Body, response::Response};\nuse axum_extra::headers::{authorization::Credentials, Authorization};\nuse bytes::Bytes;\nuse futures::{future::BoxFuture, FutureExt};\nuse hickory_resolver::{lookup_ip::LookupIpIntoIter, ResolveError, Resolver, TokioResolver};\nuse http::{header::HeaderName, HeaderMap, HeaderValue, Method, StatusCode, Version};\nuse http_body_util::Full;\nuse hyper::{body::Incoming, ext::HeaderCaseMap, Uri};\nuse hyper_openssl::client::legacy::{HttpsConnector, MaybeHttpsStream};\nuse hyper_util::{\n    client::{\n        legacy::{\n            connect::{\n                dns::Name,\n                proxy::{SocksV5, Tunnel},\n                HttpConnector,\n            },\n            Client,\n        },\n        proxy::matcher::Matcher,\n    },\n    rt::{TokioExecutor, TokioIo},\n};\nuse ipnet::IpNet;\nuse openssl::ssl::{SslConnector, SslConnectorBuilder, SslMethod, SslVerifyMode};\nuse serde::Serialize;\nuse thiserror::Error;\nuse tokio::{net::TcpStream, sync::Mutex};\nuse tower::Service;\n\nuse crate::cfg::{ProxyAddr, ProxyBypassCfg, ProxyConfig};\n\npub type CaseSensitiveHeaderMap = HashMap<String, HeaderValue>;\n\n#[derive(Debug, Error)]\npub enum Error {\n    #[error(\"failure response: {0}\")]\n    FailureStatus(StatusCode),\n\n    #[error(\"requests to this IP range are blocked (see the server configuration)\")]\n    BlockedIp,\n    #[error(\"error resolving name: {0}\")]\n    Resolve(#[from] ResolveError),\n\n    #[error(\"request timed out\")]\n    TimedOut,\n\n    #[error(\"error forming request: {0}\")]\n    InvalidHttpRequest(http::Error),\n    #[error(\"error making request: {0}\")]\n    FailedRequest(hyper_util::client::legacy::Error),\n}\n\nimpl From<hyper_util::client::legacy::Error> for Error {\n    <|fim_suffix|>\n}\n\n#[derive(Clone)]\npub struct WebhookClient {\n    client: Client<SvixHttpsConnector, Full<Bytes>>,\n    whitelist_nets: Arc<Vec<IpNet>>,\n}\n\nfn ssl_builder(disable_tls_verification: bool) -> SslConnectorBuilder {\n    // Openssl is required here -- in practice, rustls does not support many\n    // ciphers that we encounter on a regular basis:\n    let mut ssl = SslConnector::builder(SslMethod::tls()).expect(\"SslConnector build failed\");\n    if disable_tls_verification {\n        ssl.set_verify(SslVerifyMode::NONE);\n    }\n    ssl\n}\n\nimpl WebhookClient {\n    pub fn new(\n        whitelist_nets: Option<Arc<Vec<IpNet>>>,\n        whitelist_names: Option<Arc<Vec<String>>>,\n        dangerous_disable_tls_verification: bool,\n        proxy_config: Option<&ProxyConfig>,\n    ) -> Self {\n        let whitelist_nets = whitelist_nets.unwrap_or_else(|| Arc::new(Vec::new()));\n        let whitelist_names = whitelist_names.unwrap_or_else(|| Arc::new(Vec::new()));\n\n        let dns_resolver = NonLocalDnsResolver::new(whitelist_nets.clone(), whitelist_names);\n        let mut http = HttpConnector::new_with_resolver(dns_resolver);\n        http.enforce_http(false);\n\n        if dangerous_disable_tls_verification {\n            tracing::warn!(\"TLS certificate verification has been disabled by the configuration.\");\n        }\n        let https = SvixHttpsConnector::new(http, proxy_config, dangerous_disable_tls_verification)\n            .expect(\"SvixHttpsConnector build failed\");\n\n        let client = hyper_util::client::legacy::Client::builder(TokioExecutor::new())\n            .http1_ignore_invalid_headers_in_responses(true)\n            .http1_title_case_headers(true)\n            .build(https);\n\n        Self {\n            client,\n            whitelist_nets,\n        }\n    }\n\n    pub async fn execute(&self, request: Request) -> Result<Response, Error> {\n        let resp = self.execute_inner(request, true).await?;\n        Ok(resp.map(Body::new))\n    }\n\n    pub fn execute_inner(\n        &self,\n        request: Request,\n        retry: bool,\n    ) -> BoxFuture<'_, Result<Response<Incoming>, Error>> {\n        async move {\n            let org_req = request.clone();\n            if let Some(auth) = request.uri.authority() {\n                if let Ok(ip) = auth.host().parse::<IpAddr>() {\n                    if !is_allowed(ip)\n                        && !self\n                            .whitelist_nets\n                            .iter()\n                            .any(|subnet| subnet.contains(&ip))\n                    {\n                        return Err(Error::BlockedIp);\n                    }\n                }\n            }\n\n            let mut req = if let Some(body) = request.body {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::from(body))\n                    .map_err(Error::InvalidHttpRequest)?\n            } else {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::default())\n                    .map_err(Error::InvalidHttpRequest)?\n            };\n\n            *req.headers_mut() = request.headers;\n\n            if let Some(header_names) = request.header_names {\n                req.extensions_mut().insert(header_names);\n            }\n\n            let start = Instant::now();\n            let res = if let Some(timeout) = request.timeout {\n                match tokio::time::timeout(timeout, self.client.request(req)).await {\n                    Ok(Ok(resp)) => Ok(resp),\n                    Ok(Err(e)) => Err(e.into()),\n                    Err(_to) => Err(Error::TimedOut),\n                }\n            } else {\n                self.client.request(req).await.map_err(Into::into)\n            };\n\n            if !retry {\n                return res;\n            }\n\n            match res {\n                Err(Error::FailedRequest(e)) if start.elapsed() < Duration::from_millis(1000) => {\n                    tracing::info!(\"Insta-retrying: {e}\");\n                    self.execute_inner(org_req, false).await\n                }\n                res => res,\n            }\n        }\n        .boxed()\n    }\n}\n\n#[derive(Clone)]\npub struct Request {\n    method: Method,\n    uri: Uri,\n    headers: HeaderMap,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    timeout: Option<Duration>,\n    version: Version,\n}\n\npub struct RequestBuilder {\n    method: Option<Method>,\n    uri: Option<Uri>,\n    accept: Option<HeaderValue>,\n    user_agent: Option<HeaderValue>,\n    headers: Option<HeaderMap>,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    version: Option<Version>,\n    timeout: Option<Duration>,\n    basic_auth: Option<Vec<u8>>,\n\n    // Derived from body\n    content_type: Option<HeaderValue>,\n}\n\n#[derive(Debug)]\npub struct RequestBuildError(pub Vec<BuildError>);\n\nimpl std::fmt::Display for RequestBuildError {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        let mut iter = self.0.iter();\n\n        f.write_str(\"Build failed\")?;\n\n        if let Some(first) = iter.next() {\n            write!(f, \": {first}\")?;\n\n            for err in iter {\n                write!(f, \"; {err}\")?;\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Error)]\npub enum BuildError {\n    #[error(\"uri missing\")]\n    UriMissing,\n    #[error(\"version missing\")]\n    VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub fn new() -> Self {\n        Self {\n            method: None,\n            uri: None,\n            accept: None,\n            user_agent: None,\n            headers: None,\n            header_names: None,\n            body: None,\n            version: None,\n            timeout: None,\n            content_type: None,\n            basic_auth: None,\n        }\n    }\n\n    pub fn method(mut self, method: Method) -> Self {\n        self.method = Some(method);\n        sel<|fim_middle|>", "completion": "fn from(e: hyper_util::client::legacy::Error) -> Self {\n        let mut dyn_e = &e as &dyn std::error::Error;\n        loop {\n            if dyn_e\n                .to_string()\n                .contains(\"requests to this IP range are blocked\")\n            {\n                return Error::BlockedIp;\n            }\n\n            match dyn_e.source() {\n                Some(source) => dyn_e = source,\n                None => return Error::FailedRequest(e),\n            }\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "function_item", "line_range": [66, 81]}
{"prompt": "<|fim_prefix|>//! Use the `testing-docker-compose.yml` in the repo root to run the dependencies for testing,\n//! including the gcloud pubsub emulator.\n//!\n//! Use `run-tests.sh` to use the requisite environment for testing.\n\nuse std::time::Duration;\n\nuse gcloud_googleapis::pubsub::v1::{DeadLetterPolicy, PubsubMessage};\nuse gcloud_pubsub::{\n    client::{Client, ClientConfig},\n    subscription::{Subscription, SubscriptionConfig},\n    topic::Topic,\n};\nuse serde_json::json;\nuse svix_bridge_plugin_queue::{\n    config::{GcpPubSubInputOpts, QueueInputOpts},\n    sender_input::QueueSender,\n};\nuse svix_bridge_types::{\n    svix::api::MessageIn, CreateMessageRequest, SenderInput, SenderOutputOpts, SvixOptions,\n    SvixSenderOutputOpts, TransformationConfig, TransformerInput, TransformerInputFormat,\n    TransformerJob, TransformerOutput,\n};\nuse wiremock::{\n    matchers::{body_partial_json, method},\n    Mock, MockServer, ResponseTemplate,\n};\n\nconst DEFAULT_PUBSUB_EMULATOR_HOST: &str = \"localhost:8085\";\n\nfn get_test_plugin(\n    svix_url: String,\n    subscription_id: String,\n    use_transformation: Option<TransformerInputFormat>,\n) -> QueueSender {\n    QueueSender::new(\n        \"test\".into(),\n        QueueInputOpts::GcpPubSub(GcpPubSubInputOpts {\n            subscription_id,\n            credentials_file: None,\n        }),\n        use_transformation.map(|format| TransformationConfig::Explicit {\n            format,\n            src: String::from(\"function handle(x) { return x; }\"),\n        }),\n        SenderOutputOpts::Svix(SvixSenderOutputOpts {\n            token: \"xxxx\".to_string(),\n            options: Some(SvixOptions {\n                server_url: Some(svix_url),\n                ..Default::default()\n            }),\n        }),\n    )\n}\n\nasync fn mq_connection() -> Client {\n    // The `Default` impl for `ClientConfig` looks for this env var. When set it branches for\n    // local-mode use using the addr in the env var and a hardcoded project id of `local-project`.\n    if std::env::var(\"PUBSUB_EMULATOR_HOST\").is_err() {\n        std::env::set_var(\"PUBSUB_EMULATOR_HOST\", DEFAULT_PUBSUB_EMULATOR_HOST);\n    }\n    Client::new(ClientConfig::default()).await.unwrap()\n}\n\n<|fim_suffix|>\n\nasync fn create_test_queue(client: &Client) -> (Topic, Subscription) {\n    let topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();\n    // Need to define a dead letter topic to avoid the \"bad\" test cases from pulling the nacked\n    // messages again and again.\n    let dead_letter_topic_name: String = \"topic-\".chars().chain(random_chars().take(8)).collect();\n    let subscription_name: String = \"subscription-\"\n        .chars()\n        .chain(random_chars().take(8))\n        .collect();\n\n    let topic = client.create_topic(&topic_name, None, None).await.unwrap();\n    let dead_letter_topic = client\n        .create_topic(&dead_letter_topic_name, None, None)\n        .await\n        .unwrap();\n    let subscription = client\n        .create_subscription(\n            &subscription_name,\n            &topic_name,\n            SubscriptionConfig {\n                // Messages published to the topic need to supply a unique ID to make use of this\n                enable_exactly_once_delivery: true,\n                dead_letter_policy: Some(DeadLetterPolicy {\n                    dead_letter_topic: dead_letter_topic.fully_qualified_name().into(),\n                    max_delivery_attempts: MAX_DELIVERY_ATTEMPTS,\n                }),\n                ..Default::default()\n            },\n            None,\n        )\n        .await\n        .unwrap();\n\n    (topic, subscription)\n}\n\nasync fn publish(topic: &Topic, payload: &str) {\n    let publisher = topic.new_publisher(None);\n    let awaiter = publisher\n        .publish(PubsubMessage {\n            data: payload.to_owned().into_bytes(),\n            message_id: random_chars().take(6).collect(),\n            ..Default::default()\n        })\n        .await;\n    awaiter.get().await.unwrap();\n}\n\n/// General \"pause while we wait for messages to travel\" beat. If you're seeing flakes, bump this up.\nconst WAIT_MS: u64 = 100;\n/// Controls how many times a message can be nack'd before it lands on the dead letter topic.\nconst MAX_DELIVERY_ATTEMPTS: i32 = 5;\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), subscription.id(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&topic, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    subscription.delete(None).await.ok();\n    topic.delete(None).await.ok();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = mq_connection().await;\n    let (topic, subscription) = create_test_queue(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        // The transformed bit of the payload\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        subscription.id(),\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_<|fim_middle|>", "completion": "fn random_chars() -> impl Iterator<Item = char> {\n    std::iter::repeat_with(fastrand::alphanumeric)\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/gcp_pubsub_consumer.rs", "node_type": "function_item", "line_range": [65, 67]}
{"prompt": "<|fim_prefix|>use std::{net::TcpListener, sync::Arc};\n\nuse axum::{body::Body, extract::State};\nuse http::{header::USER_AGENT, HeaderValue, Request, StatusCode, Version};\nuse http_body_util::BodyExt as _;\nuse serde::{Deserialize, Serialize};\nuse svix_server::core::webhook_http_client::{Error, RequestBuilder, WebhookClient};\nuse tokio::sync::mpsc;\n\npub struct TestReceiver {\n    pub uri: String,\n    pub server_jh: tokio::task::JoinHandle<()>,\n    pub req_recv: mpsc::Receiver<Request<Body>>,\n}\n\nimpl Drop for TestReceiver {\n    fn drop(&mut self) {\n        self.server_jh.abort();\n    }\n}\n\n#[derive(Clone)]\nstruct TestAppState {\n    tx: mpsc::Sender<Request<Body>>,\n    response_status_code: StatusCode,\n}\n\nimpl TestReceiver {\n    pub fn start(resp_code: StatusCode) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n        let uri = format!(\"http://{}/\", listener.local_addr().unwrap());\n\n        let (tx, req_recv) = mpsc::channel(32);\n\n        let routes = axum::Router::new()\n            .route(\"/\", axum::routing::any(test_receiver_route))\n            .with_state(TestAppState {\n                tx,\n                response_status_code: resp_code,\n            })\n            .into_make_service();\n\n        let server_jh = tokio::spawn(async move {\n            axum::serve(listener, routes).await.unwrap();\n        });\n\n        TestReceiver {\n            uri,\n            server_jh,\n            req_recv,\n        }\n    }\n}\n\nasync fn test_receiver_route(\n    State(TestAppState {\n        ref tx,\n        response_status_code,\n    }): State<TestAppState>,\n    req: Request<Body>,\n) -> axum::http::StatusCode {\n    tx.send(req).await.unwrap();\n    response_status_code\n}\n\n#[derive(Deserialize, Serialize)]\npub struct TestSerializable {\n    test: String,\n}\n\n#[ignore]\n#[tokio::test]\nasync fn test_client_basic_operation() {\n    // Compares output to `reqwest`.\n\n    let our_client = WebhookClient::new(\n        Some(Arc::new(vec![\"127.0.0.1/0\".parse().unwrap()])),\n        None,\n        false,\n        None,\n    );\n    let reqwest_client = reqwest::Client::builder()\n        .redirect(reqwest::redirect::Policy::none())\n        .build()\n        .expect(\"Invalid reqwest Client configuration\");\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let our_req = RequestBuilder::new()\n        .uri_str(&receiver.uri)\n        .unwrap()\n        .json_body(TestSerializable {\n            test: \"value\".to_owned(),\n        })\n        .unwrap()\n        .version(Version::HTTP_11)\n        .build()\n        .unwrap();\n\n    let _resp = our_client.execute(our_req).await.unwrap();\n\n    let our_http_req = receiver.req_recv.recv().await.unwrap();\n\n    <|fim_suffix|>\n\n    let reqwest_http_req = receiver.req_recv.recv().await.unwrap();\n\n    assert_eq!(our_http_req.headers(), reqwest_http_req.headers());\n\n    let our_body = our_http_req.into_body().collect().await.unwrap().to_bytes();\n    #[rustfmt::skip]\n    let reqwest_body = reqwest_http_req.into_body().collect().await.unwrap().to_bytes();\n    assert_eq!(our_body, reqwest_body);\n}\n\n#[tokio::test]\nasync fn test_filtering() {\n    let our_client = WebhookClient::new(None, None, false, None);\n\n    let our_req = RequestBuilder::new()\n        .uri_str(\"http://127.0.0.1/\")\n        .unwrap()\n        .json_body(TestSerializable {\n            test: \"value\".to_owned(),\n        })\n        .unwrap()\n        .version(Version::HTTP_11)\n        .build()\n        .unwrap();\n\n    assert!(matches!(\n        our_client.execute(our_req).await.unwrap_err(),\n        Error::BlockedIp\n    ));\n\n    let our_req = RequestBuilder::new()\n        .uri_str(\"http://localhost/\")\n        .unwrap()\n        .json_body(TestSerializable {\n            test: \"value\".to_owned(),\n        })\n        .unwrap()\n        .version(Version::HTTP_11)\n        .build()\n        .unwrap();\n\n    assert!(matches!(\n        our_client.execute(our_req).await.unwrap_err(),\n        Error::BlockedIp\n    ));\n}\n<|fim_middle|>", "completion": "let _resp = reqwest_client\n        .post(&receiver.uri)\n        .header(\n            USER_AGENT,\n            HeaderValue::from_static(concat!(\"Svix-Webhooks/\", env!(\"CARGO_PKG_VERSION\"))),\n        )\n        .version(Version::HTTP_11)\n        .json(&TestSerializable {\n            test: \"value\".to_owned(),\n        })\n        .send()\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/integ_webhook_http_client.rs", "node_type": "let_declaration", "line_range": [107, 119]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Idempotency middleware for the Axum server.\n//!\n//! The middleware first looks up the given key for an existing response before\n//! routing to the given endpoint's function, and caches any such results\n//! such that subsequent requests to that endpoint with the same key will return\n//! the same response.\n//!\n//! Responses are cached for twelve hours by default.\n\nuse std::{collections::HashMap, convert::Infallible, future::Future, pin::Pin, time::Duration};\n\nuse axum::{\n    body::Body,\n    extract::Request,\n    http::StatusCode,\n    response::{IntoResponse, Response},\n};\nuse base64::{engine::general_purpose::STANDARD, Engine};\nuse blake2::{Blake2b512, Digest};\nuse http::request::Parts;\nuse http_body_util::BodyExt as _;\nuse serde::{Deserialize, Serialize};\nuse tower::Service;\n\nuse super::cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue};\nuse crate::error::Error;\n\n/// Returns the default expiry period for cached responses\nconst fn expiry_default() -> Duration {\n    Duration::from_secs(60 * 60 * 12)\n}\n\n/// Returns the default expiry period for the starting lock\nconst fn expiry_starting() -> Duration {\n    Duration::from_secs(5)\n}\n\n/// Returns the duration to sleep before retrying to find a [`SerializedResponse::Finished`] in the\n/// cache\nconst fn wait_duration() -> Duration {\n    Duration::from_millis(200)\n}\n\n/// The data structure containing all necessary components of a response ready to be (de)serialized\n/// from/into the cache\n#[derive(Deserialize, Serialize)]\nenum SerializedResponse {\n    Start,\n    Finished {\n        code: u16,\n        headers: Option<HashMap<String, Vec<u8>>>,\n        body: Option<Vec<u8>>,\n    },\n}\n\nkv_def!(IdempotencyKey, SerializedResponse);\n\nimpl IdempotencyKey {\n    fn new(auth_token: &str, key: &str, url: &str) -> IdempotencyKey {\n        let mut hasher = Blake2b512::new();\n\n        hasher.update(auth_token);\n        hasher.update(\":\");\n        hasher.update(key);\n        hasher.update(\":\");\n        hasher.update(url);\n\n        let res = hasher.finalize();\n        // FIXME: add (previously omitted) prefix: `SVIX_IDEMPOTENCY_CACHE`\n        IdempotencyKey(STANDARD.encode(res))\n    }\n}\n\n#[derive(thiserror::Error, Debug)]\npub enum ConversionToResponseError {\n    #[error(\"the status code is out of bounds\")]\n    StatusError(#[from] http::status::InvalidStatusCode),\n\n    #[error(\"a header name is invalid\")]\n    FromStr(#[from] http::header::InvalidHeaderName),\n    #[error(\"a header value is invalid\")]\n    InvalidHeaderValue(#[from] http::header::InvalidHeaderValue),\n}\n\n/// Will never error as long as Redis doesn't corrupt -- never use this with anything but values\n/// from Redis which were put in via the idempotency service from known good requests.\nfn finished_serialized_response_to_response(\n    code: u16,\n    headers: Option<HashMap<String, Vec<u8>>>,\n    body: Option<Vec<u8>>,\n) -> Result<Response, ConversionToResponseError> {\n    let mut out = body.unwrap_or_default().into_response();\n\n    let status = out.status_mut();\n    *status = code.try_into()?;\n\n    if let Some(resp_headers) = headers {\n        l<|fim_suffix|>        *headers = resp_headers\n            .iter()\n            .map(|(k, v)| Ok((k.parse()?, http::HeaderValue::from_bytes(v)?)))\n            .collect::<Result<_, ConversionToResponseError>>()?;\n    }\n\n    Ok(out)\n}\n\nasync fn resolve_service<S>(mut service: S, req: Request) -> Response\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    match service.call(req).await {\n        Ok(res) => res.into_response(),\n        Err(e) => match e {},\n    }\n}\n\n/// The idempotency middleware itself -- used via the [`Router::layer`] method\n#[derive(Clone)]\npub struct IdempotencyService<S: Clone> {\n    pub cache: Cache,\n    pub service: S,\n}\n\nimpl<S> Service<Request> for IdempotencyService<S>\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    type Response = Response;\n    type Error = Infallible;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(\n        &mut self,\n        cx: &mut std::task::Context<'_>,\n    ) -> std::task::Poll<Result<(), Self::Error>> {\n        self.service.poll_ready(cx)\n    }\n\n    fn call(&mut self, req: Request) -> Self::Future {\n        let mut service = self.service.clone();\n        let cache = self.cache.clone();\n\n        if !cache.is_none() {\n            Box::pin(async move {\n                let (parts, body) = req.into_parts();\n\n                // If not a POST request, simply resolve the service as usual\n                if parts.method != http::Method::POST {\n                    return Ok(resolve_service(service, Request::from_parts(parts, body)).await);\n                }\n\n                // Retrieve `IdempotencyKey` from header and URL parts, but returning the service\n                // normally in the event a key could not be created.\n                let key = if let Some(key) = get_key(&parts) {\n                    key\n                } else {\n                    return Ok(resolve_service(service, Request::from_parts(parts, body)).await);\n                };\n\n                // Set the [`SerializedResponse::Start`] lock if the key does not exist in the cache\n                // returning whether the value was set\n                let lock_acquired = if let Ok(lock_acquired) = cache\n                    .set_if_not_exists(&key, &SerializedResponse::Start, expiry_starting())\n                    .await\n                {\n                    lock_acquired\n                } else {\n                    return Ok(StatusCode::INTERNAL_SERVER_ERROR.into_response());\n                };\n\n                // If the lock was not set, first check the cache for a `Finished` cache value. If\n                // it is instead `None` or the value is a `Start` lock, then enter a loop checking\n                // it every 200ms.\n                //\n                // If the loop times out, then reset the lock and proceed to resolve the service.\n                //\n                // If at any point the cache returns an `Err`, then return 500 response\n                if !lock_acquired {\n                    match cache.get::<SerializedResponse>(&key).await {\n                        Ok(Some(SerializedResponse::Finished {\n                            code,\n                            headers,\n                            body,\n                        })) => {\n                            return Ok(finished_serialized_response_to_response(\n                                code, headers, body,\n                            )\n                            .unwrap_or_else(|_| StatusCode::INTERNAL_SERVER_ERROR.into_response()))\n                        }\n\n                        Ok(Some(SerializedResponse::Start)) | Ok(None) => {\n                            if let Ok(Some(SerializedResponse::Finished {\n                                code,\n                                headers,\n                                body,\n                            })) = lock_loop(&cache, &key).await\n                            {\n                                return Ok(finished_serialized_response_to_response(\n                                    code, headers, body,\n                                )\n                                .unwrap_or_else(|_| {\n                                    StatusCode::INTERNAL_SERVER_ERROR.into_response()\n                                }));\n                            } else {\n                                // Set the lock if it returns `Ok(None)` and continue to resolve\n                                // as normal, but return 500 if the lock cannot be set\n                                if !matches!(\n                                    cache\n                                        .set_if_not_exists(\n                                            &key,\n                                            &SerializedResponse::Start,\n                                            expiry_starting(),\n                <|fim_middle|>", "completion": "let headers = out.headers_mut();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/idempotency.rs", "node_type": "let_declaration", "line_range": [101, 101]}
{"prompt": "<|fim_prefix|>rationalWebhook::EndpointCreated(EndpointEvent::new(app.uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Create a new endpoint for the application.\n///\n/// When `secret` is `null` the secret is automatically generated (recommended)\n#[aide_annotate(op_id = \"v1.endpoint.create\")]\npub(super) async fn create_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        op_webhooks,\n        ..\n    }): State<AppState>,\n    _: Path<ApplicationPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointIn>,\n) -> Result<JsonStatus<201, EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let (endp, metadata) = create_endp_from_data(db, cfg, &op_webhooks, app, data)\n        .await\n        .trace()?;\n\n    Ok(JsonStatus((endp, metadata.data).into()))\n}\n\n/// Get an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.get\")]\npub(super) async fn get_endpoint(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointOut>> {\n    let (endp, metadata) = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .find_also_related(endpointmetadata::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let metadata = metadata.map(|m| m.data).unwrap_or_default();\n\n    Ok(Json((endp, metadata).into()))\n}\n\nasync fn update_endp_from_data(\n    db: &DatabaseConnection,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    endp: endpoint::ActiveModel,\n    metadata: endpointmetadata::ActiveModel,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.update(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    let app_uid = app.uid;\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointUpdated(EndpointEvent::new(app_uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Update an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.update\")]\npub(super) async fn update_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(mut data): ValidatedJson<EndpointUpdate>,\n) -> Result<JsonStatusUpsert<EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let models = endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n        .await\n        .trace()?;\n\n    if let Some((mut endp, mut metadata)) = models {\n        metadata.data = Set(mem::take(&mut data.metadata));\n        data.update_model(&mut endp);\n        let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Updated((endp, metadata.data).into()))\n    } else {\n        let data = data.into_in_with_default_key();\n        let (endp, metadata) = create_endp_from_data(db, cfg, op_webhooks, app, data)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Created((endp, metadata.data).into()))\n    }\n}\n\n/// Partially update an endpoint.\n#[aide_annotate]\npub(super) async fn patch_endpoint(\n    State(AppState {\n        ref db,\n        cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointPatch>,\n) -> Result<Json<EndpointOut>> {\n    if let UnrequiredNullableField::Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    if let UnrequiredField::Some(url) = &data.url {\n        validate_endpoint_url(url, cfg.endpoint_https_only)?;\n    }\n\n    let (mut endp, mut metadata) =\n        endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut patch_data = data; // need to alias so we can use data for `patch_field_non_nullable!`\n\n    let data = mem::take(&mut patch_data.metadata);\n    patch_field_non_nullable!(metadata, data);\n    patch_data.update_model(&mut endp);\n    let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n        .await\n        .trace()?;\n\n    Ok(Json((endp, metadata.data).into()))\n}\n\n/// Delete an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.delete\")]\npub(super) async fn delete_endpoint(\n    State(AppState {\n        ref db,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<NoContent> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    // Cloning the ID/UID out of endp before it's consumed below\n    let endpoint_id = endp.id.clone();\n    let endpoint_uid = endp.uid.clone();\n\n    let mut endp: endpoint::ActiveModel = endp.into();\n    endp.deleted = Set(true);\n    endp.uid = Set(None); // We don't want deleted UIDs to clash\n    endp.update(db).await?;\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointDeleted(EndpointEvent {\n                app_id: app.id,\n                app_uid: app.uid,\n                endpoint_id,\n                endpoint_uid,\n            }),\n        )\n        .await?;\n\n    Ok(NoContent)\n}\n\n/// This module is here so that our Result override doesn't conflict\nmod hack {\n    use sea_orm::FromQueryResult;\n\n    use crate::core::types::EventTypeName;\n\n    #[derive(Debug, FromQueryResult)]\n    pub struct EventTypeNameResult {\n        pub name: EventTypeName,\n    }\n}\n\nasync fn validate_event_types(\n    db: &DatabaseConnection,\n    event_types_ids: &EventTypeNameSet,\n    org_id: &OrganizationId,\n) -> Result<()> {\n    <|fim_suffix|>\n    let event_types: HashSet<EventTypeName> =\n        HashSet::from_iter(event_types.into_iter().map(|x| x.name));\n    let missing: Vec<&EventTypeName> = event_types_ids\n        .0\n        .iter()\n        .filter(|x| !event_types.contains(x))\n        .collect();\n\n    if missing.is_empty() {\n        Ok(())\n    } else {\n        let missing = missing\n            .into_iter()\n            .map(|x| &(x.0[..]))\n            .collect::<Vec<&str>>()\n            .join(\", \");\n        Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n            loc: vec![\"body\".to_owned(), \"filterTypes\".to_owned()],\n            msg: format!(\"The following event types don't exist: {missing}\"),\n            ty: \"value_error\".to_owned(),\n        }])\n        .into())\n    }\n}\n\nfn validate_endpoint_url(url: &Url, https_only: bool) -> Result<()> {\n    if !https_only {\n        return Ok(());\n    }\n\n    let scheme = url.scheme();\n    if scheme == \"https\" {\n        Ok(())\n    } else {\n        Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n            loc: vec![\"body\".to_owned(), \"url\".to_owned()],\n            msg: \"Endpoint URL schemes must be https when endpoint_https_only is set.\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }])\n        .into())\n    }\n}\n<|fim_middle|>", "completion": "let event_types: Vec<EventTypeNameResult> = eventtype::Entity::secure_find(org_id.clone())\n        .filter(eventtype::Column::Deleted.eq(false))\n        .select_only()\n        .column(eventtype::Column::Name)\n        .into_model::<EventTypeNameResult>()\n        .all(db)\n        .await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/crud.rs", "node_type": "let_declaration", "line_range": [307, 313]}
{"prompt": "<|fim_prefix|>use std::{collections::HashSet, time::Duration};\n\nuse chrono::{DateTime, FixedOffset, Utc};\nuse sea_orm::{DatabaseConnection, DatabaseTransaction, TransactionTrait};\nuse serde::{Deserialize, Serialize};\n\nuse super::types::EventTypeName;\nuse crate::{\n    core::{\n        cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue},\n        types::{\n            ApplicationId, ApplicationUid, EndpointHeaders, EndpointId, EndpointSecretInternal,\n            EventChannelSet, EventTypeNameSet, ExpiringSigningKeys, MessageAttemptTriggerType,\n            OrganizationId,\n        },\n    },\n    db::models::{application, endpoint},\n    error::{Error, Result},\n};\n\n/// The information cached during the creation of a message. Includes a [`Vec`] of all endpoints\n/// associated with the given application and organization ID.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageApp {\n    pub id: ApplicationId,\n    pub uid: Option<ApplicationUid>,\n    pub org_id: OrganizationId,\n    pub rate_limit: Option<u16>,\n    endpoints: Vec<CreateMessageEndpoint>,\n    deleted: bool,\n}\n\nimpl CreateMessageApp {\n    /// Fetch all requisite information for creating a [`CreateMessageApp`] from the PostgreSQL\n    /// database\n    async fn fetch_from_pg_by_model(\n        db: &DatabaseTransaction,\n        app: application::Model,\n    ) -> Result<CreateMessageApp> {\n        let endpoints = endpoint::Entity::secure_find(app.id.clone())\n            .all(db)\n            .await?\n            .into_iter()\n            .map(TryInto::try_into)\n            .collect::<Result<Vec<_>>>()?;\n\n        Ok(CreateMessageApp {\n            id: app.id,\n            uid: app.uid,\n            org_id: app.org_id,\n            rate_limit: app\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Application rate limit out of bounds\"))?,\n            endpoints,\n            deleted: app.deleted,\n        })\n    }\n\n    /// Fetches all information for creating a [`CreateMessageApp`] from the Redis cache if it\n    /// exists or from PostgreSQL otherwise. If the RedisCache is Some, but does not contain the\n    /// requisite information, fetch it from PostgreSQL and insert the data into the cache.\n    pub async fn layered_fetch(\n        cache: &Cache,\n        pg: &DatabaseConnection,\n        app: Option<application::Model>,\n        org_id: OrganizationId,\n        app_id: ApplicationId,\n        ttl: Duration,\n    ) -> Result<Option<CreateMessageApp>> {\n        let cache_key = AppEndpointKey::new(&org_id, &app_id);\n\n        // First check Redis\n        <|fim_suffix|>\n\n        // Then check PostgreSQL\n        let db = pg.begin().await?;\n        // Fetch the [`application::Model`] either given or from the ID\n        let app = if let Some(app) = app {\n            app\n        } else if let Some(app) = application::Entity::secure_find_by_id(org_id, app_id)\n            .one(&db)\n            .await?\n        {\n            app\n        } else {\n            return Ok(None);\n        };\n\n        // Fetch the actual [`CreateMessageApp`]\n        let out = Self::fetch_from_pg_by_model(&db, app).await?;\n\n        // Insert it into Redis\n        let _ = cache.set(&cache_key, &out, ttl).await;\n\n        if out.deleted {\n            return Ok(None);\n        }\n\n        Ok(Some(out))\n    }\n\n    pub fn filtered_endpoints(\n        &self,\n        trigger_type: MessageAttemptTriggerType,\n        event_type: &EventTypeName,\n        channels: Option<&EventChannelSet>,\n    ) -> Vec<CreateMessageEndpoint> {\n        self.endpoints\n            .iter()\n            .filter(|endpoint| {\n                // No disabled or deleted endpoints ever\n                !endpoint.disabled && !endpoint.deleted\n                    // Manual attempt types go through regardless\n                    && (trigger_type == MessageAttemptTriggerType::Manual\n                        || (\n                            // If an endpoint has event types and it matches ours, or has no event types\n                            endpoint\n                                .event_types_ids\n                                .as_ref()\n                                .map(|x| x.0.contains(event_type))\n                                .unwrap_or(true)\n                            // If an endpoint has no channels accept all messages, otherwise only if their channels overlap.\n                            // A message with no channels doesn't match an endpoint with channels.\n                            && endpoint\n                                .channels\n                                .as_ref()\n                                .map(|x| {\n                                    !x.0.is_disjoint(\n                                        channels.map(|x| &x.0).unwrap_or(&HashSet::new()),\n                                    )\n                                })\n                                .unwrap_or(true)\n                        ))\n            })\n            .cloned()\n            .collect()\n    }\n}\n\n/// The information for each individual endpoint cached with the creation of a message.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageEndpoint {\n    pub id: EndpointId,\n    pub url: String,\n    pub key: EndpointSecretInternal,\n    pub event_types_ids: Option<EventTypeNameSet>,\n    pub channels: Option<EventChannelSet>,\n    pub rate_limit: Option<u16>,\n    // Same type as the `DateTimeWithTimeZone from SeaORM used in the endpoint model\n    pub first_failure_at: Option<DateTime<FixedOffset>>,\n    pub headers: Option<EndpointHeaders>,\n    pub disabled: bool,\n    pub deleted: bool,\n    // outside of this module, valid_signing_keys should be used instead\n    old_signing_keys: Option<ExpiringSigningKeys>,\n}\n\nimpl CreateMessageEndpoint {\n    pub fn valid_signing_keys(&self) -> Vec<&EndpointSecretInternal> {\n        match self.old_signing_keys {\n            Some(ref old_keys) => std::iter::once(&self.key)\n                .chain(\n                    old_keys\n                        .0\n                        .iter()\n                        .filter(|x| x.expiration > Utc::now())\n                        .map(|x| &x.key),\n                )\n                .collect(),\n            None => vec![&self.key],\n        }\n    }\n}\n\nimpl TryFrom<endpoint::Model> for CreateMessageEndpoint {\n    type Error = Error;\n\n    fn try_from(m: endpoint::Model) -> Result<CreateMessageEndpoint> {\n        Ok(CreateMessageEndpoint {\n            id: m.id,\n            url: m.url,\n            key: m.key,\n            old_signing_keys: m.old_keys,\n            event_types_ids: m.event_types_ids,\n            channels: m.channels,\n            rate_limit: m\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Endpoint rate limit out of bounds\"))?,\n            first_failure_at: m.first_failure_at,\n            headers: m.headers,\n            disabled: m.disabled,\n            deleted: m.deleted,\n        })\n    }\n}\n\nkv_def!(AppEndpointKey, CreateMessageApp);\nimpl AppEndpointKey {\n    // FIXME: Rewrite doc comment when AppEndpointValue members are known\n    /// Returns a key for fetching all cached endpoints for a given organization and application.\n    pub fn new(org: &OrganizationId, app: &ApplicationId) -> AppEndpointKey {\n        AppEndpointKey(format!(\"SVIX_CACHE_APP_v3_{org}_{app}\"))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use chrono::Utc;\n\n    use super::CreateMessageEndpoint;\n    use crate::core::{\n        cryptography::Encryption,\n        types::{\n            EndpointId, EndpointSecret, EndpointSecretInternal, ExpiringSigningKey,\n            ExpiringSigningKeys,\n        },\n    };\n\n    #[test]\n    fn test_valid_signing_keys() {\n        let key = EndpointSecretInternal::from_endpoint_secret(\n            EndpointSecret::Symmetric(STANDARD.decode(\"MfKQ9r8GKYqrTwjUPD8ILPZIo2LaLaSw\").unwrap()),\n            &Encryption::new_noop(),\n        )\n        .unwrap();\n\n        let unexpired_old_key = ExpiringSigningKey {\n            key: key.clone(),\n            expiration: Utc::now()\n                +<|fim_middle|>", "completion": "if let Ok(Some(cma)) = cache.get::<CreateMessageApp>(&cache_key).await {\n            if cma.deleted {\n                return Ok(None);\n            } else {\n                return Ok(Some(cma));\n            }\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/message_app.rs", "node_type": "if_expression", "line_range": [75, 81]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n#![warn(clippy::all)]\n#![forbid(unsafe_code)]\n\nuse std::{borrow::Cow, sync::LazyLock, time::Duration};\n\nuse aide::axum::ApiRouter;\nuse cfg::ConfigurationInner;\nuse opentelemetry::trace::TracerProvider as _;\nuse opentelemetry_otlp::WithExportConfig;\nuse opentelemetry_sdk::{metrics::SdkMeterProvider, runtime::Tokio};\nuse queue::TaskQueueProducer;\nuse redis::RedisManager;\nuse sea_orm::DatabaseConnection;\nuse sentry::integrations::tracing::EventFilter;\nuse svix_ksuid::{KsuidLike, KsuidMs};\nuse tokio::net::TcpListener;\nu<|fim_suffix|>use tower::layer::layer_fn;\nuse tower_http::{\n    cors::{AllowHeaders, Any, CorsLayer},\n    normalize_path::NormalizePath,\n};\nuse tracing_subscriber::{layer::SubscriberExt as _, Layer as _};\n\nuse crate::{\n    cfg::{CacheBackend, Configuration},\n    core::{\n        cache,\n        cache::Cache,\n        idempotency::IdempotencyService,\n        operational_webhooks::{OperationalWebhookSender, OperationalWebhookSenderInner},\n    },\n    db::init_db,\n    expired_message_cleaner::expired_message_cleaner_loop,\n    worker::queue_handler,\n};\n\npub mod cfg;\npub mod core;\npub mod db;\npub mod error;\npub mod expired_message_cleaner;\npub mod metrics;\npub mod openapi;\npub mod queue;\npub mod redis;\npub mod v1;\npub mod worker;\n\nconst CRATE_NAME: &str = env!(\"CARGO_CRATE_NAME\");\n\nstatic SHUTTING_DOWN_TOKEN: LazyLock<CancellationToken> = LazyLock::new(CancellationToken::new);\n\n/// Has someone requested shutdown?\npub fn is_shutting_down() -> bool {\n    SHUTTING_DOWN_TOKEN.is_cancelled()\n}\n\n/// Request a CancellationToken for the application shut down\npub fn shutting_down_token() -> CancellationToken {\n    SHUTTING_DOWN_TOKEN.clone()\n}\n\n/// Shut down the application\npub fn start_shut_down() {\n    SHUTTING_DOWN_TOKEN.cancel();\n}\n\npub static INSTANCE_ID: LazyLock<String> =\n    LazyLock::new(|| hex::encode(KsuidMs::new(None, None).to_string()));\n\nasync fn graceful_shutdown_handler() {\n    let ctrl_c = async {\n        tokio::signal::ctrl_c()\n            .await\n            .expect(\"Failed to install Ctrl+C handler\");\n    };\n\n    #[cfg(unix)]\n    let sigterm = async {\n        tokio::signal::unix::signal(tokio::signal::unix::SignalKind::terminate())\n            .expect(\"Failed to install SIGTERM handler\")\n            .recv()\n            .await;\n    };\n\n    #[cfg(not(unix))]\n    let sigterm = std::future::pending::<()>();\n\n    tokio::select! {\n        _ = ctrl_c => {},\n        _ = sigterm => {},\n    }\n\n    tracing::info!(\"Received shutdown signal. Shutting down gracefully...\");\n    start_shut_down();\n}\n\npub async fn run(cfg: Configuration) {\n    let _metrics = setup_metrics(&cfg);\n    run_with_prefix(cfg.queue_prefix.clone(), cfg, None).await\n}\n\n#[derive(Clone)]\npub struct AppState {\n    db: DatabaseConnection,\n    queue_tx: TaskQueueProducer,\n    cfg: Configuration,\n    cache: Cache,\n    op_webhooks: OperationalWebhookSender,\n}\n\n// Made public for the purpose of E2E testing in which a queue prefix is necessary to avoid tests\n// consuming from each others' queues\npub async fn run_with_prefix(\n    prefix: Option<String>,\n    cfg: Configuration,\n    listener: Option<TcpListener>,\n) {\n    tracing::debug!(\"DB: Initializing pool\");\n    let pool = init_db(&cfg).await;\n    tracing::debug!(\"DB: Started\");\n\n    tracing::debug!(\"Cache: Initializing {:?}\", cfg.cache_type);\n    let cache_backend = cfg.cache_backend();\n    let cache = match &cache_backend {\n        CacheBackend::None => cache::none::new(),\n        CacheBackend::Memory => cache::memory::new(),\n        CacheBackend::Redis(_)\n        | CacheBackend::RedisCluster(_)\n        | CacheBackend::RedisSentinel(_, _) => {\n            let mgr = RedisManager::from_cache_backend(&cache_backend).await;\n            cache::redis::new(mgr)\n        }\n    };\n    tracing::debug!(\"Cache: Started\");\n\n    tracing::debug!(\"Queue: Initializing {:?}\", cfg.queue_type);\n    let (queue_tx, queue_rx) = queue::new_pair(&cfg, prefix.as_deref()).await;\n    tracing::debug!(\"Queue: Started\");\n\n    let op_webhook_sender = OperationalWebhookSenderInner::new(\n        cfg.jwt_signing_config.clone(),\n        cfg.operational_webhook_address.clone(),\n    );\n\n    // OpenAPI/aide must be initialized before any routers are constructed\n    // because its initialization sets generation-global settings which are\n    // needed at router-construction time.\n    let mut openapi = openapi::initialize_openapi();\n\n    let svc_cache = cache.clone();\n    // build our application with a route\n    let app_state = AppState {\n        db: pool.clone(),\n        queue_tx: queue_tx.clone(),\n        cfg: cfg.clone(),\n        cache: cache.clone(),\n        op_webhooks: op_webhook_sender.clone(),\n    };\n    let v1_router = v1::router().with_state::<()>(app_state);\n\n    // Initialize all routes which need to be part of OpenAPI first.\n    let app = ApiRouter::new()\n        .nest_api_service(\"/api/v1\", v1_router)\n        .finish_api(&mut openapi);\n\n    openapi::postprocess_spec(&mut openapi);\n    let docs_router = docs::router(openapi);\n    let app = app.merge(docs_router).layer((\n        layer_fn(move |service| IdempotencyService {\n            cache: svc_cache.clone(),\n            service,\n        }),\n        CorsLayer::new()\n            .allow_origin(Any)\n            .allow_methods(Any)\n            .allow_headers(AllowHeaders::mirror_request())\n            .max_age(Duration::from_secs(600)),\n    ));\n    let svc = tower::make::Shared::new(\n        // It is important that this service wraps the router instead of being\n        // applied via `Router::layer`, as it would run after routing then.\n        NormalizePath::trim_trailing_slash(app),\n    );\n\n    let with_api = cfg.api_enabled;\n    let with_worker = cfg.worker_enabled;\n    let listen_address = cfg.listen_address;\n\n    let ((), worker_loop, expired_message_cleaner_loop) = tokio::join!(\n        async {\n            if with_api {\n                let listener = match listener {\n                    Some(l) => l,\n                    None => TcpListener::bind(listen_address)\n                        .await\n                        .expect(\"Error binding to listen_address\"),\n                };\n                tracing::debug!(\"API: Listening on {}\", listener.local_addr().unwrap());\n\n                axum::serve(listener, svc)\n                    .with_graceful_shutdown(graceful_shutdown_handler())\n                    .await\n                    .unwrap();\n            } else {\n                tracing::debug!(\"API: off\");\n                graceful_shutdown_handler().await;\n            }\n        },\n        async {\n            if with_worker {\n                tracing::debug!(\"Worker: Started\");\n                queue_handler(\n                    &cfg,\n                    cache.clone(),\n                    pool.clone(),\n                    queue_tx,\n                    queue_rx,\n                    op_webhook_sender,\n                )\n                .await\n            } else {\n                tracing::debug!(\"Worker: off\");\n                Ok(())\n            }\n        },\n        async {\n            if with_worker {\n                tracing::debug!(\"Expired message cleaner: Started\");\n                expired_message_cleaner_loop(&pool).await\n            } else {\n                tracing::debug!(\"Expired message cleaner: off\");\n                Ok(())\n            }\n        }\n    );\n\n    worker_loop.expect(\"Error initializing worker\");\n    expired_message_cleaner_loop.expect(\"Error initializing expired message cleaner\")\n}\n\npub fn setup_tracing(\n    cfg: &ConfigurationInner,\n    for_test: bool,\n) -> (tracing::Dispatch, sentry::ClientInitGuard) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        let level = cfg.log_level.to_string();\n        let mut var = vec![\n            format!(\"{CRATE_NAME}={level}\"),\n            format!(\"tower_http={level}\"),\n        ];\n\n        if cfg.db_tracing {\n            var.push(format!(\"sqlx={level}\"));\n        }\n\n        var.join(\",\")\n    });\n\n    let ot<|fim_middle|>", "completion": "use tokio_util::sync::CancellationToken;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/lib.rs", "node_type": "use_declaration", "line_range": [20, 20]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\nmod crud;\nmod headers;\nmod recovery;\nmod secrets;\n\nuse std::collections::{HashMap, HashSet};\n\nuse aide::axum::{\n    routing::{get_with, post_with},\n    ApiRouter,\n};\nuse axum::{\n    extract::{Path, Query, State},\n    Json,\n};\nuse chrono::{DateTime, Duration, Utc};\nuse schemars::JsonSchema;\nuse sea_orm::{ActiveValue::Set, ColumnTrait, FromQueryResult, QuerySelect};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::{aide_annotate, ModelIn, ModelOut};\nuse url::Url;\nuse validator::{Validate, ValidationError};\n\nuse self::secrets::generate_secret;\nuse super::message::{create_message_inner, MessageIn, MessageOut, RawPayload};\nuse crate::{\n    cfg::DefaultSignatureType,\n    core::{\n        cryptography::Encryption,\n        permissions,\n        types::{\n            metadata::Metadata, ApplicationIdOrUid, EndpointHeaders, EndpointHeadersPatch,\n            EndpointId, EndpointSecret, EndpointSecretInternal, EndpointUid, EventChannelSet,\n            EventTypeName, EventTypeNameSet, MessageStatus,\n        },\n    },\n    db::models::{\n        endpoint, eventtype,\n        messageattempt::{self, Query as _},\n    },\n    error::{self, HttpError},\n    v1::utils::{\n        openapi_tag,\n        patch::{\n            patch_field_non_nullable, patch_field_nullable, UnrequiredField,\n            UnrequiredNullableField,\n        },\n        validate_no_control_characters, validate_no_control_characters_unrequired,\n        validation_error, ApplicationEndpointPath, ModelIn, ValidatedJson,\n    },\n    AppState,\n};\n\npub fn validate_event_types_ids(event_types_ids: &EventTypeNameSet) -> Result<(), ValidationError> {\n    if event_types_ids.0.is_empty() {\n        Err(validation_error(\n            Some(\"filterTypes\"),\n            Some(\"filterTypes can't be empty, it must have at least one item.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nfn validate_event_types_ids_unrequired_nullable(\n    event_types_ids: &UnrequiredNullableField<EventTypeNameSet>,\n) -> Result<(), ValidationError> {\n    m<|fim_suffix|>}\n\npub fn validate_channels_endpoint(channels: &EventChannelSet) -> Result<(), ValidationError> {\n    let len = channels.0.len();\n    if !(1..=10).contains(&len) {\n        Err(validation_error(\n            Some(\"channels\"),\n            Some(\"Channels must have at least 1 and at most 10 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nfn validate_channels_endpoint_unrequired_nullable(\n    channels: &UnrequiredNullableField<EventChannelSet>,\n) -> Result<(), ValidationError> {\n    match channels {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(channels) => validate_channels_endpoint(channels),\n    }\n}\n\npub fn validate_url(url: &Url) -> Result<(), ValidationError> {\n    let scheme = url.scheme();\n    if scheme == \"https\" || scheme == \"http\" {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"url\"),\n            Some(\"Endpoint URL schemes must be http or https\"),\n        ))\n    }\n}\n\nfn validate_url_unrequired(val: &UnrequiredField<Url>) -> Result<(), ValidationError> {\n    match val {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(val) => validate_url(val),\n    }\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nfn example_endpoint_description() -> &'static str {\n    \"An example endpoint name\"\n}\n\nfn example_filter_types() -> Vec<&'static str> {\n    vec![\"user.signup\", \"user.deleted\"]\n}\n\nfn endpoint_disabled_default() -> bool {\n    false\n}\n\nfn example_endpoint_url() -> &'static str {\n    \"https://example.com/webhook/\"\n}\n\nfn example_endpoint_version() -> u16 {\n    1\n}\n\nfn default_endpoint_version() -> Option<u16> {\n    Some(1)\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointIn {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n    /// List of message channels this endpoint listens to (omit for all)\n    #[validate(custom = \"validate_channels_endpoint\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n\n    #[validate]\n    #[serde(default)]\n    #[serde(rename = \"secret\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub key: Option<EndpointSecret>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\nimpl EndpointIn {\n    pub fn key_take_or_generate(\n        &mut self,\n        encryption: &Encryption,\n        sig_type: &DefaultSignatureType,\n    ) -> error::Result<EndpointSecretInternal> {\n        if let Some(key) = self.key.take() {\n            EndpointSecretInternal::from_endpoint_secret(key, encryption)\n        } else {\n            generate_secret(encryption, sig_type)\n        }\n    }\n}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for EndpointIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    #[allow(deprecated)]\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointIn {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            key: _,\n            metadata: _,\n        } = self;\n\n        model.description = Set(description);\n        model.rate_limit = Set(rate_limit.map(|x| x.into()));\n        model.uid = Set(uid);\n        model.url = Set(url.into());\n        model.version = Set(version.unwrap_or(1).into());\n        model.disabled = Set(disabled);\n        model.event_types_ids = Set(event_types_ids);\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\nstruct EndpointUpdate {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    p<|fim_middle|>", "completion": "match event_types_ids {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(event_type_ids) => validate_event_types_ids(event_type_ids),\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "match_expression", "line_range": [70, 73]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::time::Duration;\n\nuse aide::axum::{\n    routing::{get, get_with},\n    ApiRouter,\n};\nuse axum::{extract::State, http::StatusCode, Json};\nuse sea_orm::{query::Statement, ConnectionTrait, DatabaseBackend};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::aide_annotate;\n\nuse crate::{\n    core::cache::{kv_def, CacheBehavior, CacheKey, CacheValue},\n    queue::QueueTask,\n    v1::utils::{openapi_tag, NoContent},\n    AppState,\n};\n\nasync fn ping() -> NoContent {\n    NoContent\n}\n\n#[derive(Debug, Deserialize, Serialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum HealthStatusVariant {\n    Ok,\n    Error,\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthStatus {\n    status: HealthStatusVariant,\n    // TODO: information field\n}\n\nimpl HealthStatus {\n    pub fn new_ok() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Ok,\n        }\n    }\n\n    pub fn new_error() -> HealthStatus {\n        HealthStatus {\n            status: HealthStatusVariant::Error,\n        }\n    }\n\n    pub fn is_ok(&self) -> bool {\n        matches!(\n            self,\n            HealthStatus {\n                status: HealthStatusVariant::Ok,\n                ..\n            }\n        )\n    }\n}\nimpl<O, E> From<Result<O, E>> for HealthStatus {\n    fn from(res: Result<O, E>) -> Self {\n        m<|fim_suffix|>    }\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct HealthReport {\n    database: HealthStatus,\n\n    queue: HealthStatus,\n    cache: HealthStatus,\n}\n\n#[derive(Deserialize, Serialize, Debug, PartialEq)]\nstruct HealthCheckCacheValue(());\nkv_def!(HealthCheckCacheKey, HealthCheckCacheValue);\n\n/// Verify the API server is up and running.\n#[aide_annotate(op_id = \"v1.health.get\")]\nasync fn health(\n    State(AppState {\n        ref db,\n        queue_tx,\n        cache,\n        ..\n    }): State<AppState>,\n) -> (StatusCode, Json<HealthReport>) {\n    // SELECT 1 FROM any table\n    let database: HealthStatus = db\n        .execute(Statement::from_string(\n            DatabaseBackend::Postgres,\n            \"SELECT 1\".to_owned(),\n        ))\n        .await\n        .into();\n\n    // Send a [`HealthCheck`] through the queue\n    let queue: HealthStatus = queue_tx.send(&QueueTask::HealthCheck, None).await.into();\n\n    // Set a cache value with an expiration to ensure it works\n    let cache: HealthStatus = cache\n        .set(\n            &HealthCheckCacheKey(\"health_check_value\".to_owned()),\n            &HealthCheckCacheValue(()),\n            // Expires after this time, so it won't pollute the DB\n            Duration::from_millis(100),\n        )\n        .await\n        .into();\n\n    let status = if database.is_ok() && queue.is_ok() && cache.is_ok() {\n        StatusCode::OK\n    } else {\n        StatusCode::INTERNAL_SERVER_ERROR\n    };\n\n    (\n        status,\n        Json(HealthReport {\n            database,\n            queue,\n            cache,\n        }),\n    )\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Health\");\n\n    ApiRouter::new()\n        .api_route(\"/health/ping\", get(ping).head(ping))\n        .api_route_with(\n            \"/health\",\n            get_with(health, |op| op.response::<204, ()>().with(health_operation))\n                .head_with(health, health_operation),\n            tag,\n        )\n}\n<|fim_middle|>", "completion": "match res {\n            Ok(_) => HealthStatus::new_ok(),\n            Err(_) => HealthStatus::new_error(),\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/health.rs", "node_type": "match_expression", "line_range": [64, 67]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse reqwest::StatusCode;\nuse serde_json::json;\nuse svix_server::{\n    core::types::{EndpointUid, MessageStatus},\n    v1::{\n        endpoints::{\n            attempt::{EndpointMessageOut, MessageAttemptOut},\n            endpoint::{EndpointIn, EndpointOut},\n        },\n        utils::ListResponse,\n    },\n};\nuse wiremock::{matchers, Mock, MockServer, Respond, ResponseTemplate};\n\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, create_test_msg_with,\n        endpoint_in, get_msg_attempt_list_and_assert_count,\n    },\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n    TestReceiver,\n};\n\n#[tokio::test]\nasync fn test_expunge_attempt_response_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let sensitive_response_json = serde_json::json!({\"sensitive\":\"data\"});\n    let mut receiver = TestReceiver::start_with_body(\n        axum::http::StatusCode::OK,\n        axum::Json(sensitive_response_json.clone()),\n    );\n\n    let endpoint_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap()\n        .id;\n\n    receiver.data_recv.recv().await;\n\n    let attempt = run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endpoint_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        if attempts.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", attempts.data.len());\n        }\n        Ok(attempts.data[0].clone())\n    })\n    .await\n    .unwrap();\n\n    let attempt_response: serde_json::Value = serde_json::from_str(&attempt.response).unwrap();\n    assert_eq!(sensitive_response_json, attempt_response);\n\n    let attempt_id = &attempt.id;\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let attempt: MessageAttemptOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\"EXPUNGED\", &attempt.response);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    l<|fim_suffix|>\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n        .await\n        .unwrap();\n    let msg_4 = create_test_message(&client, &app_id, json!({ \"tes<|fim_middle|>", "completion": "let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [112, 120]}
{"prompt": "<|fim_prefix|>use std::{future::Future, pin::Pin};\n\nuse http1::Uri;\nuse hyper_util::{\n    client::legacy::connect::{\n        proxy::{SocksV5, Tunnel},\n        HttpConnector,\n    },\n    rt::TokioIo,\n};\n<|fim_suffix|>\nuse tower_service::Service;\n\n// If no TLS backend is enabled, use plain http connector.\n#[cfg(not(any(feature = \"native-tls\", feature = \"rustls-tls\")))]\ntype HttpsIfAvailable<T> = T;\n#[cfg(not(any(feature = \"native-tls\", feature = \"rustls-tls\")))]\ntype MaybeHttpsStream<T> = T;\n\n// If only native TLS is enabled, use that.\n#[cfg(all(feature = \"native-tls\", not(feature = \"rustls-tls\")))]\nuse hyper_tls::{HttpsConnector as HttpsIfAvailable, MaybeHttpsStream};\n\n// If rustls is enabled, use that.\n#[cfg(feature = \"rustls-tls\")]\nuse hyper_rustls::{HttpsConnector as HttpsIfAvailable, MaybeHttpsStream};\n\nfn wrap_connector<H>(conn: H) -> HttpsIfAvailable<H> {\n    #[cfg(not(any(feature = \"native-tls\", feature = \"rustls-tls\")))]\n    return conn;\n\n    #[cfg(feature = \"rustls-tls\")]\n    {\n        let crypto_provider = rustls::crypto::CryptoProvider::get_default()\n            .map(std::sync::Arc::clone)\n            .unwrap_or_else(|| std::sync::Arc::new(rustls::crypto::aws_lc_rs::default_provider()));\n\n        let builder = hyper_rustls::HttpsConnectorBuilder::new()\n            .with_provider_and_native_roots(crypto_provider)\n            .unwrap()\n            .https_or_http();\n\n        #[cfg(feature = \"http1\")]\n        let builder = builder.enable_http1();\n\n        #[cfg(feature = \"http2\")]\n        let builder = builder.enable_http2();\n\n        builder.wrap_connector(conn)\n    }\n\n    #[cfg(all(feature = \"native-tls\", not(feature = \"rustls-tls\")))]\n    return hyper_tls::HttpsConnector::new_with_connector(conn);\n}\n\n#[derive(Clone, Debug)]\npub(crate) enum Connector {\n    Regular(HttpsIfAvailable<HttpConnector>),\n    Socks5Proxy(HttpsIfAvailable<SocksV5<HttpConnector>>),\n    HttpProxy(HttpsIfAvailable<Tunnel<HttpConnector>>),\n}\n\n// FIXME: If we ever do a breaking release, change this\n// to be fallible and bubble the error up from `Svix::new`.\npub(crate) fn make_connector(proxy_addr: Option<String>) -> Connector {\n    let mut http = hyper_util::client::legacy::connect::HttpConnector::new();\n    if cfg!(any(feature = \"native-tls\", feature = \"rustls-tls\")) {\n        http.enforce_http(false);\n    }\n\n    let Some(proxy_addr) = proxy_addr else {\n        return Connector::Regular(wrap_connector(http));\n    };\n    let proxy_addr = match proxy_addr.parse::<Uri>() {\n        Ok(proxy_addr) => proxy_addr,\n        Err(e) => {\n            tracing::error!(\n                error = &e as &dyn std::error::Error,\n                \"invalid proxy address\"\n            );\n            return Connector::Regular(wrap_connector(http));\n        }\n    };\n\n    match proxy_addr.scheme_str() {\n        Some(\"http\" | \"https\") => {\n            let tunnel = Tunnel::new(proxy_addr, http);\n            Connector::HttpProxy(wrap_connector(tunnel))\n        }\n        Some(\"socks5\") => {\n            let socks = SocksV5::new(proxy_addr, http).local_dns(true);\n            Connector::Socks5Proxy(wrap_connector(socks))\n        }\n        Some(\"socks5h\") => {\n            let socks = SocksV5::new(proxy_addr, http);\n            Connector::Socks5Proxy(wrap_connector(socks))\n        }\n        scheme => {\n            tracing::error!(\n                scheme,\n                \"invalid proxy address: scheme must be one of http, https, socks5, socks5h\"\n            );\n            Connector::Regular(wrap_connector(http))\n        }\n    }\n}\n\nimpl Service<Uri> for Connector {\n    type Response = MaybeHttpsStream<TokioIo<TcpStream>>;\n    type Error = Box<dyn std::error::Error + Send + Sync>;\n    type Future = Pin<Box<dyn Future<Output = Result<Self::Response, Self::Error>> + Send>>;\n\n    fn poll_ready(\n        &mut self,\n        cx: &mut std::task::Context<'_>,\n    ) -> std::task::Poll<Result<(), Self::Error>> {\n        match self {\n            Self::Regular(inner) => inner.poll_ready(cx),\n            Self::Socks5Proxy(inner) => inner.poll_ready(cx),\n            Self::HttpProxy(inner) => inner.poll_ready(cx),\n        }\n    }\n\n    fn call(&mut self, req: Uri) -> Self::Future {\n        match self {\n            Self::Regular(inner) => Box::pin(inner.call(req)),\n            Self::Socks5Proxy(inner) => Box::pin(inner.call(req)),\n            Self::HttpProxy(inner) => Box::pin(inner.call(req)),\n        }\n    }\n}\n<|fim_middle|>", "completion": "use tokio::net::TcpStream;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/connector.rs", "node_type": "use_declaration", "line_range": [11, 11]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Configuration-dependent queue tests. This will depend on the set environment variables as with\n//! the e2e tests such as to allow testing multiple queue backends via the test script.\n\nuse std::{str::FromStr, time::Duration};\n\nuse http::StatusCode;\nuse redis::AsyncCommands as _;\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::Configuration,\n    core::types::{\n        ApplicationId, BaseId, EndpointId, MessageAttemptTriggerType, MessageId, OrganizationId,\n    },\n    queue::{\n        new_pair, MessageTask, QueueTask, TaskQueueConsumer, TaskQueueDelivery, TaskQueueProducer,\n    },\n    redis::RedisManager,\n    v1::endpoints::message::MessageOut,\n};\nuse tokio::time::timeout;\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id_and_prefix,\n};\n\n// TODO: Don't copy this from the Redis queue test directly, place the fn somewhere both can access\nasync fn get_pool(cfg: &Configuration) -> RedisManager {\n    RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await\n}\n\nfn task_queue_delivery_to_u16(tqd: &TaskQueueDelivery) -> u16 {\n    match &*tqd.task {\n        QueueTask::HealthCheck => panic!(\"Health check in test\"),\n        QueueTask::MessageBatch(batch) => u16::from_str(batch.msg_id.as_str()).unwrap(),\n        QueueTask::MessageV1(task) => u16::from_str(task.msg_id.as_str()).unwrap(),\n    }\n}\n\nasync fn test_many_queue_consumers_inner(prefix: &str, delay: Option<Duration>) {\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n\n    // This test assumes an empty queue, so load Redis and delete the test key\n    {\n        let pool = get_pool(&cfg).await;\n        let mut conn = pool.get().await.unwrap();\n\n        let _: () = conn\n            .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n            .await\n            .unwrap();\n    }\n\n    // Make 20 producers and 20 consumers using the same configuration\n    let mut producers_and_consumers: Vec<(TaskQueueProducer, TaskQueueConsumer)> = Vec::new();\n    for _ in 0..20 {\n        producers_and_consumers.push(new_pair(&cfg, Some(prefix)).await);\n    }\n\n    // Add 200 test messagesÂ¹ with unique message IDs to each producer for a\n    // total of 4000 unique messages\n    //\n    // Â¹ it is important for this number to be no smaller than MAX_MESSAGES in\n    //   TaskQueueConsumer::receive_all\n    for (index, (p, _c)) in producers_and_consumers.iter().enumerate() {\n        for num in 0..200 {\n            p.send(\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"{}\", index * 200 + num)),\n                    app_id: ApplicationId(\"TestApplicationId\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointId\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                }),\n                delay,\n            )\n            .await\n            .unwrap();\n        }\n    }\n\n    let mut join_handles = Vec::new();\n    // Producers need to stay alive for the remainder of the test for in-memory queue which uses\n    // [`tokio::mpsc`]s, so add them to this [`Vec`]\n    let mut producers = Vec::new();\n\n    // Ensure that consumers run on separate OS threads and receive messages until 500ms has passed\n    // without any messages\n    for (p, mut c) in producers_and_consumers {\n        producers.push(p);\n        let handle = tokio::runtime::Handle::current();\n        join_handles.push(std::thread::spawn(move || {\n            handle.block_on(async move {\n                let mut out = Vec::new();\n                let mut read = 0;\n\n                while let Ok(recv) = timeout(\n                    Duration::from_secs(1),\n                    c.receive_all(Duration::from_secs(5)),\n                )\n                .await\n                {\n                    let recv = recv.unwrap();\n                    read += recv.len();\n                    for r in recv {\n                        out.push(task_queue_delivery_to_u16(&r));\n                        r.ack().await.unwrap();\n                    }\n                }\n\n                (out, read)\n            })\n        }));\n    }\n\n    // Create a Vec with all the threads' outputs\n    let mut out = Vec::new();\n    for jh in join_handles {\n        let (mut jh_out, read): (Vec<u16>, usize) = jh.join().unwrap();\n        out.append(&mut jh_out);\n\n        if <|fim_suffix|>  }\n\n    // Sort it by the message ID\n    out.sort();\n\n    // Then assert that all the messages are there\n    assert_eq!(out.len(), 4000);\n    for (idx, &num) in out.iter().enumerate() {\n        assert_eq!(idx, num as usize);\n    }\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n// run with `cargo test -- --ignored redis` only when redis is up and configured\n#[ignore]\nasync fn test_many_queue_consumers() {\n    test_many_queue_consumers_inner(\"test_many_queue_consumers_\", None).await;\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n#[ignore]\nasync fn test_many_queue_consumers_delayed() {\n    test_many_queue_consumers_inner(\n        \"test_many_queue_consumers_delayed_\",\n        Some(Duration::from_millis(500)),\n    )\n    .await;\n}\n\n#[tokio::test]\n#[ignore]\nasync fn test_redis_streams_dlq() {\n    let mut cfg = get_default_test_config();\n    cfg.worker_enabled = false;\n    cfg.redis_pending_duration_secs = 1;\n\n    let cfg = std::sync::Arc::new(cfg);\n    let prefix = svix_ksuid::Ksuid::new(None, None).to_string();\n\n    let pool = get_pool(&cfg).await;\n    let mut conn = pool.get().await.unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n        .await\n        .unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_dlq\"))\n        .await\n        .unwrap();\n\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id_and_prefix(\n        &cfg,\n        OrganizationId::new(None, None),\n        prefix.clone(),\n    )\n    .await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let _message_1: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({\"test\": \"value\"})).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let (_p, mut c) = new_pair(&cfg, Some(&prefix)).await;\n\n    let wait_time = std::time::Duration::from_millis(1_500);\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n\n    tokio::time::sleep(wait_time).await;\n\n    // Redrive\n    client\n        .post_without_response(\n            \"/api/v1/admin/redrive-dlq\",\n            serde_json::Value::Null,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n}\n<|fim_middle|>", "completion": "if read < 20 {\n            panic!(\"Consumer starved, only read {read} messages\");\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/redis_queue.rs", "node_type": "if_expression", "line_range": [126, 128]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    borrow::Cow,\n    collections::HashSet,\n    error::Error as StdError,\n    ops::Deref,\n    sync::LazyLock,\n    time::{SystemTime, UNIX_EPOCH},\n};\n\nuse aide::{\n    transform::{TransformOperation, TransformPathItem},\n    OperationInput, OperationIo, OperationOutput,\n};\nuse axum::{\n    async_trait,\n    extract::{\n        rejection::{BytesRejection, FailedToBufferBody},\n        FromRequest, FromRequestParts, Query, Request,\n    },\n    response::IntoResponse,\n};\nuse chrono::{DateTime, Utc};\nuse http::{request::Parts, StatusCode};\nu<|fim_suffix|>use schemars::JsonSchema;\nuse sea_orm::{ColumnTrait, QueryFilter, QueryOrder, QuerySelect};\nuse serde::{de::DeserializeOwned, Deserialize, Serialize};\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::types::{\n        ApplicationIdOrUid, BaseId, EndpointIdOrUid, EventTypeName, EventTypeNameSet,\n        MessageAttemptId, MessageIdOrUid,\n    },\n    error::{Error, HttpError, Result, ValidationErrorItem},\n};\n\npub mod patch;\nuse patch::UnrequiredField;\n\nconst fn default_limit() -> PaginationLimit {\n    PaginationLimit(50)\n}\n\nconst PAGINATION_LIMIT_CAP_HARD: bool = true;\nconst PAGINATION_LIMIT_CAP_LIMIT: u64 = 250;\nstatic PAGINATION_LIMIT_ERROR: LazyLock<String> =\n    LazyLock::new(|| format!(\"Given limit must not exceed {PAGINATION_LIMIT_CAP_LIMIT}\"));\n\nstatic FUTURE_QUERY_LIMIT: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::hours(1));\nstatic LIMITED_QUERY_DURATION: LazyLock<chrono::Duration> =\n    LazyLock::new(|| chrono::Duration::days(90));\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct PaginationDescending<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n}\n\n#[derive(Clone, Debug, Deserialize, Validate, JsonSchema)]\npub struct Pagination<T: Validate + JsonSchema> {\n    /// Limit the number of returned items\n    #[validate]\n    #[serde(default = \"default_limit\")]\n    pub limit: PaginationLimit,\n    /// The iterator returned from a prior invocation\n    #[validate]\n    pub iterator: Option<T>,\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Clone, Debug, JsonSchema)]\n#[schemars(transparent)]\npub struct PaginationLimit(pub u64);\n\nimpl<'de> Deserialize<'de> for PaginationLimit {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let limit = u64::deserialize(deserializer)?;\n\n        // Want hard limits to stay the same so they can be validated\n        if !PAGINATION_LIMIT_CAP_HARD && limit > PAGINATION_LIMIT_CAP_LIMIT {\n            Ok(PaginationLimit(PAGINATION_LIMIT_CAP_LIMIT))\n        } else {\n            Ok(PaginationLimit(limit))\n        }\n    }\n}\n\nimpl Validate for PaginationLimit {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        let mut errs = validator::ValidationErrors::new();\n\n        if self.0 > PAGINATION_LIMIT_CAP_LIMIT {\n            errs.add(\n                \"limit\",\n                validation_error(Some(\"pagination\"), Some(&PAGINATION_LIMIT_ERROR)),\n            );\n        }\n\n        if errs.is_empty() {\n            Ok(())\n        } else {\n            Err(errs)\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum ReversibleIterator<T: Validate> {\n    /// Regular iteration - backwards in time.\n    Normal(T),\n    /// Reversed iteration - forwards in time.\n    Prev(T),\n}\n\nimpl<T: Validate> ReversibleIterator<T> {\n    pub(crate) fn direction(&self) -> IteratorDirection {\n        match self {\n            Self::Normal(_) => IteratorDirection::Normal,\n            Self::Prev(_) => IteratorDirection::Prev,\n        }\n    }\n}\n\nimpl<'de, T: 'static + Deserialize<'de> + Validate + From<String>> Deserialize<'de>\n    for ReversibleIterator<T>\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).map(|s| {\n            if let Some(s) = s.strip_prefix('-') {\n                ReversibleIterator::Prev(T::from(s.to_owned()))\n            } else {\n                ReversibleIterator::Normal(T::from(s))\n            }\n        })\n    }\n}\n\nimpl<T: Validate> Validate for ReversibleIterator<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            ReversibleIterator::Normal(val) => val.validate(),\n            ReversibleIterator::Prev(val) => val.validate(),\n        }\n    }\n}\n\nimpl<T: Validate + JsonSchema> JsonSchema for ReversibleIterator<T> {\n    fn schema_name() -> String {\n        format!(\"ReversibleIterator_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        T::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\n/// Applies sorting and filtration to a query from its iterator, sort column, and limit\n/// queries based on time\n/// Our rules for limiting queries are as follows\n///\n/// If `before` is passed:\n/// * lower limit on query is `before - LIMITED_QUERY_DURATION`\n/// * upper limit is `before`\n///\n/// If `after` is passed:\n/// * lower limit is `after`\n/// * upper limit is `now + FUTURE_QUERY_LIMIT`\n///\n/// If prev-iterator is passed:\n/// * lower limit is `prev-iterator`\n/// * upper limit is `prev-iterator + LIMITED_QUERY_DURATION`\n///\n/// If (normal) iterator is passed:\n/// * lower limit is `iterator - LIMITED_QUERY_DURATION`\n/// * upper limit is `iterator`\n///\n/// If no iterator is passed:\n/// * lower limit is `now() - LIMITED_QUERY_DURATION` if\n///   neither `before` nor `after` were passed\npub(crate) fn filter_and_paginate_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }\n\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket li<|fim_middle|>", "completion": "use regex::Regex;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "use_declaration", "line_range": [27, 27]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse chrono::Utc;\nu<|fim_suffix|>\nuse crate::core::types::{\n    BaseId, EndpointId, MessageAttemptId, MessageAttemptTriggerType, MessageEndpointId, MessageId,\n    MessageStatus,\n};\n\n#[derive(Clone, Debug, PartialEq, Eq, DeriveEntityModel)]\n#[sea_orm(table_name = \"messageattempt\")]\npub struct Model {\n    #[sea_orm(primary_key, auto_increment = false)]\n    pub id: MessageAttemptId,\n    pub created_at: DateTimeWithTimeZone,\n    pub msg_id: MessageId,\n    pub msg_dest_id: Option<MessageEndpointId>,\n    pub endp_id: EndpointId,\n    pub url: String,\n    pub status: MessageStatus,\n    pub response_status_code: i16,\n    #[sea_orm(column_type = \"Text\")]\n    pub response: String,\n    pub ended_at: Option<DateTimeWithTimeZone>,\n    pub trigger_type: MessageAttemptTriggerType,\n    /// Response duration in milliseconds\n    pub response_duration_ms: i64,\n    pub next_attempt: Option<DateTimeWithTimeZone>,\n    pub attempt_number: i16,\n}\n\n#[derive(Copy, Clone, Debug, EnumIter, DeriveRelation)]\npub enum Relation {\n    #[sea_orm(\n        belongs_to = \"super::message::Entity\",\n        from = \"Column::MsgId\",\n        to = \"super::message::Column::Id\",\n        on_update = \"NoAction\",\n        on_delete = \"Restrict\"\n    )]\n    Message,\n}\n\nimpl Related<super::message::Entity> for Entity {\n    fn to() -> RelationDef {\n        Relation::Message.def()\n    }\n}\n\nimpl ActiveModelBehavior for ActiveModel {\n    fn new() -> Self {\n        let timestamp = Utc::now();\n        Self {\n            id: Set(MessageAttemptId::new(timestamp.into(), None)),\n            created_at: Set(timestamp.into()),\n            ..ActiveModelTrait::default()\n        }\n    }\n}\n\nimpl Entity {\n    pub fn secure_find_by_msg(msg_id: MessageId) -> Select<Entity> {\n        Self::find().filter(Column::MsgId.eq(msg_id))\n    }\n\n    pub fn secure_find_by_endpoint(endp_id: EndpointId) -> Select<Entity> {\n        Self::find().filter(Column::EndpId.eq(endp_id))\n    }\n}\n\npub trait Query: QuerySelect + QueryFilter + QueryOrder + Sized {\n    fn after_id(self, id: MessageAttemptId) -> Self {\n        self.filter(Column::Id.gte(id))\n    }\n\n    fn after_id_exclusive(self, id: MessageAttemptId) -> Self {\n        self.filter(Column::Id.gt(id))\n    }\n\n    fn before_id(self, id: MessageAttemptId) -> Self {\n        self.filter(Column::Id.lte(id))\n    }\n\n    fn after(self, t: DateTimeUtc) -> Self {\n        self.after_id(MessageAttemptId::start_id(t))\n    }\n\n    fn before(self, t: DateTimeUtc) -> Self {\n        self.before_id(MessageAttemptId::start_id(t))\n    }\n\n    fn with_status(self, status: MessageStatus) -> Self {\n        self.filter(Column::Status.eq(status))\n    }\n\n    fn oldest_first(self) -> Self {\n        self.order_by(Column::Id, Order::Asc)\n    }\n\n    /// Only return the last attempt per message (every attempt will be part of a separate message)\n    fn latest_per_msg(self) -> Self {\n        self.distinct_on([Column::MsgId])\n            .order_by_desc(Column::MsgId)\n            .order_by_desc(Column::Id)\n    }\n}\n\nimpl Query for Select<Entity> {}\n<|fim_middle|>", "completion": "use sea_orm::{entity::prelude::*, ActiveValue::Set, Order, QueryOrder, QuerySelect};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/models/messageattempt.rs", "node_type": "use_declaration", "line_range": [5, 5]}
{"prompt": "<|fim_prefix|> .key;\n    jh.abort();\n\n    // Now add encryption and check the secret is still fine\n    let mut cfg = get_default_test_config();\n    cfg.encryption = Encryption::new([1; 32]);\n    let (client, jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading the existing secret works\n    assert_eq!(secret, secret2);\n\n    // Generate a new encrypted secret\n    client\n        .post_without_response(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/rotate/\", ep.id),\n            json!({ \"key\": secret }),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let secret2 = client\n        .get::<EndpointSecretOutTest>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap()\n        .key;\n\n    // Ensure loading and saving works for encrypted\n    assert_eq!(secret, secret2);\n    jh.abort();\n\n    // Make sure we can't read it with the secret unset\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id(&cfg, org_id.clone()).await;\n    client\n        .get::<IgnoredAny>(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/secret/\", ep.id),\n            StatusCode::INTERNAL_SERVER_ERROR,\n        )\n        .await\n        .unwrap();\n}\n\n#[tokio::test]\nasync fn test_invalid_endpoint_secret() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_too_short = \"whsec_C2FVsBQIhrscChlQIM+b5sSYspob\".to_owned();\n    let secret_too_long =\n        \"whsec_V09IYXZUaFJoSnFobnpJQkpPMXdpdGFNWnJsRzAxdXZCeTVndVpwRmxSSXFsc0oyYzBTRWRUekJhYnlaZ0JSRGNPQ3BGZG1xYjFVVmRGQ3UK\"\n            .to_owned();\n    let invalid_prefix = \"hwsec_C2FVsBQIhrscChlQIM+b5sSYspob7oDazfgh\".to_owned();\n\n    for sec in [secret_too_short, secret_too_long, invalid_prefix] {\n        let _: IgnoredAny = client\n            .post(\n                &format!(\"api/v1/app/{app_id}/endpoint/\"),\n                json!({\n                    \"url\": \"http://www.example.com\",\n                    \"version\": 1,\n                    \"secret\": sec,\n                }),\n                StatusCode::UNPROCESSABLE_ENTITY,\n            )\n            .await\n            .unwrap();\n    }\n}\n\nfn new_message_attempt_at_time(\n    timestamp: DateTime<Utc>,\n    status: MessageStatus,\n    endp_id: &EndpointId,\n    msg_id: &MessageId,\n) -> messageattempt::ActiveModel {\n    messageattempt::ActiveModel {\n        endp_id: Set(endp_id.clone()),\n        msg_id: Set(msg_id.clone()),\n        id: Set(MessageAttemptId::new(timestamp.into(), None)),\n        status: Set(status),\n        created_at: Set(timestamp.into()),\n        url: Set(\"http://www.example.com\".into()),\n        response_status_code: Set(200),\n        response_duration_ms: Set(1000),\n        response: Set(\"{}\".into()),\n        trigger_type: Set(MessageAttemptTriggerType::Scheduled),\n\n        ..ActiveModelTrait::default()\n    }\n}\n\n#[tokio::test]\nasync fn test_endpoint_stats() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let endp_id = create_test_endpoint(&client, &app_id, \"https://gabagool.deli\")\n        .await\n        .unwrap()\n        .id;\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 0);\n    assert_eq!(stats.sending, 0);\n\n    let last_msg_time = {\n        // Create the relevant Stats records manually, otherwise\n        // it's difficult to test exact state of messagedestinations.\n\n        let cfg = get_default_test_config();\n        l<|fim_suffix|>        let db = svix_server::db::init_db(&db).await;\n\n        let now = Utc::now();\n\n        let msg = message::ActiveModel {\n            app_id: Set(app_id.clone()),\n            org_id: Set(OrganizationId::new(None, None)),\n            expiration: Set(Utc::now().into()),\n            event_type: Set(EventTypeName(\"test.ing\".into())),\n            created_at: Set((now - chrono::Duration::minutes(65)).into()),\n            id: Set(MessageId::new(\n                (now - chrono::Duration::minutes(65)).into(),\n                None,\n            )),\n            ..message::ActiveModel::new()\n        }\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(60),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(45),\n            MessageStatus::Pending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap();\n\n        new_message_attempt_at_time(\n            now - chrono::Duration::minutes(30),\n            MessageStatus::Sending,\n            &endp_id,\n            &msg.id,\n        )\n        .insert(&db)\n        .await\n        .unwrap()\n        .created_at\n    };\n\n    let stats: EndpointStatsOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/stats/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats.fail, 0);\n    assert_eq!(stats.success, 0);\n    assert_eq!(stats.pending, 2);\n    assert_eq!(stats.sending, 1);\n\n    let stats_filtered: EndpointStatsOut = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}&until={}\",\n                urlencoding::encode(&last_msg_time.to_rfc3339()),\n                urlencoding::encode(&Utc::now().to_rfc3339()),\n            ),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(stats_filtered.fail, 0);\n    assert_eq!(stats_filtered.success, 0);\n    assert_eq!(stats_filtered.pending, 0);\n    assert_eq!(stats_filtered.sending, 1);\n\n    let _: IgnoredAny = client\n        .get(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{endp_id}/stats/?since={}\",\n                urlencoding::encode(&(Utc::now() - chrono::Duration::days(29)).to_rfc3339()),\n            ),\n            StatusCode::BAD_REQUEST,\n        )\n        .await\n        .unwrap();\n}\n\n/// We used to store the secret in the DB without a type marker, check loading those still works\n#[tokio::test]\nasync fn test_legacy_endpoint_secret() {\n    let cfg = get_default_test_config();\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let db = Arc::new(cfg);\n    let db = svix_server::db::init_db(&db).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let secret_throwaway = EndpointSecretInternal::generate_symmetric(&Encryption::new_noop())\n        .unwrap()\n        .into_endpoint_secret(&Encryption::new_noop())\n        .unwrap();\n    let raw_key = STANDARD.decode(\"5gasBsSw3Nvf3ugNYVJIqnRVYPW7hPts\").unwrap();\n    let secret_1 = EndpointSecret::Symmetric(raw_key.clone());\n\n    let ep_in = EndpointIn {\n        key: Some(secret_throwaway.clone()),\n        ..default_test_endpoint()\n    };\n\n    let endp_1 = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    // Set the raw value to the database (like legacy)\n    db.execute(Statement::from_sql_and_values(\n        DatabaseBackend::Postgres,\n        \"UPDATE endpoint SET key = $1 WHERE id = $2\",\n        vec![raw_key.clone().into(), endp_1.id.clone().into()],\n    ))\n    .await\n    .unwrap();\n\n    let endp_1 = get_endpoint(&client, &app_id, &endp_1.id).await.unwrap();\n\n    #[derive(Deserialize)]\n    pub struct EndpointSecretOutTest {\n        pub key: String,\n    }\n\n    let (secret, ep) = (secret_1, endp_1);\n    assert_eq!(\n        secret.serial<|fim_middle|>", "completion": "let db = Arc::new(cfg);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1484, 1484]}
{"prompt": "<|fim_prefix|>use std::{\n    borrow::Cow, collections::HashMap, convert::Infallible, fmt, io::Error, net::SocketAddr,\n    num::NonZeroUsize,\n};\n\nuse anyhow::anyhow;\nuse serde::Deserialize;\nuse shellexpand::LookupError;\n#[cfg(feature = \"kafka\")]\nuse svix_bridge_plugin_kafka::{KafkaInputOpts, KafkaOutputOpts};\nuse svix_bridge_plugin_queue::config::{QueueInputOpts, QueueOutputOpts};\nuse svix_bridge_types::{\n    svix::api::Svix, ReceiverInputOpts, ReceiverOutput, SenderInput, SenderOutputOpts, SvixOptions,\n    TransformationConfig,\n};\nuse tracing::Level;\n\n<|fim_suffix|>\n\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum EitherReceiver {\n    Webhook(WebhookReceiverConfig),\n    Poller(PollerReceiverConfig),\n}\n\n#[derive(Deserialize)]\n#[serde(deny_unknown_fields)]\npub struct Config {\n    /// Config for reading messages from plugins and forwarding to Svix.\n    #[serde(default)]\n    pub senders: Vec<WebhookSenderConfig>,\n    /// Config for receiving webhooks and forwarding them to plugins.\n    #[serde(default)]\n    pub receivers: Vec<EitherReceiver>,\n    /// The log level to run the service with. Supported: info, debug, trace\n    #[serde(default)]\n    pub log_level: LogLevel,\n    /// The log format that all output will follow. Supported: default, json\n    #[serde(default)]\n    pub log_format: LogFormat,\n    /// OpenTelemetry exporter settings\n    #[serde(default)]\n    pub opentelemetry: Option<OtelExporterConfig>,\n    #[serde(default = \"default_http_listen_address\")]\n    pub http_listen_address: SocketAddr,\n    #[serde(default = \"default_transformation_worker_count\")]\n    pub transformation_worker_count: NonZeroUsize,\n}\n\nimpl Config {\n    /// Build a Config from yaml source.\n    /// Optionally accepts a map to perform variable substitution with.\n    pub fn from_src(\n        raw_src: &str,\n        vars: Option<&HashMap<String, String>>,\n    ) -> std::io::Result<Self> {\n        let src = if let Some(vars) = vars {\n            let context = |key: &str| -> Result<Option<Cow<'_, str>>, LookupError<Infallible>> {\n                Ok(vars.get(key).map(Cow::from))\n            };\n            shellexpand::env_with_context(raw_src, context).map_err(|e: LookupError<_>| {\n                Error::other(format!(\"Variable substitution failed: {e}\"))\n            })?\n        } else {\n            Cow::Borrowed(raw_src)\n        };\n        let cfg: Self = serde_yaml::from_str(&src)\n            .map_err(|e| Error::other(format!(\"Failed to parse config: {e}\")))?;\n\n        for sc in &cfg.senders {\n            if let Some(tc) = sc.transformation() {\n                crate::runtime::validate_script(tc.source().as_str()).map_err(|e| {\n                    Error::other(format!(\n                        \"failed to parse transformation for sender `{}`: {e:?}\",\n                        &sc.name(),\n                    ))\n                })?;\n            }\n        }\n\n        for (name, tc) in cfg.receivers.iter().filter_map(|either| match either {\n            EitherReceiver::Webhook(receiver) => receiver\n                .transformation\n                .as_ref()\n                .map(|tc| (&receiver.name, tc)),\n            EitherReceiver::Poller(receiver) => receiver\n                .transformation\n                .as_ref()\n                .map(|tc| (&receiver.name, tc)),\n        }) {\n            crate::runtime::validate_script(tc.source().as_str()).map_err(|e| {\n                Error::other(format!(\n                    \"failed to parse transformation for receiver `{name}`: {e:?}\"\n                ))\n            })?;\n        }\n\n        Ok(cfg)\n    }\n}\n\nfn default_http_listen_address() -> SocketAddr {\n    \"0.0.0.0:5000\".parse().expect(\"default http listen address\")\n}\n\nfn default_transformation_worker_count() -> NonZeroUsize {\n    NonZeroUsize::new(4).expect(\"4 is greater than 0\")\n}\n\n#[derive(Deserialize)]\npub struct OtelExporterConfig {\n    /// The OpenTelemetry service name to use\n    pub service_name: Option<String>,\n    /// The OpenTelemetry address to send events to if given.\n    pub address: String,\n    /// The ratio at which to sample spans when sending to OpenTelemetry. When not given it defaults\n    /// to always sending. If the OpenTelemetry address is not set, this will do nothing.\n    pub sample_ratio: Option<f64>,\n}\n\n#[derive(Clone, Debug, Default, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogLevel {\n    #[default]\n    Info,\n    Debug,\n    Trace,\n}\n\nimpl fmt::Display for LogLevel {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Info => Level::INFO,\n            Self::Debug => Level::DEBUG,\n            Self::Trace => Level::TRACE,\n        }\n        .fmt(f)\n    }\n}\n\n#[derive(Clone, Debug, Default, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogFormat {\n    #[default]\n    Default,\n    Json,\n}\n\n/// Config for reading messages from plugins and forwarding to Svix.\n#[derive(Deserialize)]\npub struct WebhookSenderConfig {\n    pub name: String,\n    pub input: SenderInputOpts,\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: SenderOutputOpts,\n}\n\n#[derive(Deserialize)]\n#[serde(untagged)]\npub enum SenderInputOpts {\n    #[cfg(feature = \"kafka\")]\n    Kafka(KafkaInputOpts),\n    Queue(QueueInputOpts),\n}\n\nimpl WebhookSenderConfig {\n    pub fn into_sender_input(self) -> anyhow::Result<Box<dyn SenderInput>> {\n        Ok(match self.input {\n            #[cfg(feature = \"kafka\")]\n            SenderInputOpts::Kafka(input_opts) => svix_bridge_plugin_kafka::into_sender_input(\n                self.name,\n                input_opts,\n                self.transformation,\n                self.output,\n            )?,\n            SenderInputOpts::Queue(input_opts) => svix_bridge_plugin_queue::into_sender_input(\n                self.name,\n                input_opts,\n                self.transformation,\n                self.output,\n            )\n            .map_err(|e| anyhow!(\"{e}\"))?,\n        })\n    }\n}\n\nimpl WebhookSenderConfig {\n    pub fn name(&self) -> &str {\n        &self.name\n    }\n\n    pub fn transformation(&self) -> Option<&TransformationConfig> {\n        self.transformation.as_ref()\n    }\n}\n\nimpl TryFrom<WebhookSenderConfig> for Box<dyn SenderInput> {\n    type Error = anyhow::Error;\n\n    fn try_from(value: WebhookSenderConfig) -> Result<Self, Self::Error> {\n        value.into_sender_input()\n    }\n}\n\n/// Config for receiving webhooks and forwarding them to plugins.\n#[derive(Deserialize)]\npub struct WebhookReceiverConfig {\n    pub name: String,\n    pub input: ReceiverInputOpts,\n    #[serde(default)]\n    pub transformation: Option<TransformationConfig>,\n    pub output: ReceiverOutputOpts,\n}\n\n#[derive(Deserialize)]\n#[allow(clippy::large_enum_variant)] // we're talking a couple hundred bytes only\n#[serde(untagged)]\npub enum ReceiverOutputOpts {\n    Http(HttpOutputOpts),\n    #[cfg(feature = \"kafka\")]\n    Kafka(KafkaOutputOpts),\n    Queue(QueueOutputOpts),\n}\n\nimpl WebhookReceiverConfig {\n    pub async fn into_receiver_output(self) -> anyhow::Result<Box<dyn ReceiverOutput>> {\n        match self.output {\n            ReceiverOutputOpts::Http(opts) => opts.into_receiver_output(self.name),\n            #[cfg(feature = \"kafka\")]\n            ReceiverOutputOpts::Kafka(opts) => {\n                svix_bridge_plugin_kafka::into_receiver_output(self.name, opts).map_err(Into::into)\n            }\n            ReceiverOutputOpts::Queue(x) => svix_bridge_plugin_queue::into_receiver_output(\n                self.name.clone(),\n                x,\n                self.transformation.as_ref(),\n            )\n            .await\n            .map_err(Into::into),\n        }\n    }\n}\n\n#[derive(Clone, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"kebab-case\")]\npub enum PollerInputOpts {\n    SvixMessagePoller {\n        /// Identifies this client, allowing the server to track progress during iteration.\n        /// Processes should not share a consumer id. Only exclusive access is permitted.\n        consumer_id: String,\n        /// The app portion of the polling endpoint, e.g. `app_XXXX`\n        app_id: String,\n        /// The poller portion of the polling endpoint, e.g. `poll_XXXX`\n        sink_id: String,\n        /// The auth token for th<|fim_middle|>", "completion": "use crate::http_output::HttpOutputOpts;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/config/mod.rs", "node_type": "use_declaration", "line_range": [18, 18]}
{"prompt": "<|fim_prefix|>use svix::api::{MessageListOptions, Svix, SvixOptions};\n\nuse wiremock::{\n    matchers::{method, path},\n    Mock, MockServer, ResponseTemplate,\n};\n\n#[tokio::test]\nasync fn test_urlencoded_octothorpe() {\n    let mock_server = MockServer::start().await;\n\n    let json_body =\n        r#\"{\"data\":[],\"done\":true,\"iterator\":\"iterator-str\",\"prevIterator\":\"prevIterator-str\"}\"#;\n    Mock::given(method(\"GET\"))\n        .and(path(\"/api/v1/app/app_id/msg\"))\n        .respond_with(ResponseTemplate::new(200).set_body_string(json_body))\n        .mount(&mock_server)\n        .await;\n\n    <|fim_suffix|>\n\n    svx.message()\n        .list(\n            \"app_id\".to_string(),\n            Some(MessageListOptions {\n                tag: Some(\"test#test\".into()),\n                ..Default::default()\n            }),\n        )\n        .await\n        .unwrap();\n\n    let requests = mock_server\n        .received_requests()\n        .await\n        .expect(\"we should have sent a request\");\n\n    assert_eq!(1, requests.len());\n    assert_eq!(Some(\"tag=test%23test\"), requests[0].url.query());\n}\n\n#[tokio::test]\nasync fn test_idempotency_key_is_sent_for_create_request() {\n    let mock_server = MockServer::start().await;\n\n    let json_body = r#\"{\"uid\":\"unique-identifier\",\"name\":\"My first application\",\"rateLimit\":0,\"id\":\"app_1srOrx2ZWZBpBUvZwXKQmoEYga2\",\"createdAt\":\"2019-08-24T14:15:22Z\",\"updatedAt\":\"2019-08-24T14:15:22Z\",\"metadata\":{\"property1\":\"string\",\"property2\":\"string\"}}\"#;\n    Mock::given(method(\"POST\"))\n        .and(path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(200).set_body_string(json_body))\n        .mount(&mock_server)\n        .await;\n\n    let svx = Svix::new(\n        \"token\".to_string(),\n        Some(SvixOptions {\n            server_url: Some(mock_server.uri()),\n            ..Default::default()\n        }),\n    );\n\n    svx.application()\n        .create(svix::api::ApplicationIn::new(\"test app\".to_string()), None)\n        .await\n        .unwrap();\n\n    let requests = mock_server\n        .received_requests()\n        .await\n        .expect(\"we should have sent a request\");\n\n    assert_eq!(1, requests.len());\n    let idempotency_key = requests[0]\n        .headers\n        .get(\"idempotency-key\")\n        .expect(\"idempotency-key header should be present\");\n    assert!(\n        idempotency_key.to_str().unwrap().starts_with(\"auto_\"),\n        \"idempotency key should start with 'auto_', got: {idempotency_key:?}\"\n    );\n}\n\n#[tokio::test]\nasync fn test_client_provided_idempotency_key_is_not_overridden() {\n    let mock_server = MockServer::start().await;\n\n    let json_body = r#\"{\"uid\":\"unique-identifier\",\"name\":\"My first application\",\"rateLimit\":0,\"id\":\"app_1srOrx2ZWZBpBUvZwXKQmoEYga2\",\"createdAt\":\"2019-08-24T14:15:22Z\",\"updatedAt\":\"2019-08-24T14:15:22Z\",\"metadata\":{\"property1\":\"string\",\"property2\":\"string\"}}\"#;\n    Mock::given(method(\"POST\"))\n        .and(path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(200).set_body_string(json_body))\n        .mount(&mock_server)\n        .await;\n\n    let svx = Svix::new(\n        \"token\".to_string(),\n        Some(SvixOptions {\n            server_url: Some(mock_server.uri()),\n            ..Default::default()\n        }),\n    );\n\n    let client_provided_key = \"test-key-123\";\n    svx.application()\n        .create(\n            svix::api::ApplicationIn::new(\"test app\".to_string()),\n            Some(svix::api::ApplicationCreateOptions {\n                idempotency_key: Some(client_provided_key.to_string()),\n            }),\n        )\n        .await\n        .unwrap();\n\n    let requests = mock_server\n        .received_requests()\n        .await\n        .expect(\"we should have sent a request\");\n\n    assert_eq!(1, requests.len());\n    let idempotency_key = requests[0]\n        .headers\n        .get(\"idempotency-key\")\n        .expect(\"idempotency-key header should be present\");\n    assert_eq!(\n        client_provided_key,\n        idempotency_key.to_str().unwrap(),\n        \"client provided idempotency key should not be overridden\"\n    );\n}\n\n#[tokio::test]\nasync fn test_unknown_keys_are_ignored() {\n    let mock_server = MockServer::start().await;\n\n    let json_body =\n        r#\"{\"data\":[],\"done\":true,\"iterator\":null,\"prevIterator\":null,\"extra-key\":\"ignored\"}\"#;\n    Mock::given(method(\"GET\"))\n        .and(path(\"/api/v1/app\"))\n        .respond_with(ResponseTemplate::new(200).set_body_string(json_body))\n        .expect(1)\n        .mount(&mock_server)\n        .await;\n\n    let svx = Svix::new(\n        \"token\".to_string(),\n        Some(SvixOptions {\n            server_url: Some(mock_server.uri()),\n            ..Default::default()\n        }),\n    );\n\n    svx.application().list(None).await.unwrap();\n\n    mock_server.verify().await;\n}\n<|fim_middle|>", "completion": "let svx = Svix::new(\n        \"token\".to_string(),\n        Some(SvixOptions {\n            server_url: Some(mock_server.uri()),\n            ..Default::default()\n        }),\n    );", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/tests/it/wiremock_tests.rs", "node_type": "let_declaration", "line_range": [20, 26]}
{"prompt": "<|fim_prefix|>  db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                )\n                .expect(\"Error initializing RedisSentinelConnectionManager\");\n                let pool = bb8::Pool::builder()\n                    .max_size(max_conns.into())\n                    .build(mgr)\n                    .await\n                    .expect(\"Error initializing redis connection pool\");\n                RedisManager::Sentinel(pool)\n            }\n        }\n    }\n\n    async fn new_unpooled(dsn: &str, variant: RedisVariant<'_>) -> Self {\n        match variant {\n            RedisVariant::Clustered => {\n                let cli = redis::cluster::ClusterClient::builder(vec![dsn])\n                    .retries(1)\n                    .connection_timeout(REDIS_CONN_TIMEOUT)\n                    .build()\n                    .expect(\"Error initializing redis-unpooled cluster client\");\n                let con = cli\n                    .get_async_connection()\n                    .await\n                    .expect(\"Failed to get redis-cluster-unpooled connection\");\n                RedisManager::ClusteredUnpooled(con)\n            }\n            RedisVariant::NonClustered => {\n                let cli =\n                    redis::Client::open(dsn).expect(\"Error initializing redis unpooled client\");\n                let con = redis::aio::ConnectionManager::new_with_config(\n                    cli,\n                    ConnectionManagerConfig::new()\n                        .set_number_of_retries(1)\n                        .set_connection_timeout(REDIS_CONN_TIMEOUT),\n                )\n                .await\n                .expect(\"Failed to get redis-unpooled connection manager\");\n                RedisManager::NonClusteredUnpooled(con)\n            }\n            RedisVariant::Sentinel(cfg) => {\n                let tls_mode = cfg.redis_tls_mode_secure.then_some(TlsMode::Secure);\n                let protocol = if cfg.redis_use_resp3 {\n                    ProtocolVersion::RESP3\n                } else {\n                    ProtocolVersion::default()\n                };\n                let cli = redis::sentinel::SentinelClient::build(\n                    vec![dsn],\n                    cfg.service_name.clone(),\n                    Some(SentinelNodeConnectionInfo {\n                        tls_mode,\n                        redis_connection_info: Some(RedisConnectionInfo {\n                            db: cfg.redis_db.unwrap_or(0),\n                            username: cfg.redis_username.clone(),\n                            password: cfg.redis_password.clone(),\n                            protocol,\n                        }),\n                    }),\n                    redis::sentinel::SentinelServerType::Master,\n                )\n                .expect(\"Failed to build sentinel client\");\n\n                RedisManager::SentinelUnpooled(Arc::new(Mutex::new(cli)))\n            }\n        }\n    }\n\n    pub async fn from_cache_backend(cache_backend: &CacheBackend<'_>) -> Self {\n        match cache_backend {\n            CacheBackend::Redis(dsn) => Self::new_unpooled(dsn, RedisVariant::NonClustered).await,\n            CacheBackend::RedisCluster(dsn) => {\n                Self::new_unpooled(dsn, RedisVariant::Clustered).await\n            }\n            CacheBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_unpooled(dsn, RedisVariant::Sentinel(cfg)).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn from_queue_backend(queue_backend: &QueueBackend<'_>, max_conns: u16) -> Self {\n        match queue_backend {\n            QueueBackend::Redis(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::NonClustered, max_conns).await\n            }\n            QueueBackend::RedisCluster(dsn) => {\n                Self::new_pooled(dsn, RedisVariant::Clustered, max_conns).await\n            }\n            QueueBackend::RedisSentinel(dsn, cfg) => {\n                Self::new_pooled(dsn, RedisVariant::Sentinel(cfg), max_conns).await\n            }\n            _ => panic!(\"Queue type not supported with redis\"),\n        }\n    }\n\n    pub async fn get(&self) -> Result<RedisConnection<'_>, RunError<RedisError>> {\n        match self {\n            Self::Clustered(pool) => Ok(RedisConnection::Clustered(pool.get().await?)),\n            Self::NonClustered(pool) => Ok(RedisConnection::NonClustered(pool.get().await?)),\n            Self::Sentinel(pool) => Ok(RedisConnection::SentinelPooled(pool.get().await?)),\n            Self::ClusteredUnpooled(conn) => Ok(RedisConnection::ClusteredUnpooled(conn.clone())),\n            Self::NonClusteredUnpooled(conn) => {\n                Ok(RedisConnection::NonClusteredUnpooled(conn.clone()))\n            }\n            Self::SentinelUnpooled(conn) => {\n                let mut conn = conn.lock().await;\n                let con = conn\n                    .get_async_connection_with_config(\n                        &AsyncConnectionConfig::new().set_response_timeout(REDIS_CONN_TIMEOUT),\n                    )\n                    .await?;\n                Ok(RedisConnection::SentinelUnpooled(con))\n            }\n        }\n    }\n}\n\npub enum RedisConnection<'a> {\n    Clustered(bb8::PooledConnection<'a, RedisClusterConnectionManager>),\n    NonClustered(bb8::PooledConnection<'a, RedisConnectionManager>),\n    SentinelPooled(bb8::PooledConnection<'a, RedisSentinelConnectionManager>),\n    ClusteredUnpooled(redis::cluster_async::ClusterConnection),\n    NonClusteredUnpooled(redis::aio::ConnectionManager),\n    SentinelUnpooled(redis::aio::MultiplexedConnection),\n}\n\nimpl redis::aio::ConnectionLike for RedisConnection<'_> {\n    fn req_packed_command<'a>(\n        &'a mut self,\n        cmd: &'a redis::Cmd,\n    ) -> redis::RedisFuture<'a, redis::Value> {\n        match self {\n            RedisConnection::Clustered(conn) => conn.req_packed_command(cmd),\n            RedisConnection::NonClustered(conn) => conn.req_packed_command(cmd),\n            RedisConnection::ClusteredUnpooled(conn) => conn.req_packed_command(cmd),\n            RedisConnection::NonClusteredUnpooled(conn) => conn.req_packed_command(cmd),\n            RedisConnection::SentinelPooled(conn) => conn.req_packed_command(cmd),\n            RedisConnection::SentinelUnpooled(conn) => conn.req_packed_command(cmd),\n        }\n    }\n\n    fn req_packed_commands<'a>(\n        &'a mut self,\n        cmd: &'a redis::Pipeline,\n        offset: usize,\n        count: usize,\n    ) -> redis::RedisFuture<'a, Vec<redis::Value>> {\n        match self {\n            RedisConnection::Clustered(conn) => conn.req_packed_commands(cmd, offset, count),\n            RedisConnection::NonClustered(conn) => conn.req_packed_commands(cmd, offset, count),\n            RedisConnection::ClusteredUnpooled(conn) => {\n                conn.req_packed_commands(cmd, offset, count)\n            }\n            RedisConnection::NonClusteredUnpooled(conn) => {\n                conn.req_packed_commands(cmd, offset, count)\n            }\n            RedisConnection::SentinelPooled(conn) => conn.req_packed_commands(cmd, offset, count),\n            RedisConnection::SentinelUnpooled(conn) => conn.req_packed_commands(cmd, offset, count),\n        }\n    }\n\n    <|fim_suffix|>\n}\n\n#[cfg(test)]\nmod tests {\n    use redis::AsyncCommands;\n\n    use super::RedisManager;\n\n    // Ensure basic set/get works -- should test sharding as well:\n    #[tokio::test]\n    // run with `cargo test -- --ignored redis` only when redis is up and configured\n    #[ignore]\n    async fn test_set_read_random_keys() {\n        dotenvy::dotenv().ok();\n        let cfg = crate::cfg::load().unwrap();\n\n        let mgr = RedisManager::from_cache_backend(&cfg.cache_backend()).await;\n        let mut conn = mgr.get().await.unwrap();\n\n        for (val, key) in \"abcdefghijklmnopqrstuvwxyz\".chars().enumerate() {\n            let key = key.to_string();\n            let _: () = conn.set(key.clone(), val).await.unwrap();\n            assert_eq!(conn.get::<_, usize>(&key).await.unwrap(), val);\n        }\n    }\n}\n<|fim_middle|>", "completion": "fn get_db(&self) -> i64 {\n        match self {\n            RedisConnection::Clustered(conn) => conn.get_db(),\n            RedisConnection::NonClustered(conn) => conn.get_db(),\n            RedisConnection::ClusteredUnpooled(conn) => conn.get_db(),\n            RedisConnection::NonClusteredUnpooled(conn) => conn.get_db(),\n            RedisConnection::SentinelPooled(conn) => conn.get_db(),\n            RedisConnection::SentinelUnpooled(conn) => conn.get_db(),\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/redis/mod.rs", "node_type": "function_item", "line_range": [239, 248]}
{"prompt": "<|fim_prefix|>use proc_macro2::TokenStream;\nuse quote::{format_ident, quote, ToTokens as _};\nuse syn::{punctuated::Punctuated, Token};\n\npub type AideAnnotateArgumentList = Punctuated<syn::MetaNameValue, Token![,]>;\n\npub fn expand_aide_annotate(\n    args: AideAnnotateArgumentList,\n    item: syn::ItemFn,\n) -> syn::Result<TokenStream> {\n    // By default, use the function's name as the operation id.\n    let mut operation_id = item.sig.ident.to_string();\n    // The operation summary is the title-cased version of the original\n    // function name.\n    let mut operation_summary = operation_id\n        .split('_')\n        .map(title_case)\n        .collect::<Vec<String>>()\n        .join(\" \");\n    // The documentation function's name will always be the name of the\n    // original function suffixed with `_operation`.\n    let operation_ident = format_ident!(\"{}_operation\", item.sig.ident);\n    let visibility = item.vis.clone();\n\n    // Allow overriding operation ID and summary via arguments\n    for arg in args {\n        let lit = expr_to_litstr(&arg.value).ok_or_else(|| {\n            syn::Error::new_spanned(\n                &arg.value,\n                \"Unexpected expression, expected a string literal\",\n            )\n        })?;\n\n        if arg.path.is_ident(\"op_id\") {\n            operation_id = lit.value();\n        } else if arg.path.is_ident(\"op_summary\") {\n            operation_summary = lit.value();\n        } else {\n            let path = arg.path.to_token_stream().to_string();\n            return Err(syn::Error::new_spanned(\n                arg.path,\n                format_args!(\"Unknown argument `{path}`, expected `op_id` or `op_summary`\"),\n            ));\n        }\n    }\n\n    let description = doc_comment_from_attributes(&item.attrs);\n\n    <|fim_suffix|>\n\n    Ok(quote! {\n        #item\n\n        #visibility fn #operation_ident(\n            op: ::aide::transform::TransformOperation,\n        ) -> ::aide::transform::TransformOperation {\n            op\n                .id(#operation_id)\n                .summary(#operation_summary)\n                .description(#description)\n                .response_with::<401, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Unauthorized\")\n                })\n                .response_with::<403, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Forbidden\")\n                })\n                .response_with::<404, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Not Found\")\n                })\n                .response_with::<409, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Conflict\")\n                })\n                .response_with::<422, ::axum::Json<crate::error::ValidationHttpError>, _>(|op| {\n                    op.description(\"Validation Error\")\n                })\n                .response_with::<429, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Too Many Requests\")\n                })\n        }\n    })\n}\n\nfn doc_comment_from_attributes(attributes: &Vec<syn::Attribute>) -> Option<String> {\n    let mut doc_comment_lines = Vec::new();\n\n    for attr in attributes {\n        if !attr.path().is_ident(\"doc\") {\n            continue;\n        }\n\n        // Ignore bare `#[doc]` and `#[doc(foo)]` attributes, only look at `#[doc = \"foo\"]`\n        let Ok(name_val) = attr.meta.require_name_value() else {\n            continue;\n        };\n\n        // Malformed doc attribute, likely a compile error anyways\n        let Some(doc) = expr_to_litstr(&name_val.value) else {\n            continue;\n        };\n\n        doc_comment_lines.push(doc.value().trim().to_owned());\n    }\n\n    if doc_comment_lines.is_empty() {\n        return None;\n    }\n    Some(doc_comment_lines.join(\"\\n\"))\n}\n\nfn title_case(s: &str) -> String {\n    let mut c = s.chars();\n    match c.next() {\n        None => String::new(),\n        Some(f) => f.to_uppercase().collect::<String>() + c.as_str(),\n    }\n}\n\nfn expr_to_litstr(expr: &syn::Expr) -> Option<&syn::LitStr> {\n    match expr {\n        syn::Expr::Lit(l) => match &l.lit {\n            syn::Lit::Str(s) => Some(s),\n            _ => None,\n        },\n        _ => None,\n    }\n}\n<|fim_middle|>", "completion": "if description.is_none() {\n        let msg = \"An annotated handler must have a doc comment for its description.\";\n        return Err(syn::Error::new_spanned(&item.sig.ident, msg));\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server-derive/src/aide.rs", "node_type": "if_expression", "line_range": [49, 52]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse sea_orm::{\n    ColumnTrait, DatabaseConnection, DbBackend, DeleteResult, EntityTrait, QueryFilter,\n    SqlxPostgresConnector,\n};\nuse sqlx::postgres::PgPoolOptions;\n\nuse crate::{cfg::Configuration, core::types::OrganizationId};\n\npub mod models;\nuse models::{application, endpoint, eventtype, message, messageattempt};\n\nstatic MIGRATIONS: sqlx::migrate::Migrator = sqlx::migrate!();\n\nasync fn connect(dsn: &str, max_pool_size: u16) -> sqlx::Pool<sqlx::Postgres> {\n    if DbBackend::Postgres.is_prefix_of(dsn) {\n        PgPoolOptions::new()\n            .max_connections(max_pool_size.into())\n            .connect(dsn)\n            .await\n            .expect(\"Error connecting to Postgres\")\n    } else {\n        panic!(\"db_dsn format not recognized. {dsn}\")\n    }\n}\n\npub async fn init_db(cfg: &Configuration) -> DatabaseConnection {\n    SqlxPostgresConnector::from_sqlx_postgres_pool(connect(&cfg.db_dsn, cfg.db_pool_max_size).await)\n}\n\npub async fn run_migrations(cfg: &Configuration) {\n    let db = connect(&cfg.db_dsn, cfg.db_pool_max_size).await;\n    MIGRATIONS.run(&db).await.unwrap();\n}\n\n/// Wipe an organization from existence in a way that ensures the operation can be tried again on\n/// failure.\npub async fn wipe_org(cfg: &Configuration, org_id: OrganizationId) {\n    l<|fim_suffix|>\n    let applications: Vec<application::Model> = application::Entity::secure_find(org_id.clone())\n        .all(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error fetching applications associated with org ID {org_id}\"));\n\n    for application in applications {\n        let endpoints: Vec<endpoint::Model> = endpoint::Entity::secure_find(application.id.clone())\n            .all(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error fetching endpoints associated with application ID {}\",\n                    application.id\n                )\n            });\n\n        for endpoint in endpoints {\n            // First [`messageattempt`]s, then [`messagedestination`]s\n            let _: DeleteResult = messageattempt::Entity::delete_many()\n                .filter(messageattempt::Column::EndpId.eq(endpoint.id.clone()))\n                .exec(&db)\n                .await\n                .unwrap_or_else(|_| {\n                    panic!(\n                        \"Error deleting messageattempts associated with endpoint ID {}\",\n                        endpoint.id\n                    )\n                });\n        }\n\n        // Then [`message`]s, then [`endpoint`]s\n        let _: DeleteResult = message::Entity::delete_many()\n            .filter(message::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting messages associated with application ID {}\",\n                    application.id\n                )\n            });\n\n        let _: DeleteResult = endpoint::Entity::delete_many()\n            .filter(endpoint::Column::AppId.eq(application.id.clone()))\n            .exec(&db)\n            .await\n            .unwrap_or_else(|_| {\n                panic!(\n                    \"Error deleting endpoints associated with application ID {}\",\n                    application.id\n                )\n            });\n    }\n\n    // Then [`application`]s, then [`eventtype`]s\n    let _: DeleteResult = application::Entity::delete_many()\n        .filter(application::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting applications associated with org ID {org_id}\"));\n\n    let _: DeleteResult = eventtype::Entity::delete_many()\n        .filter(eventtype::Column::OrgId.eq(org_id.clone()))\n        .exec(&db)\n        .await\n        .unwrap_or_else(|_| panic!(\"Error deleting event types associated with org ID {org_id}\"));\n}\n<|fim_middle|>", "completion": "let db = init_db(cfg).await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/db/mod.rs", "node_type": "let_declaration", "line_range": [41, 41]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::{event_in::EventIn, stream_in::StreamIn};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct CreateStreamEventsIn {\n    pub events: Vec<EventIn>,\n\n    /// Optionally creates a new Stream alongside the events.\n    ///\n    /// If the stream id or uid that is used in the path already exists, this\n    /// argument is ignored.\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub stream: Option<StreamIn>,\n}\n\nimpl CreateStreamEventsIn {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(events: Vec<EventIn>) -> Self {\n        Self {\n            events,\n            stream: None,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/create_stream_events_in.rs", "node_type": "function_item", "line_range": [19, 24]}
{"prompt": "<|fim_prefix|>&key, &SerializedResponse::Start, expiry_starting())\n                    .await\n                {\n                    lock_acquired\n                } else {\n                    return Ok(StatusCode::INTERNAL_SERVER_ERROR.into_response());\n                };\n\n                // If the lock was not set, first check the cache for a `Finished` cache value. If\n                // it is instead `None` or the value is a `Start` lock, then enter a loop checking\n                // it every 200ms.\n                //\n                // If the loop times out, then reset the lock and proceed to resolve the service.\n                //\n                // If at any point the cache returns an `Err`, then return 500 response\n                if !lock_acquired {\n                    match cache.get::<SerializedResponse>(&key).await {\n                        Ok(Some(SerializedResponse::Finished {\n                            code,\n                            headers,\n                            body,\n                        })) => {\n                            return Ok(finished_serialized_response_to_response(\n                                code, headers, body,\n                            )\n                            .unwrap_or_else(|_| StatusCode::INTERNAL_SERVER_ERROR.into_response()))\n                        }\n\n                        Ok(Some(SerializedResponse::Start)) | Ok(None) => {\n                            if let Ok(Some(SerializedResponse::Finished {\n                                code,\n                                headers,\n                                body,\n                            })) = lock_loop(&cache, &key).await\n                            {\n                                return Ok(finished_serialized_response_to_response(\n                                    code, headers, body,\n                                )\n                                .unwrap_or_else(|_| {\n                                    StatusCode::INTERNAL_SERVER_ERROR.into_response()\n                                }));\n                            } else {\n                                // Set the lock if it returns `Ok(None)` and continue to resolve\n                                // as normal, but return 500 if the lock cannot be set\n                                if !matches!(\n                                    cache\n                                        .set_if_not_exists(\n                                            &key,\n                                            &SerializedResponse::Start,\n                                            expiry_starting(),\n                                        )\n                                        .await,\n                                    Ok(true)\n                                ) {\n                                    return Ok(StatusCode::INTERNAL_SERVER_ERROR.into_response());\n                                }\n                            }\n                        }\n\n                        Err(_) => return Ok(StatusCode::INTERNAL_SERVER_ERROR.into_response()),\n                    }\n                };\n\n                // If it's set or the lock or the `lock_loop` returns Ok(None), then the key has no\n                // value, so continue resolving the service while caching the response for 2xx\n                // responses\n                Ok(resolve_and_cache_response(\n                    &cache,\n                    &key,\n                    service,\n                    Request::from_parts(parts, body),\n                )\n                .await)\n            })\n        } else {\n            Box::pin(async move { Ok(service.call(req).await.into_response()) })\n        }\n    }\n}\n\n/// Retrieves an [`IdempotencyKey`] from the [`Parts`] of a [`Request`] returning None in the event\n/// that not all erquisite parts are there.\nfn get_key(parts: &Parts) -> Option<IdempotencyKey> {\n    let key = if let Some(Ok(key)) = parts.headers.get(\"idempotency-key\").map(|v| v.to_str()) {\n        key\n    } else {\n        // No idempotency-key -- pass off to service and do not cache\n        return None;\n    };\n\n    let auth = i<|fim_suffix|>\n\n    let uri = parts.uri.to_string();\n\n    Some(IdempotencyKey::new(auth, key, &uri))\n}\n\n/// If the lock could not be set, then another request with that key has been completed or is being\n/// completed, so loop until it has been completed or times out\nasync fn lock_loop(\n    cache: &Cache,\n    key: &IdempotencyKey,\n) -> Result<Option<SerializedResponse>, Error> {\n    let mut total_delay_duration = std::time::Duration::from_millis(0);\n\n    loop {\n        total_delay_duration += wait_duration();\n        tokio::time::sleep(wait_duration()).await;\n\n        match cache.get::<SerializedResponse>(key).await {\n            // Value has been retrieved from cache, so return it\n            Ok(Some(resp @ SerializedResponse::Finished { .. })) => return Ok(Some(resp)),\n\n            // Request setting the lock has not been resolved yet, so wait a little and loop again\n            Ok(Some(SerializedResponse::Start)) => {\n                if total_delay_duration > expiry_starting() {\n                    return Ok(None);\n                }\n            }\n\n            // Start value has expired\n            Ok(None) => return Ok(None),\n\n            Err(e) => return Err(Error::database(format_args!(\"{e:?}\"))),\n        }\n    }\n}\n\n/// Resolve the service and cache the result assuming the response is successful\nasync fn resolve_and_cache_response<S>(\n    cache: &Cache,\n    key: &IdempotencyKey,\n    service: S,\n    request: Request,\n) -> Response\nwhere\n    S: Service<Request, Error = Infallible> + Clone + Send + 'static,\n    S::Response: IntoResponse,\n    S::Future: Send + 'static,\n{\n    let (parts, body) = resolve_service(service, request).await.into_parts();\n\n    // If a 2xx response, cache the actual response\n    if parts.status.is_success() {\n        // TODO: Don't skip over Err value\n        let bytes = body.collect().await.ok().map(|c| c.to_bytes());\n\n        let resp = SerializedResponse::Finished {\n            code: parts.status.into(),\n            headers: Some(\n                parts\n                    .headers\n                    .iter()\n                    .map(|(k, v)| (k.as_str().to_owned(), v.as_bytes().to_owned()))\n                    .collect(),\n            ),\n            body: bytes.clone().map(|b| b.to_vec()),\n        };\n\n        if cache.set(key, &resp, expiry_default()).await.is_err() {\n            return StatusCode::INTERNAL_SERVER_ERROR.into_response();\n        }\n\n        // Assumes None to be an empty byte array\n        let bytes = bytes.unwrap_or_default();\n        Response::from_parts(parts, Body::from(bytes)).into_response()\n    }\n    // If any other status, unset the start lock and return the response\n    else {\n        if cache.delete(key).await.is_err() {\n            return StatusCode::INTERNAL_SERVER_ERROR.into_response();\n        }\n\n        Response::from_parts(parts, body).into_response()\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use std::sync::Arc;\n\n    use axum::{extract::State, routing::post, serve, Router};\n    use http::StatusCode;\n    use reqwest::Client;\n    use tokio::{net::TcpListener, sync::Mutex, task::JoinHandle};\n    use tower::ServiceBuilder;\n\n    use super::IdempotencyService;\n    use crate::core::{\n        cache,\n        security::generate_org_token,\n        types::{BaseId, OrganizationId},\n    };\n\n    #[derive(Clone)]\n    struct TestAppState {\n        count: Arc<Mutex<u16>>,\n        wait: Option<std::time::Duration>,\n    }\n\n    /// Starts a basic Axum server with one endpoint which counts the number of times the endpoint\n    /// has been polled from. This will be nested in the [`IdempotencyService`] such that, providing\n    /// a key may result in the count not increasing and a prior result being displayed.\n    ///\n    /// This function takes a variable length of time to complete with the delay input used for\n    /// testing the start lock.\n    ///\n    /// This function will return a join handle to that server, its URL and an [`Arc<Mutex<usize>>`]\n    /// that points to the count of the server such that its internal state may be monitored.\n    async fn start_service(\n        wait: Option<std::time::Du<|fim_middle|>", "completion": "if let Some(Ok(auth)) = parts.headers.get(\"Authorization\").map(|v| v.to_str()) {\n        auth\n    } else {\n        // No auth token -- pass off to service and do not cache\n        return None;\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/idempotency.rs", "node_type": "if_expression", "line_range": [261, 266]}
{"prompt": "<|fim_prefix|>   .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    fn get_unbranded_headers(msg_id: &str, signature: &str) -> HeaderMap {\n        let mut headers = HeaderMap::new();\n        headers.insert(UNBRANDED_MSG_ID_KEY, msg_id.parse().unwrap());\n        headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, signature.parse().unwrap());\n        headers.insert(\n            UNBRANDED_MSG_TIMESTAMP_KEY,\n            OffsetDateTime::now_utc()\n                .unix_timestamp()\n                .to_string()\n                .parse()\n                .unwrap(),\n        );\n        headers\n    }\n\n    #[test]\n    fn test_sign() {\n        let wh = Webhook::new(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\").unwrap();\n        assert_eq!(\n            \"v1,tZ1I4/hDygAJgO5TYxiSd6Sd0kDW6hPenDe+bTa3Kkw=\".to_owned(),\n            wh.sign(\n                \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\",\n                1649367553,\n                br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#\n            )\n            .unwrap()\n        );\n    }\n\n    #[test]\n    fn test_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            wh.verify(payload, &headers).unwrap();\n        }\n    }\n\n    #[test]\n    fn test_no_verify() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = \"v1,R3PTzyfHASBKHH98a7yexTwaJ4yNIcGhFQc1yuN+BPU=\".to_owned();\n        for headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_partial_signature() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        // Just `v1,`\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = format!(\n                \"{},\",\n                signature.split(',').collect::<Vec<&str>>().first().unwrap()\n            );\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n\n        // Non-empty but still partial signature (first few bytes)\n        for mut headers in [\n            get_svix_headers(msg_id, &signature),\n            get_unbranded_headers(msg_id, &signature),\n        ] {\n            let partial = &signature[0..8];\n            headers.insert(SVIX_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            headers.insert(UNBRANDED_MSG_SIGNATURE_KEY, partial.parse().unwrap());\n            assert!(wh.verify(payload, &headers).is_err());\n        }\n    }\n\n    #[test]\n    fn test_verify_incorrect_timestamp() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        // Checks that timestamps that are in the future or too old are rejected by\n        // `verify` but okay for `verify_ignoring_timestamp`.\n        for ts in [\n            OffsetDateTime::now_utc().unix_timestamp() - (super::TOLERANCE_IN_SECONDS + 1),\n            OffsetDateTime::now_utc().unix_timestamp() + (super::TOLERANCE_IN_SECONDS + 1),\n        ] {\n            let signature = wh.sign(msg_id, ts, payload).unwrap();\n            let mut headers = get_svix_headers(msg_id, &signature);\n            headers.insert(\n                super::SVIX_MSG_TIMESTAMP_KEY,\n                ts.to_string().parse().unwrap(),\n            );\n\n            assert!(wh.verify(payload, &headers,).is_err());\n            // Timestamp tolerance is not considered in this case.\n            assert!(wh.verify_ignoring_timestamp(payload, &headers,).is_ok());\n        }\n\n        let ts = OffsetDateTime::now_utc().unix_timestamp();\n        let signature = wh.sign(msg_id, ts, payload).unwrap();\n        let mut headers = get_svix_headers(msg_id, &signature);\n        headers.insert(\n            super::SVIX_MSG_TIMESTAMP_KEY,\n            // Timestamp mismatch!\n            (ts + 1).to_string().parse().unwrap(),\n        );\n\n        // Both versions should reject the timestamp if it's not the same one used to\n        // produce the signature.\n        assert!(wh.verify(payload, &headers,).is_err());\n        assert!(wh.verify_ignoring_timestamp(payload, &headers,).is_err());\n    }\n\n    #[test]\n    fn test_verify_with_multiple_signatures() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n\n        let multi_sig = format!(\n            \"{} {} {} {}\",\n            \"v1,tFtCZ5RDCPxzWQRWXWPgrCgE2frDBe9gjpbWQxnVfsQ=\",\n            \"v1,Mm7xgUVICxZfQ3bgf0h0Dof65L/IFx+PnZvnDWPCX6Q=\",\n            signature,\n            \"v1,9DfC1c3eeOrXB6w/5dIDydLNQaEyww5KalE5jLBZucE=\",\n        );\n\n        let headers = get_svix_headers(msg_id, &multi_sig);\n\n        wh.verify(payload, &headers).unwrap();\n    }\n\n    #[test]\n    fn test_no_verify_with_multiple_signatures() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let missing_sig = format!(\n            \"{} {} {}\",\n            \"v1,tFtCZ5RDCPxzWQRWXWPgrCgE2frDBe9gjpbWQxnVfsQ=\",\n            \"v1,Mm7xgUVICxZfQ3bgf0h0Dof65L/IFx+PnZvnDWPCX6Q=\",\n            \"v1,9DfC1c3eeOrXB6w/5dIDydLNQaEyww5KalE5jLBZucE=\",\n        );\n\n        let headers = get_svix_headers(msg_id, &missing_sig);\n\n        assert!(wh.verify(payload, &headers).is_err());\n    }\n\n    #[test]\n    fn test_missing_headers() {\n        let secret = \"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_owned();\n        let msg_id = \"msg_27UH4WbU6Z5A5EzD8u03UvzRbpk\";\n        let payload = br#\"{\"email\":\"test@example.com\",\"username\":\"test_user\"}\"#;\n        let wh = Webhook::new(&secret).unwrap();\n\n        let signature = wh\n            .sign(msg_id, OffsetDateTime::now_utc().unix_timestamp(), payload)\n            .unwrap();\n        for (mut hdr_map, hdrs) in [\n            (\n                get_svix_headers(msg_id, &signature),\n                [\n                    SVIX_MSG_ID_KEY,\n                    SVIX_MSG_SIGNATURE_KEY,\n                    SVIX_MSG_TIMESTAMP_KEY,\n                ],\n            ),\n            (\n                get_unbranded_headers(msg_id, &signature),\n                [\n                    UNBRANDED_MSG_ID_KEY,\n                    UNBRANDED_MSG_SIGNATURE_KEY,\n                    UNBRANDED_MSG_TIMESTAMP_KEY,\n                ],\n            ),\n        ] {\n            f<|fim_suffix|>        }\n    }\n}\n<|fim_middle|>", "completion": "for hdr in hdrs {\n                hdr_map.remove(hdr);\n                assert!(wh.verify(payload, &hdr_map).is_err());\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/webhooks.rs", "node_type": "for_expression", "line_range": [443, 446]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct PandaDocConfigOut {}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl PandaDocConfigOut {\n    pub fn new() -> Self {\n        Self {}\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/panda_doc_config_out.rs", "node_type": "impl_item", "line_range": [7, 11]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::message_attempt_failed_data::MessageAttemptFailedData;\n\n/// Sent when a message delivery has failed (all of the retry attempts have been\n/// exhausted) as a \"ingest.message.attempt.exhausted\" type, after it's failed\n/// four times as a \"ingest.message.attempt.failing\" event, or after it's\n/// recovered as a \"ingest.message.attempt.recovered\" event.\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct IngestMessageAttemptFailingEventData {\n    /// The Endpoint's ID.\n    #[serde(rename = \"endpointId\")]\n    pub endpoint_id: String,\n\n    #[serde(rename = \"lastAttempt\")]\n    pub last_attempt: MessageAttemptFailedData,\n\n    /// The Message's UID.\n    #[serde(rename = \"msgEventId\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub msg_event_id: Option<String>,\n\n    /// The Message's ID.\n    #[serde(rename = \"msgId\")]\n    pub msg_id: String,\n\n    /// The Source's ID.\n    #[serde(rename = \"sourceId\")]\n    pub source_id: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl IngestMessageAttemptFailingEventData {\n    pub fn new(\n        endpoint_id: String,\n        last_attempt: MessageAttemptFailedData,\n        msg_id: String,\n        source_id: String,\n    ) -> Self {\n        Self {\n            endpoint_id,\n            last_attempt,\n            msg_event_id: None,\n            msg_id,\n            source_id,\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/ingest_message_attempt_failing_event_data.rs", "node_type": "impl_item", "line_range": [33, 48]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct EventTypeListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n\n    /// When `true` archived (deleted but not expunged) items are included in\n    /// the response.\n    pub include_archived: Option<bool>,\n\n    /// When `true` the full item (including the schema) is included in the\n    /// response.\n    pub with_content: Option<bool>,\n}\n\n#[derive(Default)]\npub struct EventTypeCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EventTypeImportOpenapiOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct EventTypeDeleteOptions {\n    /// By default event types are archived when \"deleted\". Passing this to\n    /// `true` deletes them entirely.\n    pub expunge: Option<bool>,\n}\n\npub struct EventType<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> EventType<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// Return the list of event types.\n    pub async fn list(\n        &self,\n        options: Option<EventTypeListOptions>,\n    ) -> Result<ListResponseEventTypeOut> {\n        let EventTypeListOptions {\n            limit,\n            iterator,\n            order,\n            include_archived,\n            with_content,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/event-type\")\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .with_optional_query_param(\"include_archived\", include_archived)\n            .with_optional_query_param(\"with_content\", with_content)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create new or unarchive existing event type.\n    ///\n    /// Unarchiving an event type will allow endpoints to filter on it and\n    /// messages to be sent with it. Endpoints filtering on the event type\n    /// before archival will continue to filter on it. This operation does\n    /// not preserve the description and schemas.\n    pub async fn create(\n        &self,\n        event_type_in: EventTypeIn,\n        options: Option<EventTypeCreateOptions>,\n    ) -> Result<EventTypeOut> {\n        let EventTypeCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/event-type\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(event_type_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Given an OpenAPI spec, create new or update existing event types.\n    /// If an existing `archived` event type is updated, it will be unarchived.\n    ///\n    /// The importer will convert all webhooks found in the either the\n    /// `webhooks` or `x-webhooks` top-level.\n    pub async fn import_openapi(\n        &self,\n        event_type_import_open_api_in: EventTypeImportOpenApiIn,\n        options: Option<EventTypeImportOpenapiOptions>,\n    ) -> Result<EventTypeImportOpenApiOut> {\n        let EventTypeImportOpenapiOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/event-type/import/openapi\")\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(event_type_import_open_api_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an event type.\n    pub async fn get(&self, event_type_name: String) -> Result<EventTypeOut> {\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/event-type/{event_type_name}\")\n            .with_path_param(\"event_type_name\", event_type_name)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Update an event type.\n    pub async fn update(\n        &self,\n        event_type_name: String,\n        event_type_update: EventTypeUpdate,\n    ) -> Result<EventTypeOut> {\n        crate::request::Request::new(http1::Method::PUT, \"/api/v1/event-type/{event_type_name}\")\n            .with_path_param(\"event_type_name\", event_type_name)\n            .with_body_param(event_type_update)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Archive an event type.\n    ///\n    /// Endpoints already configured to filter on an event type will continue to\n    /// do so after archival. However, new messages can not be sent with it\n    /// and endpoints can not filter on it. An event type can be unarchived\n    /// with the [create operation](#operation/\n    /// create_event_type_api_v1_event_type__post).\n    pub async fn delete(\n        &self,\n        event_type_name: String,\n        options: Option<EventTypeDeleteOptions>,\n    ) -> Result<()> {\n        let EventTypeDeleteOptions { expunge } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/event-type/{event_type_name}\",\n        )\n        .with_path_param(\"event_type_name\", event_type_name)\n        .with_optional_query_param(\"expunge\", expunge)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Partially update an event type.\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub async fn patch(\n        &self,\n        event_type_name: String,\n        event_type_patch: EventTypePatch,\n    ) -> Result<EventTypeOut> {\n        crate::request::Request::new(http1::Method::PATCH, \"/api/v1/event-type/{event_type_name}\")\n            .with_path_param(\"event_type_name\", event_type_name)\n            .with_body_param(event_type_patch)\n            .execute(self.cfg)\n            .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/event_type.rs", "node_type": "function_item", "line_range": [159, 169]}
{"prompt": "<|fim_prefix|>use anyhow::Context;\nuse clap::Args;\nuse rand::{rngs::StdRng, seq::SliceRandom, SeedableRng};\nuse serde::{Deserialize, Serialize};\nuse serde_json::json;\nuse svix::api::*;\n\n#[derive(Args)]\nstruct SeedOptions {\n    /// Will clear out all the applications and event types\n    #[arg(long, default_value = \"false\")]\n    pub reset: bool,\n\n    /// The number of endpoints to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=10) , default_value = \"2\")]\n    pub endpoint_count: u8,\n\n    /// The number of messages to create (0-10)\n    #[arg(long, value_parser = clap::value_parser!(u8).range(..=100) , default_value = \"10\")]\n    pub message_count: u8,\n}\n\n#[derive(Args)]\npub struct SeedArgs {\n    #[clap(flatten)]\n    options: SeedOptions,\n}\n\n#[derive(Debug, Serialize, Default)]\n#[serde(rename_all = \"camelCase\")]\nstruct SeedOut {\n    application: ApplicationOut,\n    endpoints: Vec<String>,\n    event_types: Vec<String>,\n    messages: Vec<String>,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct PlayTokenOut {\n    token: String,\n}\n\nconst PLAY_TOKEN_GENERATE_URL: &str = \"https://api.play.svix.com/api/v1/token/generate/\";\nconst USER_EVENT_TYPES: [&str; 4] = [\"signup\", \"signin\", \"signout\", \"deleted\"];\n\npub async fn exec(\n    client: &Svix,\n    args: SeedArgs,\n    color_mode: colored_json::ColorMode,\n) -> anyhow::Result<()> {\n    let mut seed_out = SeedOut {\n        ..Default::default()\n    };\n\n    if args.options.reset {\n        let confirmation = dialoguer::Confirm::new()\n         .with_prompt(\"This will clear out all the applications and event types! Do you want to continue? \")\n         .interact()\n         .unwrap_or(false);\n\n        if confirmation {\n            reset_application(client).await?;\n            reset_event_type(client).await?;\n        } else {\n            return Ok(());\n        }\n    }\n\n    let application_in = ApplicationIn {\n        name: \"Test application\".to_string(),\n        ..Default::default()\n    };\n    let application_out = client.application().create(application_in, None).await?;\n\n    seed_out.application = application_out.clone();\n\n    let app_id = application_out.id;\n\n    let mut handles = Vec::new();\n\n    for _ in 0..args.options.endpoint_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(async move {\n            create_endpoint(client, app_id).await\n        }))\n    }\n\n    for h in handles {\n        let eo = h.await??;\n        seed_out.endpoints.push(eo.url);\n    }\n\n    for typ in USER_EVENT_TYPES {\n        let event_type_in = EventTypeIn {\n            name: format!(\"user.{typ}\"),\n            description: \"\".to_string(),\n            schemas: Some(json!(schema_example())),\n            ..Default::default()\n        };\n        let res = client.event_type().create(event_type_in, None).await;\n\n        match res {\n            Ok(event_type_out) => {\n                seed_out.event_types.push(event_type_out.name);\n            }\n            Err(err) => {\n                eprintln!(\"Failed to create event type: {err}\");\n                continue;\n            }\n        }\n    }\n    let mut handles = Vec::new();\n\n    for _ in 0..args.options.message_count {\n        let client = client.clone();\n        let app_id = app_id.clone();\n\n        handles.push(tokio::spawn(\n            async move { create_message(client, app_id).await },\n        ))\n    }\n\n    for h in handles {\n        let message_out = h.await??;\n        seed_out.messages.push(message_out.id);\n    }\n\n    let summary = format!(\n        \"Seeded {} endpoints, {} event types, {} messages to application \\\"{}\\\"\",\n        seed_out.endpoints.len(),\n        seed_out.event_types.len(),\n        seed_out.messages.len(),\n        seed_out.application.name\n    );\n\n    crate::json::print_json_output(&seed_out, color_mode)?;\n    println!(\"{summary}\");\n\n    Ok(())\n}\n\nasync fn create_endpoint(client: Svix, app_id: String) -> anyhow::Result<EndpointOut> {\n    <|fim_suffix|>\n\n    let resp = req_client\n        .post(PLAY_TOKEN_GENERATE_URL)\n        .send()\n        .await?\n        .json::<PlayTokenOut>()\n        .await\n        .context(\"Failed to get token from public api\")?;\n\n    let endpoint_in = EndpointIn {\n        url: format!(\"https://play.svix.com/in/{}/\", resp.token),\n        ..Default::default()\n    };\n    let endpoint_out = client.endpoint().create(app_id, endpoint_in, None).await?;\n    Ok(endpoint_out)\n}\n\nasync fn create_message(client: Svix, app_id: String) -> anyhow::Result<MessageOut> {\n    let mut rng = StdRng::from_entropy();\n\n    let event_type = USER_EVENT_TYPES\n        .choose(&mut rng)\n        .context(\"Couldn't pick a random event type while creating a message\")?;\n\n    let message_in = MessageIn {\n        event_type: event_type.to_string(),\n        payload: json!({\n            \"userId\": \"41376126-35bf-4eda-81ef-83d741b0e026\",\n            \"firstName\": \"John\",\n            \"lastName\": \"Doe\",\n        }),\n        ..Default::default()\n    };\n\n    let message_out = client.message().create(app_id, message_in, None).await?;\n    Ok(message_out)\n}\n\nasync fn reset_application(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.application().list(None).await?;\n    let mut handles = Vec::new();\n\n    for app_out in resp.data {\n        let client = client.clone();\n\n        handles.push(tokio::spawn(async move {\n            if let Err(err) = client.application().delete(app_out.id.clone()).await {\n                eprintln!(\"Failed to delete application {}: {}\", app_out.id, err);\n            }\n        }));\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\nasync fn reset_event_type(client: &Svix) -> anyhow::Result<()> {\n    let resp = client.event_type().list(None).await?;\n    let mut handles = Vec::new();\n\n    for event_type_out in resp.data {\n        let client = client.clone();\n\n        let handle = tokio::spawn(async move {\n            let _ = client\n                .event_type()\n                .delete(\n                    event_type_out.name,\n                    Some(EventTypeDeleteOptions {\n                        expunge: Some(true),\n                    }),\n                )\n                .await;\n        });\n        handles.push(handle);\n    }\n\n    for h in handles {\n        let _ = h.await;\n    }\n\n    Ok(())\n}\n\npub fn schema_example() -> serde_json::Value {\n    serde_json::json!({\n        \"1\": {\n            \"description\": \"A user signed up\",\n            \"properties\": {\n                \"userId\": {\n                    \"description\": \"The user id\",\n                    \"type\": \"string\"\n                },\n                \"firstName\": {\n                    \"description\": \"User first name\",\n                    \"type\": \"string\"\n                },\n                \"lastName\": {\n                    \"description\": \"User last name\",\n                    \"type\": \"string\"\n                }\n            },\n            \"required\": [\n                \"userId\",\n                \"firstName\",\n                \"lastName\",\n            ],\n            \"title\": \"User signed up Event\",\n            \"type\": \"object\",\n        }\n    })\n}\n<|fim_middle|>", "completion": "let req_client = reqwest::Client::new();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/seed.rs", "node_type": "let_declaration", "line_range": [146, 146]}
{"prompt": "<|fim_prefix|>gger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n#[derive(Parser)]\npub struct Args {\n    #[arg(long, env = \"SVIX_BRIDGE_CFG_FILE\", help = \"Path to the config file.\")]\n    cfg_file: Option<PathBuf>,\n    #[arg(\n        long,\n        env = \"SVIX_BRIDGE_CFG\",\n        help = \"Config data as a string (instead of a file on disk).\",\n        conflicts_with = \"cfg_file\"\n    )]\n    cfg: Option<String>,\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let mut config_search_paths = vec![];\n\n    if let Some(fp) = args.cfg_file {\n        config_search_paths.push(fp)\n    } else {\n        for name in [\"svix-bridge.yaml\", \"svix-bridge.yml\", \"svix-bridge.json\"] {\n            config_search_paths.push(std::env::current_dir().expect(\"current dir\").join(name));\n        }\n    }\n\n    // Clap will ensure we have only one or the other (cfg and cfg_file can't be specified together).\n    let cfg_source = match args.cfg {\n        Some(cfg_source) => cfg_source,\n        None => {\n            let fp = config_search_paths\n                .into_iter()\n                .find(|x| x.exists())\n                .expect(\"config file path\");\n            std::fs::read_to_string(&fp).map_err(|e| {\n                let p = fp.into_os_string().into_string().expect(\"config file path\");\n                Error::other(format!(\"Failed to read {p}: {e}\"))\n            })\n        }?,\n    };\n\n    let vars = std::env::vars().collect();\n    let cfg = Config::from_src(&cfg_source, Some(vars).as_ref())?;\n    setup_tracing(&cfg);\n    let _metrics = setup_metrics(&cfg);\n    tracing::info!(\"starting\");\n\n    tokio::spawn(async move {\n        let mut interval = tokio::time::interval(Duration::from_secs(15));\n        let metrics = CommonMetrics::new(&opentelemetry::global::meter(\"svix.com\"));\n        match get_allocator_stat_mibs() {\n            Ok(mibs) => {\n                tracing::debug!(\"Common Metrics Collection: Started\");\n\n                <|fim_suffix|>\n            }\n            Err(e) => tracing::error!(\"Unable to get allocator stats mibs: {e}\"),\n        }\n    });\n\n    let (xform_tx, mut xform_rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n\n    // XXX: this is a bit nasty, but might be okay to start.\n    // The nested spawns are needed to make sure we can saturate the\n    // threadpool (otherwise we'd run each job serially).\n    //\n    // Another approach would be to do what og-ingester did: give each plugin a clone of the\n    // `TpHandle`, but this would likely mean moving the runtime module over to the `-types` crate.\n    // I'd rather not do this, mostly to help keep things more unit test friendly; channels can\n    // help keep the coupling more loose, with less stateful baggage.\n    // Starting with this just to keep the JS executor stuff here in the binary.\n    tokio::spawn(async move {\n        tracing::info!(\n            \"Starting JS Transformation Workers: {}\",\n            cfg.transformation_worker_count\n        );\n\n        deno_core::JsRuntime::init_platform(None, false);\n        let pooler: runtime::JsPooler = runtime::JsPooler::new(cfg.transformation_worker_count);\n\n        while let Some(TransformerJob {\n            input,\n            script,\n            callback_tx,\n        }) = xform_rx.recv().await\n        {\n            let tp = pooler.clone();\n            tokio::spawn(async move {\n                let out = tp.run_script(input, script).await;\n                // FIXME: seeing this Err case come up during load testing.\n                //   Seems like we shouldn't be hitting this so easily while the process is not terminating.\n                //   Regularly there are group error log lines that show up right at the end of an\n                //   `oha` run, POSTing to receivers. Need to investigate why.\n                if callback_tx\n                    .send(out.map_err(|e| tracing::error!(\"{:?}\", e)))\n                    .is_err()\n                {\n                    // If the callback fails, the plugin is likely unwinding/dropping.\n                    // Not a whole lot we can do about that.\n                    tracing::error!(\"failed to send js output back to caller\");\n                }\n            });\n        }\n    });\n\n    let mut senders = Vec::with_capacity(cfg.senders.len());\n    for sc in cfg.senders {\n        let mut sender: Box<dyn SenderInput> = sc.try_into().map_err(Error::other)?;\n        sender.set_transformer(Some(xform_tx.clone()));\n        senders.push(sender);\n    }\n    if senders.is_empty() {\n        tracing::warn!(\"No senders configured.\")\n    }\n    let senders_fut = supervise_senders(senders);\n\n    if cfg.receivers.is_empty() {\n        tracing::warn!(\"No receivers configured.\")\n    }\n    let (webhook_receivers, poller_receivers): (\n        Vec<WebhookReceiverConfig>,\n        Vec<PollerReceiverConfig>,\n    ) = cfg\n        .receivers\n        .into_iter()\n        .partition_map(|either| match either {\n            EitherReceiver::Webhook(x) => Either::Left(x),\n            EitherReceiver::Poller(y) => Either::Right(y),\n        });\n\n    let webhook_receivers_fut =\n        webhook_receiver::run(cfg.http_listen_address, webhook_receivers, xform_tx.clone());\n\n    let mut pollers: Vec<Box<dyn PollerInput>> = Vec::with_capacity(poller_receivers.len());\n    for poller_cfg in poller_receivers {\n        pollers.push(poller_cfg.into_poller_input(xform_tx.clone()).await?);\n    }\n\n    let poller_receivers_fut = supervise_pollers(pollers);\n\n    match tokio::try_join!(senders_fut, webhook_receivers_fut, poller_receivers_fut) {\n        Ok(_) => tracing::error!(\"unexpectedly exiting\"),\n        Err(e) => tracing::error!(\"unexpectedly exiting: {}\", e),\n    }\n\n    tracing::info!(\"exiting...\");\n    Ok(())\n}\n<|fim_middle|>", "completion": "loop {\n                    interval.tick().await;\n\n                    if let Ok(Some((allocated, resident))) = get_allocator_stats(true, &mibs) {\n                        metrics.record_mem_allocated(allocated as _);\n                        metrics.record_mem_resident(resident as _);\n                    }\n                }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "loop_expression", "line_range": [289, 296]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{string::FromUtf8Error, time::Duration};\n\nuse ::redis::RedisError;\nuse axum::async_trait;\nuse enum_dispatch::enum_dispatch;\nuse serde::{de::DeserializeOwned, Serialize};\n\nuse crate::core::retry::run_with_retries;\n\npub mod memory;\npub mod none;\npub mod redis;\n\n/// Errors internal to the cache\n#[derive(thiserror::Error, Debug)]\npub enum Error {\n    #[error(\"error deserializing Redis value\")]\n    Deserialization(#[from] serde_json::error::Error),\n\n    #[error(\"error deserializing Redis value\")]\n    DeserializationOther,\n\n    #[error(\"error deserializing byte array\")]\n    DeserializationBytes(#[from] FromUtf8Error),\n\n    #[error(\"Redis pool error: {0}\")]\n    Pool(#[from] bb8::RunError<RedisError>),\n\n    #[error(\"Redis database error: {0}\")]\n    Database(#[from] RedisError),\n\n    #[error(\"input error: {0}\")]\n    Input(String),\n}\ntype Result<T> = std::result::Result<T, Error>;\n\n/// A valid key value for the cache -- usually just a wrapper around a [`String`]\npub trait CacheKey: AsRef<str> + Send + Sync {}\n\n/// A cache key for setting/getting raw [`String`]s -- this is just a marker\n/// trait added in the `string_kv_def macro`\npub trait StringCacheKey: AsRef<str> + Send + Sync {}\n\n/// Any (de)serializable structure usable as a value in the cache -- it is associated with a\n/// given key type to ensure type checking on creation or reading of values from the cache\npub trait CacheValue: DeserializeOwned + Serialize + Send + Sync {\n    type Key: CacheKey;\n}\n\npub trait StringCacheValue: ToString + TryFrom<String> + Send + Sync {\n    type Key: CacheKey;\n}\n\n/// An inner macro which defines everything common to the below macro. Not really meant to be used,\n/// but it can't be made private or else it couldn't be used in the outer macro.\nmacro_rules! kv_def_inner {\n    ($key_id:ident, $val_struct:ident) => {\n        #[derive(Clone, Debug)]\n        pub struct $key_id(String);\n\n        impl AsRef<str> for $key_id {\n            fn as_ref(&self) -> &str {\n                &self.0\n            }\n        }\n\n        impl CacheValue for $val_struct {\n            type Key = $key_id;\n        }\n    };\n}\np<|fim_suffix|>\n/// A macro that creates a [`CacheKey`] and ties it to any value that implements\n/// [`DeserializeOwned`] and [`Serialize`]\nmacro_rules! kv_def {\n    ($key_id:ident, $val_struct:ident) => {\n        crate::core::cache::kv_def_inner!($key_id, $val_struct);\n\n        impl CacheKey for $key_id {}\n    };\n}\npub(crate) use kv_def;\n\n/// An inner macro which defines everything common to the below macro. Not really meant to be used,\n/// but it can't be made private or else it couldn't be used in the outer macro.\n#[allow(unused_macros)]\nmacro_rules! string_kv_def_inner {\n    ($key_id:ident) => {\n        #[derive(Clone, Debug)]\n        pub struct $key_id(String);\n\n        impl AsRef<str> for $key_id {\n            fn as_ref(&self) -> &str {\n                &self.0\n            }\n        }\n    };\n}\n#[allow(unused_imports)]\npub(crate) use string_kv_def_inner;\n\n#[cfg(test)]\nmacro_rules! string_kv_def {\n    ($key_id:ident) => {\n        crate::core::cache::string_kv_def_inner!($key_id);\n\n        impl crate::core::cache::StringCacheKey for $key_id {}\n        // so key can work w/ other methods, like delete:\n        impl crate::core::cache::CacheKey for $key_id {}\n    };\n}\n#[cfg(test)]\npub(crate) use string_kv_def;\n\n#[derive(Clone)]\n#[enum_dispatch]\npub enum Cache {\n    Memory(memory::MemoryCache),\n    Redis(redis::RedisCache),\n    None(none::NoCache),\n}\n\nimpl Cache {\n    pub fn is_none(&self) -> bool {\n        matches!(*self, Cache::None(none::NoCache))\n    }\n}\n\nconst RETRY_SCHEDULE: &[Duration] = &[\n    Duration::from_millis(10),\n    Duration::from_millis(20),\n    Duration::from_millis(40),\n];\n\n#[async_trait]\n#[enum_dispatch(Cache)]\npub trait CacheBehavior: Sync + Send {\n    fn should_retry(&self, e: &Error) -> bool;\n    async fn get<T: CacheValue>(&self, key: &T::Key) -> Result<Option<T>> {\n        run_with_retries(\n            || async move {\n                self.get_raw(key.as_ref().as_bytes())\n                    .await?\n                    .map(|x| {\n                        String::from_utf8(x)\n                            .map_err(|e| e.into())\n                            .and_then(|json| serde_json::from_str(&json).map_err(|e| e.into()))\n                    })\n                    .transpose()\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn get_raw(&self, key: &[u8]) -> Result<Option<Vec<u8>>>;\n\n    async fn get_string<T: StringCacheKey>(&self, key: &T) -> Result<Option<String>> {\n        run_with_retries(\n            || async move {\n                self.get_raw(key.as_ref().as_bytes())\n                    .await?\n                    .map(|x| String::from_utf8(x).map_err(|e| e.into()))\n                    .transpose()\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set<T: CacheValue>(&self, key: &T::Key, value: &T, ttl: Duration) -> Result<()> {\n        run_with_retries(\n            || async move {\n                self.set_raw(\n                    key.as_ref().as_bytes(),\n                    serde_json::to_string(value)?.as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set_raw(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<()>;\n\n    async fn set_string<T: StringCacheKey>(\n        &self,\n        key: &T,\n        value: &str,\n        ttl: Duration,\n    ) -> Result<()> {\n        run_with_retries(\n            || async move {\n                self.set_raw(key.as_ref().as_bytes(), value.to_string().as_bytes(), ttl)\n                    .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn delete<T: CacheKey>(&self, key: &T) -> Result<()>;\n\n    async fn set_if_not_exists<T: CacheValue>(\n        &self,\n        key: &T::Key,\n        value: &T,\n        ttl: Duration,\n    ) -> Result<bool> {\n        run_with_retries(\n            || async move {\n                self.set_raw_if_not_exists(\n                    key.as_ref().as_bytes(),\n                    serde_json::to_string(value)?.as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    async fn set_raw_if_not_exists(&self, key: &[u8], value: &[u8], ttl: Duration) -> Result<bool>;\n\n    async fn set_string_if_not_exists<T: StringCacheKey>(\n        &self,\n        key: &T,\n        value: &str,\n        ttl: Duration,\n    ) -> Result<bool> {\n        run_with_retries(\n            || async move {\n                self.set_raw_if_not_exists(\n                    key.as_ref().as_bytes(),\n                    value.to_string().as_bytes(),\n                    ttl,\n                )\n                .await\n            },\n            |e| self.should_retry(e),\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub(crate) use kv_def_inner;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/cache/mod.rs", "node_type": "use_declaration", "line_range": [75, 75]}
{"prompt": "<|fim_prefix|>\n            }\n        }\n\n        impl From<$name_uid> for $name_id_or_uid {\n            fn from(v: $name_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        // Id or uid\n        #[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\n        pub struct $name_id_or_uid(pub String);\n\n        common_jsonschema_impl!(\n            $name_id_or_uid,\n            $crate::core::types::StringSchema::schema_for_uids($key_prefix)\n        );\n\n        impl From<$name_id_or_uid> for $name_uid {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        impl From<$name_id_or_uid> for $name_id {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self(v.0)\n            }\n        }\n\n        impl From<$name_id_or_uid> for sea_orm::Value {\n            fn from(v: $name_id_or_uid) -> Self {\n                Self::String(Some(Box::new(v.0)))\n            }\n        }\n\n        impl Validate for $name_id_or_uid {\n            fn validate(&self) -> Result<(), validator::ValidationErrors> {\n                validate_limited_str(&self.0)\n            }\n        }\n    };\n}\n\ncreate_id_type!(OrganizationId, \"org_\");\ncreate_id_type!(\n    MessageAttemptId,\n    \"atmpt_\",\n    crate::core::types::StringSchema {\n        string_validation: None,\n        example: Some(\"atmpt_1srOrx2ZWZBpBUvZwXKQmoEYga2\".to_string()),\n    }\n);\ncreate_id_type!(MessageEndpointId, \"msgep_\");\ncreate_id_type!(EventTypeId, \"evtype_\");\ncreate_id_type!(QueueBackgroundTaskId, \"qtask_\");\n\ncreate_all_id_types!(ApplicationId, ApplicationUid, ApplicationIdOrUid, \"app_\");\ncreate_all_id_types!(EndpointId, EndpointUid, EndpointIdOrUid, \"ep_\");\ncreate_all_id_types!(MessageId, MessageUid, MessageIdOrUid, \"msg_\");\n\nstring_wrapper!(\n    EventTypeName,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            max_length: Some(256),\n            min_length: None,\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"user.signup\".to_string()),\n    }\n);\n\nimpl Validate for EventTypeName {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        validate_limited_str(&self.0)\n    }\n}\n\nstring_wrapper!(\n    EventChannel,\n    crate::core::types::StringSchema {\n        string_validation: Some(schemars::schema::StringValidation {\n            max_length: Some(128),\n            min_length: None,\n            pattern: Some(r\"^[a-zA-Z0-9\\-_.]+$\".to_string()),\n        }),\n        example: Some(\"project_1337\".to_string()),\n    }\n);\n\nimpl Validate for EventChannel {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        validate_limited_str(&self.0)\n    }\n}\n\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventChannelSet(pub HashSet<EventChannel>);\njson_wrapper!(EventChannelSet);\n\nimpl Validate for EventChannelSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventTypeNameSet(pub HashSet<EventTypeName>);\njson_wrapper!(EventTypeNameSet);\n\nimpl Validate for EventTypeNameSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\npub struct ExpiringSigningKeys(pub Vec<ExpiringSigningKey>);\njson_wrapper!(ExpiringSigningKeys);\n\nimpl ExpiringSigningKeys {\n    pub const MAX_OLD_KEYS: usize = 10;\n    pub const OLD_KEY_EXPIRY_HOURS: i64 = 24;\n}\n\n/// The type of encryption key\n#[repr(u8)]\n#[derive(Clone, Debug, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum EndpointSecretType {\n    Hmac256 = 1,\n    Ed25519 = 2,\n    // Reserved = 3,\n}\n\nimpl EndpointSecretType {\n    pub const fn secret_prefix(&self) -> &'static str {\n        m<|fim_suffix|>    }\n\n    pub const fn public_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whpk_\",\n        }\n    }\n}\n\n/// Properties of the encryption key\n#[derive(Clone, Debug, PartialEq, Eq)]\nstruct EndpointSecretMarker {\n    type_: EndpointSecretType,\n    encrypted: bool,\n}\n\nimpl EndpointSecretMarker {\n    const ENCRYPTED_FLAG: u8 = 0b1000_0000;\n\n    fn from_u8(v: u8) -> crate::error::Result<Self> {\n        let encrypted = (v & Self::ENCRYPTED_FLAG) != 0;\n        let v = v & !Self::ENCRYPTED_FLAG;\n        let type_ = EndpointSecretType::try_from(v)\n            .map_err(|_| crate::error::Error::generic(\"Invalid marker value\"))?;\n\n        Ok(Self { type_, encrypted })\n    }\n\n    fn to_u8(&self) -> u8 {\n        let mut ret = self.type_.clone().into();\n        if self.encrypted {\n            ret |= Self::ENCRYPTED_FLAG;\n        }\n        ret\n    }\n\n    fn type_(&self) -> &EndpointSecretType {\n        &self.type_\n    }\n}\n\n/// The internal representation of the endpoint secret.\n/// This is used to store it securely in the database and cache, and to ensure it doesn't get\n/// sent externally.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub struct EndpointSecretInternal {\n    marker: EndpointSecretMarker,\n\n    key: Vec<u8>,\n}\n\nimpl EndpointSecretInternal {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MINUS_ONE: usize = Self::KEY_SIZE - 1;\n\n    fn new(\n        encryption: &Encryption,\n        type_: EndpointSecretType,\n        key: &[u8],\n    ) -> crate::error::Result<Self> {\n        Ok(Self {\n            marker: EndpointSecretMarker {\n                type_,\n                encrypted: encryption.enabled(),\n            },\n            key: encryption.encrypt(key)?,\n        })\n    }\n\n    pub fn generate_symmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let buf: [u8; Self::KEY_SIZE] = rand::thread_rng().gen();\n        Self::new(encryption, EndpointSecretType::Hmac256, &buf)\n    }\n\n    pub fn generate_asymmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let key = AsymmetricKey::generate();\n        Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())\n    }\n\n    fn into_vec(mut self) -> Vec<u8> {\n        let marker: u8 = self.marker.to_u8();\n\n        let mut vec = vec![marker];\n        vec.append(&mut self.key);\n        vec\n    }\n\n    fn from_vec(v: Vec<u8>) -> crate::error::Result<Self> {\n        // Legacy had exact size\n        match v.len() {\n            0..=Self::KEY_SIZE_MINUS_ONE => Err(crate::error::Error::generic(\"Value too small\")),\n            Self::KEY_SIZE => Ok(Self {\n                marker: EndpointSecretMarker {\n                    type_: EndpointSecretType::Hmac256,\n                    encrypted: false,\n                },\n                key: v,\n            }),\n            _ => {\n                let marker = EndpointSecretMarker::from_u8(v[0])?;\n                Ok(Self {\n                    marker,\n                    key: v[1..].to_vec(),\n                })\n            }\n        }\n    }\n\n    pub fn into_endpoint_secret(\n        self,\n        encryption: &Encryption,\n    ) -> crate::error::Result<EndpointSecret> {\n        let key = self.key(encryption)?;\n        Ok(match self.type_() {\n            EndpointSecretType::Hmac256 => EndpointSecret::Symmetric(key),\n            EndpointSecretType::Ed25519 => {\n                EndpointSecret::Asymmetric(AsymmetricKey::from_slice(&key[..])?)\n            }\n        })\n    }\n\n    pub fn from_endpoint_secret(\n        endpoint_secret: EndpointSecret,\n        encryption: &Encryption,\n    ) -> crate::error::Result<Self> {\n        Ok(match endpoint_secret {\n            EndpointSecret::Symmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Hmac256, &key)?\n            }\n            End<|fim_middle|>", "completion": "match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whsk_\",\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "match_expression", "line_range": [656, 659]}
{"prompt": "<|fim_prefix|>:json!({ \"test\": \"value\" });\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let received_payload = timeout(Duration::from_secs(2), receiver.data_recv.recv())\n        .await\n        .unwrap()\n        .unwrap();\n\n    assert_eq!(received_payload, msg_payload);\n}\n\n// This doesn't actually handle requests successfully, but it does allow us\n// to see which hostnames are requested of it.\nstruct MockProxyServer {\n    matched_hosts: Arc<Mutex<HashSet<String>>>,\n    addr: String,\n    variant: MockProxyVariant,\n}\n\nenum MockProxyVariant {\n    Http,\n    Socks5,\n}\n\nimpl MockProxyServer {\n    pub fn new(variant: MockProxyVariant) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n        let addr = match variant {\n            MockProxyVariant::Http => {\n                format!(\"http://{}\", listener.local_addr().unwrap())\n            }\n            MockProxyVariant::Socks5 => {\n                format!(\"socks5://{}\", listener.local_addr().unwrap())\n            }\n        };\n        let matched_hosts = Arc::new(Mutex::new(HashSet::new()));\n\n        match variant {\n            MockProxyVariant::Http => {\n                tokio::spawn(Self::http_listener(listener, matched_hosts.clone()))\n            }\n            MockProxyVariant::Socks5 => {\n                tokio::spawn(Self::socks5_listener(listener, matched_hosts.clone()))\n            }\n        };\n\n        Self {\n            matched_hosts,\n            addr,\n            variant,\n        }\n    }\n\n    pub async fn http_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        loop {\n            let (mut stream, _addr) = listener.accept().await.unwrap();\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let mut buffer = [0; 512];\n\n                if let Ok(size) = stream.read(&mut buffer).await {\n                    if size == 0 {\n                        return;\n                    }\n                    let request = String::from_utf8_lossy(&buffer[..size]);\n                    if let Some(host) = request\n                        .strip_prefix(\"CONNECT \")\n                        .and_then(|s| s.split(' ').next())\n                        .and_then(|s| s.strip_suffix(\":443\"))\n                    {\n                        let mut guard = matched_hosts.lock().unwrap();\n                        guard.insert(host.to_string());\n                    }\n                }\n            });\n        }\n    }\n\n    pub async fn socks5_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        use socks5_proto::{\n            handshake::{\n                Method as HandshakeMethod, Request as HandshakeRequest,\n                Response as HandshakeResponse,\n            },\n            Address, Reply, Request as SocksRequest, Response as SocksResponse,\n        };\n        loop {\n            let (mut stream, _) = match listener.accept().await {\n                Ok(v) => v,\n                Err(_) => continue,\n            };\n\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let hs_req = match HandshakeRequest::read_from(&mut stream).await {\n                    Ok(req) => req,\n                    Err(_) => {\n                        return;\n                    }\n                };\n\n                if hs_req.methods.contains(&HandshakeMethod::NONE) {\n                    if HandshakeResponse::new(HandshakeMethod::NONE)\n                        .write_to(&mut stream)\n                        .await\n                        .is_err()\n                    {\n                        return;\n                    }\n                } else {\n                    <|fim_suffix|>\n                    return;\n                }\n\n                let Ok(socks_req) = SocksRequest::read_from(&mut stream).await else {\n                    return;\n                };\n\n                let host = match &socks_req.address {\n                    Address::SocketAddress(socket_addr) => socket_addr.ip().to_string(),\n                    Address::DomainAddress(domain_bytes, _port) => {\n                        String::from_utf8_lossy(domain_bytes).to_string()\n                    }\n                };\n                if !host.is_empty() {\n                    let mut guard = matched_hosts.lock().unwrap();\n                    guard.insert(host);\n                }\n\n                let abort_resp =\n                    SocksResponse::new(Reply::ConnectionNotAllowed, Address::unspecified());\n                let _ = abort_resp.write_to(&mut stream).await;\n                let _ = stream.shutdown().await;\n            });\n        }\n    }\n\n    pub fn matches(&self) -> HashSet<String> {\n        let guard = self.matched_hosts.lock().unwrap();\n        println!(\"************ MATCHES {guard:?}\");\n        guard.clone()\n    }\n}\n\n#[tokio::test]\nasync fn test_http_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Http);\n    test_proxy_exceptions(listener).await\n}\n\n#[tokio::test]\nasync fn test_socks5_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Socks5);\n    test_proxy_exceptions(listener).await\n}\n\nasync fn test_proxy_exceptions(listener: MockProxyServer) {\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(ProxyConfig {\n        addr: ProxyAddr::new(listener.addr.clone()).unwrap(),\n        noproxy: Some(ProxyBypassCfg(\"10.0.0.0/8, 8.8.8.8, 0ec2:1652:6021:693b:f928:565d:5a0e:de9f, www.svix.com, .google.com\".to_owned())),\n    });\n    cfg.retry_schedule = vec![];\n\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"proxyTest\").await.unwrap().id;\n\n    // Note: Real hostnames here are not ideal, but difficult to test SOCKS without valid DNS entries:\n    create_test_endpoint(&client, &app_id, \"https://www.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://play.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://www.google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://8.8.8.8\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://8.8.4.4\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://10.0.0.1\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(\n        &client,\n        &app_id,\n        \"https://[0ec2:1652:6021:693b:f928:565d:5a0e:de9f]\",\n    )\n    .await\n    .unwrap();\n\n    client\n        .post::<MessageIn, serde_json::Value>(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({ \"test\": \"value\" })).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    tokio::time::sleep(std::time::Duration::from_secs(10)).await;\n\n    assert!(listener.matches().contains(\"8.8.4.4\"));\n    assert!(!listener.matches().contains(\"8.8.8.8\"));\n    assert!(!listener.matches().contains(\"10.0.0.1\"));\n    assert!(!listener\n        .matches()\n        .contains(\"[0ec2:1652:6021:693b:f928:565d:5a0e:de9f]\"));\n    match listener.variant {\n        MockProxyVariant::Http => {\n            assert!(listener.matches().contains(\"play.svix.com\"));\n\n            assert!(!listener.matches().contains(\"www.svix.com\"));\n            assert!(!listener.matches().contains(\"www.google.com\"));\n            assert!(!listener.matches().contains(\"google.com\"));\n\n            assert_eq!(listener.matches().len(), 2);\n        }\n        MockProxyVariant::Socks5 => {\n            // We can't assert hostnames here b/c DNS is resolved before\n            // calling the pro<|fim_middle|>", "completion": "let _ = HandshakeResponse::new(HandshakeMethod::UNACCEPTABLE)\n                        .write_to(&mut stream)\n                        .await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_proxy.rs", "node_type": "let_declaration", "line_range": [196, 198]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{borrow::Cow, collections::HashMap, fmt, net::SocketAddr, sync::Arc, time::Duration};\n\nuse anyhow::{bail, Context};\nuse figment::{\n    providers::{Env, Format, Toml},\n    Figment,\n};\nuse ipnet::IpNet;\nuse serde::{Deserialize, Deserializer};\nuse tracing::Level;\nuse url::Url;\nuse validator::{Validate, ValidationError};\n\nuse crate::{\n    core::{cryptography::Encryption, security::JwtSigningConfig},\n    error::Result,\n    v1::utils::validation_error,\n};\n\nf<|fim_suffix|>\n#[derive(Deserialize)]\n#[serde(untagged)]\nenum RetryScheduleDeserializer {\n    Array(Vec<u64>),\n    Legacy(String),\n}\n\nfn deserialize_retry_schedule<'de, D>(deserializer: D) -> Result<Vec<Duration>, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let buf = RetryScheduleDeserializer::deserialize(deserializer)?;\n    match buf {\n        RetryScheduleDeserializer::Array(buf) => {\n            Ok(buf.into_iter().map(|x| Duration::new(x, 0)).collect())\n        }\n        RetryScheduleDeserializer::Legacy(buf) => Ok(buf\n            .split(',')\n            .filter_map(|x| {\n                let x = x.trim();\n                if x.is_empty() {\n                    None\n                } else {\n                    Some(Duration::new(x.parse().expect(\"Error parsing duration\"), 0))\n                }\n            })\n            .collect()),\n    }\n}\n\nfn deserialize_hours<'de, D>(deserializer: D) -> Result<Duration, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let hours = u64::deserialize(deserializer)?;\n    Ok(Duration::from_secs(60 * 60 * hours))\n}\n\nconst DEFAULTS: &str = include_str!(\"../config.default.toml\");\n\npub type Configuration = Arc<ConfigurationInner>;\n\nfn default_redis_pending_duration_secs() -> u64 {\n    45\n}\n\nfn validate_operational_webhook_url(url: &str) -> Result<(), ValidationError> {\n    match Url::parse(url) {\n        Ok(url) => {\n            // Verify scheme is http or https\n            if url.scheme() != \"http\" && url.scheme() != \"https\" {\n                return Err(validation_error(\n                    Some(\"operational_webhook_address\"),\n                    Some(\"URL scheme must be http or https\"),\n                ));\n            }\n\n            // Verify there's a host\n            if url.host().is_none() {\n                return Err(validation_error(\n                    Some(\"operational_webhook_address\"),\n                    Some(\"URL must include a valid host\"),\n                ));\n            }\n        }\n        Err(_) => {\n            return Err(validation_error(\n                Some(\"operational_webhook_address\"),\n                Some(\"Invalid URL format\"),\n            ));\n        }\n    }\n\n    Ok(())\n}\n\n#[derive(Clone, Debug, Deserialize, Validate)]\n#[validate(\n    schema(function = \"validate_config_complete\"),\n    skip_on_field_errors = false\n)]\npub struct ConfigurationInner {\n    /// The address to listen on\n    pub listen_address: SocketAddr,\n\n    /// The address to send operational webhooks to. When None, operational webhooks will not be\n    /// sent. When Some, the API server with the given URL will be used to send operational webhooks.\n    #[validate(custom = \"validate_operational_webhook_url\")]\n    pub operational_webhook_address: Option<String>,\n\n    /// The main secret used by Svix. Used for client-side encryption of sensitive data, etc.\n    /// IMPORTANT: Once set, it can't be changed.\n    #[serde(\n        rename = \"main_secret\",\n        deserialize_with = \"deserialize_main_secret\",\n        default\n    )]\n    pub encryption: Encryption,\n\n    /// Contains the secret and algorithm for signing JWTs\n    #[serde(flatten)]\n    pub jwt_signing_config: Arc<JwtSigningConfig>,\n\n    /// This determines the type of key that is generated for endpoint secrets by default (when none is set).\n    /// Supported: hmac256 (default), ed25519\n    /// Note: this does not affect existing keys, which will continue signing based on the type they were created with.\n    pub default_signature_type: DefaultSignatureType,\n\n    /// The log level to run the service with. Supported: info, debug, trace\n    pub log_level: LogLevel,\n    /// The log format that all output will follow. Supported: default, json\n    pub log_format: LogFormat,\n    /// The OpenTelemetry address to send events to if given.\n    pub opentelemetry_address: Option<String>,\n    /// The ratio at which to sample spans when sending to OpenTelemetry. When not given it defaults\n    /// to always sending. If the OpenTelemetry address is not set, this will do nothing.\n    pub opentelemetry_sample_ratio: Option<f64>,\n    /// The service name to use for OpenTelemetry. If not provided, it defaults to \"svix_server\".\n    pub opentelemetry_service_name: String,\n    /// Whether to enable the logging of the databases at the configured log level. This may be\n    /// useful for analyzing their response times.\n    pub db_tracing: bool,\n    /// The Sentry DSN to use for error reporting. If this is `None`,\n    /// then sentry reporting is disabled\n    pub sentry_dsn: Option<sentry::types::Dsn>,\n    /// The environment (dev, staging, or prod) that the server is running in.\n    pub environment: Environment,\n\n    /// The wanted retry schedule in seconds. Each value is the time to wait between retries.\n    #[serde(deserialize_with = \"deserialize_retry_schedule\")]\n    pub retry_schedule: Vec<Duration>,\n\n    /// The DSN for the database. Only postgres is currently supported.\n    pub db_dsn: String,\n    // The maximum number of connections for the PostgreSQL pool\n    #[validate(range(min = 10))]\n    pub db_pool_max_size: u16,\n\n    /// The DSN for redis (can be left empty if not using redis)\n    /// Note that if using Redis Sentinel, this will be the the DSN\n    /// for a Sentinel instance.\n    pub redis_dsn: Option<String>,\n    /// The maximum number of connections for the Redis pool\n    #[validate(range(min = 10))]\n    pub redis_pool_max_size: u16,\n\n    #[serde(flatten, default)]\n    pub redis_sentinel_cfg: Option<SentinelConfig>,\n\n    /// What kind of message queue to use. Supported: memory, redis (must have redis_dsn or\n    /// queue_dsn configured).\n    pub queue_type: QueueType,\n    /// The DSN for the Redis-backed queue. Overrides `redis_dsn`. (can be left empty if not using\n    /// redis)\n    pub queue_dsn: Option<String>,\n    /// Optional prefix to use for the queue.\n    ///\n    /// Allows multiple instances of svix-server to share a single queue backend\n    /// without interfering with each other.\n    pub queue_prefix: Option<String>,\n\n    /// What kind of cache to use. Supported: memory, redis (must have redis_dsn or cache_dsn\n    /// configured), none.\n    pub cache_type: CacheType,\n    /// The DSN for the Redis-backed cache. Overrides `redis_dsn`. (can be left empty if not using\n    /// redis)\n    pub cache_dsn: Option<String>,\n\n    /// If true, headers are prefixed with `Webhook-`, otherwise with `Svix-` (default).\n    pub whitelabel_headers: bool,\n\n    /// If true, only allow https endpoints, otherwise also allow http.\n    pub endpoint_https_only: bool,\n\n    /// How long to wait when making a request (in seconds)\n    #[validate(range(min = 1, max = 30))]\n    pub worker_request_timeout: u16,\n\n    /// How long of a period an endpoint must be consistently failing to be disabled. If a message\n    /// is successfully sent during this time, then the endpoint will not disable.\n    #[serde(deserialize_with = \"deserialize_hours\")]\n    pub endpoint_failure_disable_after: Duration,\n\n    // Execution mode\n    /// Should this instance run the API\n    pub api_enabled: bool,\n    /// Should this instance run the message worker\n    pub worker_enabled: bool,\n\n    /// Subnets to whitelist for outbound webhooks. Note that allowing endpoints in private IP space\n    /// is a security risk and should only be allowed if you are using the service internally or for\n    /// testing purposes. Should be specified in CIDR notation, e.g., `[127.0.0.1/32, 172.17.0.0/16, 192.168.0.0/16]`\n    pub whitelist_subnets: Option<Arc<Vec<IpNet>>>,\n\n    /// Maximum number of concurrent worker tasks to spawn (0 is unlimited)\n    pub worker_max_tasks: u16,\n\n    /// Maximum seconds of a queue long-poll\n <|fim_middle|>", "completion": "fn deserialize_main_secret<'de, D>(deserializer: D) -> Result<Encryption, D::Error>\nwhere\n    D: Deserializer<'de>,\n{\n    let key = String::deserialize(deserializer)?;\n    // Derive a key so we get a key of the right size\n    let key = hmac_sha256::HMAC::mac(b\"main\", key.as_bytes());\n    Ok(Encryption::new(key))\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/cfg.rs", "node_type": "function_item", "line_range": [23, 31]}
{"prompt": "<|fim_prefix|>//! Allocator stats are only available when we're using jemalloc, and jemalloc doesn't work on windows.\n//!\n//! 2 impls for the helper functions are therefore provided. One set that does nothing (for windows)\n//! and another that works in the non-windows world.\n//!\n//! Care should be taken to keep the signatures aligned between these two so the callsites can be\n//! used consistently regardless of whether jemalloc is in use or not.\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\npub use supported::*;\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\n<|fim_suffix|>\n\n#[cfg(all(not(target_env = \"msvc\"), feature = \"jemalloc\"))]\nmod supported {\n    use std::sync::Arc;\n\n    use tikv_jemalloc_ctl::{epoch, stats};\n\n    pub struct AllocatorStatMibs {\n        epoch: tikv_jemalloc_ctl::epoch_mib,\n        allocated: stats::allocated_mib,\n        resident: stats::resident_mib,\n    }\n\n    pub fn get_allocator_stats(\n        bust_cache: bool,\n        mibs: &AllocatorStatMibs,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        if bust_cache {\n            // Stats are cached internally and advancing the epoch is a way to invalidate those caches.\n            mibs.epoch.advance()?;\n        }\n        let allocated = mibs.allocated.read()?;\n        let resident = mibs.resident.read()?;\n        Ok(Some((allocated, resident)))\n    }\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        let e = epoch::mib()?;\n        let allocated = stats::allocated::mib()?;\n        let resident = stats::resident::mib()?;\n\n        Ok(Arc::new(AllocatorStatMibs {\n            epoch: e,\n            allocated,\n            resident,\n        }))\n    }\n}\n\n#[cfg(any(target_env = \"msvc\", not(feature = \"jemalloc\")))]\nmod unsupported {\n    use std::sync::Arc;\n\n    use anyhow::anyhow;\n\n    pub struct AllocatorStatMibs;\n\n    pub fn get_allocator_stats(\n        _bust_cache: bool,\n        _mibs: Arc<AllocatorStatMibs>,\n    ) -> anyhow::Result<Option<(usize, usize)>> {\n        Ok(None)\n    }\n\n    pub fn get_allocator_stat_mibs() -> anyhow::Result<Arc<AllocatorStatMibs>> {\n        Err(anyhow!(\"metric collection is not supported\"))\n    }\n}\n<|fim_middle|>", "completion": "pub use unsupported::*;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/allocator.rs", "node_type": "use_declaration", "line_range": [12, 12]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Configuration-dependent tests for the main database\n\nuse http::StatusCode;\nuse sea_orm::DatabaseConnection;\nuse svix_server::{\n    core::types::{ApplicationId, BaseId, EndpointId, OrganizationId},\n    db::{\n        models::{application, endpoint, eventtype, message, messageattempt},\n        wipe_org,\n    },\n    v1::endpoints::event_type::EventTypeOut,\n};\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, create_test_message, event_type_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id,\n};\n\nasync fn test_data() -> (OrganizationId, Vec<ApplicationId>, Vec<EndpointId>) {\n    let org_id = OrganizationId::new(None, None);\n\n    l<|fim_suffix|>\n    // Make apps\n    let mut app_ids = Vec::new();\n    for _ in 0..10 {\n        app_ids.push(create_test_app(&client, \"Test1\").await.unwrap().id);\n    }\n\n    let mut endp_ids = Vec::new();\n    for app_id in &app_ids {\n        for _ in 0..2 {\n            endp_ids.push(\n                create_test_endpoint(&client, app_id, \"https://bad.url\")\n                    .await\n                    .unwrap()\n                    .id,\n            );\n        }\n    }\n\n    for app_id in &app_ids {\n        for _ in 0..2 {\n            create_test_message(&client, app_id, serde_json::json!({\"test\": \"value\"}))\n                .await\n                .unwrap();\n        }\n    }\n\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type\",\n            event_type_in(\"test\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    jh.abort();\n\n    (org_id, app_ids, endp_ids)\n}\n\nasync fn count_message_attempts(db: &DatabaseConnection, endp_id: EndpointId) -> usize {\n    messageattempt::Entity::secure_find_by_endpoint(endp_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_messages(db: &DatabaseConnection, app_id: ApplicationId) -> usize {\n    message::Entity::secure_find(app_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_endpoints(db: &DatabaseConnection, app_id: ApplicationId) -> usize {\n    endpoint::Entity::secure_find(app_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_applications(db: &DatabaseConnection, org_id: OrganizationId) -> usize {\n    application::Entity::secure_find(org_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\nasync fn count_event_types(db: &DatabaseConnection, org_id: OrganizationId) -> usize {\n    eventtype::Entity::secure_find(org_id)\n        .all(db)\n        .await\n        .unwrap()\n        .len()\n}\n\n#[ignore]\n#[tokio::test]\nasync fn test_wiping_organization() {\n    // Make two orgs, one of which will be wiped\n    let (org_id_1, app_ids_1, endp_ids_1) = test_data().await;\n\n    let (org_id_2, app_ids_2, _) = test_data().await;\n\n    // Wipe\n    let cfg = svix_server::cfg::load().unwrap();\n    wipe_org(&cfg, org_id_1.clone()).await;\n\n    // Start asserting everything is gone for org_id_1, but not org_id_2\n    let db = svix_server::db::init_db(&cfg).await;\n\n    for endp_id in endp_ids_1 {\n        assert_eq!(count_message_attempts(&db, endp_id.clone()).await, 0);\n    }\n\n    for app_id in app_ids_1 {\n        assert_eq!(count_messages(&db, app_id.clone()).await, 0);\n        assert_eq!(count_endpoints(&db, app_id.clone()).await, 0);\n    }\n\n    for app_id in app_ids_2 {\n        assert_eq!(count_messages(&db, app_id.clone()).await, 2);\n        assert_eq!(count_endpoints(&db, app_id.clone()).await, 2);\n    }\n\n    assert_eq!(count_applications(&db, org_id_1.clone()).await, 0);\n    assert_eq!(count_event_types(&db, org_id_1.clone()).await, 0);\n\n    assert_eq!(count_applications(&db, org_id_2.clone()).await, 10);\n    assert_eq!(count_event_types(&db, org_id_2.clone()).await, 1);\n}\n<|fim_middle|>", "completion": "let (client, jh) =\n        start_svix_server_with_cfg_and_org_id(&get_default_test_config(), org_id.clone()).await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/db.rs", "node_type": "let_declaration", "line_range": [25, 26]}
{"prompt": "<|fim_prefix|>// needed at router-construction time.\n    let mut openapi = openapi::initialize_openapi();\n\n    let svc_cache = cache.clone();\n    // build our application with a route\n    let app_state = AppState {\n        db: pool.clone(),\n        queue_tx: queue_tx.clone(),\n        cfg: cfg.clone(),\n        cache: cache.clone(),\n        op_webhooks: op_webhook_sender.clone(),\n    };\n    let v1_router = v1::router().with_state::<()>(app_state);\n\n    // Initialize all routes which need to be part of OpenAPI first.\n    let app = ApiRouter::new()\n        .nest_api_service(\"/api/v1\", v1_router)\n        .finish_api(&mut openapi);\n\n    openapi::postprocess_spec(&mut openapi);\n    let docs_router = docs::router(openapi);\n    let app = app.merge(docs_router).layer((\n        layer_fn(move |service| IdempotencyService {\n            cache: svc_cache.clone(),\n            service,\n        }),\n        CorsLayer::new()\n            .allow_origin(Any)\n            .allow_methods(Any)\n            .allow_headers(AllowHeaders::mirror_request())\n            .max_age(Duration::from_secs(600)),\n    ));\n    let svc = tower::make::Shared::new(\n        // It is important that this service wraps the router instead of being\n        // applied via `Router::layer`, as it would run after routing then.\n        NormalizePath::trim_trailing_slash(app),\n    );\n\n    let with_api = cfg.api_enabled;\n    let with_worker = cfg.worker_enabled;\n    let listen_address = cfg.listen_address;\n\n    let ((), worker_loop, expired_message_cleaner_loop) = tokio::join!(\n        async {\n            if with_api {\n                let listener = match listener {\n                    Some(l) => l,\n                    None => TcpListener::bind(listen_address)\n                        .await\n                        .expect(\"Error binding to listen_address\"),\n                };\n                tracing::debug!(\"API: Listening on {}\", listener.local_addr().unwrap());\n\n                axum::serve(listener, svc)\n                    .with_graceful_shutdown(graceful_shutdown_handler())\n                    .await\n                    .unwrap();\n            } else {\n                tracing::debug!(\"API: off\");\n                graceful_shutdown_handler().await;\n            }\n        },\n        async {\n            if with_worker {\n                tracing::debug!(\"Worker: Started\");\n                queue_handler(\n                    &cfg,\n                    cache.clone(),\n                    pool.clone(),\n                    queue_tx,\n                    queue_rx,\n                    op_webhook_sender,\n                )\n                .await\n            } else {\n                tracing::debug!(\"Worker: off\");\n                Ok(())\n            }\n        },\n        async {\n            if with_worker {\n                tracing::debug!(\"Expired message cleaner: Started\");\n                expired_message_cleaner_loop(&pool).await\n            } else {\n                tracing::debug!(\"Expired message cleaner: off\");\n                Ok(())\n            }\n        }\n    );\n\n    worker_loop.expect(\"Error initializing worker\");\n    expired_message_cleaner_loop.expect(\"Error initializing expired message cleaner\")\n}\n\npub fn setup_tracing(\n    cfg: &ConfigurationInner,\n    for_test: bool,\n) -> (tracing::Dispatch, sentry::ClientInitGuard) {\n    let filter_directives = std::env::var(\"RUST_LOG\").unwrap_or_else(|e| {\n        if let std::env::VarError::NotUnicode(_) = e {\n            eprintln!(\"RUST_LOG environment variable has non-utf8 contents, ignoring!\");\n        }\n\n        let level = cfg.log_level.to_string();\n        let mut var = vec![\n            format!(\"{CRATE_NAME}={level}\"),\n            format!(\"tower_http={level}\"),\n        ];\n\n        if cfg.db_tracing {\n            var.push(format!(\"sqlx={level}\"));\n        }\n\n        var.join(\",\")\n    });\n\n    let otel_layer = cfg.opentelemetry_address.as_ref().map(|addr| {\n        // Configure the OpenTelemetry tracing layer\n        opentelemetry::global::set_text_map_propagator(\n            opentelemetry_sdk::propagation::TraceContextPropagator::new(),\n        );\n\n        l<|fim_suffix|>\n        let provider = opentelemetry_otlp::new_pipeline()\n            .tracing()\n            .with_exporter(exporter)\n            .with_trace_config(\n                opentelemetry_sdk::trace::Config::default()\n                    .with_sampler(\n                        cfg.opentelemetry_sample_ratio\n                            .map(opentelemetry_sdk::trace::Sampler::TraceIdRatioBased)\n                            .unwrap_or(opentelemetry_sdk::trace::Sampler::AlwaysOn),\n                    )\n                    .with_resource(opentelemetry_sdk::Resource::new(vec![\n                        opentelemetry::KeyValue::new(\n                            \"service.name\",\n                            cfg.opentelemetry_service_name.clone(),\n                        ),\n                    ])),\n            )\n            .install_batch(Tokio)\n            .unwrap();\n\n        // Based on the private `build_batch_with_exporter` method from opentelemetry-otlp\n        let tracer = provider\n            .tracer_builder(\"opentelemetry-otlp\")\n            .with_schema_url(opentelemetry_semantic_conventions::SCHEMA_URL)\n            .build();\n\n        tracing_opentelemetry::layer().with_tracer(tracer)\n    });\n\n    let sentry_guard = sentry::init(sentry::ClientOptions {\n        dsn: cfg.sentry_dsn.clone(),\n        environment: Some(Cow::Owned(cfg.environment.to_string())),\n        release: sentry::release_name!(),\n        ..Default::default()\n    });\n\n    let sentry_layer =\n        sentry::integrations::tracing::layer().event_filter(|md| match *md.level() {\n            tracing::Level::ERROR | tracing::Level::WARN => EventFilter::Event,\n            _ => EventFilter::Ignore,\n        });\n\n    // Then create a subscriber with an additional layer printing to stdout.\n    // This additional layer is either formatted normally or in JSON format.\n    let stdout_layer = if for_test {\n        tracing_subscriber::fmt::layer().with_test_writer().boxed()\n    } else {\n        match cfg.log_format {\n            cfg::LogFormat::Default => tracing_subscriber::fmt::layer().boxed(),\n            cfg::LogFormat::Json => {\n                let fmt = tracing_subscriber::fmt::format().json().flatten_event(true);\n                let json_fields = tracing_subscriber::fmt::format::JsonFields::new();\n\n                tracing_subscriber::fmt::layer()\n                    .event_format(fmt)\n                    .fmt_fields(json_fields)\n                    .boxed()\n            }\n        }\n    };\n\n    let registry = tracing_subscriber::Registry::default()\n        .with(otel_layer)\n        .with(sentry_layer)\n        .with(stdout_layer)\n        .with(tracing_subscriber::EnvFilter::new(filter_directives))\n        .into();\n\n    (registry, sentry_guard)\n}\n\npub fn setup_metrics(cfg: &ConfigurationInner) -> Option<SdkMeterProvider> {\n    cfg.opentelemetry_address.as_ref().map(|addr| {\n        let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(addr);\n\n        opentelemetry_otlp::new_pipeline()\n            .metrics(Tokio)\n            .with_delta_temporality()\n            .with_exporter(exporter)\n            .with_resource(opentelemetry_sdk::Resource::new(vec![\n                opentelemetry::KeyValue::new(\n                    \"service.name\",\n                    cfg.opentelemetry_service_name.clone(),\n                ),\n                opentelemetry::KeyValue::new(\"instance_id\", INSTANCE_ID.to_owned()),\n                opentelemetry::KeyValue::new(\n                    \"service.version\",\n                    option_env!(\"GITHUB_SHA\").unwrap_or(\"unknown\"),\n                ),\n            ]))\n            .build()\n            .unwrap()\n    })\n}\n\npub fn setup_tracing_for_tests() {\n    use tracing_subscriber::{layer::SubscriberExt, util::SubscriberInitExt};\n\n    tracing_subscriber::registry()\n        .with(\n            tracing_subscriber::EnvFilter::try_from_default_env().unwrap_or_else(|_| {\n                // Output is only printed for failing tests, but still we shouldn't overload\n                // the output with unnecessary info. When debugging a specific<|fim_middle|>", "completion": "let exporter = opentelemetry_otlp::new_exporter()\n            .tonic()\n            .with_endpoint(addr);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/lib.rs", "node_type": "let_declaration", "line_range": [274, 276]}
{"prompt": "<|fim_prefix|>Limit(pub u64);\n\nimpl<'de> Deserialize<'de> for PaginationLimit {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let limit = u64::deserialize(deserializer)?;\n\n        // Want hard limits to stay the same so they can be validated\n        if !PAGINATION_LIMIT_CAP_HARD && limit > PAGINATION_LIMIT_CAP_LIMIT {\n            Ok(PaginationLimit(PAGINATION_LIMIT_CAP_LIMIT))\n        } else {\n            Ok(PaginationLimit(limit))\n        }\n    }\n}\n\nimpl Validate for PaginationLimit {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        let mut errs = validator::ValidationErrors::new();\n\n        if self.0 > PAGINATION_LIMIT_CAP_LIMIT {\n            errs.add(\n                \"limit\",\n                validation_error(Some(\"pagination\"), Some(&PAGINATION_LIMIT_ERROR)),\n            );\n        }\n\n        if errs.is_empty() {\n            Ok(())\n        } else {\n            Err(errs)\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum ReversibleIterator<T: Validate> {\n    /// Regular iteration - backwards in time.\n    Normal(T),\n    /// Reversed iteration - forwards in time.\n    Prev(T),\n}\n\nimpl<T: Validate> ReversibleIterator<T> {\n    pub(crate) fn direction(&self) -> IteratorDirection {\n        match self {\n            Self::Normal(_) => IteratorDirection::Normal,\n            Self::Prev(_) => IteratorDirection::Prev,\n        }\n    }\n}\n\nimpl<'de, T: 'static + Deserialize<'de> + Validate + From<String>> Deserialize<'de>\n    for ReversibleIterator<T>\n{\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        String::deserialize(deserializer).map(|s| {\n            if let Some(s) = s.strip_prefix('-') {\n                ReversibleIterator::Prev(T::from(s.to_owned()))\n            } else {\n                ReversibleIterator::Normal(T::from(s))\n            }\n        })\n    }\n}\n\nimpl<T: Validate> Validate for ReversibleIterator<T> {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        match self {\n            ReversibleIterator::Normal(val) => val.validate(),\n            ReversibleIterator::Prev(val) => val.validate(),\n        }\n    }\n}\n\nimpl<T: Validate + JsonSchema> JsonSchema for ReversibleIterator<T> {\n    fn schema_name() -> String {\n        format!(\"ReversibleIterator_{}\", T::schema_name())\n    }\n\n    fn json_schema(gen: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n        T::json_schema(gen)\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\n/// Applies sorting and filtration to a query from its iterator, sort column, and limit\n/// queries based on time\n/// Our rules for limiting queries are as follows\n///\n/// If `before` is passed:\n/// * lower limit on query is `before - LIMITED_QUERY_DURATION`\n/// * upper limit is `before`\n///\n/// If `after` is passed:\n/// * lower limit is `after`\n/// * upper limit is `now + FUTURE_QUERY_LIMIT`\n///\n/// If prev-iterator is passed:\n/// * lower limit is `prev-iterator`\n/// * upper limit is `prev-iterator + LIMITED_QUERY_DURATION`\n///\n/// If (normal) iterator is passed:\n/// * lower limit is `iterator - LIMITED_QUERY_DURATION`\n/// * upper limit is `iterator`\n///\n/// If no iterator is passed:\n/// * lower limit is `now() - LIMITED_QUERY_DURATION` if\n///   neither `before` nor `after` were passed\npub(crate) fn filter_and_paginate_time_limited<Q, I>(\n    mut query: Q,\n    sort_column: impl ColumnTrait,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    before: Option<DateTime<Utc>>,\n    after: Option<DateTime<Utc>>,\n) -> (Q, IteratorDirection)\nwhere\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    I: BaseId<Output = I> + Validate + Into<sea_orm::Value>,\n{\n    let mut limit_time = true;\n    if let Some(before) = before {\n        if limit_time {\n            query = query.filter(sort_column.gt(I::start_id(before - *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.lt(I::start_id(before)));\n    }\n\n    i<|fim_suffix|>\n    let (mut query, iter_direction) = match (&iterator, before, after) {\n        (Some(ReversibleIterator::Prev(_)), _, _) | (None, None, Some(_)) => {\n            (query.order_by_asc(sort_column), IteratorDirection::Prev)\n        }\n        _ => (query.order_by_desc(sort_column), IteratorDirection::Normal),\n    };\n\n    let now = chrono::Utc::now();\n    let future_limit = now + *FUTURE_QUERY_LIMIT;\n    match iterator {\n        Some(ReversibleIterator::Prev(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.gt(id));\n            if limit_time {\n                query = query.filter(sort_column.lt(I::end_id(ts + *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        Some(ReversibleIterator::Normal(id)) => {\n            let ts = id.timestamp();\n            query = query.filter(sort_column.lt(id));\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(ts - *LIMITED_QUERY_DURATION)));\n            }\n        }\n\n        None => {\n            if limit_time {\n                query = query.filter(sort_column.gt(I::start_id(now - *LIMITED_QUERY_DURATION)));\n            }\n        }\n    }\n\n    query = query\n        // Query for an extra element to be able to tell whether there's more\n        // data than the user requested.\n        .limit(limit + 1)\n        // Blanket limit on future\n        .filter(sort_column.lt(I::start_id(future_limit)));\n\n    (query, iter_direction)\n}\n\n/// Marker trait for any type that is used for iterating through results\n/// in the public API.\npub trait IdIterator: Validate + Into<sea_orm::Value> {}\n\nimpl<T: BaseId + Validate + Into<sea_orm::Value>> IdIterator for T {}\nimpl IdIterator for EventTypeName {}\n\npub fn apply_pagination<\n    Q: QuerySelect + QueryOrder + QueryFilter,\n    C: ColumnTrait,\n    I: IdIterator,\n>(\n    query: Q,\n    sort_column: C,\n    limit: u64,\n    iterator: Option<ReversibleIterator<I>>,\n    ordering: Ordering,\n) -> Q {\n    use Ordering::*;\n    use ReversibleIterator::*;\n\n    // Query for an extra element to be able to tell whether there's more\n    // data than the user requested.\n    let query = query.limit(limit + 1);\n\n    let iterator = if let Some(it) = iterator {\n        it\n    } else {\n        return match ordering {\n            Ascending => query.order_by_asc(sort_column),\n            Descending => query.order_by_desc(sort_column),\n        };\n    };\n\n    match (iterator, ordering) {\n        (Prev(id), Ascending) | (Normal(id), Descending) => {\n            query.order_by_desc(sort_column).filter(sort_column.lt(id))\n        }\n        (Prev(id), Descending) | (Normal(id), Ascending) => {\n            query.order_by_asc(sort_column).filter(sort_column.gt(id))\n        }\n    }\n}\n\n/// A response with no body content and a specific response code, specified by\n/// the generic parameter `N`.\npub struct NoContentWithCode<const N: u16>;\n\nimpl<const N: u16> IntoResponse for NoContentWithCode<N> {\n    fn into_response(self) -> axum::response::Response {\n        (StatusCode::from_u16(N).unwrap(), ()).into_response()\n    }\n}\n\nimpl<const N: u16> OperationOutput for NoContentWithCode<N> {\n    type Inner = Self;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        <() as OperationOutput>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(response) = Self::operation_response(ctx, operation) {\n            vec![(Some(N), response)]\n        } else {\n            vec![]\n        }\n    }\n}\n\n/// A response with no body content and HTTP status code 204, the standard code\n/// for such responses.\n#[derive(OperationIo)]\n#[aide(output_with = \"()\")]\npub struct NoContent;\n\nimpl IntoResponse for NoContent {\n    fn into_response(self) -> axum::response::Response {\n        NoContentWithCode::<204>::into_response(NoContentWithCode)\n    }\n}\n\n<|fim_middle|>", "completion": "if let Some(after) = after {\n        if limit_time {\n            query = query.filter(sort_column.lt(I::end_id(after + *LIMITED_QUERY_DURATION)));\n            limit_time = false;\n        }\n        query = query.filter(sort_column.gt(I::start_id(after)));\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "if_expression", "line_range": [222, 228]}
{"prompt": "<|fim_prefix|>r return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n/// Pollers make HTTP requests in a loop and forward what they fetch to their `ReceiverOutput`\nasync fn supervise_pollers(inputs: Vec<Box<dyn PollerInput>>) -> std::io::Result<()> {\n    let mut set = tokio::task::JoinSet::new();\n    for input in inputs {\n        set.spawn(async move {\n            // FIXME: needs much better signaling for termination\n            loop {\n                // If this future returns, the consumer terminated unexpectedly.\n                input.run().await;\n\n                tracing::warn!(\"poller input {} unexpectedly terminated\", input.name());\n                tokio::time::sleep(Duration::from_secs(1)).await;\n            }\n        });\n    }\n\n    // FIXME: add signal handling to trigger a (intentional) graceful shutdown.\n\n    // FIXME: when a plugin exits unexpectedly, what do?\n    //   Most consumers are probably stateful/brittle and may disconnect from time to time.\n    //   Ideally none of these tasks would ever return Ok or Err. They'd run forever.\n    //   Having the tasks themselves try to recover means if we see a task finish here, something\n    //   must be really wrong, so maybe we trigger a shutdown of the rest when one stops here.\n    while let Some(_res) = set.join_next().await {\n        // In order for plugins to coordinate a shutdown, maybe they could:\n        // - have a shutdown method and handle their own internal signalling, or maybe\n        // - take a oneshot channel as an arg to `run()`\n        // Basically we need something that formalizes the shutdown flow in a cross-crate\n        // friendly way.\n        todo!(\"graceful shutdown\");\n    }\n    Ok(())\n}\n\n#[derive(Parser)]\npub struct Args {\n    #[arg(long, env = \"SVIX_BRIDGE_CFG_FILE\", help = \"Path to the config file.\")]\n    cfg_file: Option<PathBuf>,\n    #[arg(\n        long,\n        env = \"SVIX_BRIDGE_CFG\",\n        help = \"Config data as a string (instead of a file on disk).\",\n        conflicts_with = \"cfg_file\"\n    )]\n    cfg: Option<String>,\n}\n\n#[tokio::main]\nasync fn main() -> Result<()> {\n    let args = Args::parse();\n\n    let mut config_search_paths = vec![];\n\n    if let Some(fp) = args.cfg_file {\n        config_search_paths.push(fp)\n    } else {\n        for name in [\"svix-bridge.yaml\", \"svix-bridge.yml\", \"svix-bridge.json\"] {\n            config_search_paths.push(std::env::current_dir().expect(\"current dir\").join(name));\n        }\n    }\n\n    // Clap will ensure we have only one or the other (cfg and cfg_file can't be specified together).\n    let cfg_source = match args.cfg {\n        Some(cfg_source) => cfg_source,\n        None => {\n            let fp = config_search_paths\n                .into_iter()\n                .find(|x| x.exists())\n                .expect(\"config file path\");\n            std::fs::read_to_string(&fp).map_err(|e| {\n                let p = fp.into_os_string().into_string().expect(\"config file path\");\n                Error::other(format!(\"Failed to read {p}: {e}\"))\n            })\n        }?,\n    };\n\n    let vars = std::env::vars().collect();\n    let cfg = Config::from_src(&cfg_source, Some(vars).as_ref())?;\n    setup_tracing(&cfg);\n    let _metrics = setup_metrics(&cfg);\n    tracing::info!(\"starting\");\n\n    tokio::spawn(async move {\n        let mut interval = tokio::time::interval(Duration::from_secs(15));\n        let metrics = CommonMetrics::new(&opentelemetry::global::meter(\"svix.com\"));\n        match get_allocator_stat_mibs() {\n            Ok(mibs) => {\n                tracing::debug!(\"Common Metrics Collection: Started\");\n\n                loop {\n                    interval.tick().await;\n\n                    if let Ok(Some((allocated, resident))) = get_allocator_stats(true, &mibs) {\n                        metrics.record_mem_allocated(allocated as _);\n                        metrics.record_mem_resident(resident as _);\n                    }\n                }\n            }\n            Err(e) => tracing::error!(\"Unable to get allocator stats mibs: {e}\"),\n        }\n    });\n\n    <|fim_suffix|>\n\n    // XXX: this is a bit nasty, but might be okay to start.\n    // The nested spawns are needed to make sure we can saturate the\n    // threadpool (otherwise we'd run each job serially).\n    //\n    // Another approach would be to do what og-ingester did: give each plugin a clone of the\n    // `TpHandle`, but this would likely mean moving the runtime module over to the `-types` crate.\n    // I'd rather not do this, mostly to help keep things more unit test friendly; channels can\n    // help keep the coupling more loose, with less stateful baggage.\n    // Starting with this just to keep the JS executor stuff here in the binary.\n    tokio::spawn(async move {\n        tracing::info!(\n            \"Starting JS Transformation Workers: {}\",\n            cfg.transformation_worker_count\n        );\n\n        deno_core::JsRuntime::init_platform(None, false);\n        let pooler: runtime::JsPooler = runtime::JsPooler::new(cfg.transformation_worker_count);\n\n        while let Some(TransformerJob {\n            input,\n            script,\n            callback_tx,\n        }) = xform_rx.recv().await\n        {\n            let tp = pooler.clone();\n            tokio::spawn(async move {\n                let out = tp.run_script(input, script).await;\n                // FIXME: seeing this Err case come up during load testing.\n                //   Seems like we shouldn't be hitting this so easily while the process is not terminating.\n                //   Regularly there are group error log lines that show up right at the end of an\n                //   `oha` run, POSTing to receivers. Need to investigate why.\n                if callback_tx\n                    .send(out.map_err(|e| tracing::error!(\"{:?}\", e)))\n                    .is_err()\n                {\n                    // If the callback fails, the plugin is likely unwinding/dropping.\n                    // Not a whole lot we can do about that.\n                    tracing::error!(\"failed to send js output back to caller\");\n                }\n            });\n        }\n    });\n\n    let mut senders = Vec::with_capacity(cfg.senders.len());\n    for sc in cfg.senders {\n        let mut sender: Box<dyn SenderInput> = sc.try_into().map_err(Error::other)?;\n        sender.set_transformer(Some(xform_tx.clone()));\n        senders.push(sender);\n    }\n    if senders.is_empty() {\n        tracing::warn!(\"No senders configured.\")\n    }\n    let senders_fut = supervise_senders(senders);\n\n    if cfg.receivers.is_empty() {\n        tracing::warn!(\"No receivers configured.\")\n    }\n    let (webhook_receivers, poller_receivers): (\n        Vec<WebhookReceiverConfig>,\n        Vec<PollerReceiverConfig>,\n    ) = cfg\n        .receivers\n        .into_iter()\n        .partition_map(|either| match either {\n            EitherReceiver::Webhook(x) => Either::Left(x),\n            EitherReceiver::Poller(y) => Either::Right(y),\n        });\n\n    let webhook_receivers_fut =\n        webhook_receiver::run(cfg.http_listen_address, webhook_receivers, xform_tx.clone());\n\n    let mut pollers: Vec<Box<dyn PollerInput>> = Vec::with_capacity(poller_receivers.len());\n    for poller_cfg in poller_receivers {\n        pollers.push(poller_cfg.into_poller_input(xform_tx.clone()).await?);\n    }\n\n    let poller_receivers_fut = supervise_pollers(pollers);\n\n    match tokio::try_join!(senders_fut, webhook_receivers_fut, poller_receivers_fut) {\n        Ok(_) => tracing::error!(\"unexpectedly exiting\"),\n        Err(e) => tracing::error!(\"unexpectedly exiting: {}\", e),\n    }\n\n    tracing::info!(\"exiting...\");\n    Ok(())\n}\n<|fim_middle|>", "completion": "let (xform_tx, mut xform_rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/main.rs", "node_type": "let_declaration", "line_range": [302, 302]}
{"prompt": "<|fim_prefix|>(\"Channels must have at least 1 and at most 10 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nfn validate_channels_endpoint_unrequired_nullable(\n    channels: &UnrequiredNullableField<EventChannelSet>,\n) -> Result<(), ValidationError> {\n    match channels {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(channels) => validate_channels_endpoint(channels),\n    }\n}\n\npub fn validate_url(url: &Url) -> Result<(), ValidationError> {\n    let scheme = url.scheme();\n    if scheme == \"https\" || scheme == \"http\" {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"url\"),\n            Some(\"Endpoint URL schemes must be http or https\"),\n        ))\n    }\n}\n\nfn validate_url_unrequired(val: &UnrequiredField<Url>) -> Result<(), ValidationError> {\n    match val {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(val) => validate_url(val),\n    }\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nfn example_endpoint_description() -> &'static str {\n    \"An example endpoint name\"\n}\n\nfn example_filter_types() -> Vec<&'static str> {\n    vec![\"user.signup\", \"user.deleted\"]\n}\n\nfn endpoint_disabled_default() -> bool {\n    false\n}\n\nfn example_endpoint_url() -> &'static str {\n    \"https://example.com/webhook/\"\n}\n\nfn example_endpoint_version() -> u16 {\n    1\n}\n\nfn default_endpoint_version() -> Option<u16> {\n    Some(1)\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointIn {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n    /// List of message channels this endpoint listens to (omit for all)\n    #[validate(custom = \"validate_channels_endpoint\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n\n    #[validate]\n    #[serde(default)]\n    #[serde(rename = \"secret\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub key: Option<EndpointSecret>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\nimpl EndpointIn {\n    pub fn key_take_or_generate(\n        &mut self,\n        encryption: &Encryption,\n        sig_type: &DefaultSignatureType,\n    ) -> error::Result<EndpointSecretInternal> {\n        if let Some(key) = self.key.take() {\n            EndpointSecretInternal::from_endpoint_secret(key, encryption)\n        } else {\n            generate_secret(encryption, sig_type)\n        }\n    }\n}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for EndpointIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    #[allow(deprecated)]\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        l<|fim_suffix|>\n        model.description = Set(description);\n        model.rate_limit = Set(rate_limit.map(|x| x.into()));\n        model.uid = Set(uid);\n        model.url = Set(url.into());\n        model.version = Set(version.unwrap_or(1).into());\n        model.disabled = Set(disabled);\n        model.event_types_ids = Set(event_types_ids);\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\nstruct EndpointUpdate {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n\n    /// List of message channels this endpoint listens to (omit for all)\n    #[validate(custom = \"validate_channels_endpoint\")]\n    #[validate]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\nimpl ModelIn for EndpointUpdate {\n    type ActiveModel = endpoint::ActiveModel;\n\n    #[allow(deprecated)]\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointUpdate {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            metadata: _,\n        } = self;\n\n        model.description = Set(description);\n        model.rate_limit = Set(rate_limit.map(|x| x.into()));\n        model.uid = Set(uid);\n        model.url = Set(url.into());\n        model.version = Set(version.unwrap_or(1).into());\n        model.disabled = Set(disabled);\n        model.event_types_ids = Set(event_types_ids);\n        model.channels = Set(channels);\n    }\n}\n\nimpl EndpointUpdate {\n    #[allow(deprecated)]\n    pub fn into_in_with_default_key(self) -> EndpointIn {\n        let EndpointUpdate {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            metadata,\n        } = self;\n\n        EndpointIn {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            metadata,\n\n            key: None,\n        }\n    }\n}\n\n#[derive(Clone, Debug, Default, Serialize, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointPatch {\n    #[serde(default)]\n    #[serde(skip_serializing_if = \"UnrequiredField::is_absent\")]\n    #[validate(custom = \"validate_no_control_characters_unrequired\")]\n    pub description: UnrequiredField<String>,\n\n    #[validate(custom = \"validate_rate_limit_patch\")]\n    #[serde(default, skip_serializing_if = \"UnrequiredNullableField::is_absent\")]\n    pub rate_limit: UnrequiredNullableField<u16>,\n\n    #[validate]\n    #[serde(default, skip_serializing_if = \"UnrequiredN<|fim_middle|>", "completion": "let EndpointIn {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            key: _,\n            metadata: _,\n        } = self;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "let_declaration", "line_range": [216, 227]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct DocusignConfigOut {}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl DocusignConfigOut {\n    pub fn new() -> Self {\n        Self {}\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/docusign_config_out.rs", "node_type": "impl_item", "line_range": [7, 11]}
{"prompt": "<|fim_prefix|>p.description(desc.as_ref())\n}\n\npub fn get_unix_timestamp() -> u64 {\n    SystemTime::now()\n        .duration_since(UNIX_EPOCH)\n        .unwrap()\n        .as_secs()\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationPath {\n    pub app_id: ApplicationIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationEndpointPath {\n    pub app_id: ApplicationIdOrUid,\n    pub endpoint_id: EndpointIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgEndpointPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n    pub endpoint_id: EndpointIdOrUid,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct ApplicationMsgAttemptPath {\n    pub app_id: ApplicationIdOrUid,\n    pub msg_id: MessageIdOrUid,\n    pub attempt_id: MessageAttemptId,\n}\n\n#[derive(Deserialize, JsonSchema)]\npub struct EventTypeNamePath {\n    pub event_type_name: EventTypeName,\n}\n\n/// JsonStatus is a wrapper over `axum::extract::Json` as a handler output.\n///\n/// Setting the `STATUS` const parameter automatically sets the response\n/// status code, as well as inserting it into the aide documentation.\npub struct JsonStatus<const STATUS: u16, T: JsonSchema + Serialize>(pub T);\n\nimpl<const STATUS: u16, T: JsonSchema + Serialize> IntoResponse for JsonStatus<STATUS, T> {\n    fn into_response(self) -> axum::response::Response {\n        (\n            StatusCode::from_u16(STATUS).unwrap(),\n            axum::extract::Json(self.0),\n        )\n            .into_response()\n    }\n}\n\nimpl<const STATUS: u16, T: JsonSchema + Serialize> OperationOutput for JsonStatus<STATUS, T> {\n    type Inner = T;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        axum::extract::Json::<T>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(resp) = Self::operation_response(ctx, operation) {\n            vec![(Some(STATUS), resp)]\n        } else {\n            vec![]\n        }\n    }\n}\n\n/// JsonStatusUpsert is a wrapper over `axum::extract::Json` as a handler\n/// output.\n///\n/// It is a special casing of `JsonStatus` for situations where a resource is\n/// either being updated or created within the same operation. In case of\n/// `Updated` HTTP 200 OK is returned, in case of `Created` HTTP 201 CREATED\n/// is returned.\npub enum JsonStatusUpsert<T: JsonSchema + Serialize> {\n    Updated(T),\n    Created(T),\n}\n\nimpl<T: JsonSchema + Serialize> IntoResponse for JsonStatusUpsert<T> {\n    fn into_response(self) -> axum::response::Response {\n        let (status, body) = match self {\n            JsonStatusUpsert::Updated(v) => (StatusCode::OK, v),\n            JsonStatusUpsert::Created(v) => (StatusCode::CREATED, v),\n        };\n        (status, axum::extract::Json(body)).into_response()\n    }\n}\n\nimpl<T: JsonSchema + Serialize> OperationOutput for JsonStatusUpsert<T> {\n    type Inner = T;\n\n    fn operation_response(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Option<aide::openapi::Response> {\n        axum::extract::Json::<T>::operation_response(ctx, operation)\n    }\n\n    fn inferred_responses(\n        ctx: &mut aide::gen::GenContext,\n        operation: &mut aide::openapi::Operation,\n    ) -> Vec<(Option<u16>, aide::openapi::Response)> {\n        if let Some(resp) = Self::operation_response(ctx, operation) {\n            vec![\n                (Some(StatusCode::OK.into()), resp.clone()),\n                (Some(StatusCode::CREATED.into()), resp),\n            ]\n        } else {\n            vec![]\n        }\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use serde_json::json;\n    use validator::Validate;\n\n    use super::{default_limit, validate_no_control_characters, validation_errors, Pagination};\n    use crate::{core::types::ApplicationUid, error::ValidationErrorItem};\n\n    #[derive(Debug, Validate)]\n    struct ValidationErrorTestStruct {\n        #[validate(range(min = 10, message = \"Below 10\"))]\n        a: u32,\n\n        #[validate]\n        b: ValidationErrorTestStructInner,\n\n        #[validate]\n        c: Vec<ValidationErrorTestStructInner>,\n    }\n\n    #[derive(Debug, Validate)]\n    struct ValidationErrorTestStructInner {\n        #[validate(range(max = 10, message = \"Above 10\"))]\n        inner: u8,\n    }\n\n    #[test]\n    fn test_validation_errors_fn() {\n        let valid = ValidationErrorTestStruct {\n            a: 11,\n            b: ValidationErrorTestStructInner { inner: 1 },\n            c: vec![\n                ValidationErrorTestStructInner { inner: 2 },\n                ValidationErrorTestStructInner { inner: 3 },\n            ],\n        };\n        let invalid = ValidationErrorTestStruct {\n            a: 9,\n            b: ValidationErrorTestStructInner { inner: 11 },\n            c: vec![\n                ValidationErrorTestStructInner { inner: 12 },\n                ValidationErrorTestStructInner { inner: 13 },\n            ],\n        };\n\n        assert_eq!(valid.validate(), Ok(()));\n\n        let errs = invalid.validate().unwrap_err();\n        let errs = validation_errors(vec![], errs);\n\n        assert_eq!(errs.len(), 4);\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"a\".to_owned()],\n            msg: \"Below 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"b\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"c\".to_owned(), \"[0]\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n        assert!(errs.contains(&ValidationErrorItem {\n            loc: vec![\"c\".to_owned(), \"[1]\".to_owned(), \"inner\".to_owned()],\n            msg: \"Above 10\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }));\n    }\n\n    #[test]\n    fn test_pagination_defaults() {\n        let p: Pagination<ApplicationUid> = serde_json::from_value(json!({})).unwrap();\n        assert_eq!(p.limit.0, default_limit().0);\n    }\n\n    #[test]\n    fn test_pagination_validation() {\n        let p: Pagination<ApplicationUid> =\n            serde_json::from_value(json!({\"iterator\": \"$$invalid-appuid\"})).unwrap();\n        assert!(p.validate().is_err());\n\n        let p: Pagination<ApplicationUid> =\n            serde_json::from_value(json!({ \"iterator\": \"valid-appuid\"})).unwrap();\n        p.validate().unwrap();\n    }\n\n    #[derive(Debug, serde::Deserialize, PartialEq)]\n    struct TestPaginationDeserializationStruct {\n        iterator: super::ReversibleIterator<crate::core::types::MessageId>,\n    }\n\n    #[test]\n    fn test_pagination_deserialization() {\n        let a = serde_json::json!({\"iterator\": \"msg_274DTsX0wVTSLvo91QopQgZrjDV\"});\n        l<|fim_suffix|>\n        assert_eq!(\n            serde_json::from_value::<TestPaginationDeserializationStruct>(a).unwrap(),\n            TestPaginationDeserializationStruct {\n                iterator: super::ReversibleIterator::Normal(crate::core::types::MessageId(\n                    \"msg_274DTsX0wVTSLvo91QopQgZrjDV\".to_owned()\n                ))\n            }\n        );\n        assert_eq!(\n            serde_json::from_value::<TestPaginationDeserializationStruct>(b).unwrap(),\n            TestPaginationDeserializationStruct {\n                iterator: super::ReversibleIterator::Prev(crate::core::types::MessageId(\n                    \"msg_274DTsX0wVTSLvo91QopQgZrjDV\".to_owned()\n                ))\n            }\n        );\n    }\n\n    #[test]\n    fn test_validate_no_control_characters() {\n        let a = \"A good string\";\n        let b = \"A\\u{0000} bad string\";\n\n        assert!(validate_no_control_characters(a).is_ok());\n        assert!(validate_no_control_characters(b).is_err());\n    }\n}\n<|fim_middle|>", "completion": "let b = serde_json::json!({\"iterator\": \"-msg_274DTsX0wVTSLvo91QopQgZrjDV\"});", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/utils/mod.rs", "node_type": "let_declaration", "line_range": [961, 961]}
{"prompt": "<|fim_prefix|>ally generated (recommended)\n#[aide_annotate(op_id = \"v1.endpoint.create\")]\npub(super) async fn create_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        op_webhooks,\n        ..\n    }): State<AppState>,\n    _: Path<ApplicationPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointIn>,\n) -> Result<JsonStatus<201, EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let (endp, metadata) = create_endp_from_data(db, cfg, &op_webhooks, app, data)\n        .await\n        .trace()?;\n\n    Ok(JsonStatus((endp, metadata.data).into()))\n}\n\n/// Get an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.get\")]\npub(super) async fn get_endpoint(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<EndpointOut>> {\n    let (endp, metadata) = endpoint::Entity::secure_find_by_id_or_uid(app.id, endpoint_id)\n        .find_also_related(endpointmetadata::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let metadata = metadata.map(|m| m.data).unwrap_or_default();\n\n    Ok(Json((endp, metadata).into()))\n}\n\nasync fn update_endp_from_data(\n    db: &DatabaseConnection,\n    op_webhooks: &OperationalWebhookSender,\n    app: application::Model,\n    endp: endpoint::ActiveModel,\n    metadata: endpointmetadata::ActiveModel,\n) -> Result<(endpoint::Model, endpointmetadata::Model)> {\n    let (endp, metadata) = {\n        let txn = db.begin().await?;\n        let endp = endp.update(&txn).await.map_err(http_error_on_conflict)?;\n        let metadata = metadata.upsert_or_delete(&txn).await.trace()?;\n        txn.commit().await?;\n        (endp, metadata)\n    };\n\n    let app_uid = app.uid;\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointUpdated(EndpointEvent::new(app_uid.as_ref(), &endp)),\n        )\n        .await?;\n\n    Ok((endp, metadata))\n}\n\n/// Update an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.update\")]\npub(super) async fn update_endpoint(\n    State(AppState {\n        ref db,\n        ref cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(mut data): ValidatedJson<EndpointUpdate>,\n) -> Result<JsonStatusUpsert<EndpointOut>> {\n    if let Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    validate_endpoint_url(&data.url, cfg.endpoint_https_only)?;\n\n    let models = endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n        .await\n        .trace()?;\n\n    if let Some((mut endp, mut metadata)) = models {\n        metadata.data = Set(mem::take(&mut data.metadata));\n        data.update_model(&mut endp);\n        let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Updated((endp, metadata.data).into()))\n    } else {\n        let data = data.into_in_with_default_key();\n        let (endp, metadata) = create_endp_from_data(db, cfg, op_webhooks, app, data)\n            .await\n            .trace()?;\n        Ok(JsonStatusUpsert::Created((endp, metadata.data).into()))\n    }\n}\n\n/// Partially update an endpoint.\n#[aide_annotate]\npub(super) async fn patch_endpoint(\n    State(AppState {\n        ref db,\n        cfg,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n    ValidatedJson(data): ValidatedJson<EndpointPatch>,\n) -> Result<Json<EndpointOut>> {\n    if let UnrequiredNullableField::Some(ref event_types_ids) = data.event_types_ids {\n        validate_event_types(db, event_types_ids, &app.org_id).await?;\n    }\n    if let UnrequiredField::Some(url) = &data.url {\n        validate_endpoint_url(url, cfg.endpoint_https_only)?;\n    }\n\n    let (mut endp, mut metadata) =\n        endpoint::ActiveModel::fetch_with_metadata(db, app.id.clone(), endpoint_id)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut patch_data = data; // need to alias so we can use data for `patch_field_non_nullable!`\n\n    let data = mem::take(&mut patch_data.metadata);\n    patch_field_non_nullable!(metadata, data);\n    patch_data.update_model(&mut endp);\n    let (endp, metadata) = update_endp_from_data(db, op_webhooks, app, endp, metadata)\n        .await\n        .trace()?;\n\n    Ok(Json((endp, metadata.data).into()))\n}\n\n/// Delete an endpoint.\n#[aide_annotate(op_id = \"v1.endpoint.delete\")]\npub(super) async fn delete_endpoint(\n    State(AppState {\n        ref db,\n        ref op_webhooks,\n        ..\n    }): State<AppState>,\n    Path(ApplicationEndpointPath { endpoint_id, .. }): Path<ApplicationEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<NoContent> {\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    // Cloning the ID/UID out of endp before it's consumed below\n    let endpoint_id = endp.id.clone();\n    let endpoint_uid = endp.uid.clone();\n\n    <|fim_suffix|>\n    endp.deleted = Set(true);\n    endp.uid = Set(None); // We don't want deleted UIDs to clash\n    endp.update(db).await?;\n\n    op_webhooks\n        .send_operational_webhook(\n            &app.org_id,\n            OperationalWebhook::EndpointDeleted(EndpointEvent {\n                app_id: app.id,\n                app_uid: app.uid,\n                endpoint_id,\n                endpoint_uid,\n            }),\n        )\n        .await?;\n\n    Ok(NoContent)\n}\n\n/// This module is here so that our Result override doesn't conflict\nmod hack {\n    use sea_orm::FromQueryResult;\n\n    use crate::core::types::EventTypeName;\n\n    #[derive(Debug, FromQueryResult)]\n    pub struct EventTypeNameResult {\n        pub name: EventTypeName,\n    }\n}\n\nasync fn validate_event_types(\n    db: &DatabaseConnection,\n    event_types_ids: &EventTypeNameSet,\n    org_id: &OrganizationId,\n) -> Result<()> {\n    let event_types: Vec<EventTypeNameResult> = eventtype::Entity::secure_find(org_id.clone())\n        .filter(eventtype::Column::Deleted.eq(false))\n        .select_only()\n        .column(eventtype::Column::Name)\n        .into_model::<EventTypeNameResult>()\n        .all(db)\n        .await?;\n    let event_types: HashSet<EventTypeName> =\n        HashSet::from_iter(event_types.into_iter().map(|x| x.name));\n    let missing: Vec<&EventTypeName> = event_types_ids\n        .0\n        .iter()\n        .filter(|x| !event_types.contains(x))\n        .collect();\n\n    if missing.is_empty() {\n        Ok(())\n    } else {\n        let missing = missing\n            .into_iter()\n            .map(|x| &(x.0[..]))\n            .collect::<Vec<&str>>()\n            .join(\", \");\n        Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n            loc: vec![\"body\".to_owned(), \"filterTypes\".to_owned()],\n            msg: format!(\"The following event types don't exist: {missing}\"),\n            ty: \"value_error\".to_owned(),\n        }])\n        .into())\n    }\n}\n\nfn validate_endpoint_url(url: &Url, https_only: bool) -> Result<()> {\n    if !https_only {\n        return Ok(());\n    }\n\n    let scheme = url.scheme();\n    if scheme == \"https\" {\n        Ok(())\n    } else {\n        Err(HttpError::unprocessable_entity(vec![ValidationErrorItem {\n            loc: vec![\"body\".to_owned(), \"url\".to_owned()],\n            msg: \"Endpoint URL schemes must be https when endpoint_https_only is set.\".to_owned(),\n            ty: \"value_error\".to_owned(),\n        }])\n        .into())\n    }\n}\n<|fim_middle|>", "completion": "let mut endp: endpoint::ActiveModel = endp.into();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/crud.rs", "node_type": "let_declaration", "line_range": [270, 270]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\n#[derive(Args, Clone)]\npub struct IntegrationListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// The sorting order of the returned items\n    #[arg(long)]\n    pub order: Option<Ordering>,\n}\n\nimpl From<IntegrationListOptions> for svix::api::IntegrationListOptions {\n    fn from(value: IntegrationListOptions) -> Self {\n        let IntegrationListOptions {\n            limit,\n            iterator,\n            order,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            order,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct IntegrationCreateOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<IntegrationCreateOptions> for svix::api::IntegrationCreateOptions {\n    fn from(value: IntegrationCreateOptions) -> Self {\n        let IntegrationCreateOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct IntegrationRotateKeyOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<IntegrationRotateKeyOptions> for svix::api::IntegrationRotateKeyOptions {\n    fn from(value: IntegrationRotateKeyOptions) -> Self {\n        let IntegrationRotateKeyOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct IntegrationArgs {\n    #[command(subcommand)]\n    pub command: IntegrationCommands,\n}\n\n#[derive(Subcommand)]\npub enum IntegrationCommands {\n    /// List the application's integrations.\n    List {\n        app_id: String,\n        #[clap(flatten)]\n        options: IntegrationListOptions,\n    },\n    /// Create an integration.\n    Create {\n        app_id: String,\n        integration_in: crate::json::JsonOf<IntegrationIn>,\n        #[clap(flatten)]\n        options: IntegrationCreateOptions,\n    },\n    /// Get an integration.\n    Get { app_id: String, id: String },\n    /// Update an integration.\n    Update {\n        app_id: String,\n        id: String,\n        integration_update: crate::json::JsonOf<IntegrationUpdate>,\n    },\n    /// Delete an integration.\n    Delete { app_id: String, id: String },\n    /// Get an integration's key.\n    GetKey { app_id: String, id: String },\n    /// Rotate the integration's key. The previous key will be immediately revoked.\n    RotateKey {\n        app_id: String,\n        id: String,\n        #[clap(flatten)]\n        options: IntegrationRotateKeyOptions,\n    },\n}\n\nimpl IntegrationCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::List { app_id, options } => {\n                <|fim_suffix|>\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                app_id,\n                integration_in,\n                options,\n            } => {\n                let resp = client\n                    .integration()\n                    .create(app_id, integration_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get { app_id, id } => {\n                let resp = client.integration().get(app_id, id).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Update {\n                app_id,\n                id,\n                integration_update,\n            } => {\n                let resp = client\n                    .integration()\n                    .update(app_id, id, integration_update.into_inner())\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Delete { app_id, id } => {\n                client.integration().delete(app_id, id).await?;\n            }\n            Self::GetKey { app_id, id } => {\n                #[allow(deprecated)]\n                let resp = client.integration().get_key(app_id, id).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::RotateKey {\n                app_id,\n                id,\n                options,\n            } => {\n                let resp = client\n                    .integration()\n                    .rotate_key(app_id, id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "let resp = client\n                    .integration()\n                    .list(app_id, Some(options.into()))\n                    .await?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/integration.rs", "node_type": "let_declaration", "line_range": [110, 113]}
{"prompt": "<|fim_prefix|>use std::{\n    collections::HashSet,\n    net::TcpListener,\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse http::StatusCode;\nuse serde::de::IgnoredAny;\nuse svix_server::{\n    cfg::{ProxyAddr, ProxyBypassCfg, ProxyConfig},\n    v1::endpoints::message::MessageIn,\n};\nuse tokio::{\n    io::{AsyncReadExt, AsyncWriteExt},\n    time::timeout,\n};\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg, TestClient, TestReceiver,\n};\n\n#[ignore] // works with microsocks running at the specified address\n#[tokio::test]\n<|fim_suffix|>\n\nfn socks_proxy_config() -> ProxyConfig {\n    ProxyConfig {\n        addr: ProxyAddr::new(\"socks5://localhost:1080\").unwrap(),\n        noproxy: None,\n    }\n}\n\n#[ignore] // works with tinyproxy running at the specified address\n#[tokio::test]\nasync fn test_message_delivery_via_http_proxy() {\n    use crate::utils::start_svix_server_with_cfg;\n\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(http_proxy_config());\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n    run_proxy_test(&client).await;\n}\n\nfn http_proxy_config() -> ProxyConfig {\n    ProxyConfig {\n        addr: ProxyAddr::new(\"http://localhost:8888\").unwrap(),\n        noproxy: None,\n    }\n}\n\nasync fn run_proxy_test(client: &TestClient) {\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let app_id = create_test_app(client, \"proxyTest\").await.unwrap().id;\n    create_test_endpoint(client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let msg_payload = serde_json::json!({ \"test\": \"value\" });\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let received_payload = timeout(Duration::from_secs(2), receiver.data_recv.recv())\n        .await\n        .unwrap()\n        .unwrap();\n\n    assert_eq!(received_payload, msg_payload);\n}\n\n// This doesn't actually handle requests successfully, but it does allow us\n// to see which hostnames are requested of it.\nstruct MockProxyServer {\n    matched_hosts: Arc<Mutex<HashSet<String>>>,\n    addr: String,\n    variant: MockProxyVariant,\n}\n\nenum MockProxyVariant {\n    Http,\n    Socks5,\n}\n\nimpl MockProxyServer {\n    pub fn new(variant: MockProxyVariant) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n        let addr = match variant {\n            MockProxyVariant::Http => {\n                format!(\"http://{}\", listener.local_addr().unwrap())\n            }\n            MockProxyVariant::Socks5 => {\n                format!(\"socks5://{}\", listener.local_addr().unwrap())\n            }\n        };\n        let matched_hosts = Arc::new(Mutex::new(HashSet::new()));\n\n        match variant {\n            MockProxyVariant::Http => {\n                tokio::spawn(Self::http_listener(listener, matched_hosts.clone()))\n            }\n            MockProxyVariant::Socks5 => {\n                tokio::spawn(Self::socks5_listener(listener, matched_hosts.clone()))\n            }\n        };\n\n        Self {\n            matched_hosts,\n            addr,\n            variant,\n        }\n    }\n\n    pub async fn http_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        loop {\n            let (mut stream, _addr) = listener.accept().await.unwrap();\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let mut buffer = [0; 512];\n\n                if let Ok(size) = stream.read(&mut buffer).await {\n                    if size == 0 {\n                        return;\n                    }\n                    let request = String::from_utf8_lossy(&buffer[..size]);\n                    if let Some(host) = request\n                        .strip_prefix(\"CONNECT \")\n                        .and_then(|s| s.split(' ').next())\n                        .and_then(|s| s.strip_suffix(\":443\"))\n                    {\n                        let mut guard = matched_hosts.lock().unwrap();\n                        guard.insert(host.to_string());\n                    }\n                }\n            });\n        }\n    }\n\n    pub async fn socks5_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        use socks5_proto::{\n            handshake::{\n                Method as HandshakeMethod, Request as HandshakeRequest,\n                Response as HandshakeResponse,\n            },\n            Address, Reply, Request as SocksRequest, Response as SocksResponse,\n        };\n        loop {\n            let (mut stream, _) = match listener.accept().await {\n                Ok(v) => v,\n                Err(_) => continue,\n            };\n\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let hs_req = match HandshakeRequest::read_from(&mut stream).await {\n                    Ok(req) => req,\n                    Err(_) => {\n                        return;\n                    }\n                };\n\n                if hs_req.methods.contains(&HandshakeMethod::NONE) {\n                    if HandshakeResponse::new(HandshakeMethod::NONE)\n                        .write_to(&mut stream)\n                        .await\n                        .is_err()\n                    {\n                        return;\n                    }\n                } else {\n                    let _ = HandshakeResponse::new(HandshakeMethod::UNACCEPTABLE)\n                        .write_to(&mut stream)\n                        .await;\n                    return;\n                }\n\n                let Ok(socks_req) = SocksRequest::read_from(&mut stream).await else {\n                    return;\n                };\n\n                let host = match &socks_req.address {\n                    Address::SocketAddress(socket_addr) => socket_addr.ip().to_string(),\n                    Address::DomainAddress(domain_bytes, _port) => {\n                        String::from_utf8_lossy(domain_bytes).to_string()\n                    }\n                };\n                if !host.is_empty() {\n                    let mut guard = matched_hosts.lock().unwrap();\n                    guard.insert(host);\n                }\n\n                let abort_resp =\n                    SocksResponse::new(Reply::ConnectionNotAllowed, Address::unspecified());\n                let _ = abort_resp.write_to(&mut stream).await;\n                let _ = stream.shutdown().await;\n            });\n        }\n    }\n\n    pub fn matches(&self) -> HashSet<String> {\n        let guard = self.matched_hosts.lock().unwrap();\n        println!(\"************ MATCHES {guard:?}\");\n        guard.clone()\n    }\n}\n\n#[tokio::test]\nasync fn test_http_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Http);\n    test_proxy_exceptions(listener).await\n}\n\n#[tokio::test]\nasync fn test_socks5_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Socks5);\n    test_proxy_exceptions(listener).await\n}\n\nasync fn test_proxy_exceptions(listener: MockProxyServer) {\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(ProxyConfig {\n        addr: ProxyAddr::new(listener.addr.clone()).unwrap(),\n        noproxy: Some(ProxyBypassCfg(\"10.0.0.0/8, 8.8.8.8, 0ec2:1652:6021:693b:f928:565d:5a0e:de9f, www.svix.com, .google.com\".to_owned())),\n    });\n    cfg.retry_schedule = vec![];\n\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"proxyTest\").await.unwrap().id;\n\n    // Note: Real hostnames here are not ideal, but difficult to test SOCKS without valid DNS entries:\n    create_test_endpoint(&client, &app_id, \"https://www.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://play.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"htt<|fim_middle|>", "completion": "async fn test_message_delivery_via_socks5() {\n    use crate::utils::start_svix_server_with_cfg;\n\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(socks_proxy_config());\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n    run_proxy_test(&client).await;\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_proxy.rs", "node_type": "function_item", "line_range": [26, 33]}
{"prompt": "<|fim_prefix|>   let mut http = HttpConnector::new_with_resolver(dns_resolver);\n        http.enforce_http(false);\n\n        if dangerous_disable_tls_verification {\n            tracing::warn!(\"TLS certificate verification has been disabled by the configuration.\");\n        }\n        let https = SvixHttpsConnector::new(http, proxy_config, dangerous_disable_tls_verification)\n            .expect(\"SvixHttpsConnector build failed\");\n\n        let client = hyper_util::client::legacy::Client::builder(TokioExecutor::new())\n            .http1_ignore_invalid_headers_in_responses(true)\n            .http1_title_case_headers(true)\n            .build(https);\n\n        Self {\n            client,\n            whitelist_nets,\n        }\n    }\n\n    pub async fn execute(&self, request: Request) -> Result<Response, Error> {\n        let resp = self.execute_inner(request, true).await?;\n        Ok(resp.map(Body::new))\n    }\n\n    pub fn execute_inner(\n        &self,\n        request: Request,\n        retry: bool,\n    ) -> BoxFuture<'_, Result<Response<Incoming>, Error>> {\n        async move {\n            let org_req = request.clone();\n            if let Some(auth) = request.uri.authority() {\n                if let Ok(ip) = auth.host().parse::<IpAddr>() {\n                    if !is_allowed(ip)\n                        && !self\n                            .whitelist_nets\n                            .iter()\n                            .any(|subnet| subnet.contains(&ip))\n                    {\n                        return Err(Error::BlockedIp);\n                    }\n                }\n            }\n\n            let mut req = if let Some(body) = request.body {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::from(body))\n                    .map_err(Error::InvalidHttpRequest)?\n            } else {\n                hyper::Request::builder()\n                    .method(request.method)\n                    .uri(request.uri)\n                    .version(request.version)\n                    .body(Full::default())\n                    .map_err(Error::InvalidHttpRequest)?\n            };\n\n            *req.headers_mut() = request.headers;\n\n            if let Some(header_names) = request.header_names {\n                req.extensions_mut().insert(header_names);\n            }\n\n            let start = Instant::now();\n            let res = if let Some(timeout) = request.timeout {\n                match tokio::time::timeout(timeout, self.client.request(req)).await {\n                    Ok(Ok(resp)) => Ok(resp),\n                    Ok(Err(e)) => Err(e.into()),\n                    Err(_to) => Err(Error::TimedOut),\n                }\n            } else {\n                self.client.request(req).await.map_err(Into::into)\n            };\n\n            if !retry {\n                return res;\n            }\n\n            match res {\n                Err(Error::FailedRequest(e)) if start.elapsed() < Duration::from_millis(1000) => {\n                    tracing::info!(\"Insta-retrying: {e}\");\n                    self.execute_inner(org_req, false).await\n                }\n                res => res,\n            }\n        }\n        .boxed()\n    }\n}\n\n#[derive(Clone)]\npub struct Request {\n    method: Method,\n    uri: Uri,\n    headers: HeaderMap,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    timeout: Option<Duration>,\n    version: Version,\n}\n\npub struct RequestBuilder {\n    method: Option<Method>,\n    uri: Option<Uri>,\n    accept: Option<HeaderValue>,\n    user_agent: Option<HeaderValue>,\n    headers: Option<HeaderMap>,\n    header_names: Option<HeaderCaseMap>,\n    body: Option<Vec<u8>>,\n    version: Option<Version>,\n    timeout: Option<Duration>,\n    basic_auth: Option<Vec<u8>>,\n\n    // Derived from body\n    content_type: Option<HeaderValue>,\n}\n\n#[derive(Debug)]\npub struct RequestBuildError(pub Vec<BuildError>);\n\nimpl std::fmt::Display for RequestBuildError {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        <|fim_suffix|>\n\n        f.write_str(\"Build failed\")?;\n\n        if let Some(first) = iter.next() {\n            write!(f, \": {first}\")?;\n\n            for err in iter {\n                write!(f, \"; {err}\")?;\n            }\n        }\n\n        Ok(())\n    }\n}\n\n#[derive(Debug, Error)]\npub enum BuildError {\n    #[error(\"uri missing\")]\n    UriMissing,\n    #[error(\"version missing\")]\n    VersionMissing,\n}\n\nfn decode_or_log(s: &str) -> String {\n    urlencoding::decode(s)\n        .map(|x| x.into_owned())\n        .unwrap_or_else(|_| {\n            tracing::error!(\"URL decoding failed\");\n            s.to_owned()\n        })\n}\n\nimpl RequestBuilder {\n    pub fn new() -> Self {\n        Self {\n            method: None,\n            uri: None,\n            accept: None,\n            user_agent: None,\n            headers: None,\n            header_names: None,\n            body: None,\n            version: None,\n            timeout: None,\n            content_type: None,\n            basic_auth: None,\n        }\n    }\n\n    pub fn method(mut self, method: Method) -> Self {\n        self.method = Some(method);\n        self\n    }\n\n    pub fn uri(mut self, uri: url::Url) -> Self {\n        let basic_auth = if uri.password().is_some() || !uri.username().is_empty() {\n            let username = decode_or_log(uri.username());\n            let password = uri.password().map(decode_or_log).unwrap_or_default();\n\n            Some(\n                Authorization::basic(&username, &password)\n                    .0\n                    .encode()\n                    .as_bytes()\n                    .to_vec(),\n            )\n        } else {\n            None\n        };\n        self.basic_auth = basic_auth;\n\n        let uri =\n            Uri::from_str(uri.as_str()).expect(\"If it's a valid url::Url, it's also a valid Uri\");\n        self.uri = Some(uri);\n        self\n    }\n\n    pub fn uri_str(self, uri: &str) -> Result<Self, url::ParseError> {\n        let uri = url::Url::from_str(uri)?;\n        Ok(self.uri(uri))\n    }\n\n    fn build_headers(\n        headers: CaseSensitiveHeaderMap,\n    ) -> (hyper::HeaderMap, hyper::ext::HeaderCaseMap) {\n        let mut hdr_map = hyper::HeaderMap::with_capacity(headers.len());\n        let mut case_sensitive_hdrs: hyper::HeaderMap<Bytes> =\n            hyper::HeaderMap::with_capacity(headers.len());\n        for (k, v) in headers.into_iter() {\n            match HeaderName::from_str(&k) {\n                Ok(key) => {\n                    hdr_map.insert(key.clone(), v);\n                    case_sensitive_hdrs.insert(key, Bytes::copy_from_slice(k.as_bytes()));\n                }\n                Err(e) => {\n                    tracing::error!(\"Failed to parse header {} {}\", k, e);\n                }\n            }\n        }\n        (hdr_map, case_sensitive_hdrs.into())\n    }\n\n    pub fn headers(mut self, headers: CaseSensitiveHeaderMap) -> Self {\n        let (hdrs, case_map) = Self::build_headers(headers);\n        self.headers = Some(hdrs);\n        self.header_names = Some(case_map);\n        self\n    }\n\n    pub fn body(mut self, body: Vec<u8>, content_type: HeaderValue) -> Self {\n        self.body = Some(body);\n        self.content_type = Some(content_type);\n        self\n    }\n\n    pub fn json_body<T: Serialize>(self, body: T) -> Result<Self, serde_json::Error> {\n        let body = serde_json::to_vec(&body)?;\n        Ok(self.body(body, HeaderValue::from_static(\"application/json\")))\n    }\n\n    pub fn version(mut self, version: Version) -> Self {\n        self.version = Some(version);\n        self\n    }\n\n    pub fn timeout(mut self, timeout: Duration) -> Self {\n        self.timeout = Some(timeout);\n        self\n    }\n\n    pub fn user_agent(mut self, user_agent: HeaderValue) -> Self {\n        self.user_agent = Some(user_agent);\n        self\n    }\n}\n\nimpl Default for RequestBuilder {\n    fn default() -> Self {\n        Self::new()\n    }\n}\n\nimpl RequestBuilder {\n    fn validate(&self) -> Result<(), RequestBuildError> {\n        let mut errs: Vec<BuildError> = Vec::new();\n        if self.uri.is_none() {\n            errs.push(BuildError::UriMissing);\n        }\n        if self.versi<|fim_middle|>", "completion": "let mut iter = self.0.iter();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/webhook_http_client.rs", "node_type": "let_declaration", "line_range": [237, 237]}
{"prompt": "<|fim_prefix|>\n\nimpl From<Delivery> for DeliveryWrapper {\n    fn from(value: Delivery) -> Self {\n        Self(value)\n    }\n}\n\nimpl DeliveryWrapper {\n    /// Delegates to the inner delivery types ack method.\n    async fn ack(self) -> Result<(), QueueError> {\n        self.0.ack().await.map_err(|(e, _)| e)\n    }\n    /// Delegates to the inner delivery types nack method.\n    async fn nack(self) -> Result<(), QueueError> {\n        self.0.nack().await.map_err(|(e, _)| e)\n    }\n\n    /// Decodes the inner delivery as String.\n    fn raw_payload(&self) -> Result<&str, QueueError> {\n        // TODO: used to be unsupported for redis. Is it now? Check for skipped tests to prove it.\n        let bytes = self.0.borrow_payload().ok_or(QueueError::NoData)?;\n        std::str::from_utf8(bytes).map_err(QueueError::generic)\n    }\n\n    /// Decodes the inner delivery as `serde_json::Value`.\n    fn payload(&self) -> Result<serde_json::Value, QueueError> {\n        self.0.payload_serde_json()?.ok_or(QueueError::NoData)\n    }\n}\n\n#[async_trait]\ntrait Consumer {\n    /// The source of the stream of messages, e.g. the name or id for the queue, subscription, etc.\n    fn source(&self) -> &str;\n    /// The name of the messaging system, e.g. rabbitmq, sqs, etc.\n    fn system(&self) -> &str;\n    /// Gets the channel sender for running transformations.\n    fn transformer_tx(&self) -> Option<&TransformerTx>;\n    /// The js source for the transformation to run on each payload.\n    fn transformation(&self) -> Option<&TransformationConfig>;\n    /// The client to use when creating messages in svix.\n    fn svix_client(&self) -> &Svix;\n\n    async fn transform(\n        &self,\n        script: String,\n        input: TransformerInput,\n    ) -> std::io::Result<JsObject> {\n        let (job, rx) = TransformerJob::new(script, input);\n        self.transformer_tx()\n            .as_ref()\n            .expect(\"transformations not configured\")\n            .send(job)\n            .map_err(|e| Error::Generic(e.to_string()))?;\n\n        let ret = rx\n            .await\n            .map_err(|_e| Error::Generic(\"transformation rx failed\".to_string()))\n            .and_then(|x| {\n                x.map_err(|_e| Error::Generic(\"transformation execution failed\".to_string()))\n            })?;\n\n        match ret {\n            TransformerOutput::Object(v) => Ok(v),\n            TransformerOutput::Invalid => {\n                Err(Error::Generic(\"transformation produced unexpected value\".to_string()).into())\n            }\n        }\n    }\n\n    /// Gets consumer (likely based on a config value), called by [`consume`].\n    async fn consumer(&self) -> std::io::Result<DynConsumer>;\n\n    /// Main consumer loop\n    async fn consume(&self) -> std::io::Result<()> {\n        let mut consumer = self.consumer().await?;\n        tracing::debug!(\"{} consuming: {}\", self.system(), self.source(),);\n        loop {\n            self.receive(&mut consumer).await?;\n        }\n    }\n\n    /// Pulls N messages off the queue and feeds them to [`Self::process`].\n    #[tracing::instrument(skip_all,\n    fields(\n        otel.kind = \"CONSUMER\",\n        messaging.system = self.system(),\n        messaging.operation = \"receive\",\n        messaging.source = self.source(),\n        svix_bridge_plugin.name = crate::PLUGIN_NAME,\n        svix_bridge_plugin.vers = crate::PLUGIN_VERS,\n    )\n    )]\n    async fn receive(&self, consumer: &mut DynConsumer) -> std::io::Result<()> {\n        // FIXME: omniqueue has a fixed batch size of 1 afaict. Would be nicer to pull N at a time.\n        let delivery = consumer.receive().await.map_err(Error::from)?;\n        self.process(delivery.into()).await?;\n        Ok(())\n    }\n\n    /// Parses the delivery as JSON and feeds it into [`create_svix_message`].\n    /// Will nack the delivery if either the JSON parse, transformation, or the request to svix fails.\n    #[tracing::instrument(skip_all, fields(messaging.operation = \"process\"))]\n    async fn process(&self, delivery: DeliveryWrapper) -> std::io::Result<()> {\n        let payload = if let Some(xform_cfg) = self.transformation() {\n            let input = match xform_cfg.format() {\n                TransformerInputFormat::Json => {\n                    let json_payload = match delivery.payload() {\n                        Ok(p) => p,\n                        Err(e) => {\n                            tracing::warn!(\"{e}\");\n                            delivery.nack().await.map_err(Error::from)?;\n                            return Ok(());\n                        }\n                    };\n                    TransformerInput::Json(json_payload)\n                }\n                TransformerInputFormat::String => {\n                    // N.b. our redis backend doesn't support string payloads, but higher up in the\n                    // call stack, during the plugin construction, we should be catching this and\n                    // giving an error about bad config.\n                    // If we get here somehow with a redis delivery, this call will panic.\n                    let raw_payload = match delivery.raw_payload() {\n                        Ok(p) => p,\n                        Err(e) => {\n                            tracing::warn!(\"{e}\");\n                            delivery.nack().await.map_err(Error::from)?;\n                            return Ok(());\n                        }\n                    };\n                    // FIXME: if we add a lifetime to `TransformerInput` we might avoid this allocation.\n                    TransformerInput::String(raw_payload.to_string())\n                }\n            };\n            <|fim_suffix|>\n            match self.transform(script, input).await {\n                Err(e) => {\n                    tracing::error!(\"nack: {e}\");\n                    delivery.nack().await.map_err(Error::from)?;\n                    return Ok(());\n                }\n                Ok(x) => serde_json::from_value(serde_json::Value::Object(x))?,\n            }\n        } else {\n            // Parse as JSON when not using a transformation because Create Message requires JSON.\n            // If this fails, the config needs to change.\n            let json_payload = match delivery.payload() {\n                Ok(p) => p,\n                Err(e) => {\n                    tracing::warn!(\"{e}\");\n                    delivery.nack().await.map_err(Error::from)?;\n                    return Ok(());\n                }\n            };\n            serde_json::from_value(json_payload)?\n        };\n\n        match create_svix_message(self.svix_client(), payload).await {\n            Ok(_) => {\n                tracing::trace!(\"ack\");\n                delivery.ack().await.map_err(Error::from)?\n            }\n            Err(e) => {\n                tracing::error!(\"nack: {e}\");\n                delivery.nack().await.map_err(Error::from)?\n            }\n        }\n        Ok(())\n    }\n}\n\nasync fn run_inner(consumer: &(impl Consumer + Send + Sync)) -> ! {\n    let mut fails: u64 = 0;\n    let mut last_fail = Instant::now();\n    let system_name = consumer.system();\n    let source = consumer.source();\n\n    tracing::info!(\"{system_name} starting: {source}\");\n\n    loop {\n        if let Err(e) = consumer.consume().await {\n            tracing::error!(\"{e}\");\n        }\n\n        tracing::error!(\"{system_name} disconnected: {source}\");\n\n        if last_fail.elapsed() > Duration::from_secs(10) {\n            // reset the fail count if we didn't have a hiccup in the past short while.\n            tracing::trace!(\"been a while since last fail, resetting count\");\n            fails = 0;\n        } else {\n            fails += 1;\n        }\n\n        last_fail = Instant::now();\n        tokio::time::sleep(Duration::from_millis((300 * fails).min(3000))).await;\n    }\n}\n\n#[tracing::instrument(skip_all, level = \"error\", fields(\n    app_id,\n    event_type = message.event_type\n))]\nasync fn create_svix_message(\n    svix: &Svix,\n    CreateMessageRequest { app_id, message }: CreateMessageRequest,\n) -> std::io::Result<()> {\n    svix.message()\n        .create(\n            app_id, message,\n            // FIXME: add a way for the caller to give an idempotency key like we have in kafka\n            None,\n        )\n        .await\n        .map_err(Error::from)?;\n    Ok(())\n}\n<|fim_middle|>", "completion": "let script = xform_cfg.source().clone();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/src/lib.rs", "node_type": "let_declaration", "line_range": [163, 163]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\nuse super::endpoint_disabled_trigger::EndpointDisabledTrigger;\n\n/// Sent when an ingest endpoint has been automatically disabled after\n/// continuous failures, or manually via an API call.\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct IngestEndpointDisabledEventData {\n    /// The Endpoint's ID.\n    #[serde(rename = \"endpointId\")]\n    pub endpoint_id: String,\n\n    /// The Endpoint's UID.\n    #[serde(rename = \"endpointUid\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub endpoint_uid: Option<String>,\n\n    #[serde(rename = \"failSince\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub fail_since: Option<String>,\n\n    /// The Source's ID.\n    #[serde(rename = \"sourceId\")]\n    pub source_id: String,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub trigger: Option<EndpointDisabledTrigger>,\n}\n\nimpl IngestEndpointDisabledEventData {\n    <|fim_suffix|>\n}\n<|fim_middle|>", "completion": "pub fn new(endpoint_id: String, source_id: String) -> Self {\n        Self {\n            endpoint_id,\n            endpoint_uid: None,\n            fail_since: None,\n            source_id,\n            trigger: None,\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/ingest_endpoint_disabled_event_data.rs", "node_type": "function_item", "line_range": [32, 40]}
{"prompt": "<|fim_prefix|>ed_body: None,\n            no_return_type: false,\n        }\n    }\n\n    pub fn with_body_param<T: serde::Serialize>(mut self, param: T) -> Self {\n        self.serialized_body = Some(serde_json::to_string(&param).unwrap());\n        self\n    }\n\n    pub fn with_optional_header_param(\n        mut self,\n        basename: &'static str,\n        param: Option<String>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.header_params.insert(basename, value);\n        }\n        self\n    }\n\n    pub fn with_query_param(mut self, basename: &'static str, param: impl QueryParamValue) -> Self {\n        self.query_params.insert(basename, param.encode());\n        self\n    }\n\n    pub fn with_optional_query_param<T: QueryParamValue>(\n        mut self,\n        basename: &'static str,\n        param: Option<T>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.query_params.insert(basename, value.encode());\n        }\n        self\n    }\n\n    pub fn with_path_param(mut self, basename: &'static str, param: String) -> Self {\n        self.path_params.insert(basename, param);\n        self\n    }\n\n    pub fn returns_nothing(mut self) -> Self {\n        self.no_return_type = true;\n        self\n    }\n\n    pub async fn execute<T: DeserializeOwned>(self, conf: &Configuration) -> Result<T, Error> {\n        match self.execute_with_backoff(conf).await? {\n            // This is a hack; if there's no_ret_type, T is (), but serde_json gives an\n            // error when deserializing \"\" into (), so deserialize 'null' into it\n            // instead.\n            // An alternate option would be to require T: Default, and then return\n            // T::default() here instead since () implements that, but then we'd\n            // need to impl default for all models.\n            None => Ok(serde_json::from_str(\"null\").expect(\"serde null value\")),\n            Some(bytes) => Ok(serde_json::from_slice(&bytes).map_err(Error::generic)?),\n        }\n    }\n\n    async fn execute_with_backoff(mut self, conf: &Configuration) -> Result<Option<Bytes>, Error> {\n        let no_return_type = self.no_return_type;\n        if self.method == http1::Method::POST && !self.header_params.contains_key(\"idempotency-key\")\n        {\n            self.header_params\n                .insert(\"idempotency-key\", format!(\"auto_{}\", uuid::Uuid::new_v4()));\n        }\n\n        const MAX_BACKOFF: Duration = Duration::from_secs(5);\n\n        let retry_schedule = match &conf.retry_schedule {\n            Some(schedule) => schedule,\n            None => &std::iter::successors(Some(Duration::from_millis(20)), |last_backoff| {\n                Some(MAX_BACKOFF.min(*last_backoff * 2))\n            })\n            .take(conf.num_retries as usize)\n            .collect(),\n        };\n        let mut retries = retry_schedule.iter();\n\n        let mut request = self.build_request(conf)?;\n        request\n            .headers_mut()\n            .insert(\"svix-req-id\", rand::rng().random::<u32>().into());\n\n        let mut retry_count = 0;\n\n        let execute_request = async |request| {\n            let response = conf.client.request(request).await.map_err(Error::generic)?;\n\n            let status = response.status();\n            if !status.is_success() {\n                Err(Error::from_response(status, response.into_body()).await)\n            } else if no_return_type {\n                Ok(None)\n            } else {\n                let bytes = response\n                    .into_body()\n                    .collect()\n                    .await\n                    .map_err(Error::generic)?\n                    .to_bytes();\n                Ok(Some(bytes))\n            }\n        };\n\n        loop {\n            let request_fut = execute_request(request.clone());\n            let res = if let Some(duration) = conf.timeout {\n                tokio::time::timeout(duration, request_fut)\n                    .await\n                    .map_err(Error::generic)?\n            } else {\n                request_fut.await\n            };\n\n            let next_backoff = retries.next().copied();\n\n            match res {\n                Ok(result) => return Ok(result),\n                e @ Err(Error::Validation(_)) => return e,\n                Err(Error::Http(err)) if err.status.as_u16() < 500 => return Err(Error::Http(err)),\n                e @ Err(_) => {\n                    if next_backoff.is_none() {\n                        return e;\n                    }\n                }\n            }\n\n            tokio::time::sleep(next_backoff.expect(\"next_backoff is always Some\")).await;\n            retry_count += 1;\n\n            request\n                .headers_mut()\n                .insert(\"svix-retry-count\", retry_count.into());\n        }\n    }\n\n    fn build_request(self, conf: &Configuration) -> Result<http1::Request<Full<Bytes>>, Error> {\n        const FRAGMENT: &AsciiSet = &CONTROLS.add(b' ').add(b'\"').add(b'<').add(b'>').add(b'`');\n        const PATH: &AsciiSet = &FRAGMENT.add(b'#').add(b'?').add(b'{').add(b'}');\n        const PATH_SEGMENT: &AsciiSet = &PATH.add(b'/').add(b'%');\n\n        let mut path = self.path.to_owned();\n        for (k, v) in self.path_params {\n            // replace {id} with the value of the id path param\n            let percent_encoded_path_param_value =\n                utf8_percent_encode(&v, PATH_SEGMENT).to_string();\n            path = path.replace(&format!(\"{{{k}}}\"), &percent_encoded_path_param_value);\n        }\n\n        let mut uri = format!(\"{}{}\", conf.base_path, path);\n\n        let mut query_string = url::form_urlencoded::Serializer::new(\"\".to_owned());\n        for (key, val) in self.query_params {\n            query_string.append_pair(key, &val);\n        }\n\n        let query_string_str = query_string.finish();\n        <|fim_suffix|>\n\n        let uri = http1::Uri::try_from(uri).map_err(Error::generic)?;\n        let mut req_builder = http1::Request::builder().uri(uri).method(self.method);\n\n        let mut request = if let Some(body) = self.serialized_body {\n            let req_headers = req_builder.headers_mut().unwrap();\n            req_headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n            req_headers.insert(CONTENT_LENGTH, body.len().into());\n            req_builder.body(Full::from(body)).map_err(Error::generic)?\n        } else {\n            req_builder.body(Full::default()).map_err(Error::generic)?\n        };\n\n        let request_headers = request.headers_mut();\n\n        // Detect the authorization type if it hasn't been set.\n        let auth = if conf.bearer_access_token.is_some() {\n            Auth::Bearer\n        } else {\n            Auth::None\n        };\n        match auth {\n            Auth::Bearer => {\n                if let Some(token) = &conf.bearer_access_token {\n                    let value = format!(\"Bearer {token}\")\n                        .try_into()\n                        .map_err(Error::generic)?;\n                    request_headers.insert(AUTHORIZATION, value);\n                }\n            }\n            Auth::None => {}\n        }\n\n        if let Some(user_agent) = &conf.user_agent {\n            let value = user_agent.try_into().map_err(Error::generic)?;\n            request_headers.insert(USER_AGENT, value);\n        }\n\n        for (k, v) in self.header_params {\n            let v = v.try_into().map_err(Error::generic)?;\n            request_headers.insert(k, v);\n        }\n\n        Ok(request)\n    }\n}\n\npub(crate) trait QueryParamValue {\n    fn encode(&self) -> String;\n}\n\nmacro_rules! impl_query_param_value {\n    ($ty:ty) => {\n        impl QueryParamValue for $ty {\n            fn encode(&self) -> String {\n                self.to_string()\n            }\n        }\n    };\n}\n\nimpl_query_param_value!(bool);\nimpl_query_param_value!(i32);\nimpl_query_param_value!(String);\nimpl_query_param_value!(models::BackgroundTaskStatus);\nimpl_query_param_value!(models::BackgroundTaskType);\nimpl_query_param_value!(models::ConnectorProduct);\nimpl_query_param_value!(models::MessageStatus);\nimpl_query_param_value!(models::Ordering);\nimpl_query_param_value!(models::StatusCodeClass);\n\nimpl QueryParamValue for Vec<String> {\n    fn encode(&self) -> String {\n        self.iter().format(\",\").to_string()\n    }\n}\n<|fim_middle|>", "completion": "if !query_string_str.is_empty() {\n            uri += \"?\";\n            uri += &query_string_str;\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/request.rs", "node_type": "if_expression", "line_range": [204, 207]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{collections::HashSet, time::Duration};\n\nuse anyhow::Result;\nuse chrono::{DateTime, Utc};\nuse reqwest::{StatusCode, Url};\nu<|fim_suffix|>use serde_json::json;\nuse svix::api::DashboardAccessOut;\nuse svix_server::{\n    core::types::{\n        metadata::Metadata, ApplicationId, EventChannel, EventTypeName, FeatureFlagSet, MessageId,\n    },\n    v1::{\n        endpoints::{\n            application::{ApplicationIn, ApplicationOut},\n            attempt::MessageAttemptOut,\n            auth::AppPortalAccessIn,\n            endpoint::{EndpointIn, EndpointOut, RecoverIn},\n            event_type::EventTypeIn,\n            message::{MessageIn, MessageOut, RawPayload},\n        },\n        utils::ListResponse,\n    },\n};\n\nuse super::{run_with_retries, TestClient};\n\n// App\n\npub fn application_in(name: &str) -> ApplicationIn {\n    ApplicationIn {\n        name: name.to_owned(),\n        ..Default::default()\n    }\n}\n\npub async fn create_test_app(client: &TestClient, name: &str) -> Result<ApplicationOut> {\n    client\n        .post(\"api/v1/app/\", application_in(name), StatusCode::CREATED)\n        .await\n}\n\npub async fn delete_test_app(client: &TestClient, id: ApplicationId) -> Result<()> {\n    client\n        .delete(&format!(\"api/v1/app/{id}/\"), StatusCode::NO_CONTENT)\n        .await\n}\n\n// Endpoint\n\npub fn default_test_endpoint() -> EndpointIn {\n    #[allow(deprecated)]\n    EndpointIn {\n        description: Default::default(),\n        rate_limit: Default::default(),\n        uid: Default::default(),\n        url: Url::parse(\"http://example.com\").unwrap(),\n        version: Some(1),\n        disabled: Default::default(),\n        event_types_ids: Default::default(),\n        channels: Default::default(),\n        key: Default::default(),\n        metadata: Default::default(),\n    }\n}\n\npub fn endpoint_in(url: &str) -> EndpointIn {\n    EndpointIn {\n        url: Url::parse(url).unwrap(),\n        ..default_test_endpoint()\n    }\n}\n\npub async fn create_test_endpoint(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    url: &str,\n) -> Result<EndpointOut> {\n    post_endpoint(client, app_id, endpoint_in(url)).await\n}\n\npub async fn post_endpoint(\n    client: &TestClient,\n    app_id: &str,\n    ep: EndpointIn,\n) -> Result<EndpointOut> {\n    client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep,\n            StatusCode::CREATED,\n        )\n        .await\n}\n\npub async fn put_endpoint(\n    client: &TestClient,\n    app_id: &str,\n    ep_id: &str,\n    ep: EndpointIn,\n) -> Result<EndpointOut> {\n    client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n            ep,\n            StatusCode::OK,\n        )\n        .await\n}\n\n// Message\n\npub fn message_in<T: Serialize>(event_type: &str, payload: T) -> Result<MessageIn> {\n    Ok(MessageIn {\n        event_type: EventTypeName(event_type.to_owned()),\n        payload: RawPayload::from_string(serde_json::to_string(&payload)?)?,\n        payload_retention_period: 5,\n        channels: None,\n        uid: None,\n        extra_params: None,\n        application: None,\n    })\n}\n\npub async fn create_test_message(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n) -> Result<MessageOut> {\n    client\n        .post(\n            &format!(\"api/v1/app/{}/msg/\", &app_id),\n            message_in(\"event.type\", payload)?,\n            StatusCode::ACCEPTED,\n        )\n        .await\n}\n\npub async fn create_test_msg_with(\n    client: &TestClient,\n    app_id: &ApplicationId,\n    payload: serde_json::Value,\n    event_type: &str,\n    channel: impl IntoIterator<Item = &str>,\n) -> MessageOut {\n    let channels: HashSet<EventChannel> = channel\n        .into_iter()\n        .map(|x| EventChannel(x.to_string()))\n        .collect();\n\n    let mut message_in = json!({\n        \"eventType\": event_type,\n        \"payload\": payload,\n        \"payloadRetentionPeriod\": 5,\n    });\n    if !channels.is_empty() {\n        message_in[\"channels\"] = json!(channels);\n    }\n\n    client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in,\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap()\n}\n\npub fn event_type_in(\n    name: &str,\n    schema: impl Into<Option<serde_json::Value>>,\n) -> Result<EventTypeIn> {\n    Ok(EventTypeIn {\n        name: EventTypeName(name.to_owned()),\n        description: \"test-event-description\".to_owned(),\n        deleted: false,\n        schemas: schema.into().map(|s| serde_json::from_value(s).unwrap()),\n        feature_flag: None,\n        deprecated: false,\n    })\n}\n\n// Common tests\npub async fn common_test_list<\n    ModelOut: DeserializeOwned + PartialEq + std::fmt::Debug,\n    ModelIn: Serialize,\n>(\n    client: &TestClient,\n    path: &str,\n    create_model: fn(usize) -> ModelIn,\n    sort_asc: bool,\n    supports_reverse: bool,\n) -> Result<()> {\n    let mut items = Vec::new();\n    for i in 0..10 {\n        let item: ModelOut = client\n            .post(path, create_model(i), StatusCode::CREATED)\n            .await\n            .unwrap();\n        // Sleep for 5ms because KsuidMs has 4ms accuracy so things got out of order\n        tokio::time::sleep(Duration::from_millis(5)).await;\n        items.push(item);\n    }\n\n    let original_list = run_with_retries(|| async {\n        let list = client\n            .get::<ListResponse<ModelOut>>(&format!(\"{path}?with_content=true\"), StatusCode::OK)\n            .await\n            .unwrap();\n\n        assert_eq!(list.data.len(), 10);\n\n        Ok(list)\n    })\n    .await\n    .unwrap();\n\n    if sort_asc {\n        for i in 0..10 {\n            assert_eq!(items.get(i), original_list.data.get(i));\n        }\n    } else {\n        for i in 0..10 {\n            assert_eq!(items.get(9 - i), original_list.data.get(i));\n        }\n    }\n\n    // Limit results\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=1\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 1);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=50\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=10\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 10);\n    assert!(list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(&format!(\"{path}?limit=6\"), StatusCode::OK)\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 6);\n    assert!(!list.done);\n\n    let list = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=6&iterator={}\", list.iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list.data.len(), 4);\n    assert!(list.done);\n\n    let prev = client\n        .get::<ListResponse<ModelOut>>(\n            &format!(\"{path}?limit=3&iterator={}\", list.prev_iterator.unwrap()),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(prev.data.len(), 3);\n    assert_eq!(\n        prev.data.first().unwrap(),\n        original_list.data.get(3).unwrap()\n    );\n\n    let _list = client\n        .get::<IgnoredAny>(\n            &format!(\"{path}?limit=6&iterator=BAD-$$$ITERATOR\"),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    if supports_reverse {\n        let opposite_order = if sort_asc { \"descending\" } else { \"ascending\" };\n\n        let opposite_1 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\"{path}?limit=3&order={opposite_order}\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let opposite_2 = client\n            .get::<ListResponse<ModelOut>>(\n                &format!(\n                    \"{path}?limit=3&order={opposite_order}&iterator={}\",\n                    opposite_1.iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n       <|fim_middle|>", "completion": "use serde::{\n    de::{DeserializeOwned, IgnoredAny},\n    Serialize,\n};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/utils/common_calls.rs", "node_type": "use_declaration", "line_range": [9, 12]}
{"prompt": "<|fim_prefix|>use deno_core::JsRuntime;\nuse serde_json::json;\n<|fim_suffix|>\n\nuse super::{run_script_inner, validate_script};\n\nfn get_test_rt() -> JsRuntime {\n    JsRuntime::new(Default::default())\n}\n\n// Really just trying to figure out if the deno runtime is working the way I hope.\n#[test]\nfn test_happy_fn() {\n    let src = r#\"\n    function handler(input) {\n        return { \"x\": 123, ...input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({ \"y\": 456 }).into(), src).unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n            assert_eq!(v[\"y\"].as_i64(), Some(456));\n        }\n        TransformerOutput::Invalid => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\nfn test_invalid_output_bool() {\n    let src = r#\"\n    function handler(input) {\n        return false;\n    }\n    \"#\n    .to_string();\n\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => panic!(\"got unexpected return value\"),\n    }\n}\n\n#[test]\n// FIXME: serde decodes arrays with keys like \"0\", \"1\"... in this situation, failing the test.\n#[ignore]\nfn test_invalid_output_array() {\n    let src = r#\"\n    function handler(input) {\n        return [1, 2];\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(&mut rt, json!({}).into(), src).unwrap();\n    match res {\n        TransformerOutput::Invalid => (),\n        TransformerOutput::Object(_) => {\n            panic!(\"got unexpected return value\");\n        }\n    }\n}\n\n/// Receives a string input, parses as JSON in js, then returns the result back to rust.\n#[test]\nfn test_string_input() {\n    let src = r#\"\n    function handler(input) {\n        return JSON.parse(input);\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(r#\"{\"x\": 123}\"#)),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"x\"].as_i64(), Some(123));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n/// Take the string input and just add it to a field in the returned object.\n/// The string should make it through, back to rust, as-is.\n#[test]\nfn test_string_input2() {\n    let src = r#\"\n    function handler(input) {\n        return { \"payload\": input };\n    }\n    \"#\n    .to_string();\n    let mut rt = get_test_rt();\n    let res = run_script_inner(\n        &mut rt,\n        TransformerInput::String(String::from(\"Hello World\")),\n        src,\n    )\n    .unwrap();\n    match res {\n        TransformerOutput::Object(v) => {\n            assert_eq!(v[\"payload\"].as_str(), Some(\"Hello World\"));\n        }\n        TransformerOutput::Invalid => (),\n    }\n}\n\n#[test]\nfn test_validate_script_bad_syntax_is_err() {\n    assert!(validate_script(\"let 123 = ';\").is_err());\n}\n\n#[test]\nfn test_validate_script_empty_handler_is_ok() {\n    assert!(validate_script(\"function handler() { }\").is_ok());\n}\n\n#[test]\nfn test_validate_script_arrow_fn_is_ok() {\n    assert!(validate_script(\"const handler = () => ({ a: 123 })\").is_ok());\n}\n\n/// Technically, this should be legal though the utility is questionable.\n#[test]\nfn test_validate_script_empty_is_ok() {\n    assert!(validate_script(\"\").is_ok());\n    assert!(validate_script(\"    \").is_ok());\n}\n<|fim_middle|>", "completion": "use svix_bridge_types::{TransformerInput, TransformerOutput};", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/runtime/tests.rs", "node_type": "use_declaration", "line_range": [3, 3]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Module defining utilities for crating `tracing` spans compatible with OpenTelemetry's\n//! conventions.\nuse std::net::SocketAddr;\n\nuse axum::extract::{ConnectInfo, MatchedPath};\nuse http::header;\nuse opentelemetry::trace::TraceContextExt;\nuse svix_ksuid::{KsuidLike, KsuidMs};\nuse tower_http::{\n    classify::ServerErrorsFailureClass,\n    trace::{MakeSpan, OnFailure, OnResponse},\n};\nuse tracing::field::{debug, Empty};\nuse tracing_opentelemetry::OpenTelemetrySpanExt;\n\n/// An implementor of [`MakeSpan`] which creates `tracing` spans populated with information about\n/// the request received by an `axum` web server.\n#[derive(Clone, Copy)]\npub struct AxumOtelSpanCreator;\n\nimpl<B> MakeSpan<B> for AxumOtelSpanCreator {\n    fn make_span(&mut self, request: &http::Request<B>) -> tracing::Span {\n        let user_agent = request\n            .headers()\n            .get(header::USER_AGENT)\n            .and_then(|header| header.to_str().ok());\n\n        let host = request\n            .headers()\n            .get(header::HOST)\n            .and_then(|header| header.to_str().ok());\n\n        let http_route = request\n            .extensions()\n            .get::<MatchedPath>()\n            .map(|p| p.as_str());\n\n        let client_ip = request\n            .extensions()\n            .get::<ConnectInfo<SocketAddr>>()\n            .map(|ConnectInfo(ip)| debug(ip));\n\n        let request_id = request\n            .headers()\n            .get(\"x-request-id\")\n            .and_then(|id| id.to_str().map(ToOwned::to_owned).ok())\n            // If `x-request-id` isn't set, check `svix-req-id`. If the `svix-req-id` isn't a\n            // valid `str`, or it isn't set, then fallback to a random [`KsuidMs`]\n            .or_else(|| {\n                request\n                    .headers()\n                    .get(\"svix-req-id\")\n                    .and_then(|v| v.to_str().map(ToOwned::to_owned).ok())\n            })\n            .unwrap_or_else(|| KsuidMs::new(None, None).to_string());\n\n        let remote_context = opentelemetry::global::get_text_map_propagator(|p| {\n            p.extract(&opentelemetry_http::HeaderExtractor(request.headers()))\n        });\n        let remote_span = remote_context.span();\n        let span_context = remote_span.span_context();\n        let trace_id = span_context\n            .is_valid()\n            .then(|| span_context.trace_id().to_string());\n\n        let idempotency_key = request\n            .headers()\n            .get(\"idempotency-key\")\n            .and_then(|v| v.to_str().ok());\n\n        let span = tracing::error_span!(\n            \"HTTP request\",\n            grpc.code = Empty,\n            http.client_ip = client_ip,\n            http.versions = ?request.version(),\n            http.host = host,\n            http.method = ?request.method(),\n            http.route = http_route,\n            http.scheme = request.uri().scheme().map(debug),\n            http.status_code = Empty,\n            http.target = request.uri().path_and_query().map(|p| p.as_str()),\n            http.user_agent = user_agent,\n            otel.kind = \"server\",\n            otel.status_code = Empty,\n            request_id,\n            trace_id,\n            idempotency_key,\n            org_id = tracing::field::Empty,\n            app_id = tracing::field::Empty,\n        );\n\n        span.set_parent(remote_context);\n\n        span\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnResponse;\n\nimpl<B> OnResponse<B> for AxumOtelOnResponse {\n    fn on_response(\n        self,\n        response: &http::Response<B>,\n        latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        let status = response.status().as_u16().to_string();\n        span.record(\"http.status_code\", tracing::field::display(status));\n        span.record(\"otel.status_code\", \"OK\");\n\n        tracing::debug!(\n            \"finished processing request latency={} ms status={}\",\n            latency.as_millis(),\n            response.status().as_u16(),\n        );\n    }\n}\n\n#[derive(Clone, Copy, Debug)]\npub struct AxumOtelOnFailure;\n\nimpl OnFailure<ServerErrorsFailureClass> for AxumOtelOnFailure {\n    fn on_failure(\n        &mut self,\n        failure_classification: ServerErrorsFailureClass,\n        _latency: std::time::Duration,\n        span: &tracing::Span,\n    ) {\n        m<|fim_suffix|>    }\n}\n<|fim_middle|>", "completion": "match failure_classification {\n            ServerErrorsFailureClass::StatusCode(status) if status.is_server_error() => {\n                span.record(\"otel.status_code\", \"ERROR\");\n            }\n            _ => {}\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/otel_spans.rs", "node_type": "match_expression", "line_range": [133, 138]}
{"prompt": "<|fim_prefix|>//! Test module for worker functionality that depends on external networking and test utilities.\n//! As such they are included with integration tests for organizational purposes.\nuse std::{net::TcpListener, sync::Arc, time::Duration};\n\nuse axum::extract::State;\nuse http::StatusCode;\nuse svix_server::v1::{\n    endpoints::{attempt::MessageAttemptOut, endpoint::EndpointOut},\n    utils::ListResponse,\n};\nuse tokio::sync::Mutex;\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, create_test_message},\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n};\n\n/// Runs a full Axum server with two endpoints. The first endpoint redirects to the second endpoint\n/// while the second endpoint records whether it has been visited. This is such that we can check\n/// that no redirection is taken by the Svix worker's `reqwest::Client`\nstruct RedirectionVisitReportingReceiver {\n    pub base_uri: String,\n    pub jh: tokio::task::JoinHandle<()>,\n    pub has_been_visited: Arc<Mutex<bool>>,\n}\n\n#[derive(Clone)]\nstruct RedirectionVisitReportingState {\n    has_been_visited: Arc<Mutex<bool>>,\n    resp_with: axum::http::StatusCode,\n}\n\nimpl RedirectionVisitReportingReceiver {\n    pub fn start(resp_with: axum::http::StatusCode) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n        let base_uri = format!(\"http://{}\", listener.local_addr().unwrap());\n\n        <|fim_suffix|>\n\n        let routes = axum::Router::new()\n            .route(\n                \"/first/\",\n                axum::routing::post(redirecting_receiver_route).get(redirecting_receiver_route),\n            )\n            .route(\n                \"/second/\",\n                axum::routing::post(visit_reporting_receiver_route)\n                    .get(visit_reporting_receiver_route),\n            )\n            .with_state(RedirectionVisitReportingState {\n                has_been_visited: has_been_visited.clone(),\n                resp_with,\n            })\n            .into_make_service();\n\n        let jh = tokio::spawn(async move {\n            axum::serve(listener, routes).await.unwrap();\n        });\n\n        RedirectionVisitReportingReceiver {\n            base_uri,\n            jh,\n            has_been_visited,\n        }\n    }\n}\n\nasync fn redirecting_receiver_route() -> axum::response::Redirect {\n    axum::response::Redirect::permanent(\"/second/\")\n}\n\nasync fn visit_reporting_receiver_route(\n    State(RedirectionVisitReportingState {\n        has_been_visited: visited,\n        resp_with,\n    }): State<RedirectionVisitReportingState>,\n) -> StatusCode {\n    *visited.lock().await = true;\n    resp_with\n}\n\n// The worker has\n#[tokio::test]\nasync fn test_no_redirects_policy() {\n    let (client, _jh) = start_svix_server().await;\n    let receiver = RedirectionVisitReportingReceiver::start(StatusCode::OK);\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let _ep_id = create_test_endpoint(&client, &app_id, &format!(\"{}/first/\", receiver.base_uri))\n        .await\n        .unwrap()\n        .id;\n    let msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n        .await\n        .unwrap()\n        .id;\n\n    run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/msg/{msg_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let attempt = attempts.data.first();\n\n        if let Some(attempt) = attempt {\n            assert_eq!(attempt.response_status_code, 308);\n            Ok(())\n        } else {\n            anyhow::bail!(\"No attempt found\");\n        }\n    })\n    .await\n    .unwrap();\n\n    // Assert that the second endpoint has not been visited\n    assert!(!*receiver.has_been_visited.lock().await);\n\n    receiver.jh.abort();\n}\n\n/// This tests that endpoints are successfully disabled after the retry schedule is exhausted\n/// multiple times without intermittent success over a period exceeding the grace period. So the\n/// tests don't take too long, these grace period and expiration period will be reconfigured to be\n/// on the order of seconds\n#[tokio::test]\nasync fn test_endpoint_disable_on_repeated_failure() {\n    let mut cfg = get_default_test_config();\n\n    if !matches!(cfg.cache_type, svix_server::cfg::CacheType::None) {\n        cfg.retry_schedule = vec![];\n        cfg.endpoint_failure_disable_after = Duration::from_secs(2);\n\n        let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n        let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n        let ep_id = create_test_endpoint(&client, &app_id, \"http://bad.url/\")\n            .await\n            .unwrap()\n            .id;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        tokio::time::sleep(Duration::from_millis(2_500)).await;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        run_with_retries(|| async {\n            let ep: EndpointOut = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if !ep.ep.disabled {\n                anyhow::bail!(\"Endpoint not disabled\")\n            } else {\n                Ok(())\n            }\n        })\n        .await\n        .unwrap();\n    }\n}\n\n/// This tests that if a consistently failing endpoint is only tried after the expiration period\n/// has been exceeded, that it will not be disabled.\n#[tokio::test]\nasync fn test_endpoint_disable_expiration_duration() {\n    let mut cfg = get_default_test_config();\n\n    if !matches!(cfg.cache_type, svix_server::cfg::CacheType::None) {\n        cfg.retry_schedule = vec![];\n        cfg.endpoint_failure_disable_after = Duration::from_millis(250);\n\n        let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n        let app_id = create_test_app(&client, \"app\").await.unwrap().id;\n        let ep_id = create_test_endpoint(&client, &app_id, \"http://bad.url/\")\n            .await\n            .unwrap()\n            .id;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        tokio::time::sleep(Duration::from_millis(1200)).await;\n\n        let _msg_id = create_test_message(&client, &app_id, serde_json::json!({}))\n            .await\n            .unwrap()\n            .id;\n\n        // Cannot run with retries as it's not disabled by default and we are checking that it remains\n        // not disabled. So another sleep is required here.\n        tokio::time::sleep(Duration::from_millis(500)).await;\n        let ep: EndpointOut = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{ep_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert!(!ep.ep.disabled);\n    }\n}\n\n/// Because the endpoint disabling system requires failures, we need a test receiver that fails\n/// some of the time, but not all of the time, such as to be able to test that a successful response\n/// after a failure clears the cache for that endpoint.\nstruct SporadicallyFailingReceiver {\n    pub base_uri: String,\n    pub jh: tokio::task::JoinHandle<()>,\n}\n\n#[derive(Clone)]\nstruct SporadicallyFailingState {\n    count: Arc<Mutex<u8>>,\n    resp_with: (http::StatusCode, http::StatusCode),\n}\n\nimpl SporadicallyFailingReceiver {\n    pub fn start(resp_with: (http::StatusCode, http::StatusCode)) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n        let base_uri = format!(\"http://{}\", listener.local_addr().unwrap());\n\n        let count = Arc::new(Mutex::new(0u8<|fim_middle|>", "completion": "let has_been_visited = Arc::new(Mutex::new(false));", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/worker.rs", "node_type": "let_declaration", "line_range": [41, 41]}
{"prompt": "<|fim_prefix|>pub last_attempt: MessageAttempetLast,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\", content = \"data\")]\n#[allow(dead_code)]\npub enum OperationalWebhookTest {\n    #[serde(rename = \"endpoint.disabled\")]\n    EndpointDisabled(EndpointDisabledEvent),\n    #[serde(rename = \"endpoint.created\")]\n    EndpointCreated(EndpointEvent),\n    #[serde(rename = \"endpoint.updated\")]\n    EndpointUpdated(EndpointEvent),\n    #[serde(rename = \"endpoint.deleted\")]\n    EndpointDeleted(EndpointEvent),\n    #[serde(rename = \"message.attempt.exhausted\")]\n    MessageAttemptExhausted(MessageAttemptEvent),\n    #[serde(rename = \"message.attempt.failing\")]\n    MessageAttemptFailing(MessageAttemptEvent),\n}\n\n/// Operational webhooks are dispatched by a special organization, so this function returns two\n/// [`TestClient`]s, one with the Svix Management org token, and one with a random org token.\n///\n/// Additionally it returns the [`OrganizationId`] of the random  organization such that it may be\n/// included as an application of the of the Operational webhooks organization.\nfn start_svix_server_with_operational_webhooks(\n    mut cfg: ConfigurationInner,\n) -> (\n    TestClient,\n    TestClient,\n    OrganizationId,\n    tokio::task::JoinHandle<()>,\n) {\n    let op_webhook_jwt = generate_org_token(&cfg.jwt_signing_config, management_org_id()).unwrap();\n\n    let org_id = OrganizationId::new(None, None);\n    let regular_jwt = generate_org_token(&cfg.jwt_signing_config, org_id.clone()).unwrap();\n\n    let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n    // Could update this fn to take a tokio TcpListener instead, but that's a pretty large diff\n    // for very little benefit (since this is just test code anyways).\n    listener.set_nonblocking(true).unwrap();\n    let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n\n    let base_url = format!(\"http://{}\", listener.local_addr().unwrap());\n\n    cfg.operational_webhook_address = Some(base_url.clone());\n    let cfg = Arc::new(cfg);\n\n    let jh = tokio::spawn(svix_server::run_with_prefix(\n        Some(svix_ksuid::Ksuid::new(None, None).to_string()),\n        cfg,\n        Some(listener),\n    ));\n\n    (\n        TestClient::new(base_url.clone(), &regular_jwt),\n        TestClient::new(base_url, &op_webhook_jwt),\n        org_id,\n        jh,\n    )\n}\n\n#[tokio::test]\nasync fn test_endpoint_create_update_and_delete() {\n    let cfg = get_default_test_config();\n\n    let (client_regular, client_op, org_id, _jh) = start_svix_server_with_operational_webhooks(cfg);\n\n    // Setup operational webhook Application and Endpoint\n    let op_webhook_app: ApplicationOut = client_op\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name: \"TestOperationalWebhookApplication\".to_owned(),\n                rate_limit: None,\n                uid: Some(ApplicationUid(org_id.to_string())),\n                metadata: Metadata::default(),\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let _op_webhook_endp: EndpointOut = client_op\n        .post(\n            &format!(\"api/v1/app/{}/endpoint/\", op_webhook_app.id),\n            EndpointIn {\n                description: \"TestOperationalWebhookEndpoint\".to_owned(),\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Setup regular Application and Endpoint\n    let regular_app = create_test_app(&client_regular, \"TestOperationalWebhookApplicationRegular\")\n        .await\n        .unwrap();\n    let regular_endp = create_test_endpoint(&client_regular, &regular_app.id, \"http://junk.url\")\n        .await\n        .unwrap();\n\n    let op_webhook_out = receiver.data_recv.recv().await.unwrap();\n    assert_eq!(\n        op_webhook_out.get(\"type\").unwrap().as_str().unwrap(),\n        \"endpoint.created\"\n    );\n    let op_webhook_out: OperationalWebhookTest = serde_json::from_value(op_webhook_out).unwrap();\n\n    m<|fim_suffix|>\n\n    // Update endpoint\n    let regular_endp: EndpointOut = client_regular\n        .put(\n            &format!(\n                \"api/v1/app/{}/endpoint/{}/\",\n                regular_app.id, regular_endp.id\n            ),\n            EndpointIn {\n                description: \"Updated description\".to_owned(),\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    let op_webhook_out = receiver.data_recv.recv().await.unwrap();\n    assert_eq!(\n        op_webhook_out.get(\"type\").unwrap().as_str().unwrap(),\n        \"endpoint.updated\"\n    );\n    let op_webhook_out: OperationalWebhookTest = serde_json::from_value(op_webhook_out).unwrap();\n\n    match op_webhook_out {\n        OperationalWebhookTest::EndpointUpdated(EndpointEvent {\n            app_id,\n            app_uid,\n            endpoint_id,\n            endpoint_uid,\n        }) => {\n            assert_eq!(app_id, regular_app.id);\n            assert_eq!(app_uid, regular_app.uid);\n            assert_eq!(endpoint_id, regular_endp.id);\n            assert_eq!(endpoint_uid, regular_endp.ep.uid);\n        }\n        _ => panic!(\"Got wrong type\"),\n    };\n\n    // Rotate secrets\n    client_regular\n        .post_without_response(\n            &format!(\n                \"api/v1/app/{}/endpoint/{}/secret/rotate/\",\n                regular_app.id, regular_endp.id\n            ),\n            EndpointSecretRotateIn::default(),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let op_webhook_out = receiver.data_recv.recv().await.unwrap();\n    assert_eq!(\n        op_webhook_out.get(\"type\").unwrap().as_str().unwrap(),\n        \"endpoint.updated\"\n    );\n\n    // And finally delete the endpoint\n    client_regular\n        .delete(\n            &format!(\n                \"api/v1/app/{}/endpoint/{}/\",\n                regular_app.id, regular_endp.id\n            ),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let op_webhook_out = receiver.data_recv.recv().await.unwrap();\n    assert_eq!(\n        op_webhook_out.get(\"type\").unwrap().as_str().unwrap(),\n        \"endpoint.deleted\"\n    );\n    let op_webhook_out: OperationalWebhookTest = serde_json::from_value(op_webhook_out).unwrap();\n\n    match op_webhook_out {\n        OperationalWebhookTest::EndpointDeleted(EndpointEvent {\n            app_id,\n            app_uid,\n            endpoint_id,\n            endpoint_uid,\n        }) => {\n            assert_eq!(app_id, regular_app.id);\n            assert_eq!(app_uid, regular_app.uid);\n            assert_eq!(endpoint_id, regular_endp.id);\n            assert_eq!(endpoint_uid, regular_endp.ep.uid);\n        }\n        _ => panic!(\"Got wrong type\"),\n    };\n}\n\n#[tokio::test]\nasync fn test_message_attempt_operational_webhooks() {\n    let mut cfg = get_default_test_config();\n\n    cfg.retry_schedule = (0..5).map(|_| Duration::from_millis(1)).collect();\n\n    let (client_regular, client_op, org_id, _jh) = start_svix_server_with_operational_webhooks(cfg);\n\n    // Setup operational webhook Application and Endpoint\n    let op_webhook_app: ApplicationOut = client_op\n        .post(\n            \"api/v1/app/\",\n            ApplicationIn {\n                name: \"TestOperationalWebhookApplication\".to_owned(),\n                rate_limit: None,\n                uid: Some(ApplicationUid(org_id.to_string())),\n                metadata: Metadata::default(),\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let _op_webhook_endp: EndpointOut = client_op\n        .post(\n            &format!(\"api/v1/app/{}/endpoint/\", op_webhook_app.id),\n            EndpointIn {\n                description: \"TestOperationalWebhookEndpoint\".to_owned(),\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // Setup re<|fim_middle|>", "completion": "match op_webhook_out {\n        OperationalWebhookTest::EndpointCreated(EndpointEvent {\n            app_id,\n            app_uid,\n            endpoint_id,\n            endpoint_uid,\n        }) => {\n            assert_eq!(app_id, regular_app.id);\n            assert_eq!(app_uid, regular_app.uid);\n            assert_eq!(endpoint_id, regular_endp.id);\n            assert_eq!(endpoint_uid, regular_endp.ep.uid);\n        }\n        _ => panic!(\"Got wrong type\"),\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_operational_webhooks.rs", "node_type": "match_expression", "line_range": [192, 205]}
{"prompt": "<|fim_prefix|>use std::{marker::PhantomData, num::NonZeroUsize, sync::Arc, time::Duration};\n\nuse omniqueue::{\n    backends::InMemoryBackend, Delivery, DynConsumer, QueueConsumer, ScheduledQueueProducer,\n};\nuse serde::{de::DeserializeOwned, Deserialize, Serialize};\n\nuse crate::{\n    cfg::{Configuration, QueueBackend},\n    core::{\n        retry::{run_with_retries, Retry},\n        types::{ApplicationId, EndpointId, MessageAttemptTriggerType, MessageId},\n    },\n    error::{Error, ErrorType, Result, Traceable},\n};\n\npub mod rabbitmq;\npub mod redis;\n\nconst RETRY_SCHEDULE: &[Duration] = &[\n    Duration::from_millis(10),\n    Duration::from_millis(20),\n    Duration::from_millis(40),\n];\n\npub type TaskQueueDelivery = SvixOmniDelivery<QueueTask>;\n\nfn should_retry(err: &Error) -> bool {\n    matches!(err.typ, ErrorType::Queue(_))\n}\n\npub async fn new_pair(\n    cfg: &Configuration,\n    prefix: Option<&str>,\n) -> (TaskQueueProducer, TaskQueueConsumer) {\n    match cfg.queue_backend() {\n        QueueBackend::Redis(_)\n        | QueueBackend::RedisCluster(_)\n        | QueueBackend::RedisSentinel(_, _) => redis::new_pair(cfg, prefix).await,\n        QueueBackend::Memory => {\n            let (producer, consumer) = InMemoryBackend::builder()\n                .build_pair()\n                .await\n                .expect(\"building in-memory queue can't fail\");\n\n            (\n                TaskQueueProducer::new(producer),\n                TaskQueueConsumer::new(consumer),\n            )\n        }\n        QueueBackend::RabbitMq(dsn) => {\n            let prefix = prefix.unwrap_or(\"\");\n            let queue = format!(\"{prefix}-message-queue\");\n            // Default to a prefetch_size of 1, as it's the safest (least likely to starve consumers)\n            let prefetch_size = cfg.rabbit_consumer_prefetch_size.unwrap_or(1);\n            rabbitmq::new_pair(dsn, queue, prefetch_size)\n                .await\n                .expect(\"can't connect to rabbit\")\n        }\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageTask {\n    pub msg_id: MessageId,\n    pub app_id: ApplicationId,\n    pub endpoint_id: EndpointId,\n    pub trigger_type: MessageAttemptTriggerType,\n    pub attempt_count: u16,\n}\n\nimpl MessageTask {\n    pub fn new_task(\n        msg_id: MessageId,\n        app_id: ApplicationId,\n        endpoint_id: EndpointId,\n        trigger_type: MessageAttemptTriggerType,\n    ) -> QueueTask {\n        QueueTask::MessageV1(Self {\n            msg_id,\n            app_id,\n            endpoint_id,\n            attempt_count: 0,\n            trigger_type,\n        })\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct MessageTaskBatch {\n    pub msg_id: MessageId,\n    pub app_id: ApplicationId,\n    pub force_endpoint: Option<EndpointId>,\n    pub trigger_type: MessageAttemptTriggerType,\n}\n\n<|fim_suffix|>\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\n#[serde(tag = \"type\")]\npub enum QueueTask {\n    HealthCheck,\n    MessageV1(MessageTask),\n    MessageBatch(MessageTaskBatch),\n}\n\nimpl QueueTask {\n    /// Returns a type string, for logging.\n    pub fn task_type(&self) -> &'static str {\n        match self {\n            QueueTask::HealthCheck => \"HealthCheck\",\n            QueueTask::MessageV1(_) => \"MessageV1\",\n            QueueTask::MessageBatch(_) => \"MessageBatch\",\n        }\n    }\n\n    pub fn msg_id(&self) -> Option<&str> {\n        match self {\n            QueueTask::HealthCheck => None,\n            QueueTask::MessageV1(v1) => Some(&v1.msg_id),\n            QueueTask::MessageBatch(batch) => Some(&batch.msg_id),\n        }\n    }\n}\n\npub type TaskQueueProducer = SvixOmniProducer<QueueTask>;\npub type TaskQueueConsumer = SvixOmniConsumer<QueueTask>;\n\npub struct SvixOmniProducer<T: OmniMessage> {\n    inner: Arc<omniqueue::DynScheduledProducer>,\n    _phantom: PhantomData<T>,\n}\n\n// Manual impl to avoid adding 'Clone' bound on T\nimpl<T: OmniMessage> Clone for SvixOmniProducer<T> {\n    fn clone(&self) -> Self {\n        Self {\n            inner: self.inner.clone(),\n            _phantom: PhantomData,\n        }\n    }\n}\n\nimpl<T: OmniMessage> SvixOmniProducer<T> {\n    pub(super) fn new(inner: impl ScheduledQueueProducer + 'static) -> Self {\n        Self {\n            inner: Arc::new(inner.into_dyn_scheduled()),\n            _phantom: PhantomData,\n        }\n    }\n\n    #[tracing::instrument(skip_all, name = \"queue_send\")]\n    pub async fn send(&self, task: &T, delay: Option<Duration>) -> Result<()> {\n        let task = Arc::new(task);\n        run_with_retries(\n            || async {\n                if let Some(delay) = delay {\n                    self.inner\n                        .send_serde_json_scheduled(task.as_ref(), delay)\n                        .await\n                } else {\n                    self.inner.send_serde_json(task.as_ref()).await\n                }\n                .map_err(Into::into)\n            },\n            should_retry,\n            RETRY_SCHEDULE,\n        )\n        .await\n    }\n\n    #[tracing::instrument(skip_all, name = \"redrive_dlq\")]\n    pub async fn redrive_dlq(&self) -> Result<()> {\n        self.inner.redrive_dlq().await.map_err(Into::into)\n    }\n}\n\npub struct SvixOmniConsumer<T: OmniMessage> {\n    inner: DynConsumer,\n    _phantom: PhantomData<T>,\n}\n\npub trait OmniMessage: Serialize + DeserializeOwned + Send + Sync {\n    fn task_id(&self) -> Option<&str>;\n}\n\nimpl OmniMessage for QueueTask {\n    fn task_id(&self) -> Option<&str> {\n        self.msg_id()\n    }\n}\n\nimpl<T: OmniMessage> SvixOmniConsumer<T> {\n    pub(super) fn new(inner: impl QueueConsumer + 'static) -> Self {\n        Self {\n            inner: inner.into_dyn(),\n            _phantom: PhantomData,\n        }\n    }\n\n    #[tracing::instrument(skip_all, name = \"queue_receive_all\")]\n    pub async fn receive_all(&mut self, deadline: Duration) -> Result<Vec<SvixOmniDelivery<T>>> {\n        pub const MAX_MESSAGES: usize = 128;\n        self.inner\n            .receive_all(MAX_MESSAGES, deadline)\n            .await\n            .map_err(Error::from)\n            .trace()?\n            .into_iter()\n            .map(|acker| {\n                Ok(SvixOmniDelivery {\n                    task: Arc::new(\n                        acker\n                            .payload_serde_json()\n                            .map_err(|e| {\n                                Error::queue(format_args!(\"Failed to decode queue task: {e:?}\"))\n                            })?\n                            .ok_or_else(|| Error::queue(\"Unexpected empty delivery\"))?,\n                    ),\n\n                    acker,\n                })\n            })\n            .collect()\n    }\n\n    pub fn max_messages(&self) -> Option<NonZeroUsize> {\n        self.inner.max_messages()\n    }\n}\n\n#[derive(Debug)]\npub struct SvixOmniDelivery<T> {\n    pub task: Arc<T>,\n    pub(super) acker: Delivery,\n}\n\nimpl<T: OmniMessage> SvixOmniDelivery<T> {\n    pub async fn set_ack_deadline(&mut self, duration: Duration) -> Result<()> {\n        Ok(self.acker.set_ack_deadline(duration).await?)\n    }\n    pub async fn ack(self) -> Result<()> {\n        tracing::trace!(\n            task_id = self.task.task_id().map(tracing::field::display),\n            \"ack\"\n        );\n\n        let mut retry = Retry::new(should_retry, RETRY_SCHEDULE);\n        let mut acker = Some(self.acker);\n        loop {\n            if let Some(result) = retry\n                .run(|| async {\n                    match acker.take() {\n                        Some(delivery) => {\n                            delivery.ack().await.map_err(|(e, delivery)| {\n                                // Put the delivery back in acker before retrying, to\n                                // satisfy the expect above.\n                                acker = Some(delivery);\n                                e.into()\n                            })\n                        }\n                        None => unreachable!(),\n                    }\n                })\n                .await\n            {\n                return result;\n            }\n        }\n    }\n\n    pub async fn nack(self) -> Result<()> {\n        tracin<|fim_middle|>", "completion": "impl MessageTaskBatch {\n    pub fn new_task(\n        msg_id: MessageId,\n        app_id: ApplicationId,\n        force_endpoint: Option<EndpointId>,\n        trigger_type: MessageAttemptTriggerType,\n    ) -> QueueTask {\n        QueueTask::MessageBatch(Self {\n            msg_id,\n            app_id,\n            force_endpoint,\n            trigger_type,\n        })\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/queue/mod.rs", "node_type": "impl_item", "line_range": [99, 113]}
{"prompt": "<|fim_prefix|>use std::time::{Duration, Instant};\n\nuse anyhow::{Context, Result};\nuse dialoguer::Input;\nuse reqwest::Client;\nuse serde::Deserialize;\n\nuse crate::{config, config::Config};\n\npub async fn prompt(_cfg: &Config) -> Result<()> {\n    print!(\"Welcome to the Svix CLI!\\n\\n\");\n\n    let selections = &[\"Login in dashboard.svix.com\", \"Input token manually\"];\n    let selection = dialoguer::Select::new()\n        .with_prompt(\"How would you like to authenticate?\")\n        .items(selections)\n        .default(0)\n        .interact()?;\n\n    let auth_token = if selection == 0 {\n        dashboard_login().await?\n    } else {\n        Input::new()\n            .with_prompt(\"Auth Token\")\n            .validate_with({\n                move |input: &String| -> Result<()> {\n                    if !input.trim().is_empty() {\n                        Ok(())\n                    } else {\n                        Err(anyhow::anyhow!(\"auth token cannot be empty\"))\n                    }\n                }\n            })\n            .interact_text()?\n            .trim()\n            .to_string()\n    };\n\n    // Load from disk and update the prompted fields.\n    // There are other fields (not prompted for) related to \"relay\" for the `listen` command\n    // that we'd rather not wipe out if `login` is invoked.\n    let mut cfg = Config::load()?;\n    cfg.auth_token = Some(auth_token);\n    let fp = config::get_config_file_path()?;\n    if let Err(e) = cfg.save_to_disk(&fp) {\n        eprintln!(\"\\n{e:#}\\n\");\n        anyhow::bail!(\n            \"Failed to configure the Svix CLI, please try again or try setting your auth \\\n             token manually `SVIX_AUTH_TOKEN` environment variable.\"\n        );\n    }\n\n    println!(\n        \"All Set! Your config has been written to `{}`\",\n        fp.display()\n    );\n    println!(\n        \"Type `{} --help` to print the Svix CLI documentation!\",\n        crate::BIN_NAME\n    );\n    Ok(())\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct CliStartLoginSessionOut {\n    session_id: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct AuthTokenOut {\n    token: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct DiscoverySessionOut {\n    pub region: String,\n}\n\nconst DASHBOARD_URL: &str = \"https://dashboard.svix.com\";\nconst LOGIN_SERVER_URL: &str = \"https://api.svix.com\";\n\npub async fn dashboard_login() -> Result<String> {\n    let client = reqwest::Client::new();\n\n    let start_session = client\n        .post(format!(\"{LOGIN_SERVER_URL}/dashboard/cli/login/start\"))\n        .send()\n        .await\n        .context(\"Failed to get session ID. Could not connect to server.\")?\n        .json::<CliStartLoginSessionOut>()\n        .await\n        .context(\"Failed to get session ID. Invalid response.\")?;\n\n    let session_id = start_session.session_id;\n    let code = &session_id[0..4].to_uppercase();\n\n    let url = format!(\"{DASHBOARD_URL}/cli/login?sessionId={session_id}&code={code}\");\n\n    println!(\"\\nPlease approve the login in your browser, then return here.\");\n    println!(\"Verification code: \\x1b[32m{code}\\x1b[0m\\n\");\n\n    if let Err(e) = open::that(&url) {\n        eprintln!(\"Failed to open browser: {e}\");\n        println!(\"Please manually open this URL in your browser: {url}\");\n    }\n\n    println!(\"Waiting for approval...\");\n\n    // First, poll the discovery endpoint to get the region\n    let discovery_poll_url = format!(\"{LOGIN_SERVER_URL}/dashboard/cli/login/discovery/complete\");\n    let discovery_data: DiscoverySessionOut =\n        poll_session(&client, &discovery_poll_url, &session_id).await?;\n\n    <|fim_suffix|>\n    let region_server_url = format!(\"https://api.{region}.svix.com\");\n    let token_poll_url = format!(\"{region_server_url}/dashboard/cli/login/token/complete\");\n\n    // Then, poll the token endpoint to get the auth token\n    let token_data: AuthTokenOut = poll_session(&client, &token_poll_url, &session_id).await?;\n\n    println!(\"Authentication successful!\\n\");\n    Ok(token_data.token)\n}\n\nconst MAX_POLL_TIME: Duration = Duration::from_secs(5 * 60);\n\nasync fn poll_session<T>(client: &Client, poll_url: &str, session_id: &str) -> Result<T>\nwhere\n    T: for<'de> serde::Deserialize<'de>,\n{\n    let start_time: Instant = Instant::now();\n\n    while start_time.elapsed() < MAX_POLL_TIME {\n        let response = client\n            .post(poll_url)\n            .json(&serde_json::json!({ \"sessionId\": session_id }))\n            .send()\n            .await\n            .context(\"Failed to connect to authentication server\")?;\n\n        if response.status().is_success() {\n            return response\n                .json::<T>()\n                .await\n                .context(\"Failed to parse authentication data\");\n        } else if response.status() != reqwest::StatusCode::NOT_FOUND {\n            // Bail if session exists but has an error (is expired or something else)\n            let error_message = match response.json::<serde_json::Value>().await {\n                Ok(json) => json\n                    .get(\"detail\")\n                    .and_then(|d| d.as_str())\n                    .unwrap_or(\"Unknown error\")\n                    .to_string(),\n                Err(_) => \"Unknown error\".to_string(),\n            };\n\n            anyhow::bail!(\"Authentication failed: {error_message}\");\n        }\n\n        std::thread::sleep(std::time::Duration::from_secs(1));\n    }\n\n    anyhow::bail!(\"Authentication failed.\");\n}\n<|fim_middle|>", "completion": "let region = discovery_data.region;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/login.rs", "node_type": "let_declaration", "line_range": [117, 117]}
{"prompt": "<|fim_prefix|> .unwrap();\n\n        assert_eq!(all_attempts.data.len(), 6);\n        assert_eq!(first_three.data.len(), 3);\n        assert_eq!(&all_attempts.data[0..3], first_three.data.as_slice());\n\n        // Forward iterator\n        let last_three_manual: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/endpoint/{}/?limit=3&iterator={}\",\n                    app.id, ep.id, all_attempts.data[2].id\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let last_three_iter_field: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/endpoint/{}/?limit=3&iterator={}\",\n                    app.id,\n                    ep.id,\n                    first_three.iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(last_three_manual.data, last_three_iter_field.data);\n\n        assert_eq!(last_three_manual.data.len(), 3);\n        assert_eq!(&all_attempts.data[3..6], last_three_manual.data.as_slice());\n        assert!(last_three_manual.done);\n\n        // `prev` iterator\n        let two_and_three: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/endpoint/{}/?limit=2&iterator={}\",\n                    app.id,\n                    ep.id,\n                    last_three_manual.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(two_and_three.data.len(), 2);\n        assert_eq!(&all_attempts.data[1..3], two_and_three.data.as_slice());\n        assert!(!two_and_three.done);\n\n        let one: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/endpoint/{}/?limit=2&iterator={}\",\n                    app.id,\n                    ep.id,\n                    two_and_three.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(one.data.len(), 1);\n        assert_eq!(all_attempts.data[0], one.data[0]);\n        assert!(one.done);\n\n        // `after` field\n        let first_three_by_time: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/endpoint/{}/?after={}\",\n                    app.id, ep.id, all_attempts.data[3].created_at,\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        assert_eq!(first_three_by_time.data.len(), 4);\n        assert_eq!(\n            &all_attempts.data[0..=3],\n            first_three_by_time.data.as_slice()\n        );\n\n        // `before field`\n        let last_three_by_time: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/endpoint/{}/?before={}\",\n                    app.id, ep.id, all_attempts.data[2].created_at,\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        assert_eq!(last_three_by_time.data.len(), 3);\n        assert_eq!(&all_attempts.data[3..6], last_three_by_time.data.as_slice());\n    }\n}\n\n#[tokio::test]\nasync fn test_pagination_by_msg() {\n    let (client, _jh) = start_svix_server().await;\n\n    // Setup six endpoints and six messages so there's a sufficient number to test pagination\n    let app = create_test_app(&client, \"app1\").await.unwrap();\n\n    let mut receivers = Vec::new();\n    for _ in 0..6 {\n        receivers.push(TestReceiver::start(StatusCode::OK));\n    }\n\n    let mut eps = Vec::new();\n    for receiver in &receivers {\n        eps.push(\n            create_test_endpoint(&client, &app.id, &receiver.endpoint)\n                .await\n                .unwrap(),\n        );\n    }\n\n    let mut messages = Vec::new();\n    f<|fim_suffix|>    messages.push(\n        create_test_msg_with(\n            &client,\n            &app.id,\n            serde_json::json!({\"test\": \"data6\"}),\n            \"balloon.popped\",\n            [\"news\"],\n        )\n        .await,\n    );\n\n    // Wait until all attempts were made\n    run_with_retries(|| async {\n        for endp_id in eps.iter().map(|ep| &ep.id) {\n            let list: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\"api/v1/app/{}/attempt/endpoint/{endp_id}/\", app.id),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n\n            if list.data.len() != 6 {\n                anyhow::bail!(\"list len {}, not 6\", list.data.len());\n            }\n\n            let list_filtered: ListResponse<MessageAttemptOut> = client\n                .get(\n                    &format!(\n                        \"api/v1/app/{}/attempt/endpoint/{endp_id}/?channel=news\",\n                        app.id\n                    ),\n                    StatusCode::OK,\n                )\n                .await\n                .unwrap();\n            assert_eq!(list_filtered.data.len(), 1);\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    // By message\n    for msg in &messages {\n        let all_attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/msg/{}/\", app.id, msg.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        // Test Limit\n        let first_three: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{}/attempt/msg/{}/?limit=3\", app.id, msg.id),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(all_attempts.data.len(), 6);\n        assert_eq!(first_three.data.len(), 3);\n\n        assert_eq!(&all_attempts.data[0..3], first_three.data.as_slice());\n\n        // Forward iterator\n        let last_three_manual: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=3&iterator={}\",\n                    app.id, msg.id, all_attempts.data[2].id\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let last_three_iter_field: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=3&iterator={}\",\n                    app.id,\n                    msg.id,\n                    first_three.iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(last_three_manual.data, last_three_iter_field.data);\n\n        assert_eq!(last_three_manual.data.len(), 3);\n        assert_eq!(&all_attempts.data[3..6], last_three_manual.data.as_slice());\n        assert!(last_three_manual.done);\n\n        // `prev` iterator\n        let two_and_three: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=2&iterator={}\",\n                    app.id,\n                    msg.id,\n                    last_three_manual.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(two_and_three.data.len(), 2);\n        assert_eq!(&all_attempts.data[1..3], two_and_three.data.as_slice());\n        assert!(!two_and_three.done);\n\n        let one: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\n                    \"api/v1/app/{}/attempt/msg/{}/?limit=2&iterator={}\",\n                    app.id,\n                    msg.id,\n                    two_and_three.prev_iterator.unwrap()\n                ),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        assert_eq!(one.data.len(), 1);\n        assert_eq!(all_attempts.data[0], one.dat<|fim_middle|>", "completion": "for i in 1..=5usize {\n        messages.push(\n            create_test_message(\n                &client,\n                &app.id,\n                serde_json::json!({\n                    \"test\": i,\n                }),\n            )\n            .await\n            .unwrap(),\n        );\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "for_expression", "line_range": [1229, 1241]}
{"prompt": "<|fim_prefix|>E_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _ep_with_nonexistent_event: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let _et: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            event_type_in(\"et1\", None).unwrap(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let ep_with_valid_event: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_events.to_owned(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_with_valid_event.ep.event_types_ids.unwrap(), expected_et);\n\n    let ep_removed_events: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_valid_event.id),\n            ep_no_events.to_owned(),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(ep_removed_events.ep.event_types_ids.is_none());\n\n    let ep_removed_events = get_endpoint(&client, &app_id, &ep_removed_events.id)\n        .await\n        .unwrap();\n\n    assert!(ep_removed_events.ep.event_types_ids.is_none());\n\n    let ep_updated_events: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_valid_event.id),\n            ep_with_events.to_owned(),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_updated_events.ep.event_types_ids.unwrap(), expected_et);\n\n    let ep_updated_events: EndpointOut = get_endpoint(&client, &app_id, &ep_with_valid_event.id)\n        .await\n        .unwrap();\n\n    assert_eq!(ep_updated_events.ep.event_types_ids.unwrap(), expected_et);\n}\n\n#[tokio::test]\nasync fn test_endpoint_filter_channels() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    // Channels must not be empty:\n    let ep_empty_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"channels\": [],\n    });\n\n    let ep_with_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n        \"channels\": [\"tag1\"],\n    });\n\n    let ep_without_channels = json!({\n        \"url\": \"http://www.example.com\",\n        \"version\": 1,\n    });\n\n    let expected_ec = EventChannelSet(HashSet::from([EventChannel(\"tag1\".to_owned())]));\n\n    let _ep_w_empty_channel: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_empty_channels,\n            StatusCode::UNPROCESSABLE_ENTITY,\n        )\n        .await\n        .unwrap();\n\n    let ep_with_channel: EndpointOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            ep_with_channels.to_owned(),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(ep_with_channel.ep.channels.unwrap(), expected_ec);\n\n    let ep_with_deleted_channel: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_id}/endpoint/{}/\", ep_with_channel.id),\n            ep_without_channels,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert!(ep_with_deleted_channel.ep.channels.is_none());\n\n    // GET / assert channels empty\n    let ep_with_deleted_channel: EndpointOut = get_endpoint(&client, &app_id, &ep_with_channel.id)\n        .await\n        .unwrap();\n\n    assert!(ep_with_deleted_channel.ep.channels.is_none());\n\n    // Update with channels:\n    let updated_ep_with_channel: EndpointOut = client\n        .put(\n            &format!(\n                \"api/v1/app/{app_id}/endpoint/{}/\",\n                ep_with_deleted_channel.id\n            ),\n            ep_with_channels,\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(updated_ep_with_channel.ep.channels.unwrap(), expected_ec);\n\n    // GET / assert channels match\n    l<|fim_suffix|>\n    assert_eq!(updated_ep_with_channel.ep.channels.unwrap(), expected_ec);\n}\n\n#[tokio::test]\nasync fn test_rate_limit() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let ep_in = EndpointIn {\n        rate_limit: Some(100),\n        ..default_test_endpoint()\n    };\n\n    let endp = post_endpoint(&client, &app_id, ep_in.clone())\n        .await\n        .unwrap();\n\n    assert_eq!(endp.ep.rate_limit.unwrap(), 100);\n\n    let endp = put_endpoint(\n        &client,\n        &app_id,\n        &endp.id,\n        EndpointIn {\n            rate_limit: None,\n            ..ep_in.clone()\n        },\n    )\n    .await\n    .unwrap();\n\n    assert!(endp.ep.rate_limit.is_none());\n\n    let endp = get_endpoint(&client, &app_id, &endp.id).await.unwrap();\n\n    assert!(endp.ep.rate_limit.is_none());\n}\n\n#[tokio::test]\nasync fn test_msg_event_types_filter() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    for et in [\n        event_type_in(\"et1\", None).unwrap(),\n        event_type_in(\"et2\", None).unwrap(),\n    ] {\n        let _: EventTypeOut = client\n            .post(\"api/v1/event-type/\", et, StatusCode::CREATED)\n            .await\n            .unwrap();\n    }\n\n    for event_types in [\n        Some(EventTypeNameSet(HashSet::from([EventTypeName(\n            \"et1\".to_owned(),\n        )]))),\n        Some(EventTypeNameSet(HashSet::from([\n            EventTypeName(\"et1\".to_owned()),\n            EventTypeName(\"et2\".to_owned()),\n        ]))),\n        None,\n    ] {\n        post_endpoint(\n            &client,\n            &app_id,\n            EndpointIn {\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                event_types_ids: event_types,\n                ..default_test_endpoint()\n            },\n        )\n        .await\n        .unwrap();\n    }\n\n    // Number of attempts should match based on event-types registered to endpoints\n    for (event_name, expected_count) in [\n        (EventTypeName(\"et1\".to_owned()), 3),\n        (EventTypeName(\"et2\".to_owned()), 2),\n    ] {\n        let msg: MessageOut = client\n            .post(\n                &format!(\"api/v1/app/{app_id}/msg/\"),\n                json!({\n                    \"eventType\": event_name,\n                    \"payload\": {},\n                    \"payloadRetentionPeriod\": 5,\n                }),\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        tokio::time::sleep(Duration::from_millis(10)).await;\n\n        let _list =\n            get_msg_attempt_list_and_assert_count(&client, &app_id, &msg.id, expected_count)\n                .await\n                .unwrap();\n    }\n}\n\n#[tokio::test]\nasync fn test_msg_channels_filter() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n\n    let ec = EventChannelSet(HashSet::from([EventChannel(\"tag1\".to_owned())]));\n\n    for channels in [Some(ec.clone()), None] {\n        let _endp = post_endpoint(\n            &client,\n            &app_id,\n            EndpointIn {\n                channels,\n                url: Url::parse(&receiver.endpoint).unwrap(),\n                ..default_test_endpoint()\n            },\n        )\n        .await\n        .unwrap();\n    }\n\n    for (channels, expected_count) in [(Some(&ec), 2), (None, 1)] {\n        let mut message_in = json!({\n            \"eventType\": \"et1\",\n            \"payload\": {},\n            \"payloadRetentionPeriod\": 5,\n        });\n        if let Some(channels) = channels {\n            message_in[\"channels\"] = json!(channels);\n        }\n\n        let msg: MessageOut = client\n            .post(\n                &format!(\"api/v1/app/{}/msg/\", &app_id),\n                message_in,\n                StatusCode::ACCEPTED,\n            )\n            .await\n            .unwrap();\n\n        let _list =\n            get_msg_attempt_list_and_assert_count<|fim_middle|>", "completion": "let updated_ep_with_channel: EndpointOut =\n        get_endpoint(&client, &app_id, &updated_ep_with_channel.id)\n            .await\n            .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [1866, 1869]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\nmod crud;\nmod headers;\nmod recovery;\nmod secrets;\n\nuse std::collections::{HashMap, HashSet};\n\nuse aide::axum::{\n    routing::{get_with, post_with},\n    ApiRouter,\n};\nuse axum::{\n    extract::{Path, Query, State},\n    Json,\n};\nuse chrono::{DateTime, Duration, Utc};\nuse schemars::JsonSchema;\nuse sea_orm::{ActiveValue::Set, ColumnTrait, FromQueryResult, QuerySelect};\nuse serde::{Deserialize, Serialize};\nuse svix_server_derive::{aide_annotate, ModelIn, ModelOut};\nuse url::Url;\nuse validator::{Validate, ValidationError};\n\nuse self::secrets::generate_secret;\nuse super::message::{create_message_inner, MessageIn, MessageOut, RawPayload};\nuse crate::{\n    cfg::DefaultSignatureType,\n    core::{\n        cryptography::Encryption,\n        permissions,\n        types::{\n            metadata::Metadata, ApplicationIdOrUid, EndpointHeaders, EndpointHeadersPatch,\n            EndpointId, EndpointSecret, EndpointSecretInternal, EndpointUid, EventChannelSet,\n            EventTypeName, EventTypeNameSet, MessageStatus,\n        },\n    },\n    db::models::{\n        endpoint, eventtype,\n        messageattempt::{self, Query as _},\n    },\n    error::{self, HttpError},\n    v1::utils::{\n        openapi_tag,\n        patch::{\n            patch_field_non_nullable, patch_field_nullable, UnrequiredField,\n            UnrequiredNullableField,\n        },\n        validate_no_control_characters, validate_no_control_characters_unrequired,\n        validation_error, ApplicationEndpointPath, ModelIn, ValidatedJson,\n    },\n    AppState,\n};\n\npub fn validate_event_types_ids(event_types_ids: &EventTypeNameSet) -> Result<(), ValidationError> {\n    if event_types_ids.0.is_empty() {\n        Err(validation_error(\n            Some(\"filterTypes\"),\n            Some(\"filterTypes can't be empty, it must have at least one item.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nfn validate_event_types_ids_unrequired_nullable(\n    event_types_ids: &UnrequiredNullableField<EventTypeNameSet>,\n) -> Result<(), ValidationError> {\n    match event_types_ids {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(event_type_ids) => validate_event_types_ids(event_type_ids),\n    }\n}\n\npub fn validate_channels_endpoint(channels: &EventChannelSet) -> Result<(), ValidationError> {\n    let len = channels.0.len();\n    if !(1..=10).contains(&len) {\n        Err(validation_error(\n            Some(\"channels\"),\n            Some(\"Channels must have at least 1 and at most 10 items, or be set to null.\"),\n        ))\n    } else {\n        Ok(())\n    }\n}\n\nfn validate_channels_endpoint_unrequired_nullable(\n    channels: &UnrequiredNullableField<EventChannelSet>,\n) -> Result<(), ValidationError> {\n    match channels {\n        UnrequiredNullableField::Absent | UnrequiredNullableField::None => Ok(()),\n        UnrequiredNullableField::Some(channels) => validate_channels_endpoint(channels),\n    }\n}\n\npub fn validate_url(url: &Url) -> Result<(), ValidationError> {\n    let scheme = url.scheme();\n    i<|fim_suffix|>}\n\nfn validate_url_unrequired(val: &UnrequiredField<Url>) -> Result<(), ValidationError> {\n    match val {\n        UnrequiredField::Absent => Ok(()),\n        UnrequiredField::Some(val) => validate_url(val),\n    }\n}\n\nfn example_channel_set() -> Vec<&'static str> {\n    vec![\"project_123\", \"group_2\"]\n}\n\nfn example_endpoint_description() -> &'static str {\n    \"An example endpoint name\"\n}\n\nfn example_filter_types() -> Vec<&'static str> {\n    vec![\"user.signup\", \"user.deleted\"]\n}\n\nfn endpoint_disabled_default() -> bool {\n    false\n}\n\nfn example_endpoint_url() -> &'static str {\n    \"https://example.com/webhook/\"\n}\n\nfn example_endpoint_version() -> u16 {\n    1\n}\n\nfn default_endpoint_version() -> Option<u16> {\n    Some(1)\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct EndpointIn {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n    pub disabled: bool,\n    #[serde(rename = \"filterTypes\")]\n    #[validate(custom = \"validate_event_types_ids\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_filter_types\", length(min = 1))]\n    pub event_types_ids: Option<EventTypeNameSet>,\n    /// List of message channels this endpoint listens to (omit for all)\n    #[validate(custom = \"validate_channels_endpoint\")]\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    #[schemars(example = \"example_channel_set\", length(min = 1, max = 10))]\n    pub channels: Option<EventChannelSet>,\n\n    #[validate]\n    #[serde(default)]\n    #[serde(rename = \"secret\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub key: Option<EndpointSecret>,\n\n    #[serde(default)]\n    pub metadata: Metadata,\n}\n\nimpl EndpointIn {\n    pub fn key_take_or_generate(\n        &mut self,\n        encryption: &Encryption,\n        sig_type: &DefaultSignatureType,\n    ) -> error::Result<EndpointSecretInternal> {\n        if let Some(key) = self.key.take() {\n            EndpointSecretInternal::from_endpoint_secret(key, encryption)\n        } else {\n            generate_secret(encryption, sig_type)\n        }\n    }\n}\n\n// FIXME: This can and should be a derive macro\nimpl ModelIn for EndpointIn {\n    type ActiveModel = endpoint::ActiveModel;\n\n    #[allow(deprecated)]\n    fn update_model(self, model: &mut Self::ActiveModel) {\n        let EndpointIn {\n            description,\n            rate_limit,\n            uid,\n            url,\n            version,\n            disabled,\n            event_types_ids,\n            channels,\n            key: _,\n            metadata: _,\n        } = self;\n\n        model.description = Set(description);\n        model.rate_limit = Set(rate_limit.map(|x| x.into()));\n        model.uid = Set(uid);\n        model.url = Set(url.into());\n        model.version = Set(version.unwrap_or(1).into());\n        model.disabled = Set(disabled);\n        model.event_types_ids = Set(event_types_ids);\n        model.channels = Set(channels);\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Deserialize, Validate, ModelIn, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\nstruct EndpointUpdate {\n    #[serde(default)]\n    #[validate(custom = \"validate_no_control_characters\")]\n    #[schemars(example = \"example_endpoint_description\")]\n    pub description: String,\n\n    #[validate(range(min = 1, message = \"Endpoint rate limits must be at least one if set\"))]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    /// Optional unique identifier for the endpoint\n    #[validate]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<EndpointUid>,\n\n    #[validate(custom = \"validate_url\")]\n    #[schemars(url, length(min = 1, max = 65_536), example = \"example_endpoint_url\")]\n    pub url: Url,\n\n    #[deprecated]\n    #[serde(default = \"default_endpoint_version\")]\n    #[validate(range(min = 1, message = \"Endpoint versions must be at least one if set\"))]\n    #[schemars(range(min = 1), example = \"example_endpoint_version\")]\n    pub version: Option<u16>,\n\n    #[serde(default)]\n    #[schemars(example = \"endpoint_disabled_default\")]\n<|fim_middle|>", "completion": "if scheme == \"https\" || scheme == \"http\" {\n        Ok(())\n    } else {\n        Err(validation_error(\n            Some(\"url\"),\n            Some(\"Endpoint URL schemes must be http or https\"),\n        ))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/endpoint/mod.rs", "node_type": "if_expression", "line_range": [99, 106]}
{"prompt": "<|fim_prefix|>        validate_limited_str(&self.0)\n    }\n}\n\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventChannelSet(pub HashSet<EventChannel>);\njson_wrapper!(EventChannelSet);\n\nimpl Validate for EventChannelSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, Eq, PartialEq, Serialize, Deserialize, JsonSchema)]\n#[schemars(transparent)]\npub struct EventTypeNameSet(pub HashSet<EventTypeName>);\njson_wrapper!(EventTypeNameSet);\n\nimpl Validate for EventTypeNameSet {\n    fn validate(&self) -> Result<(), validator::ValidationErrors> {\n        for item in self.0.iter() {\n            item.validate()?;\n        }\n        Ok(())\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\npub struct ExpiringSigningKeys(pub Vec<ExpiringSigningKey>);\njson_wrapper!(ExpiringSigningKeys);\n\nimpl ExpiringSigningKeys {\n    pub const MAX_OLD_KEYS: usize = 10;\n    pub const OLD_KEY_EXPIRY_HOURS: i64 = 24;\n}\n\n/// The type of encryption key\n#[repr(u8)]\n#[derive(Clone, Debug, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum EndpointSecretType {\n    Hmac256 = 1,\n    Ed25519 = 2,\n    // Reserved = 3,\n}\n\nimpl EndpointSecretType {\n    pub const fn secret_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whsk_\",\n        }\n    }\n\n    pub const fn public_prefix(&self) -> &'static str {\n        match self {\n            EndpointSecretType::Hmac256 => \"whsec_\",\n            EndpointSecretType::Ed25519 => \"whpk_\",\n        }\n    }\n}\n\n/// Properties of the encryption key\n#[derive(Clone, Debug, PartialEq, Eq)]\nstruct EndpointSecretMarker {\n    type_: EndpointSecretType,\n    encrypted: bool,\n}\n\nimpl EndpointSecretMarker {\n    const ENCRYPTED_FLAG: u8 = 0b1000_0000;\n\n    fn from_u8(v: u8) -> crate::error::Result<Self> {\n        let encrypted = (v & Self::ENCRYPTED_FLAG) != 0;\n        let v = v & !Self::ENCRYPTED_FLAG;\n        let type_ = EndpointSecretType::try_from(v)\n            .map_err(|_| crate::error::Error::generic(\"Invalid marker value\"))?;\n\n        Ok(Self { type_, encrypted })\n    }\n\n    fn to_u8(&self) -> u8 {\n        let mut ret = self.type_.clone().into();\n        if self.encrypted {\n            ret |= Self::ENCRYPTED_FLAG;\n        }\n        ret\n    }\n\n    fn type_(&self) -> &EndpointSecretType {\n        &self.type_\n    }\n}\n\n/// The internal representation of the endpoint secret.\n/// This is used to store it securely in the database and cache, and to ensure it doesn't get\n/// sent externally.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub struct EndpointSecretInternal {\n    marker: EndpointSecretMarker,\n\n    key: Vec<u8>,\n}\n\nimpl EndpointSecretInternal {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MINUS_ONE: usize = Self::KEY_SIZE - 1;\n\n    fn new(\n        encryption: &Encryption,\n        type_: EndpointSecretType,\n        key: &[u8],\n    ) -> crate::error::Result<Self> {\n        Ok(Self {\n            marker: EndpointSecretMarker {\n                type_,\n                encrypted: encryption.enabled(),\n            },\n            key: encryption.encrypt(key)?,\n        })\n    }\n\n    pub fn generate_symmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let buf: [u8; Self::KEY_SIZE] = rand::thread_rng().gen();\n        Self::new(encryption, EndpointSecretType::Hmac256, &buf)\n    }\n\n    pub fn generate_asymmetric(encryption: &Encryption) -> crate::error::Result<Self> {\n        let key = AsymmetricKey::generate();\n        Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())\n    }\n\n    fn into_vec(mut self) -> Vec<u8> {\n        let marker: u8 = self.marker.to_u8();\n\n        l<|fim_suffix|>        vec.append(&mut self.key);\n        vec\n    }\n\n    fn from_vec(v: Vec<u8>) -> crate::error::Result<Self> {\n        // Legacy had exact size\n        match v.len() {\n            0..=Self::KEY_SIZE_MINUS_ONE => Err(crate::error::Error::generic(\"Value too small\")),\n            Self::KEY_SIZE => Ok(Self {\n                marker: EndpointSecretMarker {\n                    type_: EndpointSecretType::Hmac256,\n                    encrypted: false,\n                },\n                key: v,\n            }),\n            _ => {\n                let marker = EndpointSecretMarker::from_u8(v[0])?;\n                Ok(Self {\n                    marker,\n                    key: v[1..].to_vec(),\n                })\n            }\n        }\n    }\n\n    pub fn into_endpoint_secret(\n        self,\n        encryption: &Encryption,\n    ) -> crate::error::Result<EndpointSecret> {\n        let key = self.key(encryption)?;\n        Ok(match self.type_() {\n            EndpointSecretType::Hmac256 => EndpointSecret::Symmetric(key),\n            EndpointSecretType::Ed25519 => {\n                EndpointSecret::Asymmetric(AsymmetricKey::from_slice(&key[..])?)\n            }\n        })\n    }\n\n    pub fn from_endpoint_secret(\n        endpoint_secret: EndpointSecret,\n        encryption: &Encryption,\n    ) -> crate::error::Result<Self> {\n        Ok(match endpoint_secret {\n            EndpointSecret::Symmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Hmac256, &key)?\n            }\n            EndpointSecret::Asymmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())?\n            }\n        })\n    }\n\n    pub fn sign(&self, encryption: &Encryption, bytes: &[u8]) -> Vec<u8> {\n        let key = self.key(encryption).unwrap();\n        // FIXME: remove unwrap\n        match self.marker.type_() {\n            EndpointSecretType::Hmac256 => hmac_sha256::HMAC::mac(bytes, key).to_vec(),\n            EndpointSecretType::Ed25519 => AsymmetricKey::from_slice(&key[..])\n                .unwrap()\n                .0\n                .sk\n                .sign(bytes, None)\n                .to_vec(),\n        }\n    }\n\n    fn key(&self, encryption: &Encryption) -> crate::error::Result<Vec<u8>> {\n        Ok(if self.marker.encrypted {\n            if encryption.enabled() {\n                encryption.decrypt(&self.key)?\n            } else {\n                return Err(crate::error::Error::generic(\n                    \"main_secret unset, can't decrypt key\",\n                ));\n            }\n        } else {\n            self.key.to_vec()\n        })\n    }\n\n    pub fn type_(&self) -> &EndpointSecretType {\n        self.marker.type_()\n    }\n}\n\nimpl Serialize for EndpointSecretInternal {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        serializer.serialize_str(&STANDARD.encode(self.clone().into_vec()))\n    }\n}\n\nimpl<'de> Deserialize<'de> for EndpointSecretInternal {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n\n        String::deserialize(deserializer).and_then(|string| {\n            // For backwards compat when loading from ExpiringSigningKeys. Going forward we just b64 it\n            if string.starts_with(EndpointSecretType::Hmac256.secret_prefix()) {\n                Ok(Self {\n                    marker: EndpointSecretMarker {\n                        type_: EndpointSecretType::Hmac256,\n                        encrypted: false,\n                    },\n                    key: string\n                        .get(EndpointSecretType::Hmac256.secret_prefix().len()..)\n                        .ok_or_else(|| Error::custom(\"invalid prefix\".to_string()))\n                        .and_then(|string| {\n                            STANDARD\n                                .decode(string)\n                                .map_err(|err| Error::custom(err.to_string()))\n                        })?,\n                })\n            } else {\n                let buf = STANDARD<|fim_middle|>", "completion": "let mut vec = vec![marker];", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "let_declaration", "line_range": [747, 747]}
{"prompt": "<|fim_prefix|>  /// `[\"query\", \"field_name\"]`, etc. They may, however, be arbitrarily deep.\n    pub loc: Vec<String>,\n\n    /// The message accompanying the validation error item.\n    pub msg: String,\n\n    /// The type of error, often \"type_error\" or \"value_error\", but sometimes with more context like\n    /// as \"value_error.number.not_ge\"\n    #[serde(rename = \"type\")]\n    pub ty: String,\n}\n\n#[derive(Debug, Clone)]\npub struct HttpError {\n    pub status: StatusCode,\n    body: HttpErrorBody,\n}\n\nimpl HttpError {\n    fn new_standard(status: StatusCode, code: String, detail: String) -> Self {\n        Self {\n            status,\n            body: HttpErrorBody::Standard(StandardHttpError { code, detail }),\n        }\n    }\n\n    pub fn bad_request(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::BAD_REQUEST,\n            code.unwrap_or_else(|| \"generic_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Generic error\".to_owned()),\n        )\n    }\n\n    pub fn not_found(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_FOUND,\n            code.unwrap_or_else(|| \"not_found\".to_owned()),\n            detail.unwrap_or_else(|| \"Entity not found\".to_owned()),\n        )\n    }\n\n    pub fn unauthorized(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::UNAUTHORIZED,\n            code.unwrap_or_else(|| \"authentication_failed\".to_owned()),\n            detail.unwrap_or_else(|| \"Incorrect authentication credentials.\".to_owned()),\n        )\n    }\n\n    pub fn permission_denied(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::FORBIDDEN,\n            code.unwrap_or_else(|| \"insufficient access\".to_owned()),\n            detail.unwrap_or_else(|| \"Insufficient access for the given operation.\".to_owned()),\n        )\n    }\n\n    pub fn conflict(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::CONFLICT,\n            code.unwrap_or_else(|| \"conflict\".to_owned()),\n            detail.unwrap_or_else(|| \"A conflict has occurred\".to_owned()),\n        )\n    }\n\n    pub fn unprocessable_entity(detail: Vec<ValidationErrorItem>) -> Self {\n        Self {\n            status: StatusCode::UNPROCESSABLE_ENTITY,\n            body: HttpErrorBody::Validation(ValidationHttpError { detail }),\n        }\n    }\n\n    pub fn internal_server_error(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::INTERNAL_SERVER_ERROR,\n            code.unwrap_or_else(|| \"server_error\".to_owned()),\n            detail.unwrap_or_else(|| \"Internal Server Error\".to_owned()),\n        )\n    }\n\n    pub fn not_implemented(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::NOT_IMPLEMENTED,\n            code.unwrap_or_else(|| \"not_implemented\".to_owned()),\n            detail.unwrap_or_else(|| \"This API endpoint is not yet implemented.\".to_owned()),\n        )\n    }\n\n    pub fn too_large(code: Option<String>, detail: Option<String>) -> Self {\n        Self::new_standard(\n            StatusCode::PAYLOAD_TOO_LARGE,\n            code.unwrap_or_else(|| \"payload_too_large\".to_owned()),\n            detail.unwrap_or_else(|| \"Request payload is too large.\".to_owned()),\n        )\n    }\n}\n\nimpl From<HttpError> for Error {\n    fn from(err: HttpError) -> Error {\n        Error::http(err)\n    }\n}\n\nimpl fmt::Display for HttpError {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match &self.body {\n            HttpErrorBody::Standard(StandardHttpError { code, detail }) => write!(\n                f,\n                \"status={} code=\\\"{code}\\\" detail=\\\"{detail}\\\"\",\n                self.status\n            ),\n\n            HttpErrorBody::Validation(ValidationHttpError { detail }) => {\n                write!(\n                    f,\n                    \"status={} detail={}\",\n                    self.status,\n                    serde_json::to_string(&detail)\n                        .unwrap_or_else(|e| format!(\"\\\"unserializable error for {e}\\\"\"))\n                )\n            }\n        }\n    }\n}\n\nimpl IntoResponse for HttpError {\n    fn into_response(self) -> Response {\n        (self.status, Json(self.body)).into_response()\n    }\n}\n\nimpl From<ErrorType> for Error {\n    fn from(typ: ErrorType) -> Self {\n        Self { trace: vec![], typ }\n    }\n}\n\n// FIXME - delete\nimpl From<crate::core::webhook_http_client::Error> for Error {\n    fn from(err: webhook_http_client::Error) -> Error {\n        match err {\n            webhook_http_client::Error::TimedOut => Self::timeout(err),\n            _ => Error::generic(err),\n        }\n    }\n}\n\n/// Utility function for Converting a [`DbErr`] into an [`Error`].\n///\n/// The error \"duplicate key value violates unique constraint\" is converted to\n/// an HTTP \"conflict\" error. This is to be used in `map_err` calls on\n/// creation/update of records.\npub fn http_error_on_conflict(db_err: DbErr) -> Error {\n    if is_conflict_err(&db_err) {\n        HttpError::conflict(None, None).into()\n    } else {\n        Error::database(db_err)\n    }\n}\n\npub fn is_conflict_err(db_err: &DbErr) -> bool {\n    u<|fim_suffix|>    let rt_err = match db_err {\n        E::Exec(e) | E::Query(e) | E::Conn(e) => e,\n        // If sqlx ever extends this enum, I want a compile time error so we're forced to update this function.\n        // Hence we list out all the enumerations, rather than using a default match statement\n        E::TryIntoErr { .. }\n        | E::ConvertFromU64(_)\n        | E::UnpackInsertId\n        | E::UpdateGetPrimaryKey\n        | E::RecordNotFound(_)\n        | E::AttrNotSet(_)\n        | E::Custom(_)\n        | E::Type(_)\n        | E::Json(_)\n        | E::Migration(_)\n        | E::RecordNotInserted\n        | E::RecordNotUpdated\n        | E::ConnectionAcquire(_) => return false,\n    };\n\n    let sqlx_err = match rt_err {\n        RuntimeErr::SqlxError(e) => e,\n        RuntimeErr::Internal(_) => return false,\n    };\n\n    sqlx_err\n        .as_database_error()\n        .and_then(|e| e.code())\n        .filter(|code| code == \"23505\")\n        .is_some()\n}\n\npub fn is_timeout_error(db_err: &DbErr) -> bool {\n    let runtime_err = match &db_err {\n        DbErr::Conn(e) | DbErr::Exec(e) | DbErr::Query(e) => e,\n        _ => return false,\n    };\n\n    let sqlx_err = match runtime_err {\n        RuntimeErr::SqlxError(e) => e,\n        RuntimeErr::Internal(_) => return false,\n    };\n\n    match sqlx_err.as_database_error() {\n        // STUPID - no other good way to ID statement timeouts\n        Some(e) => e\n            .message()\n            .contains(\"canceling statement due to statement timeout\"),\n        None => false,\n    }\n}\n\n/// Returns true if the DbErr results from weirdness with a slow/long connection.\n/// This is distinct from [is_timeout_error], which reports whether the underlying\n/// query actually timed out on the pg side.\n///\n/// [is_connection_timeout_error] reports whether the connection to pg itself was slow\n/// for some reason.\npub fn is_connection_timeout_error(db_err: &DbErr) -> bool {\n    use DbErr as E;\n    let rt_err = match db_err {\n        E::ConnectionAcquire(_) | E::Conn(_) => return true,\n        E::Exec(e) | E::Query(e) => e.to_string(),\n\n        // If sqlx ever extends this enum, I want a compile time error so we're forced to update this function.\n        // Hence we list out all the enumerations, rather than using a default match statement\n        E::TryIntoErr { .. }\n        | E::ConvertFromU64(_)\n        | E::UnpackInsertId\n        | E::UpdateGetPrimaryKey\n        | E::RecordNotFound(_)\n        | E::AttrNotSet(_)\n        | E::Custom(_)\n        | E::Type(_)\n        | E::Json(_)\n        | E::Migration(_)\n        | E::RecordNotInserted\n        | E::RecordNotUpdated => return false,\n    };\n\n    const ERRORS: [&str; 3] = [\n        \"Connection pool timed out\",\n        \"Connection reset by peer\",\n        \"unexpected end of file\",\n    ];\n    for e in ERRORS {\n        if rt_err.contains(e) {\n            return true;\n        }\n    }\n\n    false\n}\n<|fim_middle|>", "completion": "use DbErr as E;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/error.rs", "node_type": "use_declaration", "line_range": [456, 456]}
{"prompt": "<|fim_prefix|>println!(\"read loop terminated: {e:#}\"))\n            }\n        });\n\n        set.spawn(async move {\n            send_to_ws_loop(remote_rx, ws_tx)\n                .await\n                .inspect_err(|e| eprintln!(\"write loop terminated: {e:#}\"))\n        });\n\n        // If any task terminates, trash the rest so we can reconnect.\n        if set.join_next().await.is_some() {\n            set.shutdown().await;\n        }\n\n        Ok(())\n    }\n}\n\npub async fn listen(\n    local_url: url::Url,\n    relay_token: String,\n    relay_debug_url: Option<&str>,\n    relay_disable_security: bool,\n    disable_tls_verification: bool,\n) -> Result<()> {\n    let scheme = if relay_disable_security { \"ws\" } else { \"wss\" };\n    let api_host = relay_debug_url.unwrap_or(DEFAULT_API_HOST);\n    let token = format!(\"c_{relay_token}\");\n\n    let websocket_url = format!(\"{scheme}://{api_host}/{API_PREFIX}/listen/\").parse()?;\n\n    let http_client = HttpClient::builder()\n        .danger_accept_invalid_certs(disable_tls_verification)\n        .build()?;\n\n    let mut client = Client {\n        token,\n        websocket_url,\n        local_url,\n        http_client,\n    };\n\n    const MAX_BACKOFF: Duration = Duration::from_millis(5000);\n    let backoff_schedule = [\n        Duration::ZERO,\n        Duration::from_millis(100),\n        Duration::from_millis(1000),\n        MAX_BACKOFF,\n    ];\n\n    let mut attempt_count = 0;\n    let mut last_attempt = Instant::now();\n\n    // We may ditch this token, generating a new one on the fly, depending on how the server\n    // responds when we connect.\n    let orig_token = client.token.clone();\n    loop {\n        // Any termination Ok or Err... try to reconnect.\n        let show_welcome_message = attempt_count == 0 || orig_token != client.token;\n\n        if let Err(e) = client.connect(show_welcome_message).await {\n            eprintln!(\"Failed to connect to Webhook Relay: {e:#}\");\n            if e.downcast_ref::<TokenInUse>().is_some() {\n                eprintln!(\"Generating a new token for this session.\");\n                client.token = {\n                    let relay_token = generate_token()?;\n                    format!(\"c_{relay_token}\")\n                };\n            }\n        } else {\n            eprintln!(\"Failed to connect to Webhook Relay\");\n        }\n\n        // Reset the backoff schedule if it's been a while since we've seen a disconnect.\n        if last_attempt.elapsed() > MAX_BACKOFF * 2 {\n            // N.b. attempt_count `0` is special because that's what prompts the printing of a\n            // welcome message in `Client::connect`.\n            // When we reset here, starting at `0` here will still avoid the\n            // re-print because we increment after selecting the sleep duration.\n            attempt_count = 0;\n        }\n\n        let backoff = *backoff_schedule.get(attempt_count).unwrap_or(&MAX_BACKOFF);\n        eprintln!(\"Reattempting connection in: {}ms\", backoff.as_millis());\n\n        attempt_count += 1;\n        last_attempt = Instant::now();\n\n        tokio::time::sleep(backoff).await;\n    }\n}\n\nfn receive_url(token: &str) -> String {\n    format!(\"https://play.svix.com/in/{token}/\")\n}\n\nfn view_url(token: &str) -> String {\n    format!(\"https://play.svix.com/view/{token}/\")\n}\n\ntype S = WebSocketStream<MaybeTlsStream<TcpStream>>;\n\nstruct WsConnection {\n    stream: S,\n}\n\nimpl WsConnection {\n    async fn new(websocket_url: &url::Url) -> Result<Self> {\n        let request = websocket_url.to_string().into_client_request()?;\n        let (stream, _resp) = connect_async(request)\n            .await\n            .inspect_err(|e| eprintln!(\"{e}\"))\n            .context(\"failed to connect to websocket server\")?;\n\n        Ok(Self { stream })\n    }\n}\n\nasync fn read_from_ws_loop(\n    mut rx: SplitStream<S>,\n    tx: UnboundedSender<MessageOut>,\n    local_url: url::Url,\n    client: HttpClient,\n) -> Result<()> {\n    // We expect to see roughly _at least one Ping_ in each `SERVER_PING_PERIOD`.\n    // Other messages may arrive ahead of this schedule.\n    // Tracking the time each message is received, we can know if the server has been quiet for too\n    // long, possibly requiring us to reconnect.\n    let mut last_msg = Instant::now();\n\n    loop {\n        const REMOTE_SERVER_CLOSED: &str = \"remote server closed connection\";\n\n        match tokio::time::timeout(SERVER_PING_PERIOD, rx.next()).await {\n            Err(_timeout_hit) => {\n                // Generous. 1.5x the ping frequency. If we go that long without\n                // seeing anything from the server, force a reconnect.\n                if last_msg.elapsed() > SERVER_PING_PERIOD + (SERVER_PING_PERIOD / 2) {\n                    anyhow::bail!(REMOTE_SERVER_CLOSED);\n                }\n            }\n            // Stream empty/closed\n            Ok(None) => break,\n            Ok(Some(msg)) => {\n                last_msg = Instant::now();\n\n                let data = match msg? {\n                    // Control messages.\n                    Message::Close(_) => anyhow::bail!(REMOTE_SERVER_CLOSED),\n                    Message::Ping(_) | Message::Pong(_) | Message::Frame(_) => continue,\n\n                    // Messages that carry data we care to process.\n                    Message::Text(s) => s.into(),\n                    Message::Binary(bytes) => bytes,\n                };\n\n                handle_incoming_message(client.clone(), data, &local_url, tx.clone()).await;\n            }\n        }\n    }\n\n    Ok(())\n}\n\nasync fn send_to_ws_loop(\n    mut rx: UnboundedReceiver<MessageOut>,\n    mut tx: SplitSink<S, Message>,\n) -> Result<()> {\n    while let Some(msg) = rx.recv().await {\n        tokio::time::timeout(\n            WRITE_WAIT,\n            tx.send(Message::Binary(\n                serde_json::to_vec(&msg)\n                    .expect(\"trivial serialization\")\n                    .into(),\n            )),\n        )\n        .await?\n        .context(\"Websocket write timeout\")?;\n    }\n\n    Ok(())\n}\n\nasync fn make_local_request(\n    client: HttpClient,\n    url: &url::Url,\n    data: MessageInEvent,\n) -> Result<LocalServerResponse> {\n    let method = data.method.parse()?;\n    // FIXME: deprecation warning\n    #[allow(deprecated)]\n    let body = base64::decode(&data.body)?;\n    let mut headers = HeaderMap::with_capacity(data.headers.len());\n    for (k, v) in &data.headers {\n        // FIXME: there's a remark about the Go client freaking out if there's more than one host header set.\n        //   Do we care now that we're not using Go? TBD.\n        headers.insert(\n            HeaderName::try_from(k.as_str())?,\n            HeaderValue::try_from(v.as_str())?,\n        );\n    }\n    Ok(client\n        .request(method, url.clone())\n        .timeout(DEFAULT_TIMEOUT)\n        .body(body)\n        .headers(headers)\n        .send()\n        .await?)\n}\n\nfn format_resp_headers(headers: &HeaderMap) -> Result<HashMap<String, String>> {\n    let mut out = HashMap::new();\n    for (k, v) in headers {\n        out.insert(k.to_string(), v.to_str()?.to_string());\n    }\n    Ok(out)\n}\n\nasync fn handle_incoming_message(\n    client: HttpClient,\n    bytes: Bytes,\n    local_url: &url::Url,\n    tx: UnboundedSender<MessageOut>,\n) {\n    match serde_json::from_slice::<MessageIn>(&bytes) {\n        Ok(MessageIn::Event { data, .. }) => {\n            let msg_id = data.id.clone();\n            println!(\"<- Forwarding message id={msg_id} to: {local_url}\");\n            <|fim_suffix|>\n        }\n        Ok(MessageIn::Start { .. }) => { /* nothing to do */ }\n        Err(_err) => {\n            eprintln!(\"Received invalid webhook message... skipping\");\n        }\n    }\n}\n\nasync fn process_response(\n    id: String,\n    resp: LocalServerResponse,\n    tx: UnboundedSender<MessageOut>,\n) -> Result<()> {\n    let status = resp.status().as_u16();\n    let headers = format_resp_headers(resp.headers())?;\n    #[allow(deprecated)]\n    let body = base64::encode(resp.bytes().await?);\n    let msg = MessageOut::Event {\n        version: message::VERSION,\n        data: MessageOutEvent {\n            id,\n            body,\n            headers,\n            status,\n        },\n    };\n\n    println!(\"-> Received \\\"{status}\\\" response from local server, forwarding to webhook sender\");\n    Ok(tx.send(msg)?)\n}\n<|fim_middle|>", "completion": "match make_local_request(client, local_url, data).await {\n                Err(err) => {\n                    eprintln!(\"Failed to make request to local server: \\n{err:#}\");\n                }\n                Ok(resp) => {\n                    if let Err(err) = process_response(msg_id, resp, tx).await {\n                        eprintln!(\"Failed to read response from local server: \\n{err:#}\");\n                    }\n                }\n            }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/mod.rs", "node_type": "match_expression", "line_range": [424, 433]}
{"prompt": "<|fim_prefix|>use std::sync::Arc;\n\nuse axum::{\n    body::Body,\n    http::{Request, StatusCode},\n};\nuse serde_json::json;\nuse svix_bridge_types::{\n    async_trait, svix::webhooks::Webhook, BoxError, ForwardRequest, ReceiverOutput,\n    TransformationConfig, TransformerInput, TransformerInputFormat, TransformerJob,\n    TransformerOutput,\n};\nuse tower::{Service, ServiceExt};\n\nuse super::router;\nuse crate::webhook_receiver::{\n    types::{IntegrationState, InternalState},\n    verification::{NoVerifier, SvixVerifier},\n};\n\nstruct FakeReceiverOutput {\n    tx: tokio::sync::mpsc::UnboundedSender<serde_json::Value>,\n}\n\nimpl FakeReceiverOutput {\n    pub fn new() -> (\n        Self,\n        tokio::sync::mpsc::UnboundedReceiver<serde_json::Value>,\n    ) {\n        let (tx, rx) = tokio::sync::mpsc::unbounded_channel();\n        (Self { tx }, rx)\n    }\n}\n\n#[async_trait]\nimpl ReceiverOutput for FakeReceiverOutput {\n    fn name(&self) -> &str {\n        \"fake output\"\n    }\n\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError> {\n        self.tx.send(request.payload)?;\n        Ok(())\n    }\n}\n\n#[tokio::test]\nasync fn test_forwarding_no_verification() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    <|fim_suffix|>\n    let state = InternalState::new(state_map, tx);\n    let app = router().with_state(state);\n    let response = app\n        .oneshot(\n            Request::builder()\n                .uri(\"/webhook/a\")\n                .method(\"POST\")\n                .header(\"content-type\", \"application/json\")\n                .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n                .unwrap(),\n        )\n        .await\n        .unwrap();\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n}\n\n/// Registers 2 receivers and sends 1 request to each.\n#[tokio::test]\nasync fn test_forwarding_multiple_receivers() {\n    let (tx, _rx) = tokio::sync::mpsc::unbounded_channel();\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"a\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: None,\n            },\n        ),\n        (\n            \"b\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/a\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"a\": true}));\n\n    let request = Request::builder()\n        .uri(\"/webhook/b\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n/// Registers 2 receivers, one with a transformation and one without. Sends 1 request to each.\n#[tokio::test]\nasync fn test_transformation_json() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let mut input = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            input.insert(\"__TRANSFORMED__\".into(), json!(true));\n            let out = json!({ \"payload\": input });\n\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let (b_output, mut b_rx) = FakeReceiverOutput::new();\n    let state_map = [\n        (\n            \"transformed\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(a_output)),\n                transformation: Some(\n                    \"handler = (x) => ({ payload: {__TRANSFORMED__: true, ...x }})\".into(),\n                ),\n            },\n        ),\n        (\n            \"as-is\".into(),\n            IntegrationState {\n                verifier: NoVerifier.into(),\n                output: Arc::new(Box::new(b_output)),\n                transformation: None,\n            },\n        ),\n    ]\n    .into_iter()\n    .collect();\n    let state = InternalState::new(state_map, tx);\n\n    let mut app = router().with_state(state);\n\n    let request = Request::builder()\n        .uri(\"/webhook/transformed\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"a\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = a_rx.try_recv().unwrap();\n    // The `__TRANSFORMED__` key should have been added\n    assert_eq!(\n        json!(forwarded),\n        json!({\"a\": true, \"__TRANSFORMED__\": true})\n    );\n\n    let request = Request::builder()\n        .uri(\"/webhook/as-is\")\n        .method(\"POST\")\n        .header(\"content-type\", \"application/json\")\n        .body(axum::body::Body::from(json!({ \"b\": true }).to_string()))\n        .unwrap();\n\n    let response = ServiceExt::<Request<Body>>::ready(&mut app)\n        .await\n        .unwrap()\n        .call(request)\n        .await\n        .unwrap();\n\n    assert_eq!(response.status(), StatusCode::NO_CONTENT);\n    let forwarded = b_rx.try_recv().unwrap();\n    // The same payload should come through, without any transformation.\n    assert_eq!(json!(forwarded), json!({\"b\": true}));\n\n    // Both channels should be empty at this point.\n    assert!(a_rx.try_recv().is_err());\n    assert!(b_rx.try_recv().is_err());\n}\n\n#[tokio::test]\nasync fn test_transformation_string() {\n    let (tx, mut rx) = tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = rx.recv().await {\n            let out = match x.input {\n                TransformerInput::String(input) => json!({\"payload\": { \"got\": input }})\n                    .as_object()\n                    .cloned()\n                    .unwrap(),\n                _ => unreachable!(),\n            };\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n\n    let (a_output, mut a_rx) = FakeReceiverOutput::new();\n    let state_map = [(\n        \"transformed\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: Some(TransformationConfig::Explicit {\n                format: TransformerInputFormat::String,\n                src: String::from(\"handler = (x) => ({ payload: { got: x }})\"),\n            }),\n        },\n    )]\n    .into_iter()\n    .collect();\n<|fim_middle|>", "completion": "let state_map = [(\n        \"a\".into(),\n        IntegrationState {\n            verifier: NoVerifier.into(),\n            output: Arc::new(Box::new(a_output)),\n            transformation: None,\n        },\n    )]\n    .into_iter()\n    .collect();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/webhook_receiver/tests.rs", "node_type": "let_declaration", "line_range": [51, 60]}
{"prompt": "<|fim_prefix|> Client {\n    for (var, val) in &DEFAULT_CFG {\n        if std::env::var(var).is_err() {\n            std::env::set_var(var, val);\n        }\n    }\n    let config = aws_config::from_env().endpoint_url(ROOT_URL).load().await;\n    Client::new(&config)\n}\n\nasync fn create_test_queue(client: &Client) -> String {\n    let name: String = std::iter::repeat_with(fastrand::alphanumeric)\n        .take(8)\n        .collect();\n    client\n        .create_queue()\n        .queue_name(&name)\n        .send()\n        .await\n        .unwrap();\n\n    name\n}\n\nasync fn publish(client: &Client, url: &str, payload: &str) {\n    client\n        .send_message()\n        .queue_url(url)\n        .message_body(payload)\n        .send()\n        .await\n        .unwrap();\n}\n\n/// General \"pause while we wait for messages to travel\" beat. If you're seeing flakes, bump this up.\nconst WAIT_MS: u64 = 300;\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request.\n#[tokio::test]\nasync fn test_consume_ok() {\n    let client = mq_connection().await;\n    let queue_name = create_test_queue(&client).await;\n\n    let queue_url = format!(\"{ROOT_URL}/queue/{queue_name}\");\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            \"hi\": \"there\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), queue_url.clone(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&client, &queue_url, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    client\n        .delete_queue()\n        .queue_url(&queue_url)\n        .send()\n        .await\n        .unwrap();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = mq_connection().await;\n    let queue_name = create_test_queue(&client).await;\n\n    let queue_url = format!(\"{ROOT_URL}/queue/{queue_name}\");\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            \"_SVIX_APP_ID\": \"app_1234\",\n            \"_SVIX_EVENT_TYPE\": \"testing.things\",\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        queue_url.clone(),\n        Some(TransformerInputFormat::Json),\n    );\n    <|fim_suffix|>\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&client, &queue_url, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    client\n        .delete_queue()\n        .queue_url(&queue_url)\n        .send()\n        .await\n        .unwrap();\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_string_ok() {\n    let client = mq_connection().await;\n    let queue_name = create_test_queue(&client).await;\n\n    let queue_url = format!(\"{ROOT_URL}/queue/{queue_name}\");\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(\n            json!({ \"payload\": { \"hello\": \"world\" } }),\n        ))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"hello\": \"world\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        queue_url.clone(),\n        Some(TransformerInputFormat::String),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let input = match x.input {\n                TransformerInput::String(input) => input,\n                _ => unreachable!(),\n            };\n            // Build a create-message-compatible object, using the string input as a field in the payload.\n            let out = json!({\n                \"appId\": \"app_1234\",\n                \"message\": {\n                    \"eventType\": \"testing.things\",\n                    \"payload\": {\n                        \"hello\": input,\n                    }\n                }\n            });\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    publish(&client, &queue_url, \"world\").await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    client\n  <|fim_middle|>", "completion": "let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/sqs_consumer.rs", "node_type": "let_declaration", "line_range": [185, 186]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse crate::{error::Result, models::*, Configuration};\n\n#[derive(Default)]\npub struct IntegrationListOptions {\n    /// Limit the number of returned items\n    pub limit: Option<i32>,\n\n    /// The iterator returned from a prior invocation\n    pub iterator: Option<String>,\n\n    /// The sorting order of the returned items\n    pub order: Option<Ordering>,\n}\n\n#[derive(Default)]\npub struct IntegrationCreateOptions {\n    pub idempotency_key: Option<String>,\n}\n\n#[derive(Default)]\npub struct IntegrationRotateKeyOptions {\n    pub idempotency_key: Option<String>,\n}\n\npub struct Integration<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> Integration<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    /// List the application's integrations.\n    pub async fn list(\n        &self,\n        app_id: String,\n        options: Option<IntegrationListOptions>,\n    ) -> Result<ListResponseIntegrationOut> {\n        let IntegrationListOptions {\n            limit,\n            iterator,\n            order,\n        } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::GET, \"/api/v1/app/{app_id}/integration\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_query_param(\"limit\", limit)\n            .with_optional_query_param(\"iterator\", iterator)\n            .with_optional_query_param(\"order\", order)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Create an integration.\n    pub async fn create(\n        &self,\n        app_id: String,\n        integration_in: IntegrationIn,\n        options: Option<IntegrationCreateOptions>,\n    ) -> Result<IntegrationOut> {\n        let IntegrationCreateOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(http1::Method::POST, \"/api/v1/app/{app_id}/integration\")\n            .with_path_param(\"app_id\", app_id)\n            .with_optional_header_param(\"idempotency-key\", idempotency_key)\n            .with_body_param(integration_in)\n            .execute(self.cfg)\n            .await\n    }\n\n    /// Get an integration.\n    pub async fn get(&self, app_id: String, integ_id: String) -> Result<IntegrationOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/integration/{integ_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Update an integration.\n    <|fim_suffix|>\n\n    /// Delete an integration.\n    pub async fn delete(&self, app_id: String, integ_id: String) -> Result<()> {\n        crate::request::Request::new(\n            http1::Method::DELETE,\n            \"/api/v1/app/{app_id}/integration/{integ_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .returns_nothing()\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Get an integration's key.\n    #[deprecated]\n    pub async fn get_key(&self, app_id: String, integ_id: String) -> Result<IntegrationKeyOut> {\n        crate::request::Request::new(\n            http1::Method::GET,\n            \"/api/v1/app/{app_id}/integration/{integ_id}/key\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .execute(self.cfg)\n        .await\n    }\n\n    /// Rotate the integration's key. The previous key will be immediately\n    /// revoked.\n    pub async fn rotate_key(\n        &self,\n        app_id: String,\n        integ_id: String,\n        options: Option<IntegrationRotateKeyOptions>,\n    ) -> Result<IntegrationKeyOut> {\n        let IntegrationRotateKeyOptions { idempotency_key } = options.unwrap_or_default();\n\n        crate::request::Request::new(\n            http1::Method::POST,\n            \"/api/v1/app/{app_id}/integration/{integ_id}/key/rotate\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .with_optional_header_param(\"idempotency-key\", idempotency_key)\n        .execute(self.cfg)\n        .await\n    }\n}\n<|fim_middle|>", "completion": "pub async fn update(\n        &self,\n        app_id: String,\n        integ_id: String,\n        integration_update: IntegrationUpdate,\n    ) -> Result<IntegrationOut> {\n        crate::request::Request::new(\n            http1::Method::PUT,\n            \"/api/v1/app/{app_id}/integration/{integ_id}\",\n        )\n        .with_path_param(\"app_id\", app_id)\n        .with_path_param(\"integ_id\", integ_id)\n        .with_body_param(integration_update)\n        .execute(self.cfg)\n        .await\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/integration.rs", "node_type": "function_item", "line_range": [86, 101]}
{"prompt": "<|fim_prefix|>use std::time::{Duration, Instant};\n\nuse anyhow::{Context, Result};\nuse dialoguer::Input;\nuse reqwest::Client;\nuse serde::Deserialize;\n\nuse crate::{config, config::Config};\n\npub async fn prompt(_cfg: &Config) -> Result<()> {\n    print!(\"Welcome to the Svix CLI!\\n\\n\");\n\n    let selections = &[\"Login in dashboard.svix.com\", \"Input token manually\"];\n    let selection = dialoguer::Select::new()\n        .with_prompt(\"How would you like to authenticate?\")\n        .items(selections)\n        .default(0)\n        .interact()?;\n\n    let auth_token = if selection == 0 {\n        dashboard_login().await?\n    } else {\n        Input::new()\n            .with_prompt(\"Auth Token\")\n            .validate_with({\n                move |input: &String| -> Result<()> {\n                    if !input.trim().is_empty() {\n                        Ok(())\n                    } else {\n                        Err(anyhow::anyhow!(\"auth token cannot be empty\"))\n                    }\n                }\n            })\n            .interact_text()?\n            .trim()\n            .to_string()\n    };\n\n    // Load from disk and update the prompted fields.\n    // There are other fields (not prompted for) related to \"relay\" for the `listen` command\n    // that we'd rather not wipe out if `login` is invoked.\n    let mut cfg = Config::load()?;\n    cfg.auth_token = Some(auth_token);\n    let fp = config::get_config_file_path()?;\n    if let Err(e) = cfg.save_to_disk(&fp) {\n        eprintln!(\"\\n{e:#}\\n\");\n        anyhow::bail!(\n            \"Failed to configure the Svix CLI, please try again or try setting your auth \\\n             token manually `SVIX_AUTH_TOKEN` environment variable.\"\n        );\n    }\n\n    println!(\n        \"All Set! Your config has been written to `{}`\",\n        fp.display()\n    );\n    println!(\n        \"Type `{} --help` to print the Svix CLI documentation!\",\n        crate::BIN_NAME\n    );\n    Ok(())\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct CliStartLoginSessionOut {\n    session_id: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct AuthTokenOut {\n    token: String,\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(rename_all = \"camelCase\")]\nstruct DiscoverySessionOut {\n    pub region: String,\n}\n\nconst DASHBOARD_URL: &str = \"https://dashboard.svix.com\";\nconst LOGIN_SERVER_URL: &str = \"https://api.svix.com\";\n\npub async fn dashboard_login() -> Result<String> {\n    let client = reqwest::Client::new();\n\n    let start_session = client\n        .post(format!(\"{LOGIN_SERVER_URL}/dashboard/cli/login/start\"))\n        .send()\n        .await\n        .context(\"Failed to get session ID. Could not connect to server.\")?\n        .json::<CliStartLoginSessionOut>()\n        .await\n        .context(\"Failed to get session ID. Invalid response.\")?;\n\n    let session_id = start_session.session_id;\n    let code = &session_id[0..4].to_uppercase();\n\n    let url = format!(\"{DASHBOARD_URL}/cli/login?sessionId={session_id}&code={code}\");\n\n    println!(\"\\nPlease approve the login in your browser, then return here.\");\n    println!(\"Verification code: \\x1b[32m{code}\\x1b[0m\\n\");\n\n    if let Err(e) = open::that(&url) {\n        eprintln!(\"Failed to open browser: {e}\");\n        println!(\"Please manually open this URL in your browser: {url}\");\n    }\n\n    println!(\"Waiting for approval...\");\n\n    // First, poll the discovery endpoint to get the region\n    let discovery_poll_url = format!(\"{LOGIN_SERVER_URL}/dashboard/cli/login/discovery/complete\");\n    let discovery_data: DiscoverySessionOut =\n        poll_session(&client, &discovery_poll_url, &session_id).await?;\n\n    let region = discovery_data.region;\n    let region_server_url = format!(\"https://api.{region}.svix.com\");\n    let token_poll_url = format!(\"{region_server_url}/dashboard/cli/login/token/complete\");\n\n    // Then, poll the token endpoint to get the auth token\n    let token_data: AuthTokenOut = poll_session(&client, &token_poll_url, &session_id).await?;\n\n    println!(\"Authentication successful!\\n\");\n    Ok(token_data.token)\n}\n\nconst MAX_POLL_TIME: Duration = Duration::from_secs(5 * 60);\n\nasync fn poll_session<T>(client: &Client, poll_url: &str, session_id: &str) -> Result<T>\nwhere\n    T: for<'de> serde::Deserialize<'de>,\n{\n    let start_time: Instant = Instant::now();\n\n    while start_time.elapsed() < MAX_POLL_TIME {\n        <|fim_suffix|>\n\n        if response.status().is_success() {\n            return response\n                .json::<T>()\n                .await\n                .context(\"Failed to parse authentication data\");\n        } else if response.status() != reqwest::StatusCode::NOT_FOUND {\n            // Bail if session exists but has an error (is expired or something else)\n            let error_message = match response.json::<serde_json::Value>().await {\n                Ok(json) => json\n                    .get(\"detail\")\n                    .and_then(|d| d.as_str())\n                    .unwrap_or(\"Unknown error\")\n                    .to_string(),\n                Err(_) => \"Unknown error\".to_string(),\n            };\n\n            anyhow::bail!(\"Authentication failed: {error_message}\");\n        }\n\n        std::thread::sleep(std::time::Duration::from_secs(1));\n    }\n\n    anyhow::bail!(\"Authentication failed.\");\n}\n<|fim_middle|>", "completion": "let response = client\n            .post(poll_url)\n            .json(&serde_json::json!({ \"sessionId\": session_id }))\n            .send()\n            .await\n            .context(\"Failed to connect to authentication server\")?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/login.rs", "node_type": "let_declaration", "line_range": [137, 142]}
{"prompt": "<|fim_prefix|>alse,\n                },\n                key: v,\n            }),\n            _ => {\n                let marker = EndpointSecretMarker::from_u8(v[0])?;\n                Ok(Self {\n                    marker,\n                    key: v[1..].to_vec(),\n                })\n            }\n        }\n    }\n\n    pub fn into_endpoint_secret(\n        self,\n        encryption: &Encryption,\n    ) -> crate::error::Result<EndpointSecret> {\n        let key = self.key(encryption)?;\n        Ok(match self.type_() {\n            EndpointSecretType::Hmac256 => EndpointSecret::Symmetric(key),\n            EndpointSecretType::Ed25519 => {\n                EndpointSecret::Asymmetric(AsymmetricKey::from_slice(&key[..])?)\n            }\n        })\n    }\n\n    pub fn from_endpoint_secret(\n        endpoint_secret: EndpointSecret,\n        encryption: &Encryption,\n    ) -> crate::error::Result<Self> {\n        Ok(match endpoint_secret {\n            EndpointSecret::Symmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Hmac256, &key)?\n            }\n            EndpointSecret::Asymmetric(key) => {\n                Self::new(encryption, EndpointSecretType::Ed25519, key.0.sk.as_slice())?\n            }\n        })\n    }\n\n    pub fn sign(&self, encryption: &Encryption, bytes: &[u8]) -> Vec<u8> {\n        let key = self.key(encryption).unwrap();\n        // FIXME: remove unwrap\n        match self.marker.type_() {\n            EndpointSecretType::Hmac256 => hmac_sha256::HMAC::mac(bytes, key).to_vec(),\n            EndpointSecretType::Ed25519 => AsymmetricKey::from_slice(&key[..])\n                .unwrap()\n                .0\n                .sk\n                .sign(bytes, None)\n                .to_vec(),\n        }\n    }\n\n    fn key(&self, encryption: &Encryption) -> crate::error::Result<Vec<u8>> {\n        Ok(if self.marker.encrypted {\n            if encryption.enabled() {\n                encryption.decrypt(&self.key)?\n            } else {\n                return Err(crate::error::Error::generic(\n                    \"main_secret unset, can't decrypt key\",\n                ));\n            }\n        } else {\n            self.key.to_vec()\n        })\n    }\n\n    pub fn type_(&self) -> &EndpointSecretType {\n        self.marker.type_()\n    }\n}\n\nimpl Serialize for EndpointSecretInternal {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        serializer.serialize_str(&STANDARD.encode(self.clone().into_vec()))\n    }\n}\n\nimpl<'de> Deserialize<'de> for EndpointSecretInternal {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n\n        String::deserialize(deserializer).and_then(|string| {\n            // For backwards compat when loading from ExpiringSigningKeys. Going forward we just b64 it\n            if string.starts_with(EndpointSecretType::Hmac256.secret_prefix()) {\n                Ok(Self {\n                    marker: EndpointSecretMarker {\n                        type_: EndpointSecretType::Hmac256,\n                        encrypted: false,\n                    },\n                    key: string\n                        .get(EndpointSecretType::Hmac256.secret_prefix().len()..)\n                        .ok_or_else(|| Error::custom(\"invalid prefix\".to_string()))\n                        .and_then(|string| {\n                            STANDARD\n                                .decode(string)\n                                .map_err(|err| Error::custom(err.to_string()))\n                        })?,\n                })\n            } else {\n                let buf = STANDARD\n                    .decode(string)\n                    .map_err(|err| Error::custom(err.to_string()))?;\n                Self::from_vec(buf).map_err(|err| Error::custom(err.to_string()))\n            }\n        })\n    }\n}\n\nimpl From<EndpointSecretInternal> for sea_orm::Value {\n    fn from(v: EndpointSecretInternal) -> Self {\n        Self::Bytes(Some(Box::new(v.into_vec())))\n    }\n}\n\nimpl sea_orm::TryGetable for EndpointSecretInternal {\n    f<|fim_suffix|>\n    fn try_get(\n        res: &sea_orm::QueryResult,\n        pre: &str,\n        col: &str,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get(res, pre, col) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }\n}\n\nimpl sea_orm::sea_query::Nullable for EndpointSecretInternal {\n    fn null() -> sea_orm::Value {\n        sea_orm::Value::Bytes(None)\n    }\n}\n\nimpl sea_orm::sea_query::ValueType for EndpointSecretInternal {\n    fn try_from(v: sea_orm::Value) -> Result<Self, sea_orm::sea_query::ValueTypeErr> {\n        match v {\n            sea_orm::Value::Bytes(Some(x)) => {\n                EndpointSecretInternal::from_vec(*x).map_err(|_| sea_orm::sea_query::ValueTypeErr)\n            }\n            _ => Err(sea_orm::sea_query::ValueTypeErr),\n        }\n    }\n\n    fn type_name() -> String {\n        stringify!(EndpointSecretInternal).to_owned()\n    }\n\n    fn column_type() -> sea_orm::sea_query::ColumnType {\n        sea_orm::sea_query::ColumnType::Binary(\n            Self::KEY_SIZE\n                .try_into()\n                .expect(\"Key size is not more than u32::MAX\"),\n        )\n    }\n\n    fn array_type() -> sea_orm::sea_query::ArrayType {\n        sea_orm::sea_query::ArrayType::Bytes\n    }\n}\n\n/// The external representation of the endpoint secret.\n/// This one is used for serializing to and from customers.\n#[derive(Clone, Debug, PartialEq, Eq)]\npub enum EndpointSecret {\n    Symmetric(Vec<u8>),\n    Asymmetric(AsymmetricKey),\n}\n\nimpl EndpointSecret {\n    // IMPORTANT: has to be at least 24 bytes because of how we encode the type (and legacy ones\n    // didn't have type encoded).\n    // XXX Also: can't change withuot breaking from_vec\n    const KEY_SIZE: usize = 24;\n    // Needed because of rust limitations\n    const KEY_SIZE_MAX: usize = 75;\n\n    pub fn serialize_secret_key(&self) -> String {\n        match self {\n            Self::Symmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Hmac256.secret_prefix(),\n                    STANDARD.encode(key)\n                )\n            }\n            Self::Asymmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Ed25519.secret_prefix(),\n                    &STANDARD.encode(key.0.sk.as_slice())\n                )\n            }\n        }\n    }\n\n    pub fn serialize_public_key(&self) -> String {\n        match self {\n            Self::Symmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Hmac256.public_prefix(),\n                    STANDARD.encode(key)\n                )\n            }\n            Self::Asymmetric(key) => {\n                format!(\n                    \"{}{}\",\n                    EndpointSecretType::Ed25519.public_prefix(),\n                    &STANDARD.encode(key.pubkey())\n                )\n            }\n        }\n    }\n}\n\nimpl Serialize for EndpointSecret {\n    fn serialize<S>(&self, serializer: S) -> Result<S::Ok, S::Error>\n    where\n        S: serde::Serializer,\n    {\n        serializer.serialize_str(&self.serialize_public_key())\n    }\n}\n\nimpl<'de> Deserialize<'de> for EndpointSecret {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        use serde::de::Error;\n        let invalid_prefix = Error::custom(\"invalid prefix\".to_string());\n        String::deserialize(deserializer).and_then(|string| {\n            if string.starts_with(EndpointSecretType::Ed25519.secret_prefix()) {\n                Ok(Self::Asymmetric(\n                    AsymmetricKey::from_base64(\n                        string\n                            .get(EndpointSecretType::Ed25519.secret_prefix().len()..)\n                            .ok_or(invalid_prefix)?,\n                    )\n                    .map_err(|e| Error::custom(e.to_string()))?,\n                ))\n            } else if string.starts_with<|fim_middle|>", "completion": "fn try_get_by<I: sea_orm::ColIdx>(\n        res: &sea_orm::QueryResult,\n        index: I,\n    ) -> Result<Self, sea_orm::TryGetError> {\n        match Vec::<u8>::try_get_by(res, index) {\n            Ok(v) => EndpointSecretInternal::from_vec(v)\n                .map_err(|x| sea_orm::TryGetError::DbErr(sea_orm::DbErr::Type(x.to_string()))),\n            Err(e) => Err(e),\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "function_item", "line_range": [883, 892]}
{"prompt": "<|fim_prefix|>Time<Utc>>,\n}\n\n#[derive(Serialize, JsonSchema)]\nstruct MessageAttemptEndpointOut {\n    #[serde(flatten)]\n    common_: MessageAttemptOut,\n}\n\nimpl From<MessageAttemptOut> for MessageAttemptEndpointOut {\n    fn from(common_: MessageAttemptOut) -> Self {\n        Self { common_ }\n    }\n}\n\n/// DEPRECATED: please use list_attempts with endpoint_id as a query parameter instead.\n///\n/// List the message attempts for a particular endpoint.\n///\n/// Returning the endpoint.\n///\n/// The `before` parameter lets you filter all items created before a certain date and is ignored if an iterator is passed.\n#[aide_annotate(op_id = \"v1.message-attempt.list-by-endpoint-deprecated\")]\nasync fn list_attempts_for_endpoint(\n    state: State<AppState>,\n    pagination: ValidatedQuery<PaginationDescending<ReversibleIterator<MessageAttemptId>>>,\n    ValidatedQuery(ListAttemptsForEndpointQueryParams {\n        channel,\n        status,\n        before,\n        after,\n    }): ValidatedQuery<ListAttemptsForEndpointQueryParams>,\n    event_types_query: EventTypesQueryParams,\n    Path(ApplicationMsgEndpointPath {\n        app_id,\n        msg_id,\n        endpoint_id,\n    }): Path<ApplicationMsgEndpointPath>,\n    auth_app: permissions::Application,\n) -> Result<Json<ListResponse<MessageAttemptEndpointOut>>> {\n    list_messageattempts(\n        state,\n        pagination,\n        ValidatedQuery(AttemptListFetchQueryParams {\n            endpoint_id: Some(endpoint_id),\n            channel,\n            status,\n            before,\n            after,\n        }),\n        event_types_query,\n        Path(ApplicationMsgPath { app_id, msg_id }),\n        auth_app,\n    )\n    .await\n    .map(|Json(list)| {\n        let new_data = list\n            .data\n            .into_iter()\n            .map(MessageAttemptEndpointOut::from)\n            .collect();\n        Json(ListResponse {\n            data: new_data,\n            done: list.done,\n            iterator: list.iterator,\n            prev_iterator: list.prev_iterator,\n        })\n    })\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct AttemptListFetchQueryParams {\n    /// Filter the attempts based on the attempted endpoint\n    #[validate]\n    pub endpoint_id: Option<EndpointIdOrUid>,\n    /// Filter response based on the channel\n    #[validate]\n    pub channel: Option<EventChannel>,\n    /// Filter response based on the delivery status\n    pub status: Option<MessageStatus>,\n    /// Only include items created before a certain date\n    pub before: Option<DateTime<Utc>>,\n    /// Only include items created after a certain date\n    pub after: Option<DateTime<Utc>>,\n}\n\n/// Deprecated: Please use \"List Attempts by Endpoint\" and \"List Attempts by Msg\" instead.\n///\n/// `msg_id`: Use a message id or a message `eventId`\n#[aide_annotate(op_id = \"v1.message-attempt.list-by-msg-deprecated\")]\nasync fn list_messageattempts(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<\n        PaginationDescending<ReversibleIterator<MessageAttemptId>>,\n    >,\n    ValidatedQuery(AttemptListFetchQueryParams {\n        endpoint_id,\n        channel,\n        status,\n        before,\n        after,\n    }): ValidatedQuery<AttemptListFetchQueryParams>,\n    EventTypesQueryParams(event_types): EventTypesQueryParams,\n    Path(ApplicationMsgPath { msg_id, .. }): Path<ApplicationMsgPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<ListResponse<MessageAttemptOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id.clone(), msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let mut query = messageattempt::Entity::secure_find_by_msg(msg.id);\n\n    if let Some(endpoint_id) = endpoint_id {\n        let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n            .one(db)\n            .await?\n            .ok_or_else(|| HttpError::not_found(None, None))?;\n        query = query.filter(messageattempt::Column::EndpId.eq(endp.id))\n    }\n\n    i<|fim_suffix|>\n    if let Some(channel) = channel {\n        query = query.filter(Expr::cust_with_values(\"channels @> $1\", [channel.jsonb()]));\n    }\n\n    if let Some(EventTypeNameSet(event_types)) = event_types {\n        query = query.filter(message::Column::EventType.is_in(event_types));\n    }\n\n    let (query, iter_direction) = filter_and_paginate_time_limited(\n        query,\n        messageattempt::Column::Id,\n        limit,\n        pagination.iterator,\n        before,\n        after,\n    );\n    let out = query.all(db).await?.into_iter().map(Into::into).collect();\n\n    Ok(Json(MessageAttemptOut::list_response(\n        out,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\n/// `msg_id`: Use a message id or a message `eventId`\n#[aide_annotate(op_id = \"v1.message-attempt.get\")]\nasync fn get_messageattempt(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgAttemptPath {\n        msg_id, attempt_id, ..\n    }): Path<ApplicationMsgAttemptPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<Json<MessageAttemptOut>> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let attempt = messageattempt::Entity::secure_find_by_msg(msg.id)\n        .filter(messageattempt::Column::Id.eq(attempt_id))\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n    Ok(Json(attempt.into()))\n}\n\n/// Resend a message to the specified endpoint.\n#[aide_annotate(op_id = \"v1.message-attempt.resend\")]\nasync fn resend_webhook(\n    State(AppState {\n        ref db, queue_tx, ..\n    }): State<AppState>,\n    Path(ApplicationMsgEndpointPath {\n        msg_id,\n        endpoint_id,\n        ..\n    }): Path<ApplicationMsgEndpointPath>,\n    permissions::Application { app }: permissions::Application,\n) -> Result<NoContentWithCode<202>> {\n    let (msg, msg_content) = message::Entity::secure_find_by_id_or_uid(app.id.clone(), msg_id)\n        .find_also_related(messagecontent::Entity)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    let msg_content = match msg_content {\n        Some(m) => serde_json::from_slice(&m.payload).ok(),\n        None => msg.legacy_payload,\n    };\n    if msg_content.is_none() {\n        return Err(HttpError::bad_request(\n            Some(\"missing_payload\".to_string()),\n            Some(\"Unable to resend message. Payload is missing (probably expired).\".to_string()),\n        )\n        .into());\n    }\n\n    let endp = endpoint::Entity::secure_find_by_id_or_uid(app.id.clone(), endpoint_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, None))?;\n\n    queue_tx\n        .send(\n            &MessageTask::new_task(\n                msg.id.clone(),\n                app.id,\n                endp.id,\n                MessageAttemptTriggerType::Manual,\n            ),\n            None,\n        )\n        .await?;\n    Ok(NoContentWithCode)\n}\n\n/// Deletes the given attempt's response body. Useful when an endpoint accidentally returned sensitive content.\n#[aide_annotate(op_id = \"v1.message-attempt.expunge-content\")]\nasync fn expunge_attempt_content(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationMsgAttemptPath {\n        msg_id, attempt_id, ..\n    }): Path<ApplicationMsgAttemptPath>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n) -> Result<StatusCode> {\n    let msg = message::Entity::secure_find_by_id_or_uid(app.id, msg_id)\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, Some(\"Message not found\".to_string())))?;\n\n    let mut attempt = messageattempt::Entity::secure_find_by_msg(msg.id)\n        .filter(messageattempt::Column::Id.eq(attempt_id))\n        .one(db)\n        .await?\n        .ok_or_else(|| HttpError::not_found(None, Some(\"Message attempt not found\".to_string())))?\n        .into_active_model();\n\n    attempt.response = sea_orm::Set(\"EXPUNGED\".to_string());\n    attempt.update(<|fim_middle|>", "completion": "if let Some(status) = status {\n        query = query.filter(messageattempt::Column::Status.eq(status))\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/attempt.rs", "node_type": "if_expression", "line_range": [927, 929]}
{"prompt": "<|fim_prefix|>f) -> Option<&str> {\n        self.cache_dsn.as_deref().or(self.redis_dsn.as_deref())\n    }\n\n    /// Fetches the configured backend information for the queue. May panic is the configuration has\n    /// not been validated\n    pub fn queue_backend(&self) -> QueueBackend<'_> {\n        let err = \"Called [`queue_backend`] before validating configuration\";\n\n        match self.queue_type {\n            QueueType::Memory => QueueBackend::Memory,\n            QueueType::Redis => QueueBackend::Redis(self.queue_dsn().expect(err)),\n            QueueType::RedisCluster => QueueBackend::RedisCluster(self.queue_dsn().expect(err)),\n            QueueType::RedisSentinel => QueueBackend::RedisSentinel(\n                self.queue_dsn().expect(err),\n                self.redis_sentinel_cfg.as_ref().expect(err),\n            ),\n            QueueType::RabbitMQ => QueueBackend::RabbitMq(self.rabbit_dsn.as_ref().expect(err)),\n        }\n    }\n\n    /// Fetches the configured backend information for the cache, or `None` if the [`CacheType`] is\n    ///  `None`. May panic is the configuration has not been validated\n    pub fn cache_backend(&self) -> CacheBackend<'_> {\n        let err = \"Called [`cache_backend`] before validating configuration\";\n\n        match self.cache_type {\n            CacheType::None => CacheBackend::None,\n            CacheType::Memory => CacheBackend::Memory,\n            CacheType::Redis => CacheBackend::Redis(self.cache_dsn().expect(err)),\n            CacheType::RedisCluster => CacheBackend::RedisCluster(self.cache_dsn().expect(err)),\n            CacheType::RedisSentinel => CacheBackend::RedisSentinel(\n                self.cache_dsn().expect(err),\n                self.redis_sentinel_cfg.as_ref().expect(err),\n            ),\n        }\n    }\n}\n\n#[derive(Clone, Debug, Deserialize)]\npub struct InternalConfig {\n    /// The region to use in the Svix URL given in th dashboard access endpoint\n    #[serde(default = \"default_region\")]\n    pub region: String,\n\n    /// The base url to use for the app portal\n    #[serde(default = \"default_app_portal_url\")]\n    pub app_portal_url: String,\n}\n\nfn default_region() -> String {\n    \"self_hosted\".to_owned()\n}\n\nfn default_app_portal_url() -> String {\n    \"https://app.svix.com\".to_owned()\n}\n\n#[derive(Debug, Eq, PartialEq)]\npub enum QueueBackend<'a> {\n    Memory,\n    Redis(&'a str),\n    RedisCluster(&'a str),\n    RedisSentinel(&'a str, &'a SentinelConfig),\n    RabbitMq(&'a str),\n}\n\n#[derive(Debug, Eq, PartialEq)]\npub enum CacheBackend<'a> {\n    None,\n    Memory,\n    Redis(&'a str),\n    RedisCluster(&'a str),\n    RedisSentinel(&'a str, &'a SentinelConfig),\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogLevel {\n    Info,\n    Debug,\n    Trace,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum LogFormat {\n    Default,\n    Json,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum QueueType {\n    Memory,\n    Redis,\n    RedisCluster,\n    RedisSentinel,\n    RabbitMQ,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum CacheType {\n    Memory,\n    Redis,\n    RedisCluster,\n    RedisSentinel,\n    None,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum DefaultSignatureType {\n    Hmac256,\n    Ed25519,\n}\n\n#[derive(Clone, Debug, Deserialize)]\n#[serde(rename_all = \"lowercase\")]\npub enum Environment {\n    Dev,\n    Staging,\n    Prod,\n}\n\nimpl std::fmt::Display for Environment {\n    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n        write!(\n            f,\n            \"{}\",\n            match self {\n                Environment::Dev => \"dev\",\n                Environment::Staging => \"staging\",\n                Environment::Prod => \"prod\",\n            }\n        )\n    }\n}\n\nimpl fmt::Display for LogLevel {\n    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {\n        match self {\n            Self::Info => Level::INFO,\n            Self::Debug => Level::DEBUG,\n            Self::Trace => Level::TRACE,\n        }\n        .fmt(f)\n    }\n}\n\n#[derive(Clone, Debug, Deserialize, Eq, PartialEq)]\npub struct SentinelConfig {\n    #[serde(rename = \"sentinel_service_name\")]\n    pub service_name: String,\n    #[serde(default)]\n    pub redis_tls_mode_secure: bool,\n    pub redis_db: Option<i64>,\n    pub redis_username: Option<String>,\n    pub redis_password: Option<String>,\n    #[serde(default)]\n    pub redis_use_resp3: bool,\n}\n\nimpl From<SentinelConfig> for omniqueue::backends::redis::SentinelConfig {\n    fn from(val: SentinelConfig) -> Self {\n        let SentinelConfig {\n            service_name,\n            redis_tls_mode_secure,\n            redis_db,\n            redis_username,\n            redis_password,\n            redis_use_resp3,\n        } = val;\n        omniqueue::backends::redis::SentinelConfig {\n            service_name,\n            redis_tls_mode_secure,\n            redis_db,\n            redis_username,\n            redis_password,\n            redis_use_resp3,\n        }\n    }\n}\n\n/// Try to extract a [`ConfigurationInner`] from the provided [`Figment`]. Any error message should\n/// indicate the missing required field(s).\nfn try_extract(figment: Figment) -> anyhow::Result<ConfigurationInner> {\n    // Explicitly override error if `jwt_secret` is not set, as the default error does not mention\n    // the field name due it coming from an inlined field `ConfigurationInner::jwt_signing_config`\n    // See: <https://github.com/SergioBenitez/Figment/issues/80>\n    if !figment.contains(\"jwt_secret\") {\n        bail!(\"missing field `jwt_secret`\");\n    }\n\n    Ok(figment.extract()?)\n}\n\npub fn load() -> anyhow::Result<Arc<ConfigurationInner>> {\n    if let Ok(db_url) = std::env::var(\"DATABASE_URL\") {\n        // If we have DATABASE_URL set, we should potentially use it.\n        const DB_DSN: &str = \"SVIX_DB_DSN\";\n        if std::env::var_os(DB_DSN).is_none() {\n            std::env::set_var(DB_DSN, db_url);\n        }\n    }\n\n    l<|fim_suffix|>\n    let config = try_extract(merged).context(\"failed to extract configuration\")?;\n\n    config\n        .validate()\n        .context(\"failed to validate configuration\")?;\n    Ok(Arc::from(config))\n}\n\n#[cfg(test)]\nmod tests {\n    use std::sync::Arc;\n\n    use figment::{\n        providers::{Format as _, Toml},\n        Figment,\n    };\n\n    use super::{load, try_extract, CacheBackend, CacheType, QueueBackend, QueueType};\n    use crate::core::security::{JWTAlgorithm, JwtSigningConfig};\n\n    #[test]\n    fn test_cache_or_queue_dsn_priority() {\n        let mut cfg = load().unwrap();\n        let cfg = Arc::make_mut(&mut cfg);\n\n        // Override all relevant values\n        cfg.queue_type = QueueType::Redis;\n        cfg.cache_type = CacheType::Redis;\n        cfg.queue_dsn = Some(\"test_a\".to_owned());\n        cfg.cache_dsn = Some(\"test_b\".to_owned());\n        cfg.redis_dsn = Some(\"this_value_should_be_overridden\".to_owned());\n\n        // Assert that the queue_dsn and cache_dsn overwrite the `redis_dsn`\n        assert_eq!(cfg.queue_backend(), QueueBackend::Redis(\"test_a\"));\n        assert_eq!(cfg.cache_backend(), CacheBackend::Redis(\"test_b\"));\n    }\n\n    #[test]\n    fn test_try_extract_missing_jwt_secret() {\n        let defaults = Figment::new();\n\n        let actual = try_extract(defaults);\n\n        let err = actual.unwrap_err();\n        assert_eq!(err.to_string(), \"missing field `jwt_secret`\");\n    }\n\n    #[test]\n    fn test_jwt_signing_fallback() {\n        let raw_config = r#\"\njwt_secret = \"not_actually_a_secret\"\n        \"#;\n\n        let actual: JwtSigningConfig = Figment::new()\n            .merge(Toml::string(raw_config))\n            .extract()\n            .unwrap();\n\n        assert!(matches!(actual, JwtSigningConfig::Default { .. }));\n    }\n\n    #[test]\n    fn test_jwt_select_algorithm() {\n        let raw_config = r#\"\njwt_secret = \"not_actually_a_secret\"\njwt_algorithm = \"HS512\"\n        \"#;\n\n        let actual: JwtSigningConfig = Figment::new()\n            .merge(Toml::string(raw_config))\n            .extract()\n            .unwrap();\n\n        assert!(matches!(\n            actual,\n            JwtSigningConfig::Advanced(JWTAlgorithm::HS512(_))\n        ));\n    }\n}\n<|fim_middle|>", "completion": "let merged = Figment::new()\n        .merge(Toml::string(DEFAULTS))\n        .merge(Toml::file(\"config.toml\"))\n        .merge(Env::prefixed(\"SVIX_\"));", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/cfg.rs", "node_type": "let_declaration", "line_range": [603, 606]}
{"prompt": "<|fim_prefix|>use proc_macro2::TokenStream;\nuse quote::{format_ident, quote, ToTokens as _};\nuse syn::{punctuated::Punctuated, Token};\n\npub type AideAnnotateArgumentList = Punctuated<syn::MetaNameValue, Token![,]>;\n\npub fn expand_aide_annotate(\n    args: AideAnnotateArgumentList,\n    item: syn::ItemFn,\n) -> syn::Result<TokenStream> {\n    // By default, use the function's name as the operation id.\n    <|fim_suffix|>\n    // The operation summary is the title-cased version of the original\n    // function name.\n    let mut operation_summary = operation_id\n        .split('_')\n        .map(title_case)\n        .collect::<Vec<String>>()\n        .join(\" \");\n    // The documentation function's name will always be the name of the\n    // original function suffixed with `_operation`.\n    let operation_ident = format_ident!(\"{}_operation\", item.sig.ident);\n    let visibility = item.vis.clone();\n\n    // Allow overriding operation ID and summary via arguments\n    for arg in args {\n        let lit = expr_to_litstr(&arg.value).ok_or_else(|| {\n            syn::Error::new_spanned(\n                &arg.value,\n                \"Unexpected expression, expected a string literal\",\n            )\n        })?;\n\n        if arg.path.is_ident(\"op_id\") {\n            operation_id = lit.value();\n        } else if arg.path.is_ident(\"op_summary\") {\n            operation_summary = lit.value();\n        } else {\n            let path = arg.path.to_token_stream().to_string();\n            return Err(syn::Error::new_spanned(\n                arg.path,\n                format_args!(\"Unknown argument `{path}`, expected `op_id` or `op_summary`\"),\n            ));\n        }\n    }\n\n    let description = doc_comment_from_attributes(&item.attrs);\n\n    if description.is_none() {\n        let msg = \"An annotated handler must have a doc comment for its description.\";\n        return Err(syn::Error::new_spanned(&item.sig.ident, msg));\n    }\n\n    Ok(quote! {\n        #item\n\n        #visibility fn #operation_ident(\n            op: ::aide::transform::TransformOperation,\n        ) -> ::aide::transform::TransformOperation {\n            op\n                .id(#operation_id)\n                .summary(#operation_summary)\n                .description(#description)\n                .response_with::<401, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Unauthorized\")\n                })\n                .response_with::<403, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Forbidden\")\n                })\n                .response_with::<404, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Not Found\")\n                })\n                .response_with::<409, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Conflict\")\n                })\n                .response_with::<422, ::axum::Json<crate::error::ValidationHttpError>, _>(|op| {\n                    op.description(\"Validation Error\")\n                })\n                .response_with::<429, ::axum::Json<crate::error::StandardHttpError>, _>(|op| {\n                    op.description(\"Too Many Requests\")\n                })\n        }\n    })\n}\n\nfn doc_comment_from_attributes(attributes: &Vec<syn::Attribute>) -> Option<String> {\n    let mut doc_comment_lines = Vec::new();\n\n    for attr in attributes {\n        if !attr.path().is_ident(\"doc\") {\n            continue;\n        }\n\n        // Ignore bare `#[doc]` and `#[doc(foo)]` attributes, only look at `#[doc = \"foo\"]`\n        let Ok(name_val) = attr.meta.require_name_value() else {\n            continue;\n        };\n\n        // Malformed doc attribute, likely a compile error anyways\n        let Some(doc) = expr_to_litstr(&name_val.value) else {\n            continue;\n        };\n\n        doc_comment_lines.push(doc.value().trim().to_owned());\n    }\n\n    if doc_comment_lines.is_empty() {\n        return None;\n    }\n    Some(doc_comment_lines.join(\"\\n\"))\n}\n\nfn title_case(s: &str) -> String {\n    let mut c = s.chars();\n    match c.next() {\n        None => String::new(),\n        Some(f) => f.to_uppercase().collect::<String>() + c.as_str(),\n    }\n}\n\nfn expr_to_litstr(expr: &syn::Expr) -> Option<&syn::LitStr> {\n    match expr {\n        syn::Expr::Lit(l) => match &l.lit {\n            syn::Lit::Str(s) => Some(s),\n            _ => None,\n        },\n        _ => None,\n    }\n}\n<|fim_middle|>", "completion": "let mut operation_id = item.sig.ident.to_string();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server-derive/src/aide.rs", "node_type": "let_declaration", "line_range": [12, 12]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct MessageOut {\n    /// List of free-form identifiers that endpoints can filter by\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub channels: Option<Vec<String>>,\n\n    #[serde(rename = \"deliverAt\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub deliver_at: Option<String>,\n\n    /// Optional unique identifier for the message\n    #[serde(rename = \"eventId\")]\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub event_id: Option<String>,\n\n    /// The event type's name\n    #[serde(rename = \"eventType\")]\n    pub event_type: String,\n\n    /// The Message's ID.\n    pub id: String,\n\n    pub payload: serde_json::Value,\n\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub tags: Option<Vec<String>>,\n\n    pub timestamp: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl MessageOut {\n    pub fn new(\n        event_type: String,\n        id: String,\n        payload: serde_json::Value,\n        timestamp: String,\n    ) -> Self {\n        Self {\n            channels: None,\n            deliver_at: None,\n            event_id: None,\n            event_type,\n            id,\n            payload,\n            tags: None,\n            timestamp,\n        }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/message_out.rs", "node_type": "impl_item", "line_range": [34, 52]}
{"prompt": "<|fim_prefix|>use std::{collections::HashSet, time::Duration};\n\nuse chrono::{DateTime, FixedOffset, Utc};\nuse sea_orm::{DatabaseConnection, DatabaseTransaction, TransactionTrait};\nuse serde::{Deserialize, Serialize};\n\nuse super::types::EventTypeName;\nuse crate::{\n    core::{\n        cache::{kv_def, Cache, CacheBehavior, CacheKey, CacheValue},\n        types::{\n            ApplicationId, ApplicationUid, EndpointHeaders, EndpointId, EndpointSecretInternal,\n            EventChannelSet, EventTypeNameSet, ExpiringSigningKeys, MessageAttemptTriggerType,\n            OrganizationId,\n        },\n    },\n    db::models::{application, endpoint},\n    error::{Error, Result},\n};\n\n/// The information cached during the creation of a message. Includes a [`Vec`] of all endpoints\n/// associated with the given application and organization ID.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageApp {\n    pub id: ApplicationId,\n    pub uid: Option<ApplicationUid>,\n    pub org_id: OrganizationId,\n    pub rate_limit: Option<u16>,\n    endpoints: Vec<CreateMessageEndpoint>,\n    deleted: bool,\n}\n\nimpl CreateMessageApp {\n    /// Fetch all requisite information for creating a [`CreateMessageApp`] from the PostgreSQL\n    /// database\n    async fn fetch_from_pg_by_model(\n        db: &DatabaseTransaction,\n        app: application::Model,\n    ) -> Result<CreateMessageApp> {\n        let endpoints = endpoint::Entity::secure_find(app.id.clone())\n            .all(db)\n            .await?\n            .into_iter()\n            .map(TryInto::try_into)\n            .collect::<Result<Vec<_>>>()?;\n\n        Ok(CreateMessageApp {\n            id: app.id,\n            uid: app.uid,\n            org_id: app.org_id,\n            rate_limit: app\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Application rate limit out of bounds\"))?,\n            endpoints,\n            deleted: app.deleted,\n        })\n    }\n\n    /// Fetches all information for creating a [`CreateMessageApp`] from the Redis cache if it\n    /// exists or from PostgreSQL otherwise. If the RedisCache is Some, but does not contain the\n    /// requisite information, fetch it from PostgreSQL and insert the data into the cache.\n    pub async fn layered_fetch(\n        cache: &Cache,\n        pg: &DatabaseConnection,\n        app: Option<application::Model>,\n        org_id: OrganizationId,\n        app_id: ApplicationId,\n        ttl: Duration,\n    ) -> Result<Option<CreateMessageApp>> {\n        let cache_key = AppEndpointKey::new(&org_id, &app_id);\n\n        // First check Redis\n        if let Ok(Some(cma)) = cache.get::<CreateMessageApp>(&cache_key).await {\n            if cma.deleted {\n                return Ok(None);\n            } else {\n                return Ok(Some(cma));\n            }\n        }\n\n        // Then check PostgreSQL\n        let db = pg.begin().await?;\n        // Fetch the [`application::Model`] either given or from the ID\n        let app = if let Some(app) = app {\n            app\n        } else if let Some(app) = application::Entity::secure_find_by_id(org_id, app_id)\n            .one(&db)\n            .await?\n        {\n            app\n        } else {\n            return Ok(None);\n        };\n\n        // Fetch the actual [`CreateMessageApp`]\n        let out = Self::fetch_from_pg_by_model(&db, app).await?;\n\n        // Insert it into Redis\n        <|fim_suffix|>\n\n        if out.deleted {\n            return Ok(None);\n        }\n\n        Ok(Some(out))\n    }\n\n    pub fn filtered_endpoints(\n        &self,\n        trigger_type: MessageAttemptTriggerType,\n        event_type: &EventTypeName,\n        channels: Option<&EventChannelSet>,\n    ) -> Vec<CreateMessageEndpoint> {\n        self.endpoints\n            .iter()\n            .filter(|endpoint| {\n                // No disabled or deleted endpoints ever\n                !endpoint.disabled && !endpoint.deleted\n                    // Manual attempt types go through regardless\n                    && (trigger_type == MessageAttemptTriggerType::Manual\n                        || (\n                            // If an endpoint has event types and it matches ours, or has no event types\n                            endpoint\n                                .event_types_ids\n                                .as_ref()\n                                .map(|x| x.0.contains(event_type))\n                                .unwrap_or(true)\n                            // If an endpoint has no channels accept all messages, otherwise only if their channels overlap.\n                            // A message with no channels doesn't match an endpoint with channels.\n                            && endpoint\n                                .channels\n                                .as_ref()\n                                .map(|x| {\n                                    !x.0.is_disjoint(\n                                        channels.map(|x| &x.0).unwrap_or(&HashSet::new()),\n                                    )\n                                })\n                                .unwrap_or(true)\n                        ))\n            })\n            .cloned()\n            .collect()\n    }\n}\n\n/// The information for each individual endpoint cached with the creation of a message.\n#[derive(Deserialize, Serialize, Debug, Clone)]\npub struct CreateMessageEndpoint {\n    pub id: EndpointId,\n    pub url: String,\n    pub key: EndpointSecretInternal,\n    pub event_types_ids: Option<EventTypeNameSet>,\n    pub channels: Option<EventChannelSet>,\n    pub rate_limit: Option<u16>,\n    // Same type as the `DateTimeWithTimeZone from SeaORM used in the endpoint model\n    pub first_failure_at: Option<DateTime<FixedOffset>>,\n    pub headers: Option<EndpointHeaders>,\n    pub disabled: bool,\n    pub deleted: bool,\n    // outside of this module, valid_signing_keys should be used instead\n    old_signing_keys: Option<ExpiringSigningKeys>,\n}\n\nimpl CreateMessageEndpoint {\n    pub fn valid_signing_keys(&self) -> Vec<&EndpointSecretInternal> {\n        match self.old_signing_keys {\n            Some(ref old_keys) => std::iter::once(&self.key)\n                .chain(\n                    old_keys\n                        .0\n                        .iter()\n                        .filter(|x| x.expiration > Utc::now())\n                        .map(|x| &x.key),\n                )\n                .collect(),\n            None => vec![&self.key],\n        }\n    }\n}\n\nimpl TryFrom<endpoint::Model> for CreateMessageEndpoint {\n    type Error = Error;\n\n    fn try_from(m: endpoint::Model) -> Result<CreateMessageEndpoint> {\n        Ok(CreateMessageEndpoint {\n            id: m.id,\n            url: m.url,\n            key: m.key,\n            old_signing_keys: m.old_keys,\n            event_types_ids: m.event_types_ids,\n            channels: m.channels,\n            rate_limit: m\n                .rate_limit\n                .map(|v| v.try_into())\n                .transpose()\n                .map_err(|_| Error::validation(\"Endpoint rate limit out of bounds\"))?,\n            first_failure_at: m.first_failure_at,\n            headers: m.headers,\n            disabled: m.disabled,\n            deleted: m.deleted,\n        })\n    }\n}\n\nkv_def!(AppEndpointKey, CreateMessageApp);\nimpl AppEndpointKey {\n    // FIXME: Rewrite doc comment when AppEndpointValue members are known\n    /// Returns a key for fetching all cached endpoints for a given organization and application.\n    pub fn new(org: &OrganizationId, app: &ApplicationId) -> AppEndpointKey {\n        AppEndpointKey(format!(\"SVIX_CACHE_APP_v3_{org}_{app}\"))\n    }\n}\n\n#[cfg(test)]\nmod tests {\n    use base64::{engine::general_purpose::STANDARD, Engine};\n    use chrono::Utc;\n\n    use super::CreateMessageEndpoint;\n    use crate::core::{\n        cryptography::Encryption,\n        types::{\n            EndpointId, EndpointSecret, EndpointSecretInternal, ExpiringSigningKey,\n            ExpiringSigningKeys,\n        },\n    };\n\n    #[test]\n    fn test_valid_signing_keys() {\n        let key = EndpointSecretInternal::from_endpoint_secret(\n            EndpointSecret::Symmetric(STANDARD.decode(\"MfKQ9r8GKYqrTwjUPD8ILPZIo2LaLaSw\").unwrap()),\n            &Encryption::new_noo<|fim_middle|>", "completion": "let _ = cache.set(&cache_key, &out, ttl).await;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/message_app.rs", "node_type": "let_declaration", "line_range": [101, 101]}
{"prompt": "<|fim_prefix|>use std::time::Duration;\n\npub use async_trait::async_trait;\nuse serde::{Deserialize, Serialize};\npub use svix;\nuse svix::api::{MessageIn, SvixOptions as _SvixOptions};\nuse tokio::sync::{mpsc, oneshot};\n\n#[derive(Deserialize, Default, Eq, PartialEq, Copy, Clone)]\n#[serde(rename_all = \"lowercase\")]\npub enum TransformerInputFormat {\n    String,\n    #[default]\n    Json,\n}\n\n#[derive(Deserialize, Clone)]\n#[serde(untagged)]\npub enum TransformationConfig {\n    /// If the config has a string value, we assume it expects the input parsed as json\n    /// ```yaml\n    /// transformation: function handler(x) {return { payload: x.foobar }; }\n    /// ```\n    ImplicitJson(String),\n    /// When the config has format/src fields, then you can optionally set the format to `string`,\n    /// in which case you have to parse it yourself inside the transformation.\n    /// ```yaml\n    /// transformation:\n    ///   format: string\n    ///   src: function handler(x) { return { payload: JSON.parse(x).foobar }; }\n    /// ```\n    Explicit {\n        format: TransformerInputFormat,\n        src: String,\n    },\n}\n\nimpl TransformationConfig {\n    pub fn source(&self) -> &String {\n        match self {\n            TransformationConfig::ImplicitJson(src) => src,\n            TransformationConfig::Explicit { src, .. } => src,\n        }\n    }\n\n    pub fn format(&self) -> TransformerInputFormat {\n        match self {\n            TransformationConfig::ImplicitJson(_) => TransformerInputFormat::Json,\n            TransformationConfig::Explicit { format, .. } => *format,\n        }\n    }\n}\n\nimpl<S> From<S> for TransformationConfig\nwhere\n    S: Into<String>,\n{\n    fn from(value: S) -> Self {\n        Self::ImplicitJson(value.into())\n    }\n}\n\n#[derive(Serialize)]\n#[serde(untagged)]\npub enum TransformerInput {\n    /// Transformations accept arbitrary json here, not restricted to an Object type.\n    /// The thing receiving the value will error if it can't marshall into a type it needs.\n    Json(serde_json::Value),\n    /// Aka \"raw\", we take the input as a utf-8 string and the transformation does whatever it\n    /// wants with it.\n    String(String),\n}\n\nimpl From<serde_json::Value> for TransformerInput {\n    fn from(value: serde_json::Value) -> Self {\n        Self::Json(value)\n    }\n}\n\nimpl From<String> for TransformerInput {\n    fn from(value: String) -> Self {\n        Self::String(value)\n    }\n}\n\n/// Plain old JSON objects are what the transformations expect to receive and produce.\npub type JsObject = serde_json::Map<String, serde_json::Value>;\n/// A channel for plugins to send payloads/scripts to for execution.\npub type TransformerTx = mpsc::UnboundedSender<TransformerJob>;\n/// The receiver side for transformations. The JS executor reads from this.\npub type TransformerRx = mpsc::UnboundedReceiver<TransformerJob>;\n/// A oneshot channel for the JS executor to \"publish\" return values to once complete.\n// FIXME: better error type?\npub type TransformerCallbackTx = oneshot::Sender<Result<TransformerOutput, ()>>;\n/// Used by the caller of the transformer to await the execution's output.\n// FIXME: better error type?\npub type TransformerCallbackRx = oneshot::Receiver<Result<TransformerOutput, ()>>;\n\n/// A transformation job sent to the JS executor.\n/// Once the script has been run on the payload, the transformed payload is sent back through the\n/// callback channel.\npub struct TransformerJob {\n    pub callback_tx: TransformerCallbackTx,\n    pub input: TransformerInput,\n    pub script: String,\n}\n\n#[derive(Debug)]\npub enum TransformerOutput {\n    /// A successfully transformed payload.\n    // Both senders and receivers require a map type (Object) but have different requirements which\n    // are best validated after the fact. For now, we validate only that we get a map type back.\n    Object(JsObject),\n    /// For cases where the JS script executes successfully but produces an unexpected output.\n    Invalid,\n}\n\nimpl TransformerJob {\n    <|fim_suffix|>\n}\n\n/// Effectively a black box to the supervisor.\n///\n/// Plugins should run until they are done, and likely they should not be \"done\" until the program\n/// exits.\n#[async_trait]\npub trait SenderInput: Send {\n    fn name(&self) -> &str;\n    /// For plugins that want to run JS transformations on payloads.\n    /// Giving them a sender lets them pass messages to the JS executor.\n    fn set_transformer(&mut self, _tx: Option<TransformerTx>) {}\n    async fn run(&self);\n}\n\n#[async_trait]\npub trait PollerInput: Send {\n    fn name(&self) -> &str;\n    fn set_transformer(&mut self, _tx: Option<TransformerTx>) {}\n    async fn run(&self);\n}\n\npub type BoxError = Box<dyn std::error::Error + Send + Sync>;\n\n/// Represents something we can hand a webhook payload to.\n/// Aka a \"forwarder.\"\n///\n/// To start, we're only using this in conjunction with an HTTP server \"owned\" by the bridge binary.\n#[async_trait]\npub trait ReceiverOutput: Send + Sync {\n    fn name(&self) -> &str;\n    async fn handle(&self, request: ForwardRequest) -> Result<(), BoxError>;\n}\n\n#[derive(Deserialize, Debug, Clone, Default)]\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum WebhookVerifier {\n    Svix {\n        endpoint_secret: String,\n    },\n    #[default]\n    None,\n}\n\n#[derive(Debug, Clone, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"kebab-case\")]\npub enum ReceiverInputOpts {\n    Webhook {\n        path_id: String,\n        #[serde(default)]\n        verification: WebhookVerifier,\n    },\n    SvixWebhook {\n        path_id: String,\n        endpoint_secret: String,\n    },\n}\n\nimpl ReceiverInputOpts {\n    pub fn path_id(&self) -> &str {\n        match self {\n            ReceiverInputOpts::Webhook { path_id, .. }\n            | ReceiverInputOpts::SvixWebhook { path_id, .. } => path_id,\n        }\n    }\n}\n\n// N.b. the codegen types we get from openapi don't impl Deserialize so we need our own version.\n#[derive(Clone, Debug, Default, Deserialize)]\npub struct SvixOptions {\n    #[serde(default)]\n    pub debug: bool,\n    pub server_url: Option<String>,\n    pub timeout_secs: Option<u64>,\n    pub num_retries: Option<u32>,\n    pub retry_schedule_ms: Option<Vec<u64>>,\n    pub proxy_address: Option<String>,\n}\n\nimpl From<SvixOptions> for _SvixOptions {\n    fn from(\n        SvixOptions {\n            debug,\n            server_url,\n            timeout_secs,\n            num_retries,\n            retry_schedule_ms,\n            proxy_address,\n        }: SvixOptions,\n    ) -> Self {\n        _SvixOptions {\n            debug,\n            server_url,\n            timeout: timeout_secs.map(Duration::from_secs),\n            num_retries,\n            retry_schedule: retry_schedule_ms\n                .map(|sched| sched.into_iter().map(Duration::from_millis).collect()),\n            proxy_address,\n        }\n    }\n}\n\n#[derive(Debug, Deserialize)]\n#[serde(tag = \"type\", rename_all = \"lowercase\")]\npub enum SenderOutputOpts {\n    Svix(SvixSenderOutputOpts),\n}\n\n#[derive(Debug, Deserialize)]\npub struct SvixSenderOutputOpts {\n    /// Svix API token for the client.\n    pub token: String,\n    /// Options for the Svix client.\n    #[serde(default)]\n    pub options: Option<SvixOptions>,\n}\n\n/// Senders convert messages into Create Message API calls so the JSON pulled out of message queues\n/// or produced by transformations need to conform to this shape.\n#[derive(Clone, Deserialize, Serialize)]\n#[serde(rename_all = \"camelCase\")]\npub struct CreateMessageRequest {\n    pub app_id: String,\n    pub message: MessageIn,\n}\n\n/// Receivers convert HTTP bodies into messages forwarded to (currently only) message queues, etc.\n///\n/// The `payload` field represents the message body given to the producer, and other fields may be\n/// added in the future allowing transformations to dynamically customize the producer behavior.\n#[derive(Clone, Deserialize, Serialize)]\npub struct ForwardRequest {\n    /// This is the payload that will be fed into a Receiver Output\n    // XXX: right now I think any arbitrary json value can work, but individual outputs may have\n    // more strict requirements.\n    // The fact this is represented as a field on a json object demands at least that the value can\n    // be represented in json.\n    // FIXME: can we leverage RawValue here?\n    pub pay<|fim_middle|>", "completion": "pub fn new(script: String, input: TransformerInput) -> (Self, TransformerCallbackRx) {\n        let (callback_tx, callback_rx) = oneshot::channel();\n        (\n            Self {\n                input,\n                script,\n                callback_tx,\n            },\n            callback_rx,\n        )\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-types/src/lib.rs", "node_type": "function_item", "line_range": [119, 129]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\n//! Configuration-dependent queue tests. This will depend on the set environment variables as with\n//! the e2e tests such as to allow testing multiple queue backends via the test script.\n\nuse std::{str::FromStr, time::Duration};\n\nuse http::StatusCode;\nuse redis::AsyncCommands as _;\nuse svix_ksuid::KsuidLike;\nuse svix_server::{\n    cfg::Configuration,\n    core::types::{\n        ApplicationId, BaseId, EndpointId, MessageAttemptTriggerType, MessageId, OrganizationId,\n    },\n    queue::{\n        new_pair, MessageTask, QueueTask, TaskQueueConsumer, TaskQueueDelivery, TaskQueueProducer,\n    },\n    redis::RedisManager,\n    v1::endpoints::message::MessageOut,\n};\nuse tokio::time::timeout;\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg_and_org_id_and_prefix,\n};\n\n// TODO: Don't copy this from the Redis queue test directly, place the fn somewhere both can access\nasync fn get_pool(cfg: &Configuration) -> RedisManager {\n    RedisManager::from_queue_backend(&cfg.queue_backend(), cfg.redis_pool_max_size).await\n}\n\nfn task_queue_delivery_to_u16(tqd: &TaskQueueDelivery) -> u16 {\n    match &*tqd.task {\n        QueueTask::HealthCheck => panic!(\"Health check in test\"),\n        QueueTask::MessageBatch(batch) => u16::from_str(batch.msg_id.as_str()).unwrap(),\n        QueueTask::MessageV1(task) => u16::from_str(task.msg_id.as_str()).unwrap(),\n    }\n}\n\nasync fn test_many_queue_consumers_inner(prefix: &str, delay: Option<Duration>) {\n    dotenvy::dotenv().ok();\n    let cfg = svix_server::cfg::load().expect(\"Error loading configuration\");\n\n    // This test assumes an empty queue, so load Redis and delete the test key\n    {\n        let pool = get_pool(&cfg).await;\n        let mut conn = pool.get().await.unwrap();\n\n        let _: () = conn\n            .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n            .await\n            .unwrap();\n    }\n\n    // Make 20 producers and 20 consumers using the same configuration\n    let mut producers_and_consumers: Vec<(TaskQueueProducer, TaskQueueConsumer)> = Vec::new();\n    for _ in 0..20 {\n        producers_and_consumers.push(new_pair(&cfg, Some(prefix)).await);\n    }\n\n    // Add 200 test messagesÂ¹ with unique message IDs to each producer for a\n    // total of 4000 unique messages\n    //\n    // Â¹ it is important for this number to be no smaller than MAX_MESSAGES in\n    //   TaskQueueConsumer::receive_all\n    for (index, (p, _c)) in producers_and_consumers.iter().enumerate() {\n        for num in 0..200 {\n            p.send(\n                &QueueTask::MessageV1(MessageTask {\n                    msg_id: MessageId(format!(\"{}\", index * 200 + num)),\n                    app_id: ApplicationId(\"TestApplicationId\".to_owned()),\n                    endpoint_id: EndpointId(\"TestEndpointId\".to_owned()),\n                    trigger_type: MessageAttemptTriggerType::Manual,\n                    attempt_count: 0,\n                }),\n                delay,\n            )\n            .await\n            .unwrap();\n        }\n    }\n\n    let mut join_handles = Vec::new();\n    // Producers need to stay alive for the remainder of the test for in-memory queue which uses\n    // [`tokio::mpsc`]s, so add them to this [`Vec`]\n    let mut producers = Vec::new();\n\n    // Ensure that consumers run on separate OS threads and receive messages until 500ms has passed\n    // without any messages\n    for (p, mut c) in producers_and_consumers {\n        producers.push(p);\n        let handle = tokio::runtime::Handle::current();\n        join_handles.push(std::thread::spawn(move || {\n            handle.block_on(async move {\n                let mut out = Vec::new();\n                let mut read = 0;\n\n                while let Ok(recv) = timeout(\n                    Duration::from_secs(1),\n                    c.receive_all(Duration::from_secs(5)),\n                )\n                .await\n                {\n                    let recv = recv.unwrap();\n                    read += recv.len();\n                    for r in recv {\n                        out.push(task_queue_delivery_to_u16(&r));\n                        r.ack().await.unwrap();\n                    }\n                }\n\n                (out, read)\n            })\n        }));\n    }\n\n    // Create a Vec with all the threads' outputs\n    let mut out = Vec::new();\n    for jh in join_handles {\n        let (mut jh_out, read): (Vec<u16>, usize) = jh.join().unwrap();\n        out.append(&mut jh_out);\n\n        if read < 20 {\n            panic!(\"Consumer starved, only read {read} messages\");\n        }\n    }\n\n    // Sort it by the message ID\n    out.sort();\n\n    // Then assert that all the messages are there\n    assert_eq!(out.len(), 4000);\n    for (idx, &num) in out.iter().enumerate() {\n        assert_eq!(idx, num as usize);\n    }\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n// run with `cargo test -- --ignored redis` only when redis is up and configured\n#[ignore]\nasync fn test_many_queue_consumers() {\n    test_many_queue_consumers_inner(\"test_many_queue_consumers_\", None).await;\n}\n\n// Without the `multi_thread` and `worker_threads` directive, the `block_on` call will never return\n// and the test will hang.\n#[tokio::test(flavor = \"multi_thread\", worker_threads = 4)]\n#[ignore]\nasync fn test_many_queue_consumers_delayed() {\n    test_many_queue_consumers_inner(\n        \"test_many_queue_consumers_delayed_\",\n        Some(Duration::from_millis(500)),\n    )\n    .await;\n}\n\n#[tokio::test]\n#[ignore]\nasync fn test_redis_streams_dlq() {\n    let mut cfg = get_default_test_config();\n    cfg.worker_enabled = false;\n    cfg.redis_pending_duration_secs = 1;\n\n    let cfg = std::sync::Arc::new(cfg);\n    let prefix = svix_ksuid::Ksuid::new(None, None).to_string();\n\n    let pool = get_pool(&cfg).await;\n    let mut conn = pool.get().await.unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_v3_main\"))\n        .await\n        .unwrap();\n\n    let _: () = conn\n        .del(format!(\"{prefix}{{queue}}_svix_dlq\"))\n        .await\n        .unwrap();\n\n    let (client, _jh) = start_svix_server_with_cfg_and_org_id_and_prefix(\n        &cfg,\n        OrganizationId::new(None, None),\n        prefix.clone(),\n    )\n    .await;\n\n    let app_id = create_test_app(&client, \"v1MessageCRTestApp\")\n        .await\n        .unwrap()\n        .id;\n\n    let _endp_id = create_test_endpoint(&client, &app_id, \"http://localhost:2/bad/url/\")\n        .await\n        .unwrap()\n        .id;\n\n    let<|fim_suffix|>   let (_p, mut c) = new_pair(&cfg, Some(&prefix)).await;\n\n    let wait_time = std::time::Duration::from_millis(1_500);\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n\n    tokio::time::sleep(wait_time).await;\n\n    // Redrive\n    client\n        .post_without_response(\n            \"/api/v1/admin/redrive-dlq\",\n            serde_json::Value::Null,\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    for _ in 0..3 {\n        let res = c.receive_all(wait_time).await.unwrap();\n        assert!(!res.is_empty());\n        for j in res {\n            j.nack().await.unwrap();\n        }\n    }\n\n    let res = c.receive_all(wait_time).await.unwrap();\n    assert!(res.is_empty());\n}\n<|fim_middle|>", "completion": "let _message_1: MessageOut = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, serde_json::json!({\"test\": \"value\"})).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/redis_queue.rs", "node_type": "let_declaration", "line_range": [202, 209]}
{"prompt": "<|fim_prefix|>// this file is @generated\n<|fim_suffix|>\nuse crate::Configuration;\n\npub struct OperationalWebhook<'a> {\n    cfg: &'a Configuration,\n}\n\nimpl<'a> OperationalWebhook<'a> {\n    pub(super) fn new(cfg: &'a Configuration) -> Self {\n        Self { cfg }\n    }\n\n    pub fn endpoint(&self) -> OperationalWebhookEndpoint<'a> {\n        OperationalWebhookEndpoint::new(self.cfg)\n    }\n}\n<|fim_middle|>", "completion": "use super::OperationalWebhookEndpoint;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/api/operational_webhook.rs", "node_type": "use_declaration", "line_range": [2, 2]}
{"prompt": "<|fim_prefix|>}\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize, ModelOut, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub struct ApplicationOut {\n    // FIXME: Do we want to use serde(flatten) or just duplicate the keys?\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub uid: Option<ApplicationUid>,\n    #[schemars(example = \"application_name_example\")]\n    pub name: String,\n    #[serde(skip_serializing_if = \"Option::is_none\")]\n    pub rate_limit: Option<u16>,\n\n    pub id: ApplicationId,\n    pub created_at: DateTime<Utc>,\n    pub updated_at: DateTime<Utc>,\n    pub metadata: Metadata,\n}\n\nimpl From<(application::Model, applicationmetadata::Model)> for ApplicationOut {\n    fn from((app, metadata): (application::Model, applicationmetadata::Model)) -> Self {\n        Self {\n            uid: app.uid,\n            name: app.name,\n            rate_limit: app.rate_limit.map(|x| x as u16),\n            id: app.id,\n            created_at: app.created_at.into(),\n            updated_at: app.updated_at.into(),\n            metadata: metadata.metadata(),\n        }\n    }\n}\n\n/// List of all the organization's applications.\n#[aide_annotate(op_id = \"v1.application.list\")]\nasync fn list_applications(\n    State(AppState { ref db, .. }): State<AppState>,\n    ValidatedQuery(pagination): ValidatedQuery<Pagination<ReversibleIterator<ApplicationId>>>,\n    permissions::Organization { org_id }: permissions::Organization,\n) -> Result<Json<ListResponse<ApplicationOut>>> {\n    let PaginationLimit(limit) = pagination.limit;\n    let iterator = pagination.iterator;\n    let iter_direction = iterator\n        .as_ref()\n        .map_or(IteratorDirection::Normal, |iter| iter.direction());\n\n    let query = apply_pagination(\n        application::Entity::secure_find(org_id),\n        application::Column::Id,\n        limit,\n        iterator,\n        pagination.order.unwrap_or(Ordering::Ascending),\n    );\n\n    let results: Vec<ApplicationOut> = query\n        .find_also_related(applicationmetadata::Entity)\n        .all(db)\n        .await?\n        .into_iter()\n        .map(|(app, metadata)| {\n            let metadata =\n                metadata.unwrap_or_else(|| applicationmetadata::Model::new(app.id.clone()));\n            (app, metadata)\n        })\n        .map(ApplicationOut::from)\n        .collect();\n\n    Ok(Json(ApplicationOut::list_response(\n        results,\n        limit as usize,\n        iter_direction,\n    )))\n}\n\nfn default_as_false() -> bool {\n    false\n}\n\n#[derive(Debug, Deserialize, Validate, JsonSchema)]\npub struct CreateApplicationQueryParams {\n    /// Get an existing application, or create a new one if doesn't exist. It's two separate functions in the libs.\n    #[serde(default = \"default_as_false\")]\n    get_if_exists: bool,\n}\n\n/// Create a new application.\n#[aide_annotate(op_id = \"v1.application.create\")]\nasync fn create_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    query: ValidatedQuery<CreateApplicationQueryParams>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<ApplicationIn>,\n) -> Result<JsonStatusUpsert<ApplicationOut>> {\n    if let Some(ref uid) = data.uid {\n        if let Some((app, metadata)) =\n            application::Model::fetch_with_metadata(db, org_id.clone(), uid.clone().into())\n                .await\n                .trace()?\n        {\n            if query.get_if_exists {\n                // Technically not updated, but it fits.\n                return Ok(JsonStatusUpsert::Updated((app, metadata).into()));\n            }\n            return Err(HttpError::conflict(\n                None,\n                Some(\"An application with that id or uid already exists\".into()),\n            )\n            .into());\n        };\n    }\n\n    let (app, metadata) = create_app_from_app_in(db, data, org_id).await?;\n\n    Ok(JsonStatusUpsert::Created((app, metadata).into()))\n}\n\npub async fn create_app_from_app_in(\n    db: &DatabaseConnection,\n    app_in: ApplicationIn,\n    org_id: OrganizationId,\n) -> Result<(application::Model, applicationmetadata::Model)> {\n    l<|fim_suffix|>    let metadata = applicationmetadata::ActiveModel::new(app.id.clone().unwrap(), None);\n\n    let mut model = (app, metadata);\n    app_in.update_model(&mut model);\n    let (app, metadata) = model;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            async move {\n                let app_result = app.insert(txn).await.map_err(http_error_on_conflict)?;\n                let metadata = metadata.upsert_or_delete(txn).await.trace()?;\n                Ok((app_result, metadata))\n            }\n            .boxed()\n        })\n        .await?;\n\n    Ok((app, metadata))\n}\n\n/// Get an application.\n#[aide_annotate(op_id = \"v1.application.get\")]\nasync fn get_application(\n    permissions::ApplicationWithMetadata { app, metadata }: permissions::ApplicationWithMetadata,\n) -> Result<Json<ApplicationOut>> {\n    Ok(Json((app, metadata).into()))\n}\n\n/// Update an application.\n#[aide_annotate(op_id = \"v1.application.update\")]\nasync fn update_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    Path(ApplicationPath { app_id }): Path<ApplicationPath>,\n    permissions::Organization { org_id }: permissions::Organization,\n    ValidatedJson(data): ValidatedJson<ApplicationIn>,\n) -> Result<JsonStatusUpsert<ApplicationOut>> {\n    let (app, metadata, create_models) = if let Some((app, metadata)) =\n        application::Model::fetch_with_metadata(db, org_id.clone(), app_id)\n            .await\n            .trace()?\n    {\n        (app.into(), metadata.into(), false)\n    } else {\n        let app = application::ActiveModel::new(org_id);\n        let metadata = applicationmetadata::ActiveModel::new(app.id.clone().unwrap(), None);\n        (app, metadata, true)\n    };\n\n    let mut models = (app, metadata);\n    data.update_model(&mut models);\n    let (app, metadata) = models;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            async move {\n                let app = if create_models {\n                    app.insert(txn).await.map_err(http_error_on_conflict)?\n                } else {\n                    app.update(txn).await.map_err(http_error_on_conflict)?\n                };\n                let metadata = metadata.upsert_or_delete(txn).await?;\n                Ok((app, metadata))\n            }\n            .boxed()\n        })\n        .await?;\n\n    if create_models {\n        Ok(JsonStatusUpsert::Created((app, metadata).into()))\n    } else {\n        Ok(JsonStatusUpsert::Updated((app, metadata).into()))\n    }\n}\n\n/// Partially update an application.\n#[aide_annotate]\nasync fn patch_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n    ValidatedJson(data): ValidatedJson<ApplicationPatch>,\n) -> Result<Json<ApplicationOut>> {\n    let metadata = app.fetch_or_create_metadata(db).await.trace()?;\n    let app: application::ActiveModel = app.into();\n\n    let mut model = (app, metadata);\n    data.update_model(&mut model);\n    let (app, metadata) = model;\n\n    let (app, metadata) = db\n        .transaction(|txn| {\n            async move {\n                let app = app.update(txn).await.map_err(http_error_on_conflict)?;\n                let metadata = metadata.upsert_or_delete(txn).await.trace()?;\n                Ok((app, metadata))\n            }\n            .boxed()\n        })\n        .await?;\n\n    Ok(Json((app, metadata).into()))\n}\n\n/// Delete an application.\n#[aide_annotate(op_id = \"v1.application.delete\")]\nasync fn delete_application(\n    State(AppState { ref db, .. }): State<AppState>,\n    permissions::OrganizationWithApplication { app }: permissions::OrganizationWithApplication,\n) -> Result<NoContent> {\n    let mut app: application::ActiveModel = app.into();\n    app.deleted = Set(true);\n    app.uid = Set(None); // We don't want deleted UIDs to clash\n    app.update(db).await?;\n    Ok(NoContent)\n}\n\npub fn router() -> ApiRouter<AppState> {\n    let tag = openapi_tag(\"Application\");\n    ApiRouter::new()\n        .api_route_with(\n            \"/app\",\n            post_with(create_application, create_application_operation)\n       <|fim_middle|>", "completion": "let app = application::ActiveModel::new(org_id);", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/v1/endpoints/application.rs", "node_type": "let_declaration", "line_range": [283, 283]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse clap::{Args, Subcommand};\nuse svix::api::*;\n\nuse super::message_poller::MessagePollerArgs;\n\n#[derive(Args, Clone)]\npub struct MessageListOptions {\n    /// Limit the number of returned items\n    #[arg(long)]\n    pub limit: Option<i32>,\n    /// The iterator returned from a prior invocation\n    #[arg(long)]\n    pub iterator: Option<String>,\n    /// Filter response based on the channel.\n    #[arg(long)]\n    pub channel: Option<String>,\n    /// Only include items created before a certain date.\n    #[arg(long)]\n    pub before: Option<chrono::DateTime<chrono::Utc>>,\n    /// Only include items created after a certain date.\n    #[arg(long)]\n    pub after: Option<chrono::DateTime<chrono::Utc>>,\n    /// When `true` message payloads are included in the response.\n    #[arg(long)]\n    pub with_content: Option<bool>,\n    /// Filter messages matching the provided tag.\n    #[arg(long)]\n    pub tag: Option<String>,\n    /// Filter response based on the event type\n    #[arg(long)]\n    pub event_types: Option<Vec<String>>,\n}\n\nimpl From<MessageListOptions> for svix::api::MessageListOptions {\n    fn from(value: MessageListOptions) -> Self {\n        let MessageListOptions {\n            limit,\n            iterator,\n            channel,\n            before,\n            after,\n            with_content,\n            tag,\n            event_types,\n        } = value;\n        Self {\n            limit,\n            iterator,\n            channel,\n            before: before.map(|dt| dt.to_rfc3339()),\n            after: after.map(|dt| dt.to_rfc3339()),\n            with_content,\n            tag,\n            event_types,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageCreateOptions {\n    /// When `true`, message payloads are included in the response.\n    #[arg(long)]\n    pub with_content: Option<bool>,\n\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<MessageCreateOptions> for svix::api::MessageCreateOptions {\n    fn from(value: MessageCreateOptions) -> Self {\n        let MessageCreateOptions {\n            with_content,\n            idempotency_key,\n        } = value;\n        Self {\n            with_content,\n            idempotency_key,\n        }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageExpungeAllContentsOptions {\n    #[arg(long)]\n    pub idempotency_key: Option<String>,\n}\n\nimpl From<MessageExpungeAllContentsOptions> for svix::api::MessageExpungeAllContentsOptions {\n    fn from(value: MessageExpungeAllContentsOptions) -> Self {\n        let MessageExpungeAllContentsOptions { idempotency_key } = value;\n        Self { idempotency_key }\n    }\n}\n\n#[derive(Args, Clone)]\npub struct MessageGetOptions {\n    /// When `true` message payloads are included in the response.\n    #[arg(long)]\n    pub with_content: Option<bool>,\n}\n\nimpl From<MessageGetOptions> for svix::api::MessageGetOptions {\n    <|fim_suffix|>\n}\n\n#[derive(Args)]\n#[command(args_conflicts_with_subcommands = true, flatten_help = true)]\npub struct MessageArgs {\n    #[command(subcommand)]\n    pub command: MessageCommands,\n}\n\n#[derive(Subcommand)]\npub enum MessageCommands {\n    Poller(MessagePollerArgs),\n    /// List all of the application's messages.\n    ///\n    /// The `before` and `after` parameters let you filter all items created before or after a certain date. These can be\n    /// used alongside an iterator to paginate over results within a certain window.\n    ///\n    /// Note that by default this endpoint is limited to retrieving 90 days' worth of data\n    /// relative to now or, if an iterator is provided, 90 days before/after the time indicated\n    /// by the iterator ID. If you require data beyond those time ranges, you will need to explicitly\n    /// set the `before` or `after` parameter as appropriate.\n    List {\n        app_id: String,\n        #[clap(flatten)]\n        options: MessageListOptions,\n    },\n    /// Creates a new message and dispatches it to all of the application's endpoints.\n    ///\n    /// The `eventId` is an optional custom unique ID. It's verified to be unique only up to a day, after that no verification will be made.\n    /// If a message with the same `eventId` already exists for the application, a 409 conflict error will be returned.\n    ///\n    /// The `eventType` indicates the type and schema of the event. All messages of a certain `eventType` are expected to have the same schema. Endpoints can choose to only listen to specific event types.\n    /// Messages can also have `channels`, which similar to event types let endpoints filter by them. Unlike event types, messages can have multiple channels, and channels don't imply a specific message content or schema.\n    ///\n    /// The `payload` property is the webhook's body (the actual webhook message). Svix supports payload sizes of up to 1MiB, though it's generally a good idea to keep webhook payloads small, probably no larger than 40kb.\n    Create {\n        app_id: String,\n        message_in: crate::json::JsonOf<MessageIn>,\n        #[clap(flatten)]\n        options: MessageCreateOptions,\n    },\n    /// Delete all message payloads for the application.\n    ///\n    /// This operation is only available in the <a href=\"https://svix.com/pricing\" target=\"_blank\">Enterprise</a> plan.\n    ///\n    /// A completed task will return a payload like the following:\n    /// ```json\n    /// {\n    ///   \"id\": \"qtask_33qen93MNuelBAq1T9G7eHLJRsF\",\n    ///   \"status\": \"finished\",\n    ///   \"task\": \"application.purge_content\",\n    ///   \"data\": {\n    ///     \"messagesPurged\": 150\n    ///   }\n    /// }\n    /// ```\n    ExpungeAllContents {\n        app_id: String,\n        #[clap(flatten)]\n        options: MessageExpungeAllContentsOptions,\n    },\n    /// Get a message by its ID or eventID.\n    Get {\n        app_id: String,\n        id: String,\n        #[clap(flatten)]\n        options: MessageGetOptions,\n    },\n    /// Delete the given message's payload.\n    ///\n    /// Useful in cases when a message was accidentally sent with sensitive content.\n    /// The message can't be replayed or resent once its payload has been deleted or expired.\n    ExpungeContent {\n        app_id: String,\n        id: String,\n    },\n}\n\nimpl MessageCommands {\n    pub async fn exec(\n        self,\n        client: &Svix,\n        color_mode: colored_json::ColorMode,\n    ) -> anyhow::Result<()> {\n        match self {\n            Self::Poller(args) => {\n                args.command.exec(client, color_mode).await?;\n            }\n            Self::List { app_id, options } => {\n                let resp = client.message().list(app_id, Some(options.into())).await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Create {\n                app_id,\n                message_in,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .create(app_id, message_in.into_inner(), Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ExpungeAllContents { app_id, options } => {\n                let resp = client\n                    .message()\n                    .expunge_all_contents(app_id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::Get {\n                app_id,\n                id,\n                options,\n            } => {\n                let resp = client\n                    .message()\n                    .get(app_id, id, Some(options.into()))\n                    .await?;\n                crate::json::print_json_output(&resp, color_mode)?;\n            }\n            Self::ExpungeContent { app_id, id } => {\n                client.message().expunge_content(app_id, id).await?;\n            }\n        }\n\n        Ok(())\n    }\n}\n<|fim_middle|>", "completion": "fn from(value: MessageGetOptions) -> Self {\n        let MessageGetOptions { with_content } = value;\n        Self { with_content }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/cmds/api/message.rs", "node_type": "function_item", "line_range": [104, 107]}
{"prompt": "<|fim_prefix|>// this file is @generated\nuse serde::{Deserialize, Serialize};\n\n#[derive(Clone, Debug, Default, PartialEq, Deserialize, Serialize)]\npub struct EndpointSecretOut {\n    /// The endpoint's verification secret.\n    ///\n    /// Format: `base64` encoded random bytes optionally prefixed with `whsec_`.\n    /// It is recommended to not set this and let the server generate the\n    /// secret.\n    pub key: String,\n}\n\n<|fim_suffix|>\n<|fim_middle|>", "completion": "impl EndpointSecretOut {\n    pub fn new(key: String) -> Self {\n        Self { key }\n    }\n}", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/models/endpoint_secret_out.rs", "node_type": "impl_item", "line_range": [14, 18]}
{"prompt": "<|fim_prefix|>// SPDX-FileCopyrightText: Â© 2022 Svix Authors\n// SPDX-License-Identifier: MIT\n\nuse std::{\n    sync::{Arc, Mutex},\n    time::Duration,\n};\n\nuse reqwest::StatusCode;\nuse serde_json::json;\nuse svix_server::{\n    core::types::{EndpointUid, MessageStatus},\n    v1::{\n        endpoints::{\n            attempt::{EndpointMessageOut, MessageAttemptOut},\n            endpoint::{EndpointIn, EndpointOut},\n        },\n        utils::ListResponse,\n    },\n};\nuse wiremock::{matchers, Mock, MockServer, Respond, ResponseTemplate};\n\nuse crate::utils::{\n    common_calls::{\n        create_test_app, create_test_endpoint, create_test_message, create_test_msg_with,\n        endpoint_in, get_msg_attempt_list_and_assert_count,\n    },\n    get_default_test_config, run_with_retries, start_svix_server, start_svix_server_with_cfg,\n    TestReceiver,\n};\n\n#[tokio::test]\nasync fn test_expunge_attempt_response_body() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let sensitive_response_json = serde_json::json!({\"sensitive\":\"data\"});\n    let mut receiver = TestReceiver::start_with_body(\n        axum::http::StatusCode::OK,\n        axum::Json(sensitive_response_json.clone()),\n    );\n\n    let endpoint_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    l<|fim_suffix|>\n    receiver.data_recv.recv().await;\n\n    let attempt = run_with_retries(|| async {\n        let attempts: ListResponse<MessageAttemptOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/attempt/endpoint/{endpoint_id}/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        if attempts.data.len() != 1 {\n            anyhow::bail!(\"list len {}, not 1\", attempts.data.len());\n        }\n        Ok(attempts.data[0].clone())\n    })\n    .await\n    .unwrap();\n\n    let attempt_response: serde_json::Value = serde_json::from_str(&attempt.response).unwrap();\n    assert_eq!(sensitive_response_json, attempt_response);\n\n    let attempt_id = &attempt.id;\n    client\n        .delete(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/content/\"),\n            StatusCode::NO_CONTENT,\n        )\n        .await\n        .unwrap();\n\n    let attempt: MessageAttemptOut = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/msg/{msg_id}/attempt/{attempt_id}/\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(\"EXPUNGED\", &attempt.response);\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages() {\n    let (client, _jh) = start_svix_server().await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver_1 = TestReceiver::start(axum::http::StatusCode::OK);\n    let receiver_2 = TestReceiver::start(axum::http::StatusCode::OK);\n\n    let endp_id_1 = create_test_endpoint(&client, &app_id, &receiver_1.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    // Let's have an endpoint with a UID too\n    let mut endp2 = endpoint_in(&receiver_2.endpoint);\n    endp2.uid = Some(EndpointUid(\"test\".to_owned()));\n    let endp_id_2 = client\n        .post::<EndpointIn, EndpointOut>(\n            &format!(\"api/v1/app/{app_id}/endpoint/\"),\n            endp2,\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data2\"}))\n        .await\n        .unwrap();\n    let msg_3 = create_test_msg_with(\n        &client,\n        &app_id,\n        serde_json::json!({\"test\": \"data3\"}),\n        \"balloon.popped\",\n        [\"news\"],\n    )\n    .await;\n\n    run_with_retries(|| async {\n        let list_1: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n        let list_2: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_2}/msg/\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        let list_2_uid: ListResponse<EndpointMessageOut> = client\n            .get(\n                &format!(\"api/v1/app/{app_id}/endpoint/{}/msg/\", \"test\"),\n                StatusCode::OK,\n            )\n            .await\n            .unwrap();\n\n        for list in [list_1, list_2, list_2_uid] {\n            if list.data.len() != 3 {\n                anyhow::bail!(\"list len {}, not 3\", list.data.len());\n            }\n\n            assert!(list.data.iter().any(|x| x.msg == msg_1));\n            assert!(list.data.iter().any(|x| x.msg == msg_2));\n            assert!(list.data.iter().any(|x| x.msg == msg_3));\n        }\n\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    let list_filtered: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?channel=news\"),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_filtered.data.len(), 1);\n    assert!(list_filtered.data[0].msg == msg_3);\n\n    // Test 'event_types' query parameter\n\n    let list_balloon_popped: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_balloon_popped.data.len(), 1);\n    assert!(list_balloon_popped.data[0].msg == msg_3);\n\n    let list_event_type: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(list_event_type.data.len(), 2);\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_event_type.data.iter().any(|x| x.msg == msg_2));\n\n    let list_both_event_types: ListResponse<EndpointMessageOut> = client\n        .get(\n            &format!(\"api/v1/app/{app_id}/endpoint/{endp_id_1}/msg/?event_types=event.type,balloon.popped\",),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    assert_eq!(list_both_event_types.data.len(), 3);\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_1));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_2));\n    assert!(list_both_event_types.data.iter().any(|x| x.msg == msg_3));\n}\n\n#[tokio::test]\nasync fn test_list_attempted_messages_failed() {\n    let mut cfg = get_default_test_config();\n    cfg.retry_schedule = vec![Duration::from_millis(1)];\n    let (client, _jh) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"app1\").await.unwrap().id;\n\n    let receiver = TestReceiver::start(StatusCode::OK);\n    let endp_id = create_test_endpoint(&client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap()\n        .id;\n\n    let msg_1 = create_test_message(&client, &app_id, json!({ \"test\": \"data1\" }))\n        .await\n        .unwrap();\n    let msg_2 = create_test_message(&client, &app_id, json!({ \"test\": \"data2\" }))\n        .await\n        .unwrap();\n\n    run_with_retries(async || {\n        for status in [\"0\"] {\n            let list_success: ListResponse<EndpointMessageOut> = client\n                .get(\n                    &format!(\"api/v1/app/{app_id}/endpoint/{endp_id}/msg/?status={status}\"),\n                    StatusCode::OK,\n                )\n                .await?;\n\n            anyhow::ensure!(list_success.data.len() == 2);\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_1));\n            anyhow::ensure!(list_success.data.iter().any(|x| x.msg == msg_2));\n        }\n        Ok(())\n    })\n    .await\n    .unwrap();\n\n    receiver.set_response_status_code(StatusCode::INTERNAL_SERVER_ERROR);\n\n    let msg_3 = create_test_message(&client, &app_id, json!({ \"test\": \"data3\" }))\n       <|fim_middle|>", "completion": "let msg_id = create_test_message(&client, &app_id, serde_json::json!({\"test\": \"data1\"}))\n        .await\n        .unwrap()\n        .id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_attempt.rs", "node_type": "let_declaration", "line_range": [49, 52]}
{"prompt": "<|fim_prefix|>is://localhost:1234\"\n      max_connections: 4\n      queue_key: \"my_queue\"\n\n  - name: \"forward-to-sqs-example\"\n    input:\n      type: \"webhook\"\n      path_id: \"sqs\"\n      verification:\n        type: \"none\"\n    output:\n      # Note that the SQS forwarder requires credentials to be set as environment vars:\n      # - `AWS_DEFAULT_REGION`\n      # - `AWS_ACCESS_KEY_ID`\n      # - `AWS_SECRET_ACCESS_KEY`\n      type: \"sqs\"\n      queue_dsn: \"https://example.aws.com/my-queue\"\n\n  - name: \"forward-to-kafka-example\"\n    input:\n      type: \"webhook\"\n      path_id: \"kafka\"\n      verification:\n        type: \"none\"\n    output:\n      type: \"kafka\"\n      kafka_bootstrap_brokers: \"localhost:9094\"\n      kafka_topic: \"foobar\"\n      # Other valid values: \"plaintext\", \"ssl\"\n      kafka_security_protocol: \"sasl_ssl\"\n      # Only for SASL\n      kafka_sasl_username: \"user\"\n      kafka_sasl_password: \"pass\"\n\n\"#;\n\n#[test]\nfn test_sender_parses_ok() {\n    let conf: Result<WebhookSenderConfig, _> = serde_yaml::from_str(\n        r#\"\nname: \"from-rabbit-local-to-svix\"\ninput:\n    type: \"rabbitmq\"\n    queue_name: \"local\"\n    uri: \"amqp://example.com/%2f\"\ntransformation: |\n    handler = (x) => ({ appId: \"app_1234\", message: { eventType: \"foo.bar\", payload: x }})\noutput:\n    type: \"svix\"\n    token: \"XXXX\"\n    \"#,\n    );\n    conf.unwrap();\n}\n\n#[test]\nfn test_senders_parses_ok() {\n    let conf: Result<Vec<WebhookSenderConfig>, _> = serde_yaml::from_str(\n        r#\"\n\n- name: \"from-rabbit-local-to-svix\"\n  input:\n    type: \"rabbitmq\"\n    queue_name: \"local\"\n    uri: \"amqp://example.com/%2f\"\n  # Implicit json transformation\n  transformation: |\n    handler = (x) => ({ appId: \"app_1234\", message: { eventType: \"foo.bar\", payload: x }})\n  output:\n    type: \"svix\"\n    token: \"XXXX\"\n- name: \"from-SQS-to-svix\"\n  input:\n    type: \"sqs\"\n    queue_dsn: \"http://sqs.example.com/foo/bar\"\n  # Explicit string transformation\n  transformation:\n    format: string\n    src: |\n        function handler(x) {\n            return { appId: \"app_1234\", message: { eventType: \"foo.bar\", payload: x }}\n        }\n  output:\n    type: \"svix\"\n    token: \"YYYY\"\n\"#,\n    );\n    let conf = conf.unwrap();\n    assert_eq!(conf.len(), 2);\n}\n\n#[test]\nfn test_omnibus_parses_ok() {\n    let conf: Result<Config, _> = serde_yaml::from_str(OMNIBUS);\n    conf.unwrap();\n}\n\n#[test]\nfn test_empty() {\n    let conf: Config = serde_yaml::from_str(\"\").unwrap();\n    assert!(conf.senders.is_empty());\n    assert!(conf.receivers.is_empty());\n    assert_eq!(conf.http_listen_address, \"0.0.0.0:5000\".parse().unwrap());\n    assert!(conf.opentelemetry.is_none());\n    assert!(matches!(conf.log_format, LogFormat::Default));\n    assert!(matches!(conf.log_level, LogLevel::Info));\n}\n\n/// Don't particularly care about the parsed specifics here.\n/// This is more about making sure the examples we have in the repo actually parse.\n#[test]\nfn test_receivers_example() {\n    let fp = concat!(\n        env!(\"CARGO_MANIFEST_DIR\"),\n        \"/../svix-bridge.example.receivers.yaml\"\n    );\n    let conf: Config = serde_yaml::from_slice(&std::fs::read(fp).unwrap()).unwrap();\n    assert!(conf.senders.is_empty());\n    assert!(!conf.receivers.is_empty());\n}\n\n/// Don't particularly care about the parsed specifics here.\n/// This is more about making sure the examples we have in the repo actually parse.\n#[test]\nfn test_senders_example() {\n    let fp = concat!(\n        env!(\"CARGO_MANIFEST_DIR\"),\n        \"/../svix-bridge.example.senders.yaml\"\n    );\n    let conf: Config = serde_yaml::from_slice(&std::fs::read(fp).unwrap()).unwrap();\n    assert!(!conf.senders.is_empty());\n    assert!(conf.receivers.is_empty());\n}\n\n#[test]\nfn test_variable_substitution_missing_vars() {\n    let src = r#\"\n    opentelemetry:\n        address: \"${OTEL_ADDR}\"\n    \"#;\n    let vars = HashMap::new();\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n    let otel = cfg.opentelemetry.unwrap();\n    // when lookups in the vars map fail, the original token text is preserved.\n    assert_eq!(&otel.address, \"${OTEL_ADDR}\");\n}\n\n#[test]\nfn test_variable_substitution_available_vars() {\n    <|fim_suffix|>\n    let mut vars = HashMap::new();\n    vars.insert(\n        String::from(\"OTEL_ADDR\"),\n        String::from(\"http://127.0.0.1:8080\"),\n    );\n    vars.insert(String::from(\"OTEL_SAMPLE_RATIO\"), String::from(\"0.25\"));\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n    // when lookups succeed, the token should be replaced.\n    let otel = cfg.opentelemetry.unwrap();\n    assert_eq!(&otel.address, \"http://127.0.0.1:8080\");\n    assert_eq!(otel.sample_ratio, Some(0.25));\n}\n\n#[test]\nfn test_variable_substitution_braces_optional() {\n    let src = r#\"\n    opentelemetry:\n        # Formerly failing to use ${} notation means the port number would not be substituted.\n        # Today, it works. Test that it continues to.\n        address: \"${OTEL_SCHEME}://${OTEL_HOST}:$OTEL_PORT\"\n    \"#;\n    let mut vars = HashMap::new();\n    vars.insert(String::from(\"OTEL_SCHEME\"), String::from(\"https\"));\n    vars.insert(String::from(\"OTEL_HOST\"), String::from(\"127.0.0.1\"));\n    vars.insert(String::from(\"OTEL_PORT\"), String::from(\"9999\"));\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n    // when lookups succeed, the token should be replaced.\n    let otel = cfg.opentelemetry.unwrap();\n    // Not the user-intended outcome, but it simplifies the parsing requirements.\n    assert_eq!(&otel.address, \"https://127.0.0.1:9999\");\n}\n\n#[test]\nfn test_variable_substitution_missing_numeric_var_is_err() {\n    // Unfortunate side-effect of templating yaml.\n    //\n    // If the variable is missing, usually you've got three options:\n    // - retain the token text that failed the lookup (envsubst-rs does this)\n    // - replace the token with an empty string (the CLI `envsubst` does this)\n    // - mark it an error (neither do this, but we can if we roll our own impl)\n    //\n    // For yaml, the field typings are heavily/poorly inferred so for an optional float like\n    // `sample_ratio` an empty string would parse as a `None`, which could be a bad fallback since\n    // otel considers this a 1.0 ratio (send everything).\n    //\n    // For this specific case, retaining the token text produces an error, which happens to be useful.\n    // For fields that happen to be strings anyway, errors may show up later (after the config parsing).\n    // Ex: using `${QUEUE_NAME}` in a rabbit sender input will surface in logs as an error when we\n    // try to connect: \"no such queue '${QUEUE_NAME}'\".\n\n    let src = r#\"\n    opentelemetry:\n        address: \"${OTEL_ADDR}\"\n        # This var will be missing, causing the template token to\n        # be retained causing a parse failure :(\n        sample_ratio: ${OTEL_SAMPLE_RATIO}\n    \"#;\n    let vars = HashMap::new();\n    let err = Config::from_src(src, Some(&vars)).err().unwrap();\n    let want = \"Failed to parse config: opentelemetry.sample_ratio: invalid type: \\\n                    string \\\"${OTEL_SAMPLE_RATIO}\\\", expected f64 at line 6 column 23\";\n    assert_eq!(want, err.to_string());\n}\n\n/// This is probably a given, but we should expect a single variable can be referenced multiple\n/// times within the config.\n/// The concrete use case: auth tokens.\n#[test]\nfn test_variable_substitution_repeated_lookups() {\n    let src = r#\"\n    senders:\n      - name: \"rabbitmq-1\"\n        input:\n          type: \"rabbitmq\"\n          uri: \"${RABBIT_URI}\"\n          queue_name: \"${QUEUE_NAME_1}\"\n        output:\n          type: \"svix\"\n          token: \"${SVIX_TOKEN}\"\n      - name: \"rabbitmq-2\"\n        input:\n          type: \"rabbitmq\"\n          uri: \"${RABBIT_URI}\"\n          queue_name: \"${QUEUE_NAME_2}\"\n        output:\n          type: \"svix\"\n          token: \"${SVIX_TOKEN}\"\n    \"#;\n    let mut vars = HashMap::new();\n    vars.insert(\n        String::from(\"RABBIT_URI\"),\n        String::from(\"amqp://guest:guest@localhost:5672/%2f\"),\n    );\n    vars.insert(String::from(\"QUEUE_NAME_1\"), String::from(\"one\"));\n    vars.insert(String::from(\"QUEUE_NAME_2\"), String::from(\"two\"));\n    vars.insert(String::from(\"SVIX_TOKEN\"), String::from(\"x\"));\n    let cfg = Config::from_src(src, Some(&vars)).unwrap();\n\n    if let WebhookSenderConfig {\n        input<|fim_middle|>", "completion": "let src = r#\"\n    opentelemetry:\n        address: \"${OTEL_ADDR}\"\n        sample_ratio: ${OTEL_SAMPLE_RATIO}\n    \"#;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge/src/config/tests.rs", "node_type": "let_declaration", "line_range": [400, 404]}
{"prompt": "<|fim_prefix|>t, OperationalWebhook,\n            OperationalWebhookSender,\n        },\n        types::{\n            ApplicationId, ApplicationUid, BaseId, EndpointHeaders, EndpointId,\n            EndpointSecretInternal, EndpointSecretType, MessageAttemptId,\n            MessageAttemptTriggerType, MessageId, MessageStatus, MessageUid, OrganizationId,\n        },\n        webhook_http_client::{Error as WebhookClientError, RequestBuilder, WebhookClient},\n    },\n    db::models::{endpoint, message, messageattempt, messagecontent, messagedestination},\n    error::{Error, ErrorType, HttpError, Result},\n    queue::{MessageTask, QueueTask, TaskQueueConsumer, TaskQueueProducer},\n    v1::utils::get_unix_timestamp,\n};\n\npub type CaseSensitiveHeaderMap = HashMap<String, HeaderValue>;\n\n// The maximum variation from the retry schedule when applying jitter to a resent webhook event in\n// percent deviation\nconst JITTER_DELTA: f32 = 0.2;\nconst OVERLOAD_PENALTY_SECS: u64 = 60;\n\nconst USER_AGENT: &str = concat!(\"Svix-Webhooks/\", env!(\"CARGO_PKG_VERSION\"));\n\n/// Send the MessageAttemptFailingEvent after exceeding this number of failed attempts\nconst OP_WEBHOOKS_SEND_FAILING_EVENT_AFTER: usize = 4;\n\nconst RESPONSE_MAX_SIZE: usize = 20000;\n\n/// A simple struct noting the context of the wrapped [`DateTimeUtc`]. This struct is returned when\n/// you are to disable disable an endpoint. This is optionally returned by [`process_failure_cache`]\n/// which is to be called after all retry events are exhausted.\n#[repr(transparent)]\nstruct EndpointDisableInfo {\n    first_failure_at: DateTimeUtc,\n}\n\n/// The first_failure_at time is only stored in Postgres after the endpoint has been disabled.\n/// Otherwise, it is stored in the cache with an expiration.\n#[derive(Deserialize, Serialize)]\npub struct FailureCacheValue {\n    pub first_failure_at: DateTimeUtc,\n}\n\nkv_def!(FailureCacheKey, FailureCacheValue);\n\nimpl FailureCacheKey {\n    pub fn new(\n        org_id: &OrganizationId,\n        app_id: &ApplicationId,\n        endp_id: &EndpointId,\n    ) -> FailureCacheKey {\n        FailureCacheKey(format!(\"SVIX_FAILURE_CACHE_{org_id}_{app_id}_{endp_id}\"))\n    }\n}\n\n/// Called upon the successful dispatch of an endpoint. Simply clears the cache of a\n/// [`FailureCacheKey`]/[`FailureCacheValue`] pair associated with a given endpoint. This is such\n/// that an endpoint that was previously not responding is not disabled after responding again.\n///\n/// If the key value pair does not already exist in the cache, indicating that the endpoint never\n/// stopped responding, no operation is performed.\n#[tracing::instrument(skip_all)]\nasync fn process_endpoint_success(\n    cache: &Cache,\n    app_id: &ApplicationId,\n    org_id: &OrganizationId,\n    endp: &CreateMessageEndpoint,\n) -> Result<()> {\n    let key = FailureCacheKey::new(org_id, app_id, &endp.id);\n\n    cache.delete(&key).await.map_err(Error::cache)\n}\n\n/// Called upon endpoint failure. Returns whether to disable the endpoint based on the time of first\n/// failure stored in the cache.\n///\n/// If no failure has previously been reported, then now is cached as the time of first failure and\n/// the endpoint is not disabled.\n///\n/// If there has been a  previous failure, then it is compared to the configured grace period, where\n/// if there have been only failures within the grace period, then the endpoint is disabled.\n///\n/// All cache values are set with an expiration time greater that the grace period, so occasional\n/// failures will not cause an endpoint to be disabled.\n#[tracing::instrument(skip_all)]\nasync fn process_endpoint_failure(\n    cache: &Cache,\n    app_id: &ApplicationId,\n    org_id: &OrganizationId,\n    endp: &CreateMessageEndpoint,\n    disable_in: Duration,\n) -> Result<Option<EndpointDisableInfo>> {\n    let key = FailureCacheKey::new(org_id, app_id, &endp.id);\n    let now = Utc::now();\n\n    // If it already exists in the cache, see if the grace period has already elapsed\n    if let Some(FailureCacheValue { first_failure_at }) = cache\n        .get::<FailureCacheValue>(&key)\n        .await\n        .map_err(Error::generic)?\n    {\n        i<|fim_suffix|>    }\n    // If it does not yet exist in the cache, set the first_failure_at value to now\n    else {\n        cache\n            .set(\n                &key,\n                &FailureCacheValue {\n                    first_failure_at: now,\n                },\n                // Failures are forgiven after double the `disable_in` `Duration` with the expiry of\n                // the Redis key\n                disable_in * 2,\n            )\n            .await\n            .map_err(Error::generic)?;\n\n        Ok(None)\n    }\n}\n\n/// Sign a message\nfn sign_msg(\n    main_secret: &Encryption,\n    timestamp: i64,\n    body: &str,\n    msg_id: &MessageId,\n    endpoint_signing_keys: &[&EndpointSecretInternal],\n) -> String {\n    let to_sign = format!(\"{msg_id}.{timestamp}.{body}\");\n\n    endpoint_signing_keys\n        .iter()\n        .format_with(\" \", |x, f| {\n            let sig = x.sign(main_secret, to_sign.as_bytes());\n            let version = match x.type_() {\n                EndpointSecretType::Hmac256 => \"v1\",\n                EndpointSecretType::Ed25519 => \"v1a\",\n            };\n\n            f(&format_args!(\"{version},{}\", STANDARD.encode(sig)))\n        })\n        .to_string()\n}\n\n/// Generates a set of headers for any one webhook event\nfn generate_msg_headers(\n    timestamp: i64,\n    msg_id: &MessageId,\n    signatures: String,\n    whitelabel_headers: bool,\n    configured_headers: Option<&EndpointHeaders>,\n    _endpoint_url: &str,\n) -> Result<CaseSensitiveHeaderMap> {\n    let mut headers = CaseSensitiveHeaderMap::new();\n    let id_hdr = msg_id\n        .0\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message id: {e:?}\")))?;\n    let timestamp = timestamp\n        .to_string()\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message timestamp: {e:?}\")))?;\n    let signatures_str = signatures\n        .parse()\n        .map_err(|e| Error::generic(format_args!(\"Error parsing message signatures: {e:?}\")))?;\n    if whitelabel_headers {\n        headers.insert(\"webhook-id\".to_owned(), id_hdr);\n        headers.insert(\"webhook-timestamp\".to_owned(), timestamp);\n        headers.insert(\"webhook-signature\".to_owned(), signatures_str);\n    } else {\n        headers.insert(\"svix-id\".to_owned(), id_hdr);\n        headers.insert(\"svix-timestamp\".to_owned(), timestamp);\n        headers.insert(\"svix-signature\".to_owned(), signatures_str);\n    }\n    headers.insert(\n        \"user-agent\".to_owned(),\n        USER_AGENT.to_string().parse().unwrap(),\n    );\n    headers.insert(\n        \"content-type\".to_owned(),\n        \"application/json\".parse().unwrap(),\n    );\n    if let Some(configured_headers) = configured_headers {\n        for (k, v) in &configured_headers.0 {\n            match v.parse() {\n                Ok(v) => {\n                    headers.insert(k.clone(), v);\n                }\n                Err(e) => {\n                    tracing::error!(\"Invalid HeaderValue {}: {}\", v, e);\n                }\n            }\n        }\n    }\n\n    Ok(headers)\n}\n\n#[derive(Clone)]\nstruct WorkerContext<'a> {\n    cfg: &'a Configuration,\n    cache: &'a Cache,\n    db: &'a DatabaseConnection,\n    queue_tx: &'a TaskQueueProducer,\n    op_webhook_sender: &'a OperationalWebhookSender,\n    webhook_client: &'a WebhookClient,\n}\n\nstruct FailedDispatch(messageattempt::ActiveModel, Error);\nstruct SuccessfulDispatch(messageattempt::ActiveModel);\n\n#[allow(clippy::large_enum_variant)]\nenum IncompleteDispatch {\n    Pending(PendingDispatch),\n    #[allow(dead_code)]\n    Failed(FailedDispatch),\n}\n\nstruct PendingDispatch {\n    method: http::Method,\n    url: String,\n    headers: CaseSensitiveHeaderMap,\n    payload: String,\n    request_timeout: u64,\n    created_at: DateTimeUtc,\n}\n\n// Clippy fails to compute the first variant's size, stating it as\n// \"at least 0 bytes\". They're actually very similar in size.\n#[allow(clippy::large_enum_variant)]\nenum CompletedDispatch {\n    Failed(FailedDispatch),\n    Successful(SuccessfulDispatch),\n}\n\n#[tracing::instrument(skip_all)]\nasync fn prepare_dispatch(\n    WorkerContext { cfg, .. }: &WorkerContext<'_>,\n<|fim_middle|>", "completion": "if now - first_failure_at\n            > chrono::Duration::from_std(disable_in).expect(\"Given `disable_in` is too large\")\n        {\n            Ok(Some(EndpointDisableInfo { first_failure_at }))\n        } else {\n            Ok(None)\n        }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/worker.rs", "node_type": "if_expression", "line_range": [138, 144]}
{"prompt": "<|fim_prefix|>mars::schema::Metadata{\n                description: Some(\"The endpoint's verification secret. If `null` is passed, a secret is automatically generated. Format: `base64` encoded random bytes optionally prefixed with `whsec_`. Recommended size: 24.\".to_string()),\n                .. Default::default()\n            }));\n            obj.extensions.insert(\n                \"example\".to_string(),\n                serde_json::Value::String(\"whsec_C2FVsBQIhrscChlQIMV+b5sSYspob7oD\".to_string()),\n            );\n        }\n        schema\n    }\n\n    fn is_referenceable() -> bool {\n        false\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Deserialize)]\npub struct ExpiringSigningKey {\n    #[serde(rename = \"signingKey\")]\n    pub key: EndpointSecretInternal,\n    pub expiration: DateTime<Utc>,\n}\n\nconst FORBIDDEN_KEYS: [&str; 19] = [\n    \"user-agent\",\n    \"keep-alive\",\n    \"proxy-authenticate\",\n    \"proxy-authorization\",\n    \"te\",\n    \"trailers\",\n    \"transfer-encoding\",\n    \"upgrade\",\n    \"age\",\n    \"cache-control\",\n    \"clear-site-data\",\n    \"expires\",\n    \"pragma\",\n    \"warning\",\n    \"content-length\",\n    \"content-type\",\n    \"content-encoding\",\n    \"content-language\",\n    \"content-location\",\n];\n\nconst FORBIDDEN_PREFIXES: [&str; 10] = [\n    \"x-amz-\", \"x-amzn-\", \"x-google\", \"x-goog-\", \"x-gfe\", \"x-amz-\", \"x-azure-\", \"x-fd-\", \"x-svix-\",\n    \"svix-\",\n];\n\nfn validate_header_key(k: &str, errors: &mut ValidationErrors) {\n    let k = &k.to_lowercase();\n    if let Err(_e) = http::header::HeaderName::try_from(k) {\n        errors.add(\n            ALL_ERROR,\n            validation_error(Some(\"header\"), Some(\"Invalid Header Name.\")),\n        );\n    }\n    if FORBIDDEN_KEYS.contains(&k.as_str()) {\n        errors.add(\n            ALL_ERROR,\n            validation_error(Some(\"header\"), Some(\"Header uses a forbidden key.\")),\n        );\n    }\n    FORBIDDEN_PREFIXES.iter().for_each(|p| {\n        if k.starts_with(p) {\n            errors.add(\n                ALL_ERROR,\n                validation_error(\n                    Some(\"header\"),\n                    Some(\"Header starts with a forbidden prefix.\"),\n                ),\n            )\n        }\n    })\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Default, JsonSchema)]\n#[schemars(transparent)]\npub struct EndpointHeaders(pub HashMap<String, String>);\njson_wrapper!(EndpointHeaders);\n\nconst HEADER_MAX_LENGTH: usize = 4096;\n\nimpl<'de> Deserialize<'de> for EndpointHeaders {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        let headers: HashMap<String, String> = HashMap::deserialize(deserializer)?;\n\n        validate_header_map(&headers).map_err(serde::de::Error::custom)?;\n\n        Ok(EndpointHeaders(headers))\n    }\n}\n\nfn validate_header_map(headers: &HashMap<String, String>) -> Result<(), ValidationErrors> {\n    let mut errors = ValidationErrors::new();\n    for (k, v) in headers {\n        validate_header_key(k, &mut errors);\n\n        if let Err(_e) = http::header::HeaderValue::try_from(v) {\n            errors.add(\n                ALL_ERROR,\n                validation_error(Some(\"header\"), Some(\"Invalid Header Value.\")),\n            );\n        }\n\n        if v.len() > HEADER_MAX_LENGTH {\n            errors.add(\n                ALL_ERROR,\n                validation_error(Some(\"header\"), Some(\"Maximum header length is 4096 bytes\")),\n            );\n        }\n    }\n    if errors.is_empty() {\n        Ok(())\n    } else {\n        Err(errors)\n    }\n}\n\n#[derive(Clone, Debug, PartialEq, Eq, Serialize, Default, JsonSchema)]\n#[schemars(transparent)]\npub struct EndpointHeadersPatch(pub HashMap<String, Option<String>>);\njson_wrapper!(EndpointHeadersPatch);\n\nimpl<'de> Deserialize<'de> for EndpointHeadersPatch {\n    fn deserialize<D>(deserializer: D) -> Result<Self, D::Error>\n    where\n        D: serde::Deserializer<'de>,\n    {\n        HashMap::deserialize(deserializer)\n            .map(|x: HashMap<String, Option<String>>| x.into_iter().collect())\n            .map(EndpointHeadersPatch)\n    }\n}\n\nimpl Validate for EndpointHeadersPatch {\n    f<|fim_suffix|>}\n\n/// A macro to which you pass the list of variants of an enum using `repr(N)`\n/// and it returns a `Vec<(N, String)>`, where each element is `(value, \"VariantStringified\")`\nmacro_rules! repr_enum {\n    ($($variant:ident),+) => {\n        vec![\n            $(($variant.into(), stringify!($variant).to_string())),+\n        ]\n    }\n}\n\n/// Generates a `JsonSchema` implementation for an enum using `repr(N)`. The\n/// enum must also derive `IntoPrimitive`.\n///\n/// Arguments are:\n/// 1. Name of the enum type, `Foo`\n/// 2. The repr type used, e.g. in case of `repr(i16)` it must be `i16`\n/// 3. The string description to be used in the docs.\n///\n/// Remaining arguments must be the variants in order. For example:\n///\n/// ```ignore\n/// #[derive(IntoPrimitive)]\n/// #[repr(u8)]\n/// enum MyEnum {\n///     Foo = 0,\n///     Bar = 1,\n///     Qux = 5,\n/// }\n///\n/// jsonschema_for_repr_enum! {\n///     MyEnum,\n///     u8,\n///     \"My nice little enum\",\n///     Foo, Bar, Qux\n/// }\n/// ```\nmacro_rules! jsonschema_for_repr_enum {\n    ($tyname:ty, $repr_ty:ty, $descr:expr, $($variant:ident),+) => {\n        impl JsonSchema for $tyname {\n            fn schema_name() -> String {\n                stringify!($tyname).to_string()\n            }\n\n            fn json_schema(_: &mut schemars::gen::SchemaGenerator) -> schemars::schema::Schema {\n                use schemars::schema::{InstanceType, Metadata, Schema, SchemaObject, SingleOrVec};\n                use $tyname::*;\n\n                // A list of variant values and their corresponding name.\n                let variants: Vec<($repr_ty, String)> = repr_enum!($($variant),+);\n                // The list of possible enum primitive values.\n                let values = variants.iter().map(|(value, _)| serde_json::json!(value)).collect();\n                // The list of nice variant names the above values correspond to.\n                let variant_names = variants.iter().map(|(_, name)| serde_json::Value::String(name.clone())).collect();\n\n                Schema::Object(SchemaObject{\n                    metadata: Some(Box::new(Metadata {\n                        title: Some(Self::schema_name()),\n                        description: Some($descr.to_string()),\n                        ..Default::default()\n                    })),\n                    instance_type: Some(SingleOrVec::Single(Box::new(InstanceType::Integer))),\n                    enum_values: Some(values),\n                    extensions: FromIterator::from_iter([\n                        (\"x-enum-varnames\".to_string(), serde_json::Value::Array(variant_names)),\n                    ]),\n                    ..Default::default()\n                })\n            }\n        }\n    }\n}\n\n#[repr(i16)]\n#[derive(Clone, Debug, Copy, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum MessageAttemptTriggerType {\n    Scheduled = 0,\n    Manual = 1,\n}\n\njsonschema_for_repr_enum! {\n    MessageAttemptTriggerType,\n    i16,\n    \"The reason an attempt was made:\\n- Scheduled = 0\\n- Manual = 1\",\n    Scheduled, Manual\n}\n\n#[repr(i16)]\n#[derive(Clone, Debug, Copy, PartialEq, IntoPrimitive, TryFromPrimitive, Hash, Eq)]\npub enum MessageStatus {\n    Success = 0,\n    Pending = 1,\n    Fail = 2,\n    Sending = 3,\n}\n\njsonschema_for_repr_enum! {\n    MessageStatus,\n    i16,\n    \"The sending status of the message:\\n- Success = 0\\n- Pending = 1\\n- Fail = 2\\n- Sending = 3\",\n    Success, Pending, Fail, Sending\n}\n\n#[derive(Clone, Debug, Copy, PartialEq, Eq, Serialize, Deserialize, JsonSchema)]\n#[serde(rename_all = \"camelCase\")]\npub enum MessageStatusText {\n    Success,\n    Pending,\n    #[serde(alias = \"failed\")]\n    Fail,\n    Sending,\n}\n\nimpl From<MessageStatus> for MessageStatusText {\n    fn from(status: MessageStatus) -> Self {\n        match status {\n            MessageStatus::Success => Self::Success,\n            MessageStatus::Pending => Self::Pending,\n            MessageStatus::Fail => Self::Fail,\n            MessageStatus::Sending => Self::Sending,\n        }\n    }\n}\n\n#[repr(i16)]\n#[derive(Clone, Debug, Copy, PartialEq, Eq, IntoPrimitive, TryFromPrimitive)]\npub enum StatusCodeClass {\n    Cod<|fim_middle|>", "completion": "fn validate(&self) -> Result<(), ValidationErrors> {\n        let mut errors = ValidationErrors::new();\n        self.0\n            .iter()\n            .for_each(|(k, _)| validate_header_key(k, &mut errors));\n        if errors.is_empty() {\n            Ok(())\n        } else {\n            Err(errors)\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/core/types/mod.rs", "node_type": "function_item", "line_range": [1229, 1239]}
{"prompt": "<|fim_prefix|>g, ProxyConfig},\n    v1::endpoints::message::MessageIn,\n};\nuse tokio::{\n    io::{AsyncReadExt, AsyncWriteExt},\n    time::timeout,\n};\n\nuse crate::utils::{\n    common_calls::{create_test_app, create_test_endpoint, message_in},\n    get_default_test_config, start_svix_server_with_cfg, TestClient, TestReceiver,\n};\n\n#[ignore] // works with microsocks running at the specified address\n#[tokio::test]\nasync fn test_message_delivery_via_socks5() {\n    use crate::utils::start_svix_server_with_cfg;\n\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(socks_proxy_config());\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n    run_proxy_test(&client).await;\n}\n\nfn socks_proxy_config() -> ProxyConfig {\n    ProxyConfig {\n        addr: ProxyAddr::new(\"socks5://localhost:1080\").unwrap(),\n        noproxy: None,\n    }\n}\n\n#[ignore] // works with tinyproxy running at the specified address\n#[tokio::test]\nasync fn test_message_delivery_via_http_proxy() {\n    use crate::utils::start_svix_server_with_cfg;\n\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(http_proxy_config());\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n    run_proxy_test(&client).await;\n}\n\nfn http_proxy_config() -> ProxyConfig {\n    ProxyConfig {\n        addr: ProxyAddr::new(\"http://localhost:8888\").unwrap(),\n        noproxy: None,\n    }\n}\n\nasync fn run_proxy_test(client: &TestClient) {\n    let mut receiver = TestReceiver::start(StatusCode::OK);\n\n    let app_id = create_test_app(client, \"proxyTest\").await.unwrap().id;\n    create_test_endpoint(client, &app_id, &receiver.endpoint)\n        .await\n        .unwrap();\n\n    let msg_payload = serde_json::json!({ \"test\": \"value\" });\n\n    let _: IgnoredAny = client\n        .post(\n            &format!(\"api/v1/app/{app_id}/msg/\"),\n            message_in(&app_id, msg_payload.clone()).unwrap(),\n            StatusCode::ACCEPTED,\n        )\n        .await\n        .unwrap();\n\n    let received_payload = timeout(Duration::from_secs(2), receiver.data_recv.recv())\n        .await\n        .unwrap()\n        .unwrap();\n\n    assert_eq!(received_payload, msg_payload);\n}\n\n// This doesn't actually handle requests successfully, but it does allow us\n// to see which hostnames are requested of it.\nstruct MockProxyServer {\n    matched_hosts: Arc<Mutex<HashSet<String>>>,\n    addr: String,\n    variant: MockProxyVariant,\n}\n\nenum MockProxyVariant {\n    Http,\n    Socks5,\n}\n\nimpl MockProxyServer {\n    pub fn new(variant: MockProxyVariant) -> Self {\n        let listener = TcpListener::bind(\"127.0.0.1:0\").unwrap();\n        listener.set_nonblocking(true).unwrap();\n        let listener = tokio::net::TcpListener::from_std(listener).unwrap();\n        let addr = match variant {\n            MockProxyVariant::Http => {\n                format!(\"http://{}\", listener.local_addr().unwrap())\n            }\n            MockProxyVariant::Socks5 => {\n                format!(\"socks5://{}\", listener.local_addr().unwrap())\n            }\n        };\n        let matched_hosts = Arc::new(Mutex::new(HashSet::new()));\n\n        match variant {\n            MockProxyVariant::Http => {\n                tokio::spawn(Self::http_listener(listener, matched_hosts.clone()))\n            }\n            MockProxyVariant::Socks5 => {\n                tokio::spawn(Self::socks5_listener(listener, matched_hosts.clone()))\n            }\n        };\n\n        Self {\n            matched_hosts,\n            addr,\n            variant,\n        }\n    }\n\n    pub async fn http_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        loop {\n            let (mut stream, _addr) = listener.accept().await.unwrap();\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let mut buffer = [0; 512];\n\n                if let Ok(size) = stream.read(&mut buffer).await {\n                    if size == 0 {\n                        return;\n                    }\n                    let request = String::from_utf8_lossy(&buffer[..size]);\n                    <|fim_suffix|>\n                }\n            });\n        }\n    }\n\n    pub async fn socks5_listener(\n        listener: tokio::net::TcpListener,\n        matched_hosts: Arc<Mutex<HashSet<String>>>,\n    ) {\n        use socks5_proto::{\n            handshake::{\n                Method as HandshakeMethod, Request as HandshakeRequest,\n                Response as HandshakeResponse,\n            },\n            Address, Reply, Request as SocksRequest, Response as SocksResponse,\n        };\n        loop {\n            let (mut stream, _) = match listener.accept().await {\n                Ok(v) => v,\n                Err(_) => continue,\n            };\n\n            let matched_hosts = matched_hosts.clone();\n\n            tokio::spawn(async move {\n                let hs_req = match HandshakeRequest::read_from(&mut stream).await {\n                    Ok(req) => req,\n                    Err(_) => {\n                        return;\n                    }\n                };\n\n                if hs_req.methods.contains(&HandshakeMethod::NONE) {\n                    if HandshakeResponse::new(HandshakeMethod::NONE)\n                        .write_to(&mut stream)\n                        .await\n                        .is_err()\n                    {\n                        return;\n                    }\n                } else {\n                    let _ = HandshakeResponse::new(HandshakeMethod::UNACCEPTABLE)\n                        .write_to(&mut stream)\n                        .await;\n                    return;\n                }\n\n                let Ok(socks_req) = SocksRequest::read_from(&mut stream).await else {\n                    return;\n                };\n\n                let host = match &socks_req.address {\n                    Address::SocketAddress(socket_addr) => socket_addr.ip().to_string(),\n                    Address::DomainAddress(domain_bytes, _port) => {\n                        String::from_utf8_lossy(domain_bytes).to_string()\n                    }\n                };\n                if !host.is_empty() {\n                    let mut guard = matched_hosts.lock().unwrap();\n                    guard.insert(host);\n                }\n\n                let abort_resp =\n                    SocksResponse::new(Reply::ConnectionNotAllowed, Address::unspecified());\n                let _ = abort_resp.write_to(&mut stream).await;\n                let _ = stream.shutdown().await;\n            });\n        }\n    }\n\n    pub fn matches(&self) -> HashSet<String> {\n        let guard = self.matched_hosts.lock().unwrap();\n        println!(\"************ MATCHES {guard:?}\");\n        guard.clone()\n    }\n}\n\n#[tokio::test]\nasync fn test_http_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Http);\n    test_proxy_exceptions(listener).await\n}\n\n#[tokio::test]\nasync fn test_socks5_proxy_exceptions() {\n    let listener = MockProxyServer::new(MockProxyVariant::Socks5);\n    test_proxy_exceptions(listener).await\n}\n\nasync fn test_proxy_exceptions(listener: MockProxyServer) {\n    let mut cfg = get_default_test_config();\n    cfg.proxy_config = Some(ProxyConfig {\n        addr: ProxyAddr::new(listener.addr.clone()).unwrap(),\n        noproxy: Some(ProxyBypassCfg(\"10.0.0.0/8, 8.8.8.8, 0ec2:1652:6021:693b:f928:565d:5a0e:de9f, www.svix.com, .google.com\".to_owned())),\n    });\n    cfg.retry_schedule = vec![];\n\n    let (client, _) = start_svix_server_with_cfg(&cfg).await;\n\n    let app_id = create_test_app(&client, \"proxyTest\").await.unwrap().id;\n\n    // Note: Real hostnames here are not ideal, but difficult to test SOCKS without valid DNS entries:\n    create_test_endpoint(&client, &app_id, \"https://www.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://play.svix.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://www.google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://google.com\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client, &app_id, \"https://8.8.8.8\")\n        .await\n        .unwrap();\n\n    create_test_endpoint(&client,<|fim_middle|>", "completion": "if let Some(host) = request\n                        .strip_prefix(\"CONNECT \")\n                        .and_then(|s| s.split(' ').next())\n                        .and_then(|s| s.strip_suffix(\":443\"))\n                    {\n                        let mut guard = matched_hosts.lock().unwrap();\n                        guard.insert(host.to_string());\n                    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_proxy.rs", "node_type": "if_expression", "line_range": [147, 154]}
{"prompt": "<|fim_prefix|>tion::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&client, &key, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    delete_test_stream(&client, &key).await;\n}\n\n/// Push a msg on the queue.\n/// Check to see if the svix server sees a request, but this time transform the payload.\n#[tokio::test]\nasync fn test_consume_transformed_json_ok() {\n    let client = redis_connection().await;\n    let key = create_test_stream(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(json!({ \"payload\": { \"good\": \"bye\" } })))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"good\": \"bye\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        key.clone(),\n        Some(TransformerInputFormat::Json),\n    );\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let mut out = match x.input {\n                TransformerInput::Json(input) => input.as_object().unwrap().clone(),\n                _ => unreachable!(),\n            };\n            // Prune out the \"hi\" key.\n            out[\"message\"][\"payload\"]\n                .as_object_mut()\n                .unwrap()\n                .remove(\"hi\");\n            // Add the \"good\" key.\n            out[\"message\"][\"payload\"][\"good\"] = json!(\"bye\");\n            x.callback_tx.send(Ok(TransformerOutput::Object(out))).ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    let msg = CreateMessageRequest {\n        app_id: \"app_1234\".into(),\n        message: MessageIn::new(\"testing.things\".into(), json!({\"hi\": \"there\"})),\n    };\n\n    publish(&client, &key, &serde_json::to_string(&msg).unwrap()).await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    delete_test_stream(&client, &key).await;\n}\n\n#[tokio::test]\nasync fn test_consume_transformed_string_ok() {\n    let client = redis_connection().await;\n    let key = create_test_stream(&client).await;\n\n    let mock_server = MockServer::start().await;\n    // The mock will make asserts on drop (i.e. when the body of the test is returning).\n    // The `expect` call should ensure we see exactly 1 POST request.\n    // <https://docs.rs/wiremock/latest/wiremock/struct.Mock.html#method.expect>\n    let mock = Mock::given(method(\"POST\"))\n        .and(body_partial_json(\n            json!({ \"payload\": { \"hello\": \"world\" } }),\n        ))\n        .respond_with(ResponseTemplate::new(202).set_body_json(json!({\n          \"eventType\": \"testing.things\",\n          \"payload\": {\n            // The adjustment made via the transformation...\n            \"hello\": \"world\",\n          },\n          \"id\": \"msg_xxxx\",\n          \"timestamp\": \"2023-04-25T00:00:00Z\"\n        })))\n        .named(\"create_message\")\n        .expect(1);\n    mock_server.register(mock).await;\n\n    <|fim_suffix|>\n    let (transformer_tx, mut transformer_rx) =\n        tokio::sync::mpsc::unbounded_channel::<TransformerJob>();\n    let _handle = tokio::spawn(async move {\n        while let Some(x) = transformer_rx.recv().await {\n            let input = match x.input {\n                TransformerInput::String(input) => input,\n                _ => unreachable!(),\n            };\n            // Build a create-message-compatible object, using the string input as a field in the payload.\n            let out = json!({\n                \"appId\": \"app_1234\",\n                \"message\": {\n                    \"eventType\": \"testing.things\",\n                    \"payload\": {\n                        \"hello\": input,\n                    }\n                }\n            });\n            x.callback_tx\n                .send(Ok(TransformerOutput::Object(\n                    out.as_object().unwrap().clone(),\n                )))\n                .ok();\n        }\n    });\n    plugin.set_transformer(Some(transformer_tx));\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    publish(&client, &key, \"world\").await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    handle.abort();\n\n    delete_test_stream(&client, &key).await;\n}\n\n#[tokio::test]\nasync fn test_missing_app_id_nack() {\n    let client = redis_connection().await;\n    let key = create_test_stream(&client).await;\n\n    let mock_server = MockServer::start().await;\n    let mock = Mock::given(method(\"POST\"))\n        // The response doesn't really matter, but we need to define it to be able to `expect(0)`.\n        .respond_with(ResponseTemplate::new(400))\n        .named(\"create_message\")\n        // No requests should be made when the event type or app id are missing.\n        .expect(0);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), key.clone(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    publish(\n        &client,\n        &key,\n        &serde_json::to_string(&json!({\n            // No app id\n            \"message\": {\n                \"eventType\": \"testing.things\",\n                \"payload\": {\n                    \"hi\": \"there\",\n                }\n            },\n\n        }))\n        .unwrap(),\n    )\n    .await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n    handle.abort();\n\n    delete_test_stream(&client, &key).await\n}\n\n#[tokio::test]\nasync fn test_missing_event_type_nack() {\n    let client = redis_connection().await;\n    let key = create_test_stream(&client).await;\n\n    let mock_server = MockServer::start().await;\n    let mock = Mock::given(method(\"POST\"))\n        // The response doesn't really matter, but we need to define it to be able to `expect(0)`.\n        .respond_with(ResponseTemplate::new(400))\n        .named(\"create_message\")\n        // No requests should be made when the event type or app id are missing.\n        .expect(0);\n    mock_server.register(mock).await;\n\n    let plugin = get_test_plugin(mock_server.uri(), key.clone(), None);\n\n    let handle = tokio::spawn(async move {\n        let fut = plugin.run();\n        fut.await\n    });\n\n    // Wait for the consumer to connect\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n\n    publish(\n        &client,\n        &key,\n        &serde_json::to_string(&json!({\n            \"appId\": \"app_1234\",\n            \"message\": {\n                // No event type\n                \"payload\": {\n                    \"hi\": \"there\",\n                }\n            },\n        }))\n        .unwrap(),\n    )\n    .await;\n\n    // Wait for the consumer to consume.\n    tokio::time::sleep(Duration::from_millis(WAIT_MS)).await;\n    handle.abort();\n\n    delete_test_stream(&client, &key).await\n}\n\n/// Check<|fim_middle|>", "completion": "let mut plugin = get_test_plugin(\n        mock_server.uri(),\n        key.clone(),\n        Some(TransformerInputFormat::String),\n    );", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/bridge/svix-bridge-plugin-queue/tests/it/redis_stream_consumer.rs", "node_type": "let_declaration", "line_range": [234, 238]}
{"prompt": "<|fim_prefix|>use anyhow::Result;\nuse rand::distributions::{Distribution, Uniform};\n\nconst BASE62: &[u8; 62] = b\"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\";\n\npub fn generate_token() -> Result<String> {\n    let between = Uniform::from(0..BASE62.len());\n    let mut rng = rand::thread_rng();\n    let mut bytes = [0_u8; 27];\n    for b in &mut bytes {\n        *b = BASE62[between.sample(&mut rng)];\n    }\n    Ok(String::from_utf8(bytes.to_vec())?)\n}\n\n#[cfg(test)]\nmod tests {\n    use super::generate_token;\n\n    #[test]\n    fn test_generate_token() {\n        <|fim_suffix|>\n        let out2 = generate_token().unwrap();\n        let out3 = generate_token().unwrap();\n\n        // Sort of weak as far as assertions go, but this is the least we can expect, right?\n        assert_ne!(out1, out2, \"random tokens should be different\");\n        assert_ne!(out2, out3, \"random tokens should be different\");\n        assert_ne!(out3, out1, \"random tokens should be different\");\n    }\n}\n<|fim_middle|>", "completion": "let out1 = generate_token().unwrap();", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/svix-cli/src/relay/token.rs", "node_type": "let_declaration", "line_range": [22, 22]}
{"prompt": "<|fim_prefix|>mod redis;\n\npub fn init_metric<T, E: std::fmt::Debug>(result: Result<T, E>) -> Option<T> {\n    <|fim_suffix|>\n}\n\npub use self::redis::{RedisQueueMetrics, RedisQueueType};\n<|fim_middle|>", "completion": "match result {\n        Ok(t) => Some(t),\n        Err(e) => {\n            tracing::error!(error = ?e, \"Failed to initialize metric\");\n            None\n        }\n    }", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/src/metrics/mod.rs", "node_type": "match_expression", "line_range": [4, 10]}
{"prompt": "<|fim_prefix|>alse,\n        }\n    }\n\n    pub fn with_body_param<T: serde::Serialize>(mut self, param: T) -> Self {\n        self.serialized_body = Some(serde_json::to_string(&param).unwrap());\n        self\n    }\n\n    pub fn with_optional_header_param(\n        mut self,\n        basename: &'static str,\n        param: Option<String>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.header_params.insert(basename, value);\n        }\n        self\n    }\n\n    pub fn with_query_param(mut self, basename: &'static str, param: impl QueryParamValue) -> Self {\n        self.query_params.insert(basename, param.encode());\n        self\n    }\n\n    pub fn with_optional_query_param<T: QueryParamValue>(\n        mut self,\n        basename: &'static str,\n        param: Option<T>,\n    ) -> Self {\n        if let Some(value) = param {\n            self.query_params.insert(basename, value.encode());\n        }\n        self\n    }\n\n    pub fn with_path_param(mut self, basename: &'static str, param: String) -> Self {\n        self.path_params.insert(basename, param);\n        self\n    }\n\n    pub fn returns_nothing(mut self) -> Self {\n        self.no_return_type = true;\n        self\n    }\n\n    pub async fn execute<T: DeserializeOwned>(self, conf: &Configuration) -> Result<T, Error> {\n        match self.execute_with_backoff(conf).await? {\n            // This is a hack; if there's no_ret_type, T is (), but serde_json gives an\n            // error when deserializing \"\" into (), so deserialize 'null' into it\n            // instead.\n            // An alternate option would be to require T: Default, and then return\n            // T::default() here instead since () implements that, but then we'd\n            // need to impl default for all models.\n            None => Ok(serde_json::from_str(\"null\").expect(\"serde null value\")),\n            Some(bytes) => Ok(serde_json::from_slice(&bytes).map_err(Error::generic)?),\n        }\n    }\n\n    async fn execute_with_backoff(mut self, conf: &Configuration) -> Result<Option<Bytes>, Error> {\n        let no_return_type = self.no_return_type;\n        if self.method == http1::Method::POST && !self.header_params.contains_key(\"idempotency-key\")\n        {\n            self.header_params\n                .insert(\"idempotency-key\", format!(\"auto_{}\", uuid::Uuid::new_v4()));\n        }\n\n        const MAX_BACKOFF: Duration = Duration::from_secs(5);\n\n        let retry_schedule = match &conf.retry_schedule {\n            Some(schedule) => schedule,\n            None => &std::iter::successors(Some(Duration::from_millis(20)), |last_backoff| {\n                Some(MAX_BACKOFF.min(*last_backoff * 2))\n            })\n            .take(conf.num_retries as usize)\n            .collect(),\n        };\n        let mut retries = retry_schedule.iter();\n\n        let mut request = self.build_request(conf)?;\n        request\n            .headers_mut()\n            .insert(\"svix-req-id\", rand::rng().random::<u32>().into());\n\n        let mut retry_count = 0;\n\n        let execute_request = async |request| {\n            let response = conf.client.request(request).await.map_err(Error::generic)?;\n\n            let status = response.status();\n            if !status.is_success() {\n                Err(Error::from_response(status, response.into_body()).await)\n            } else if no_return_type {\n                Ok(None)\n            } else {\n                let bytes = response\n                    .into_body()\n                    .collect()\n                    .await\n                    .map_err(Error::generic)?\n                    .to_bytes();\n                Ok(Some(bytes))\n            }\n        };\n\n        loop {\n            let request_fut = execute_request(request.clone());\n            let res = if let Some(duration) = conf.timeout {\n                tokio::time::timeout(duration, request_fut)\n                    .await\n                    .map_err(Error::generic)?\n            } else {\n                request_fut.await\n            };\n\n            let next_backoff = retries.next().copied();\n\n            match res {\n                Ok(result) => return Ok(result),\n                e @ Err(Error::Validation(_)) => return e,\n                Err(Error::Http(err)) if err.status.as_u16() < 500 => return Err(Error::Http(err)),\n                e @ Err(_) => {\n                    if next_backoff.is_none() {\n                        return e;\n                    }\n                }\n            }\n\n            tokio::time::sleep(next_backoff.expect(\"next_backoff is always Some\")).await;\n            retry_count += 1;\n\n            request\n                .headers_mut()\n                .insert(\"svix-retry-count\", retry_count.into());\n        }\n    }\n\n    fn build_request(self, conf: &Configuration) -> Result<http1::Request<Full<Bytes>>, Error> {\n        const FRAGMENT: &AsciiSet = &CONTROLS.add(b' ').add(b'\"').add(b'<').add(b'>').add(b'`');\n        const PATH: &AsciiSet = &FRAGMENT.add(b'#').add(b'?').add(b'{').add(b'}');\n        const PATH_SEGMENT: &AsciiSet = &PATH.add(b'/').add(b'%');\n\n        let mut path = self.path.to_owned();\n        for (k, v) in self.path_params {\n            // replace {id} with the value of the id path param\n            let percent_encoded_path_param_value =\n                utf8_percent_encode(&v, PATH_SEGMENT).to_string();\n            path = path.replace(&format!(\"{{{k}}}\"), &percent_encoded_path_param_value);\n        }\n\n        let mut uri = format!(\"{}{}\", conf.base_path, path);\n\n        let mut query_string = url::form_urlencoded::Serializer::new(\"\".to_owned());\n        for (key, val) in self.query_params {\n            query_string.append_pair(key, &val);\n        }\n\n        let query_string_str = query_string.finish();\n        if !query_string_str.is_empty() {\n            uri += \"?\";\n            uri += &query_string_str;\n        }\n\n        <|fim_suffix|>\n        let mut req_builder = http1::Request::builder().uri(uri).method(self.method);\n\n        let mut request = if let Some(body) = self.serialized_body {\n            let req_headers = req_builder.headers_mut().unwrap();\n            req_headers.insert(CONTENT_TYPE, HeaderValue::from_static(\"application/json\"));\n            req_headers.insert(CONTENT_LENGTH, body.len().into());\n            req_builder.body(Full::from(body)).map_err(Error::generic)?\n        } else {\n            req_builder.body(Full::default()).map_err(Error::generic)?\n        };\n\n        let request_headers = request.headers_mut();\n\n        // Detect the authorization type if it hasn't been set.\n        let auth = if conf.bearer_access_token.is_some() {\n            Auth::Bearer\n        } else {\n            Auth::None\n        };\n        match auth {\n            Auth::Bearer => {\n                if let Some(token) = &conf.bearer_access_token {\n                    let value = format!(\"Bearer {token}\")\n                        .try_into()\n                        .map_err(Error::generic)?;\n                    request_headers.insert(AUTHORIZATION, value);\n                }\n            }\n            Auth::None => {}\n        }\n\n        if let Some(user_agent) = &conf.user_agent {\n            let value = user_agent.try_into().map_err(Error::generic)?;\n            request_headers.insert(USER_AGENT, value);\n        }\n\n        for (k, v) in self.header_params {\n            let v = v.try_into().map_err(Error::generic)?;\n            request_headers.insert(k, v);\n        }\n\n        Ok(request)\n    }\n}\n\npub(crate) trait QueryParamValue {\n    fn encode(&self) -> String;\n}\n\nmacro_rules! impl_query_param_value {\n    ($ty:ty) => {\n        impl QueryParamValue for $ty {\n            fn encode(&self) -> String {\n                self.to_string()\n            }\n        }\n    };\n}\n\nimpl_query_param_value!(bool);\nimpl_query_param_value!(i32);\nimpl_query_param_value!(String);\nimpl_query_param_value!(models::BackgroundTaskStatus);\nimpl_query_param_value!(models::BackgroundTaskType);\nimpl_query_param_value!(models::ConnectorProduct);\nimpl_query_param_value!(models::MessageStatus);\nimpl_query_param_value!(models::Ordering);\nimpl_query_param_value!(models::StatusCodeClass);\n\nimpl QueryParamValue for Vec<String> {\n    fn encode(&self) -> String {\n        self.iter().format(\",\").to_string()\n    }\n}\n<|fim_middle|>", "completion": "let uri = http1::Uri::try_from(uri).map_err(Error::generic)?;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/rust/src/request.rs", "node_type": "let_declaration", "line_range": [209, 209]}
{"prompt": "<|fim_prefix|>ke an event type to set it to\n    let _: EventTypeOut = client\n        .post(\n            \"api/v1/event-type/\",\n            json!({\n                \"description\": \"a test event type\",\n                \"name\": \"test\",\n            }),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"filterTypes\": [\"test\"],\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(\n        out.ep.event_types_ids,\n        Some(EventTypeNameSet(HashSet::from([EventTypeName(\n            \"test\".to_owned()\n        )])))\n    );\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that event type IDs may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"filterTypes\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.event_types_ids, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.channels, None);\n\n    // Test that channels may be set\n    let _: EndpointOut = client\n        .patch(\n            &url,\n            json!({\n                \"channels\": [\"test\"],\n            }),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(\n        out.ep.channels,\n        Some(EventChannelSet(HashSet::from([EventChannel(\n            \"test\".to_owned()\n        )])))\n    );\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n\n    // Test that channels may be unset\n    let _: EndpointOut = client\n        .patch(&url, json!({ \"channels\": null }), StatusCode::OK)\n        .await\n        .unwrap();\n\n    // Assert the change was made\n    let out = client\n        .get::<EndpointOut>(&url, StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(out.ep.channels, None);\n    // Assert that no other changes were made\n    assert_eq!(out.ep.description, \"test\".to_owned());\n    assert_eq!(out.ep.rate_limit, None);\n    assert_eq!(out.ep.uid, None);\n    assert_eq!(out.ep.url, \"http://bad.url2/\".to_owned());\n    assert_eq!(out.ep.version, 2);\n    assert!(out.ep.disabled);\n    assert_eq!(out.ep.event_types_ids, None);\n}\n\n#[allow(deprecated)]\n#[tokio::test]\nasync fn test_crud() {\n    let (client, _jh) = start_svix_server().await;\n\n    const APP_NAME_1: &str = \"v1EndpointCrudTestApp1\";\n    const APP_NAME_2: &str = \"v1EndpointCrudTestApp2\";\n\n    const EP_URI_APP_1_EP_1_VER_1: &str = \"http://v1EndpointCrudTestApp1Ep1Ver1.test/foo\";\n    const EP_URI_APP_1_EP_1_VER_2: &str = \"http://v1EndpointCrudTestApp1Ep1Ver2.test/\";\n    const EP_URI_APP_1_EP_2: &str = \"http://v1EndpointCrudTestApp1Ep2.test/\";\n    const EP_URI_APP_2_EP_1: &str = \"http://v1EndpointCrudTestApp2Ep1.test/\";\n    const EP_URI_APP_2_EP_2: &str = \"http://v1EndpointCrudTestApp2Ep2.test/\";\n\n    l<|fim_suffix|>    let app_2 = create_test_app(&client, APP_NAME_2).await.unwrap().id;\n\n    // CREATE\n    let app_1_ep_1 = create_test_endpoint(&client, &app_1, EP_URI_APP_1_EP_1_VER_1)\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_1.ep.url, EP_URI_APP_1_EP_1_VER_1.to_lowercase());\n    assert_eq!(app_1_ep_1.ep.version, 1);\n\n    let app_1_ep_2 = create_test_endpoint(&client, &app_1, EP_URI_APP_1_EP_2)\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_2.ep.url, EP_URI_APP_1_EP_2.to_lowercase());\n    assert_eq!(app_1_ep_2.ep.version, 1);\n\n    let app_2_ep_1 = create_test_endpoint(&client, &app_2, EP_URI_APP_2_EP_1)\n        .await\n        .unwrap();\n    assert_eq!(app_2_ep_1.ep.url, EP_URI_APP_2_EP_1.to_lowercase());\n    assert_eq!(app_2_ep_1.ep.version, 1);\n\n    let app_2_ep_2 = create_test_endpoint(&client, &app_2, EP_URI_APP_2_EP_2)\n        .await\n        .unwrap();\n    assert_eq!(app_2_ep_2.ep.url, EP_URI_APP_2_EP_2.to_lowercase());\n    assert_eq!(app_2_ep_2.ep.version, 1);\n\n    // READ\n\n    // Can read from correct app\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_1.id).await.unwrap(),\n        app_1_ep_1\n    );\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_2.id).await.unwrap(),\n        app_1_ep_2\n    );\n    assert_eq!(\n        get_endpoint(&client, &app_2, &app_2_ep_1.id).await.unwrap(),\n        app_2_ep_1\n    );\n    assert_eq!(\n        get_endpoint(&client, &app_2, &app_2_ep_2.id).await.unwrap(),\n        app_2_ep_2\n    );\n\n    // Can't read from incorrect app\n    get_endpoint_404(&client, &app_2, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_1, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_1, &app_2_ep_2.id)\n        .await\n        .unwrap();\n\n    // UPDATE\n    let app_1_ep_1_id = app_1_ep_1.id;\n    let app_1_ep_1: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_1}/endpoint/{app_1_ep_1_id}/\"),\n            endpoint_in(EP_URI_APP_1_EP_1_VER_2),\n            StatusCode::OK,\n        )\n        .await\n        .unwrap();\n    assert_eq!(app_1_ep_1.ep.url, EP_URI_APP_1_EP_1_VER_2.to_lowercase());\n\n    // CONFIRM UPDATE\n    assert_eq!(\n        get_endpoint(&client, &app_1, &app_1_ep_1_id).await.unwrap(),\n        app_1_ep_1\n    );\n\n    // Test that PUT with an invalid ID creates an endpoint\n    let app_1_ep_3: EndpointOut = client\n        .put(\n            &format!(\"api/v1/app/{app_1}/endpoint/fake-id/\"),\n            endpoint_in(EP_URI_APP_1_EP_1_VER_2),\n            StatusCode::CREATED,\n        )\n        .await\n        .unwrap();\n\n    // LIST\n    let list_app_1: ListResponse<EndpointOut> = client\n        .get(&format!(\"api/v1/app/{app_1}/endpoint/\"), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list_app_1.data.len(), 3);\n    assert!(list_app_1.data.contains(&app_1_ep_1));\n    assert!(list_app_1.data.contains(&app_1_ep_2));\n    assert!(list_app_1.data.contains(&app_1_ep_3));\n\n    let list_app_2: ListResponse<EndpointOut> = client\n        .get(&format!(\"api/v1/app/{}/endpoint/\", &app_2), StatusCode::OK)\n        .await\n        .unwrap();\n    assert_eq!(list_app_2.data.len(), 2);\n    assert!(list_app_2.data.contains(&app_2_ep_1));\n    assert!(list_app_2.data.contains(&app_2_ep_2));\n\n    // DELETE\n    delete_endpoint(&client, &app_1, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_1, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_2, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    delete_endpoint(&client, &app_2, &app_2_ep_2.id)\n        .await\n        .unwrap();\n\n    // CONFIRM DELETION\n    get_endpoint_404(&client, &app_1, &app_1_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_1, &app_1_ep_2.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_2_ep_1.id)\n        .await\n        .unwrap();\n    get_endpoint_404(&client, &app_2, &app_2_ep_2.id)\n        .await<|fim_middle|>", "completion": "let app_1 = create_test_app(&client, APP_NAME_1).await.unwrap().id;", "file": "/Users/vikra/bound-datagen/fill-in-the-middle/data/svix-webhooks/server/svix-server/tests/it/e2e_endpoint.rs", "node_type": "let_declaration", "line_range": [458, 458]}
